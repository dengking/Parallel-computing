# [6.3.2. Sequential Consistency](https://www.e-reading.club/chapter.php/143358/218/Tanenbaum_-_Distributed_operating_systems.html)

While strict consistency is the ideal programming model, it is nearly impossible to implement in a **distributed system**. Furthermore, experience shows that programmers can often manage quite well with **weaker models**. For example, all textbooks on operating systems discuss **critical sections** and the **mutual exclusion problem**. This discussion always includes the caveat that properly-written parallel programs (such as the **producer-consumer problem**) should not make any assumptions about the relative speeds of the processes or how their statements will **interleave** in time. Counting on two events within one process to happen so quickly that the other process will not be able to do something in between is looking for trouble. Instead, the reader is taught to program in such a way that the exact **order** of statement execution (in fact, memory references) does not matter. When the order of events is essential, **semaphores** or other **synchronization operations** should be used. Accepting this argument（观点） in fact means learning to live with a **weaker memory model**. With some practice, most parallel programmers are able to adapt to it.

> 虽然严格一致性是理想的编程模型，但在分布式系统中几乎不可能实现。此外，经验表明，程序员通常可以很好地管理较弱的模型。例如，操作系统上的所有教科书都讨论了关键部分和互斥问题。这个讨论总是包括一个警告，即正确编写的并行程序（例如生产者 - 消费者问题）不应该对过程的相对速度或它们的语句如何及时交错做出任何假设。计算一个进程内的两个事件发生得如此之快以至于其他进程无法在两者之间做某事正在寻找麻烦。相反，教导读者以这样的方式编程，即语句执行的确切顺序（实际上是内存引用）无关紧要。当事件顺序必不可少时，应使用信号量或其他同步操作。接受这个论点实际上意味着学会用较弱的记忆模型来生活。通过一些练习，大多数并行程序员都能够适应它。

**Sequential consistency** is a slightly weaker memory model than **strict consistency**. It was first defined by Lamport (1979), who said that a **sequentially consistent memory** is one that satisfies the following condition:

> The result of any execution is the same as if the operations of all processors were executed in some **sequential order**, and the operations of each individual processor appear in this **sequence** in the order specified by its program.

What this definition means is that when processes run in parallel on different machines (or even in pseudoparallel on a timesharing system), any valid interleaving is acceptable behavior, but *all processes must see the same sequence of memory references.* (这个定义的含义是，当进程在不同的机器上并行运行时（甚至在分时系统上并行运行）时，任何有效的交错都是可接受的行为，但所有进程必须看到相同的内存引用序列) A memory in which one process (or processor) sees one interleaving and another process sees a different one is not a **sequentially consistent memory**. Note that nothing is said about **time**; that is, there is no reference to the "most recent" store. Note that in this context, a process "sees" **writes** from all processes but only its own **reads**.

That **time** does not play a role can be seen from Fig. 6-13. A memory behaving as shown in Fig. 6-13(a) is **sequentially consistent** even though the first read done by *P*2 returns the initial value of 0 instead of the new value of 1.

![Distributed operating systems](https://www.e-reading.club/illustrations/143/143358-Any2FbImgLoader150)

Fig. 6-13. Two possible results of running the same program.

**Sequentially consistent memory** does not guarantee that a read returns the value written by another process a nanosecond earlier, or a microsecond earlier, or even a minute earlier. It merely guarantees that all processes see all memory references in the same order. If the program that generated Fig. 6-13(a) is run again, it might give the result of Fig. 6-13(b). The results are not deterministic. Running a program again may not give the same result in the absence of explicit **synchronization operations**.

![Distributed operating systems](https://www.e-reading.club/illustrations/143/143358-Any2FbImgLoader151)

Fig. 6-14. Three parallel processes.

To make this point more explicit, let us consider the example of Fig. 6-14 (Dubois et al., 1988). Here we see the code for three processes that run in parallel on three different processors. All three processes share the same **sequentially consistent distributed shared memory**, and all have access to the variables *a, b,* and *c.* From a memory reference point of view, an assignment should be seen as a write, and a print statement should be seen as a simultaneous read of its two parameters. All statements are assumed to be **atomic**.

Various **interleaved execution sequences** are possible. With six independent statements, there are potentially 720 (6!) possible execution sequences, although some of these violate program order. Consider the 120 (5!) sequences that begin with *a*=1. Half of these have *print(a, c)* before *b=*1 and thus violate program order. Half also have *print(a, b)* before *c=*1 and also violate program order. Only 1/4 of the 120 sequences or 30 are valid. Another 30 valid sequences are possible starting with *b=*1 and another 30 can begin with *c*=1, for a total of 90 valid execution sequences. Four of these are shown in Fig. 6-15.

| a = 1;            | a = 1;            | b = 1;            | b = 1;            |
| ----------------- | ----------------- | ----------------- | ----------------- |
| print(b, c);      | b = 1;            | c= 1;             | a= 1;             |
| b = 1;            | print(a, c);      | print(a, b);      | c = 1;            |
| print(a, c);      | print(b, c);      | print(a, c);      | print(a, c);      |
| c = 1;            | c = 1;            | a = 1;            | print(b, c);      |
| print(a, b);      | print(a, b);      | print(b, c);      | print(a, b);      |
| Prints: 001011    | Prints: 101011    | Prints: 010111    | Prints: 111111    |
| Signature: 001011 | Signature: 101011 | Signature: 110101 | Signature: 111111 |
| (a)               | (b)               | (c)               | (d)               |

Fig. 6-15. Four valid execution sequences for the program of Fig. 6-14. The vertical axis is time, increasing downward. 

In Fig. 6-15 (a), the three processes are run in order, first *P*1*,* then *P*2, then *P*3. The other three examples demonstrate different, but equally valid, inter-leavings of the statements in time. Each of the three processes prints two variables. Since the only values each variable can take on are the initial value (0), or the assigned value (1), each process produces a 2-bit string. The numbers after *Prints* are the actual outputs that appear on the output device.

If we concatenate the output of *P*1, *P*2, and *P*3 in that order, we get a 6-bit string that characterizes a particular interleaving of statements (and thus memory references). This is the string listed as the *Signature* in Fig. 6-15. Below we will characterize each ordering by its signature rather than by its printout.

Not all 64 signature patterns are allowed. As a trivial example, 000000 is not permitted, because that would imply that the print statements ran before the assignment statements, violating Lamport's requirement that **statements are executed in program order**. A more subtle example is 001001. The first two bits, 00, mean that *b* and *c* were both 0 when *P*1 did its printing. This situation occurs only when *P*1 executes both statements before *P*2 or *P*3 starts. The next two bits, 10, mean that *P*2 must run after *P*1 has started but before *P*3 has started. The last two bits, 01, mean that *P*3, must complete before *P*1 starts, but we have already seen that *P*1 must go first. Therefore, 001001 is not allowed. 

In short, the 90 different valid statement orderings produce a variety of different program results (less than 64, though) that are allowed under the assumption of sequential consistency. The contract between the software and memory here is that the software must accept all of these as valid results. In other words, the software must accept the four results shown in Fig. 6-15 and all the other valid results as proper answers, and must work correctly if any of them occurs. A program that works for some of these results and not for others violates the contract with the memory and is incorrect.

A **sequentially consistent memory** can be implemented on a DSM or multiprocessor system that replicates writable pages by ensuring that no memory operation is started until all the previous ones have been completed. In a system with an efficient, totally-ordered reliable broadcast mechanism, for example, all shared variables could be grouped together on one or more pages, and operations to the shared pages could be broadcast. The exact order in which the operations are interleaved does not matter as long as all processes agree on the order of all operations on the shared memory. 

Various formal systems have been proposed for expressing sequential consistency (and other models). Let us briefly consider the system of Ahamad et al. (1993). In their method, the sequence of read and write operations of process *i* is designated by *Hi*, (the history of *Pi*). Figure 6-12(b) shows two such sequences, *H*1 and *H*2 for *P*1 and *P*2, respectively, as follows:

*H1=W(x)1*

*H2=R(x)0 R(x)1*

The set of all such sequences is called *H.*

To get the relative order in which the operations appear to be executed, we must merge the operation strings in *H* into a single string, *S,* in which each operation appearing in *H* appears in *S* once. Intuitively, *S* gives the order that the operations would have been carried out had there been a single centralized memory. All legal values for *S* must obey two constraints:

\1. Program order must be maintained.

\2. Memory coherence must be respected.

The first constraint means that if a read or write access, *A,* appears before another access, *B,* in one of the strings in *H, A* must also appear before *B* in *S.* If this constraint is true for all pairs of operations, the resulting *S* will not show any operations in an order that violates any of the programs.

The second constraint, called memory coherence, means that a read to some location, *x,* must always return the value most recently written to *x;* that is, the value v written by the most recent *W(x)v* before the *R(x).* Memory coherence examines in isolation each location and the sequence of operations on it, without regard to other locations. Consistency, in contrast, deals with writes to *different* locations and their ordering.

For Fig. 6-12(b) there is only one legal value of *S:*

*S=R(x)0 W(x)1 R(x)1,*

For more complicated examples there might be several legal values of *S.* The behavior of a program is said to be correct if its operation sequence corresponds to some legal value of *S.*

Although sequential consistency is a programmer-friendly model, it has a serious performance problem. Lipton and Sandberg (1988) proved that if the read time is *r,* the write time is *w,* and the minimal packet transfer time between nodes is *t,* then it is always true that *r+w>=t.* In other words, for any sequentially consistent memory, changing the protocol to improve the read performance makes the write performance worse, and vice versa. For this reason, researchers have investigated other (weaker) models. In the following sections we will discuss some of them.

[6.3.1. Strict Consistency](https://www.e-reading.club/chapter.php/143358/217/Tanenbaum_-_Distributed_operating_systems.html) | [Distributed operating systems](https://www.e-reading.club/book.php?book=143358) | [6.3.3. Causal Consistency](https://www.e-reading.club/chapter.php/143358/219/Tanenbaum_-_Distributed_operating_systems.html)





# [一致性模型之Sequential Consistency](https://blog.csdn.net/qianfeng_dashuju/article/details/90642303)

## Sequential Consistency的定义

Sequential Consistency的精确定义来自于Leslie Lamport老哥(以后我们会多次提到他)。他本来是定义了基于共享内存的多CPU并行计算的一致性模型，但是也可以推广到分布式系统中，实际上多CPU并行计算也都可以认为是分布式系统。模型的定义是:

> the result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program

放到分布式系统里，意思就是不管系统怎么运行，得到的结果就好像把所有节点的所有操作按照某个sequential order排序后运行，但是在这个sequential order顺序中，来自同一个节点的操作仍然保持着它们在节点中被指定的顺序（也就是他们在program中指定的顺序）。

## Sequential Consistency的例子

Leslie Lamport老哥的说法一贯的佶屈聱牙，我们通过几个例子来看一下。图中从左向右表示物理时间，W(a)表示写入数据a，R(a)表示读出数据a。

　　

![例子1](https://user-gold-cdn.xitu.io/2019/5/28/16afd5b49d0642ca?w=648&h=247&f=png&s=4477)

 

　　

![例子2](https://user-gold-cdn.xitu.io/2019/5/28/16afd5b4a04845b0?w=652&h=252&f=png&s=4727)

 

可以看出，这两个系统都不是很完美，但是它们的模型都可以看做**Sequential Consistency**，因为通过如下变换，总是可以自圆其说，也就是可以找到符合定义的**sequential order**。

　　

![变换1](https://user-gold-cdn.xitu.io/2019/5/28/16afd5b49e6e8e25?w=638&h=371&f=png&s=11702)

 

　　

![变换2](https://user-gold-cdn.xitu.io/2019/5/28/16afd5b49fdea9e0?w=644&h=382&f=png&s=11286)

 

## Sequential Consistency和硬件

也许有人会问，同一个进程中保留操作顺序不是显而易见的么?实际上随着硬件技术，尤其是多核、多CPU技术的发展，一个CPU核心运行的进程，不一定能观测到另一个核心进程的操作顺序。

在论文中，Leslie Lamport老哥举了这样一个例子，有一个互斥算法，要求两个进程不能同时执行临界区方法，a和b两个变量初始值为0。正常情况下，最多一个进程执行临界区方法。

　　进程1执行序列如下：

　　a = 1

　　if (b!=0){

　　临界区方法

　　}

　　进程2执行序列如下：

　　b = 1

　　if (a!=0){

　　临界区方法

　　}

　　这个程序在多核CPU机器上运行时，有可能两个进程同时进入临界区。为什么呢?

　　我们先看一下现代CPU的架构

　　

![图片描述](https://user-gold-cdn.xitu.io/2019/5/28/16afd5b4a753d25f?w=500&h=333&f=png&s=19914)

 

　　CPU一般具有多个核心，每个核心都有自己的L1 cache和L2 cache，cache之上还有Load Buffer和Store Buffer。写入时，处理器很有可能仅仅将数据写入Store Buffer，稍后再将Store Buffer中的数据统一写回cache，有可能再过一会儿才将cache的数据写回内存。同样，一个核心读取的数据说不定也已经被另一个核心修改过，只是它不知道而已。

　　所以上述进程对a和b的赋值，很有可能没被对方感知。

　　为了保证**Sequential Consistency**，Leslie Lamport老哥在论文中提出了两个要求:

　　Each processor issues memory requests in the order specified by its program

　　Memory requests from all processors issued to an individual memory module are serviced from a single FIFO queue. Issuing a memory request consists of entering the request on this queue.

　　但是如果在硬件层满足Sequential Consistency，肯定会大大降低效率，所以一般这些工作就会交给上层的软件开发人员来做。