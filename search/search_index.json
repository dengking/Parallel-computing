{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\u5173\u4e8e\u672c\u5de5\u7a0b Parallel computing \u3001 Distributed computing \u3001 Concurrent computing \u8fd9\u4e09\u4e2acomputing\u662f\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u7ecf\u5e38\u78b0\u5230\u7684\uff0c\u5b83\u4eec\u7d27\u5bc6\u5173\u8054\u53c8\u5404\u6709\u4e0d\u540c\uff0c\u6b63\u5982\u5728 Distributed computing \u7684 Parallel and distributed computing \u7ae0\u8282\u6240\u8ff0\uff1a The terms \" concurrent computing \", \" parallel computing \", and \"distributed computing\" have a lot of overlap, and no clear distinction exists between them. The same system may be characterized both as \"parallel\" and \"distributed\"; the processors in a typical distributed system run concurrently in parallel. Parallel computing may be seen as a particular tightly coupled form of distributed computing, and distributed computing may be seen as a loosely coupled form of parallel computing. \u6240\u4ee5\uff0c\u6709\u5fc5\u8981\u5bf9\u5b83\u4eec\u8fdb\u884c\u7814\u7a76\u3002\u672c\u5de5\u7a0b\u6309\u7167\u7ef4\u57fa\u767e\u79d1\u7684 Parallel computing \u8fdb\u884c\u7ec4\u7ec7\uff0c\u5b83\u5c06 Distributed computing \u3001 Concurrent computing \u90fd\u5f52\u5165 Parallel computing \u7684\u8303\u8f74\uff0c\u672c\u5de5\u7a0b\u5bf9\u76f8\u5173\u7406\u8bba\u77e5\u8bc6\u8fdb\u884c\u68b3\u7406\u3002 \u65f6\u4ee3\u80cc\u666f\u3001\u53d1\u5c55\u8d8b\u52bf \u4ee5\u53ca why need parallel computing \u5173\u4e8e\u5f53\u524d\u7684\u65f6\u4ee3\u80cc\u666f\u3001\u53d1\u5c55\u8d8b\u52bf\uff0c\u5728\u4e0b\u9762\u7ae0\u8282\u4e2d\u8fdb\u884c\u4e86\u63cf\u8ff0: 1\u3001\u5de5\u7a0b hardware \u7684 Modern-CPU\\Tendency-toward-parallel-computing \u7ae0\u8282 2\u3001 Book-Designing-Data-Intensive-Applications \u4e0b\u9762\u662f\u622a\u53d6\u81ea Book-Designing-Data-Intensive-Applications : 4\u3001CPU clock speeds are barely increasing, but multi-core processors are standard, and networks are getting faster. This means parallelism is only going to increase. NOTE: CPU clock speed\u51e0\u4e4e\u4e0d\u589e\u52a0\uff0cmulti-core processors\u88ab\u5e7f\u6cdb\u91c7\u7528\uff0c\u7f51\u7edc\u8d8a\u6765\u8d8a\u5feb\uff0c\u8fd9\u4e9b\u90fd\u610f\u5473\u7740\u201cparallelism\u201d\u5c06\u4f1a\u589e\u52a0\u3002 Why need parallel computing ? \u91cd\u590d\u53d1\u6325computation power\uff0c\u514b\u670d\u8d8a\u6765\u8d8a\u591a\u7684\u6311\u6218\u3002 Unit of parallel computing \"Unit of parallel computing\"\u5373\"\u5e76\u884c\u7684\u5355\u4f4d\"\uff08\u53c2\u89c1<\u6587\u7ae0Unit>\uff09\uff0c\u7c7b\u4f3c\u4e8e\u5728\u7ef4\u57fa\u767e\u79d1 Parallel computing \u6240\u8ff0\u7684 Types of parallelism \u3001\u6216\u8005\u8bf4\u662f\uff1a\u5e76\u53d1\u7684\u7ea7\u522b\u3002unit\u662f\u4e00\u4e2a\u66f4\u52a0\u62bd\u8c61/\u6982\u62ec\u7684\u6982\u5ff5\uff0c\u80fd\u591f\u8ba9\u6211\u4eec\u6b63\u5728\u66f4\u52a0\u9ad8\u7684\u89d2\u5ea6\u6765\u7406\u89e3\u548c\u5206\u6790\u5728parallel computing\u4e2d\u7684\u5404\u79cd\u95ee\u9898\uff0c\u80fd\u591f\u8ba9\u6211\u4eec\u6e05\u695a\u5730\u770b\u5230\u5728\u4e0d\u540c\u5c42\u7ea7\u7684parallel computing\u4e2d\uff0c\u90fd\u4f1a\u9762\u4e34\u7684\u95ee\u9898\uff0c\u6bd4\u5982\u65e0\u8bba\u662fmultiple process\u3001multiple thread\u90fd\u6d89\u53ca\u901a\u4fe1\u95ee\u9898\u3002\u5728\u540e\u9762\u6211\u4eec\u6709\u65f6\u5019\u4e5f\u4f1a\u4f7f\u7528\u201centity\u201d\u6765\u8868\u793a\uff0c\u540e\u9762\u6211\u4eec\u5c06\u4f7f\u7528unit/entity\u7684\u6982\u5ff5\u6765\u63cf\u8ff0\u5728\u5404\u79cdparallel computing\u4e2d\u7684\u5404\u79cd\u95ee\u9898\uff0c\u5982\uff1a parallel\u7684entity/unit\u662f\u4ec0\u4e48\uff1f \u8fd9\u4e9bentity\u4e4b\u95f4\u5982\u4f55\u8fdb\u884c\u901a\u4fe1\uff1f\u8fd9\u5728 Inter-entity-communication \u4e2d\u8fdb\u884c\u4e86\u603b\u7ed3\u3002 \u8fd9\u4e9bentity\u4e4b\u95f4\u5982\u4f55\u8fdb\u884csynchroniz\uff1f\u8fd9\u5728 Synchronization \u4e2d\u8fdb\u884c\u4e86\u603b\u7ed3\u3002 \u6709\u54ea\u4e9b\u901a\u7528\u7684model\uff1f\u8fd9\u5728 Model \u4e2d\u8fdb\u884c\u4e86\u603b\u7ed3\u3002 \u8fd9\u4e9bentity\u4e4b\u95f4\u5982\u4f55\u8fbe\u6210\u5171\u8bc6\uff1f","title":"Home"},{"location":"#_1","text":"Parallel computing \u3001 Distributed computing \u3001 Concurrent computing \u8fd9\u4e09\u4e2acomputing\u662f\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u7ecf\u5e38\u78b0\u5230\u7684\uff0c\u5b83\u4eec\u7d27\u5bc6\u5173\u8054\u53c8\u5404\u6709\u4e0d\u540c\uff0c\u6b63\u5982\u5728 Distributed computing \u7684 Parallel and distributed computing \u7ae0\u8282\u6240\u8ff0\uff1a The terms \" concurrent computing \", \" parallel computing \", and \"distributed computing\" have a lot of overlap, and no clear distinction exists between them. The same system may be characterized both as \"parallel\" and \"distributed\"; the processors in a typical distributed system run concurrently in parallel. Parallel computing may be seen as a particular tightly coupled form of distributed computing, and distributed computing may be seen as a loosely coupled form of parallel computing. \u6240\u4ee5\uff0c\u6709\u5fc5\u8981\u5bf9\u5b83\u4eec\u8fdb\u884c\u7814\u7a76\u3002\u672c\u5de5\u7a0b\u6309\u7167\u7ef4\u57fa\u767e\u79d1\u7684 Parallel computing \u8fdb\u884c\u7ec4\u7ec7\uff0c\u5b83\u5c06 Distributed computing \u3001 Concurrent computing \u90fd\u5f52\u5165 Parallel computing \u7684\u8303\u8f74\uff0c\u672c\u5de5\u7a0b\u5bf9\u76f8\u5173\u7406\u8bba\u77e5\u8bc6\u8fdb\u884c\u68b3\u7406\u3002","title":"\u5173\u4e8e\u672c\u5de5\u7a0b"},{"location":"#why#need#parallel#computing","text":"\u5173\u4e8e\u5f53\u524d\u7684\u65f6\u4ee3\u80cc\u666f\u3001\u53d1\u5c55\u8d8b\u52bf\uff0c\u5728\u4e0b\u9762\u7ae0\u8282\u4e2d\u8fdb\u884c\u4e86\u63cf\u8ff0: 1\u3001\u5de5\u7a0b hardware \u7684 Modern-CPU\\Tendency-toward-parallel-computing \u7ae0\u8282 2\u3001 Book-Designing-Data-Intensive-Applications \u4e0b\u9762\u662f\u622a\u53d6\u81ea Book-Designing-Data-Intensive-Applications : 4\u3001CPU clock speeds are barely increasing, but multi-core processors are standard, and networks are getting faster. This means parallelism is only going to increase. NOTE: CPU clock speed\u51e0\u4e4e\u4e0d\u589e\u52a0\uff0cmulti-core processors\u88ab\u5e7f\u6cdb\u91c7\u7528\uff0c\u7f51\u7edc\u8d8a\u6765\u8d8a\u5feb\uff0c\u8fd9\u4e9b\u90fd\u610f\u5473\u7740\u201cparallelism\u201d\u5c06\u4f1a\u589e\u52a0\u3002","title":"\u65f6\u4ee3\u80cc\u666f\u3001\u53d1\u5c55\u8d8b\u52bf \u4ee5\u53ca why need parallel computing"},{"location":"#why#need#parallel#computing_1","text":"\u91cd\u590d\u53d1\u6325computation power\uff0c\u514b\u670d\u8d8a\u6765\u8d8a\u591a\u7684\u6311\u6218\u3002","title":"Why need parallel computing ?"},{"location":"#unit#of#parallel#computing","text":"\"Unit of parallel computing\"\u5373\"\u5e76\u884c\u7684\u5355\u4f4d\"\uff08\u53c2\u89c1<\u6587\u7ae0Unit>\uff09\uff0c\u7c7b\u4f3c\u4e8e\u5728\u7ef4\u57fa\u767e\u79d1 Parallel computing \u6240\u8ff0\u7684 Types of parallelism \u3001\u6216\u8005\u8bf4\u662f\uff1a\u5e76\u53d1\u7684\u7ea7\u522b\u3002unit\u662f\u4e00\u4e2a\u66f4\u52a0\u62bd\u8c61/\u6982\u62ec\u7684\u6982\u5ff5\uff0c\u80fd\u591f\u8ba9\u6211\u4eec\u6b63\u5728\u66f4\u52a0\u9ad8\u7684\u89d2\u5ea6\u6765\u7406\u89e3\u548c\u5206\u6790\u5728parallel computing\u4e2d\u7684\u5404\u79cd\u95ee\u9898\uff0c\u80fd\u591f\u8ba9\u6211\u4eec\u6e05\u695a\u5730\u770b\u5230\u5728\u4e0d\u540c\u5c42\u7ea7\u7684parallel computing\u4e2d\uff0c\u90fd\u4f1a\u9762\u4e34\u7684\u95ee\u9898\uff0c\u6bd4\u5982\u65e0\u8bba\u662fmultiple process\u3001multiple thread\u90fd\u6d89\u53ca\u901a\u4fe1\u95ee\u9898\u3002\u5728\u540e\u9762\u6211\u4eec\u6709\u65f6\u5019\u4e5f\u4f1a\u4f7f\u7528\u201centity\u201d\u6765\u8868\u793a\uff0c\u540e\u9762\u6211\u4eec\u5c06\u4f7f\u7528unit/entity\u7684\u6982\u5ff5\u6765\u63cf\u8ff0\u5728\u5404\u79cdparallel computing\u4e2d\u7684\u5404\u79cd\u95ee\u9898\uff0c\u5982\uff1a parallel\u7684entity/unit\u662f\u4ec0\u4e48\uff1f \u8fd9\u4e9bentity\u4e4b\u95f4\u5982\u4f55\u8fdb\u884c\u901a\u4fe1\uff1f\u8fd9\u5728 Inter-entity-communication \u4e2d\u8fdb\u884c\u4e86\u603b\u7ed3\u3002 \u8fd9\u4e9bentity\u4e4b\u95f4\u5982\u4f55\u8fdb\u884csynchroniz\uff1f\u8fd9\u5728 Synchronization \u4e2d\u8fdb\u884c\u4e86\u603b\u7ed3\u3002 \u6709\u54ea\u4e9b\u901a\u7528\u7684model\uff1f\u8fd9\u5728 Model \u4e2d\u8fdb\u884c\u4e86\u603b\u7ed3\u3002 \u8fd9\u4e9bentity\u4e4b\u95f4\u5982\u4f55\u8fbe\u6210\u5171\u8bc6\uff1f","title":"Unit of parallel computing"},{"location":"Application/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u8ba8\u8bbadistributing computing\u7684\u5e94\u7528\uff0c\u5176\u4e2d\u4f1a\u4ecb\u7ecd\u5404\u79cd\u5404\u6837\u7684\u57fa\u4e8edistributing computing\u7684software\u3002 Distributed version control Git Redis Celery: Distributed Task Queue Distributed computing architecture Distributed hash table Distributed database","title":"Introduction"},{"location":"Application/#_1","text":"\u672c\u7ae0\u8ba8\u8bbadistributing computing\u7684\u5e94\u7528\uff0c\u5176\u4e2d\u4f1a\u4ecb\u7ecd\u5404\u79cd\u5404\u6837\u7684\u57fa\u4e8edistributing computing\u7684software\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Application/#distributed#version#control","text":"Git","title":"Distributed version control"},{"location":"Application/#redis","text":"","title":"Redis"},{"location":"Application/#celery#distributed#task#queue","text":"","title":"Celery: Distributed Task Queue"},{"location":"Application/#distributed#computing#architecture","text":"","title":"Distributed computing architecture"},{"location":"Application/#distributed#hash#table","text":"","title":"Distributed hash table"},{"location":"Application/#distributed#database","text":"","title":"Distributed database"},{"location":"Application/Blockchain/","text":"\u524d\u8a00 \u5728\u5b66\u4e60\u4e86 git \u540e\uff0c\u6211\u5bf9\u5b83\u4f5c\u4e3a distributed version control \u7684\u8bf8\u591a\u7279\u6027\u4ee5\u53ca\u5b83\u7684\u539f\u7406\u5370\u8c61\u6df1\u523b\uff0c\u6bd4\u5982\u5b83\u7684 Data Assurance \u7279\u6027\u3001\u4f7f\u7528 peer-to-peer \u3001 decentralized system \u7b49\u3002\u540e\u6765\u6211\u8054\u60f3\u5230\u4e86\u5728\u9605\u8bfb\u65b0\u95fb\u65f6\u6d4f\u89c8\u8fc7\u7684 \u533a\u5757\u94fe \uff0c\u8bb0\u5f97\u4ecb\u7ecd\u8bf4\u5b83\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a \u5206\u5e03\u5f0f\u8d26\u672c \uff0c\u5b83\u6240\u91c7\u7528\u7684\u4e5f\u662f peer-to-peer \uff0c\u4e5f\u662f decentralized system \u3002\u8fd9\u6837\u5bf9\u6bd4\u4e86\u4e00\u4e0b\u540e\uff0c\u53d1\u73b0\u4e24\u8005\u633a\u7c7b\u4f3c\u7684\u3002\u540e\u6765\u6211\u6df1\u5165\u601d\u8003\u4e24\u8005\uff0c\u53d1\u73b0\uff0cLedger \u548c Version control\u672c\u8d28\u4e0a\u4e5f\u6709\u7740\u975e\u5e38\u7c7b\u4f3c\uff1a Ledger VS Version control \u4e24\u8005\u4e4b\u95f4\u662f\u5b58\u5728\u7740\u5171\u901a\u4e4b\u5904\u7684\uff1a \u4e24\u8005\u90fd\u9700\u8981\u8ffd\u6eaf\u5386\u53f2 \u4e24\u8005\u90fd\u9700\u8981\u8bb0\u5f55\u53d8\u5316 \u4e24\u8005\u5bf9\u51c6\u786e\u6027\u90fd\u6709\u7740\u8981\u6c42 \u5728 \u767e\u5ea6\u767e\u79d1\u533a\u5757\u94fe \u4e2d\uff0c\u5bf9\u533a\u5757\u94fe\u7279\u6027\u6709\u5982\u4e0b\u5b9a\u4e49\uff1a \u533a\u5757\u94fe\u662f\u4e00\u4e2a \u4fe1\u606f\u6280\u672f \u9886\u57df\u7684\u672f\u8bed\u3002\u4ece\u672c\u8d28\u4e0a\u8bb2\uff0c\u5b83\u662f\u4e00\u4e2a\u5171\u4eab \u6570\u636e\u5e93 \uff0c\u5b58\u50a8\u4e8e\u5176\u4e2d\u7684\u6570\u636e\u6216\u4fe1\u606f\uff0c\u5177\u6709\u201c\u4e0d\u53ef\u4f2a\u9020\u201d\u201c\u5168\u7a0b\u7559\u75d5\u201d\u201c\u53ef\u4ee5\u8ffd\u6eaf\u201d\u201c\u516c\u5f00\u900f\u660e\u201d\u201c\u96c6\u4f53\u7ef4\u62a4\u201d\u7b49\u7279\u5f81\u3002\u57fa\u4e8e\u8fd9\u4e9b\u7279\u5f81\uff0c \u533a\u5757\u94fe\u6280\u672f \u5960\u5b9a\u4e86\u575a\u5b9e\u7684\u201c\u4fe1\u4efb\u201c\u57fa\u7840\uff0c\u521b\u9020\u4e86\u53ef\u9760\u7684\u201c\u5408\u4f5c\u201d\u673a\u5236\uff0c\u5177\u6709\u5e7f\u9614\u7684\u8fd0\u7528\u524d\u666f\u3002 \u770b\u5230\u4e86\u8fd9\u6bb5\u8bdd\u540e\uff0c\u6211\u60f3\u5230\u4e86 git \u7684 Data Assurance \u7279\u6027\uff1a It is also impossible to change any file, date, commit message, or any other data in a Git repository without changing the IDs of everything after it. This means that if you have a commit ID, you can be assured not only that your project is exactly the same as when it was committed, but that nothing in its history was changed. \u6211\u731c\u60f3\uff0c\u6b63\u662f\u7531\u4e8eLedger \u548c Version control \u4e24\u8005\u4e4b\u95f4\u7684\u4e00\u4e9b\u76f8\u540c\u7684\u9700\u6c42\uff0c\u6240\u4ee5\u4e24\u8005\u5728\u5b9e\u73b0\u8fd9\u4e9b\u9700\u6c42\u7684\u65f6\u5019\u624d\u4f1a\u9009\u62e9\u7c7b\u4f3c\u7684\u5b9e\u73b0\u65b9\u5f0f\u3002 \u6240\u4ee5\u5f00\u59cb\u4e86 Blockchain \u7684\u5b66\u4e60\uff0c\u7531\u4e8e\u6211\u662f\u662f\u7531 git \u800c\u8054\u60f3\u5230Blockchain\u7684\uff0c\u6240\u4ee5\u4e0b\u9762\u5728\u5bf9Blockchain\u8fdb\u884c\u4ecb\u7ecd\u7684\u65f6\u5019\uff0c\u6211\u4f1a\u4f7f\u7528git\u4e2d\u7684\u4e00\u4e9b\u6982\u5ff5\u6765\u8fdb\u884c\u7c7b\u6bd4\u3002 Blockchain A blockchain , originally block chain , is a growing list of records , called blocks , that are linked using cryptography . Each block contains a cryptographic hash of the previous block, a timestamp , and transaction data (generally represented as a Merkle tree ). NOTE: \u4e0b\u9762\u662fBlockchain\u7684\u7b80\u5355\u6a21\u578b\uff1a \u539f\u6587\u94fe\u63a5\uff1a https://www.researchgate.net/figure/The-Bitcoin-blockchain-is-a-hash-chain-of-blocks-Each-block-has-a-Merkle-tree-of_fig1_316789505 The Bitcoin blockchain is a hash chain of blocks. Each block has a Merkle tree of transactions. \u53ef\u4ee5\u52a0\u4e2ablockchain\u7b80\u5355\u5730\u770b\u505a\u662f hash chain \u3002 By design, a blockchain is resistant to modification of the data . It is \"an open, distributed ledger that can record transactions between two parties efficiently and in a verifiable and permanent way\". NOTE: \u8fd9\u79cd\u7279\u6027\uff08resistant to modification of the data\uff09\u5bf9\u4e8e\u4e00\u4e2a ledger \uff08\u8d26\u672c\uff09\u800c\u8a00\u662f\u5fc5\u987b\u7684\uff0c\u5426\u5219\u8fd9\u4e2aledger\u5c31\u5b8c\u5168\u6ca1\u6709\u610f\u4e49\u4e86\u3002 For use as a distributed ledger , a blockchain is typically managed by a peer-to-peer network collectively adhering to a protocol for inter-node communication and validating new blocks . Once recorded, the data in any given block cannot be altered retroactively without alteration of all subsequent blocks, which requires consensus of the network majority. Although blockchain records are not unalterable, blockchains may be considered secure by design and exemplify a distributed computing system with high Byzantine fault tolerance . Decentralized consensus has therefore been claimed with a blockchain. NOTE: blockchain\u7684\u8bbe\u8ba1\u662fresistant to modification of the data\uff0c\u4e5f\u5c31\u662f\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7684 secure by design \uff0c\u5e76\u4e14\u5b83\u5df2\u7ecf\u8fbe\u5230\u4e86\u6bd4\u8f83\u9ad8\u7684 Byzantine fault tolerance \u6807\u51c6\uff0c\u6240\u4ee5\u867d\u7136\u4ece\u6280\u672f\u4e0a\u6765\u8bf4\u662f\u662f\u80fd\u591f\u7be1\u6539record\u7684\uff0c\u4f46\u662f\u8fd9\u5e76\u4e0d\u80fd\u591f\u5426\u5b9ablockchain\u7684\u53ef\u7528\u6027\u3002 History The first work on a cryptographically secured chain of blocks was described in 1991 by Stuart Haber and W. Scott Stornetta. They wanted to implement a system where document timestamps could not be tampered with. In 1992, Bayer, Haber and Stornetta incorporated Merkle trees to the design, which improved its efficiency by allowing several document certificates to be collected into one block. NOTE: git\u4e2d\u4e5f\u4f7f\u7528\u4e86 Merkle trees NOTE: \u4e0a\u8ff0\u63cf\u8ff0\u7684\u95ee\u9898\u662f\u4e00\u7c7b\u53eb\u505a\uff1a Trusted timestamping \u7684\u95ee\u9898 Structure A blockchain is a decentralized , distributed , and oftentimes public, digital ledger that is used to record transactions across many computers so that any involved record cannot be altered retroactively, without the alteration of all subsequent blocks. This allows the participants to verify and audit transactions independently and relatively inexpensively. A blockchain database is managed autonomously using a peer-to-peer network and a distributed timestamping server. They are authenticated by mass collaboration powered by collective self-interests . Such a design facilitates robust workflow where participants' uncertainty regarding data security is marginal. The use of a blockchain removes the characteristic of infinite reproducibility from a digital asset. It confirms that each unit of value was transferred only once, solving the long-standing problem of double spending . A blockchain has been described as a value-exchange protocol . A blockchain can maintain title rights because, when properly set up to detail the exchange agreement, it provides a record that compels offer and acceptance . Blocks Blocks hold batches of valid transactions that are hashed and encoded into a Merkle tree . Each block includes the cryptographic hash of the prior block in the blockchain, linking the two. The linked blocks form a chain. This iterative process confirms the integrity of the previous block, all the way back to the original genesis block \uff08\u6e90\u5934\uff09. Sometimes separate blocks can be produced concurrently, creating a temporary fork. In addition to a secure hash-based history, any blockchain has a specified algorithm for scoring different versions of the history so that one with a higher score can be selected over others. Blocks not selected for inclusion in the chain are called orphan blocks . Peers supporting the database have different versions of the history from time to time. They keep only the highest-scoring version of the database known to them. Whenever a peer receives a higher-scoring version (usually the old version with a single new block added) they extend or overwrite their own database and retransmit the improvement to their peers. There is never an absolute guarantee that any particular entry will remain in the best version of the history forever. Blockchains are typically built to add the score of new blocks onto old blocks and are given incentives to extend with new blocks rather than overwrite old blocks. NOTE: database\u4e2d\u4fdd\u5b58\u4ec0\u4e48\u6570\u636e\u5462\uff1f\u662f\u6574\u4e2ablockchain\uff1f Decentralization NOTE: \u5176\u5b9e\u8fd9\u4e00\u6bb5\u6240\u63cf\u8ff0\u7684peer-to-peer network\u7684\u4f18\u52bf\u3002 Git VS Blockchain **Blockchain**\u7684block\uff0c\u975e\u5e38\u7c7b\u4f3c\u4e8egit\u7684commit\uff1b blockchain\u4e2dchain\u7684\u542b\u4e49\u5c31\u662flinked block\u3002git\u4e2d\u7684**branch**\u5176\u5b9e\u5bf9\u5e94\u7684\u5c31\u662flinked commit\u3002 blockchain\u4f7f\u7528 cryptographic hash \u6765\u4f5c\u4e3a\u4e24\u4e2ablock\u7684link\uff0c\u8fd9\u5176\u5b9e\u5c31\u662f Hash Chain \u3002blockchain\u7684\u8fd9\u79cd\u7ec4\u7ec7\u65b9\u5f0f\u5b9e\u73b0\u4e86Data Assurance\uff08\u5728\u524d\u9762\u7ae0\u8282\u4e2d\u6240\u4ecb\u7ecd\u7684\uff09: Once recorded, the data in any given block cannot be altered retroactively without alteration of all subsequent blocks, which requires consensus of the network majority. git\u4e5f\u91c7\u7528\u4e86\u7c7b\u4f3c\u7684link\u6765\u7ec4\u7ec7commit\uff0cgit\u7684\u8fd9\u79cd\u7ec4\u7ec7\u65b9\u5f0f\u5b9e\u73b0Data Assurance\uff0c\u53c2\u89c1\uff1a Data Assurance Cryptographic authentication of history \u53ef\u4ee5\u770b\u5230\uff0c\u4e24\u8005\u90fd\u4f7f\u7528\u4e86 Hash Chain \uff0c\u8fd9\u79cd\u94fe\u5f0f\u4f9d\u8d56\u7684\u7ed3\u6784\u80fd\u591f\u975e\u5e38\u597d\u7684\u5b9e\u73b0Data Assurance\u3002 blockchain\u4e2d\u4f7f\u7528 Merkle tree \u6765\u7ec4\u7ec7 transaction data\u3002git\u4e2d\u4f7f\u7528 Merkle tree \u6765\u7ec4\u7ec7 tracked file\u3002 blockchain\u4e2d\u6dfb\u52a0\u4e00\u4e2a\u65b0\u7684block\u9700\u8981network\u4e2d\u7684\u6240\u6709\u7684peer\u7684consensus \u3002git\u4e2d\u6dfb\u52a0\u4e00\u4e2a\u65b0\u7684commit\u4e0d\u6d89\u53ca\u8fd9\u4e9b\u95ee\u9898\uff0cgit\u4e2d\u4f7f\u7528pull\u548cpush\u6765\u5728\u5404\u4e2apeer\uff08repository\uff09\u4e4b\u95f4\u7684\u540c\u6b65\u3002 blockchain\u4f7f\u7528peer-to-peer\u67b6\u6784\uff1bgit\u4f7f\u7528peer-to-peer\u67b6\u6784\uff1b \u601d\u8003\uff1agit\u7684\u6bcf\u4e2apeer\u90fd\u6709\u4e00\u4e2a\u81ea\u5df1\u7684repository\u6765\u4fdd\u5b58\u5b8c\u6574\u7684commit\u6570\u636e\uff0cblockchain\u5462\uff1f\u662f\u5426\u4e5f\u662f\u7c7b\u4f3c\u8fd9\u6837\u7684\uff1f\u5e94\u8be5\u662f\u8fd9\u6837\u7684\u3002 git\u548cblockchain\u90fd\u53ef\u4ee5\u770b\u505a\u662f Merkle tree \uff0c\u4e0d\u540c\u7684\u662fgit\u662f\u652f\u6301branch\uff0c\u6240\u4ee5git\u7684commit\u6240\u7ec4\u6210\u7684\u662ftree\uff0c\u800cblockchain\u5219\u662f\u7ebf\u6027\u7684\u3002 \u770b\u770b\u522b\u4eba\u600e\u4e48\u8bf4 Why is Git not considered a \u201cblock chain\u201d?","title":"Introduction"},{"location":"Application/Blockchain/#_1","text":"\u5728\u5b66\u4e60\u4e86 git \u540e\uff0c\u6211\u5bf9\u5b83\u4f5c\u4e3a distributed version control \u7684\u8bf8\u591a\u7279\u6027\u4ee5\u53ca\u5b83\u7684\u539f\u7406\u5370\u8c61\u6df1\u523b\uff0c\u6bd4\u5982\u5b83\u7684 Data Assurance \u7279\u6027\u3001\u4f7f\u7528 peer-to-peer \u3001 decentralized system \u7b49\u3002\u540e\u6765\u6211\u8054\u60f3\u5230\u4e86\u5728\u9605\u8bfb\u65b0\u95fb\u65f6\u6d4f\u89c8\u8fc7\u7684 \u533a\u5757\u94fe \uff0c\u8bb0\u5f97\u4ecb\u7ecd\u8bf4\u5b83\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a \u5206\u5e03\u5f0f\u8d26\u672c \uff0c\u5b83\u6240\u91c7\u7528\u7684\u4e5f\u662f peer-to-peer \uff0c\u4e5f\u662f decentralized system \u3002\u8fd9\u6837\u5bf9\u6bd4\u4e86\u4e00\u4e0b\u540e\uff0c\u53d1\u73b0\u4e24\u8005\u633a\u7c7b\u4f3c\u7684\u3002\u540e\u6765\u6211\u6df1\u5165\u601d\u8003\u4e24\u8005\uff0c\u53d1\u73b0\uff0cLedger \u548c Version control\u672c\u8d28\u4e0a\u4e5f\u6709\u7740\u975e\u5e38\u7c7b\u4f3c\uff1a","title":"\u524d\u8a00"},{"location":"Application/Blockchain/#ledger#vs#version#control","text":"\u4e24\u8005\u4e4b\u95f4\u662f\u5b58\u5728\u7740\u5171\u901a\u4e4b\u5904\u7684\uff1a \u4e24\u8005\u90fd\u9700\u8981\u8ffd\u6eaf\u5386\u53f2 \u4e24\u8005\u90fd\u9700\u8981\u8bb0\u5f55\u53d8\u5316 \u4e24\u8005\u5bf9\u51c6\u786e\u6027\u90fd\u6709\u7740\u8981\u6c42 \u5728 \u767e\u5ea6\u767e\u79d1\u533a\u5757\u94fe \u4e2d\uff0c\u5bf9\u533a\u5757\u94fe\u7279\u6027\u6709\u5982\u4e0b\u5b9a\u4e49\uff1a \u533a\u5757\u94fe\u662f\u4e00\u4e2a \u4fe1\u606f\u6280\u672f \u9886\u57df\u7684\u672f\u8bed\u3002\u4ece\u672c\u8d28\u4e0a\u8bb2\uff0c\u5b83\u662f\u4e00\u4e2a\u5171\u4eab \u6570\u636e\u5e93 \uff0c\u5b58\u50a8\u4e8e\u5176\u4e2d\u7684\u6570\u636e\u6216\u4fe1\u606f\uff0c\u5177\u6709\u201c\u4e0d\u53ef\u4f2a\u9020\u201d\u201c\u5168\u7a0b\u7559\u75d5\u201d\u201c\u53ef\u4ee5\u8ffd\u6eaf\u201d\u201c\u516c\u5f00\u900f\u660e\u201d\u201c\u96c6\u4f53\u7ef4\u62a4\u201d\u7b49\u7279\u5f81\u3002\u57fa\u4e8e\u8fd9\u4e9b\u7279\u5f81\uff0c \u533a\u5757\u94fe\u6280\u672f \u5960\u5b9a\u4e86\u575a\u5b9e\u7684\u201c\u4fe1\u4efb\u201c\u57fa\u7840\uff0c\u521b\u9020\u4e86\u53ef\u9760\u7684\u201c\u5408\u4f5c\u201d\u673a\u5236\uff0c\u5177\u6709\u5e7f\u9614\u7684\u8fd0\u7528\u524d\u666f\u3002 \u770b\u5230\u4e86\u8fd9\u6bb5\u8bdd\u540e\uff0c\u6211\u60f3\u5230\u4e86 git \u7684 Data Assurance \u7279\u6027\uff1a It is also impossible to change any file, date, commit message, or any other data in a Git repository without changing the IDs of everything after it. This means that if you have a commit ID, you can be assured not only that your project is exactly the same as when it was committed, but that nothing in its history was changed. \u6211\u731c\u60f3\uff0c\u6b63\u662f\u7531\u4e8eLedger \u548c Version control \u4e24\u8005\u4e4b\u95f4\u7684\u4e00\u4e9b\u76f8\u540c\u7684\u9700\u6c42\uff0c\u6240\u4ee5\u4e24\u8005\u5728\u5b9e\u73b0\u8fd9\u4e9b\u9700\u6c42\u7684\u65f6\u5019\u624d\u4f1a\u9009\u62e9\u7c7b\u4f3c\u7684\u5b9e\u73b0\u65b9\u5f0f\u3002 \u6240\u4ee5\u5f00\u59cb\u4e86 Blockchain \u7684\u5b66\u4e60\uff0c\u7531\u4e8e\u6211\u662f\u662f\u7531 git \u800c\u8054\u60f3\u5230Blockchain\u7684\uff0c\u6240\u4ee5\u4e0b\u9762\u5728\u5bf9Blockchain\u8fdb\u884c\u4ecb\u7ecd\u7684\u65f6\u5019\uff0c\u6211\u4f1a\u4f7f\u7528git\u4e2d\u7684\u4e00\u4e9b\u6982\u5ff5\u6765\u8fdb\u884c\u7c7b\u6bd4\u3002","title":"Ledger VS Version control"},{"location":"Application/Blockchain/#blockchain","text":"A blockchain , originally block chain , is a growing list of records , called blocks , that are linked using cryptography . Each block contains a cryptographic hash of the previous block, a timestamp , and transaction data (generally represented as a Merkle tree ). NOTE: \u4e0b\u9762\u662fBlockchain\u7684\u7b80\u5355\u6a21\u578b\uff1a \u539f\u6587\u94fe\u63a5\uff1a https://www.researchgate.net/figure/The-Bitcoin-blockchain-is-a-hash-chain-of-blocks-Each-block-has-a-Merkle-tree-of_fig1_316789505 The Bitcoin blockchain is a hash chain of blocks. Each block has a Merkle tree of transactions. \u53ef\u4ee5\u52a0\u4e2ablockchain\u7b80\u5355\u5730\u770b\u505a\u662f hash chain \u3002 By design, a blockchain is resistant to modification of the data . It is \"an open, distributed ledger that can record transactions between two parties efficiently and in a verifiable and permanent way\". NOTE: \u8fd9\u79cd\u7279\u6027\uff08resistant to modification of the data\uff09\u5bf9\u4e8e\u4e00\u4e2a ledger \uff08\u8d26\u672c\uff09\u800c\u8a00\u662f\u5fc5\u987b\u7684\uff0c\u5426\u5219\u8fd9\u4e2aledger\u5c31\u5b8c\u5168\u6ca1\u6709\u610f\u4e49\u4e86\u3002 For use as a distributed ledger , a blockchain is typically managed by a peer-to-peer network collectively adhering to a protocol for inter-node communication and validating new blocks . Once recorded, the data in any given block cannot be altered retroactively without alteration of all subsequent blocks, which requires consensus of the network majority. Although blockchain records are not unalterable, blockchains may be considered secure by design and exemplify a distributed computing system with high Byzantine fault tolerance . Decentralized consensus has therefore been claimed with a blockchain. NOTE: blockchain\u7684\u8bbe\u8ba1\u662fresistant to modification of the data\uff0c\u4e5f\u5c31\u662f\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7684 secure by design \uff0c\u5e76\u4e14\u5b83\u5df2\u7ecf\u8fbe\u5230\u4e86\u6bd4\u8f83\u9ad8\u7684 Byzantine fault tolerance \u6807\u51c6\uff0c\u6240\u4ee5\u867d\u7136\u4ece\u6280\u672f\u4e0a\u6765\u8bf4\u662f\u662f\u80fd\u591f\u7be1\u6539record\u7684\uff0c\u4f46\u662f\u8fd9\u5e76\u4e0d\u80fd\u591f\u5426\u5b9ablockchain\u7684\u53ef\u7528\u6027\u3002","title":"Blockchain"},{"location":"Application/Blockchain/#history","text":"The first work on a cryptographically secured chain of blocks was described in 1991 by Stuart Haber and W. Scott Stornetta. They wanted to implement a system where document timestamps could not be tampered with. In 1992, Bayer, Haber and Stornetta incorporated Merkle trees to the design, which improved its efficiency by allowing several document certificates to be collected into one block. NOTE: git\u4e2d\u4e5f\u4f7f\u7528\u4e86 Merkle trees NOTE: \u4e0a\u8ff0\u63cf\u8ff0\u7684\u95ee\u9898\u662f\u4e00\u7c7b\u53eb\u505a\uff1a Trusted timestamping \u7684\u95ee\u9898","title":"History"},{"location":"Application/Blockchain/#structure","text":"A blockchain is a decentralized , distributed , and oftentimes public, digital ledger that is used to record transactions across many computers so that any involved record cannot be altered retroactively, without the alteration of all subsequent blocks. This allows the participants to verify and audit transactions independently and relatively inexpensively. A blockchain database is managed autonomously using a peer-to-peer network and a distributed timestamping server. They are authenticated by mass collaboration powered by collective self-interests . Such a design facilitates robust workflow where participants' uncertainty regarding data security is marginal. The use of a blockchain removes the characteristic of infinite reproducibility from a digital asset. It confirms that each unit of value was transferred only once, solving the long-standing problem of double spending . A blockchain has been described as a value-exchange protocol . A blockchain can maintain title rights because, when properly set up to detail the exchange agreement, it provides a record that compels offer and acceptance .","title":"Structure"},{"location":"Application/Blockchain/#blocks","text":"Blocks hold batches of valid transactions that are hashed and encoded into a Merkle tree . Each block includes the cryptographic hash of the prior block in the blockchain, linking the two. The linked blocks form a chain. This iterative process confirms the integrity of the previous block, all the way back to the original genesis block \uff08\u6e90\u5934\uff09. Sometimes separate blocks can be produced concurrently, creating a temporary fork. In addition to a secure hash-based history, any blockchain has a specified algorithm for scoring different versions of the history so that one with a higher score can be selected over others. Blocks not selected for inclusion in the chain are called orphan blocks . Peers supporting the database have different versions of the history from time to time. They keep only the highest-scoring version of the database known to them. Whenever a peer receives a higher-scoring version (usually the old version with a single new block added) they extend or overwrite their own database and retransmit the improvement to their peers. There is never an absolute guarantee that any particular entry will remain in the best version of the history forever. Blockchains are typically built to add the score of new blocks onto old blocks and are given incentives to extend with new blocks rather than overwrite old blocks. NOTE: database\u4e2d\u4fdd\u5b58\u4ec0\u4e48\u6570\u636e\u5462\uff1f\u662f\u6574\u4e2ablockchain\uff1f","title":"Blocks"},{"location":"Application/Blockchain/#decentralization","text":"NOTE: \u5176\u5b9e\u8fd9\u4e00\u6bb5\u6240\u63cf\u8ff0\u7684peer-to-peer network\u7684\u4f18\u52bf\u3002","title":"Decentralization"},{"location":"Application/Blockchain/#git#vs#blockchain","text":"**Blockchain**\u7684block\uff0c\u975e\u5e38\u7c7b\u4f3c\u4e8egit\u7684commit\uff1b blockchain\u4e2dchain\u7684\u542b\u4e49\u5c31\u662flinked block\u3002git\u4e2d\u7684**branch**\u5176\u5b9e\u5bf9\u5e94\u7684\u5c31\u662flinked commit\u3002 blockchain\u4f7f\u7528 cryptographic hash \u6765\u4f5c\u4e3a\u4e24\u4e2ablock\u7684link\uff0c\u8fd9\u5176\u5b9e\u5c31\u662f Hash Chain \u3002blockchain\u7684\u8fd9\u79cd\u7ec4\u7ec7\u65b9\u5f0f\u5b9e\u73b0\u4e86Data Assurance\uff08\u5728\u524d\u9762\u7ae0\u8282\u4e2d\u6240\u4ecb\u7ecd\u7684\uff09: Once recorded, the data in any given block cannot be altered retroactively without alteration of all subsequent blocks, which requires consensus of the network majority. git\u4e5f\u91c7\u7528\u4e86\u7c7b\u4f3c\u7684link\u6765\u7ec4\u7ec7commit\uff0cgit\u7684\u8fd9\u79cd\u7ec4\u7ec7\u65b9\u5f0f\u5b9e\u73b0Data Assurance\uff0c\u53c2\u89c1\uff1a Data Assurance Cryptographic authentication of history \u53ef\u4ee5\u770b\u5230\uff0c\u4e24\u8005\u90fd\u4f7f\u7528\u4e86 Hash Chain \uff0c\u8fd9\u79cd\u94fe\u5f0f\u4f9d\u8d56\u7684\u7ed3\u6784\u80fd\u591f\u975e\u5e38\u597d\u7684\u5b9e\u73b0Data Assurance\u3002 blockchain\u4e2d\u4f7f\u7528 Merkle tree \u6765\u7ec4\u7ec7 transaction data\u3002git\u4e2d\u4f7f\u7528 Merkle tree \u6765\u7ec4\u7ec7 tracked file\u3002 blockchain\u4e2d\u6dfb\u52a0\u4e00\u4e2a\u65b0\u7684block\u9700\u8981network\u4e2d\u7684\u6240\u6709\u7684peer\u7684consensus \u3002git\u4e2d\u6dfb\u52a0\u4e00\u4e2a\u65b0\u7684commit\u4e0d\u6d89\u53ca\u8fd9\u4e9b\u95ee\u9898\uff0cgit\u4e2d\u4f7f\u7528pull\u548cpush\u6765\u5728\u5404\u4e2apeer\uff08repository\uff09\u4e4b\u95f4\u7684\u540c\u6b65\u3002 blockchain\u4f7f\u7528peer-to-peer\u67b6\u6784\uff1bgit\u4f7f\u7528peer-to-peer\u67b6\u6784\uff1b \u601d\u8003\uff1agit\u7684\u6bcf\u4e2apeer\u90fd\u6709\u4e00\u4e2a\u81ea\u5df1\u7684repository\u6765\u4fdd\u5b58\u5b8c\u6574\u7684commit\u6570\u636e\uff0cblockchain\u5462\uff1f\u662f\u5426\u4e5f\u662f\u7c7b\u4f3c\u8fd9\u6837\u7684\uff1f\u5e94\u8be5\u662f\u8fd9\u6837\u7684\u3002 git\u548cblockchain\u90fd\u53ef\u4ee5\u770b\u505a\u662f Merkle tree \uff0c\u4e0d\u540c\u7684\u662fgit\u662f\u652f\u6301branch\uff0c\u6240\u4ee5git\u7684commit\u6240\u7ec4\u6210\u7684\u662ftree\uff0c\u800cblockchain\u5219\u662f\u7ebf\u6027\u7684\u3002","title":"Git VS Blockchain"},{"location":"Application/Blockchain/#_2","text":"","title":"\u770b\u770b\u522b\u4eba\u600e\u4e48\u8bf4"},{"location":"Application/Blockchain/#why#is#git#not#considered#a#block#chain","text":"","title":"Why is Git not considered a \u201cblock chain\u201d?"},{"location":"Application/Blockchain/Is-git-a-block-chain/","text":"ycombinator Is Git a Block Chain? \u200b ntonozzi on Apr 25, 2015 Both git repositories and bitcoin are specialized Merkle trees. Merkle trees are incredibly useful and general; they are used in many kinds of verification, especially of large chunks of data. http://en.wikipedia.org/wiki/Merkle_tree ejholmes on Apr 25, 2015 The power of the Merkle tree is pretty amazing. IPFS is a good example ngoldbaum on Apr 25, 2015 I thought it was a directed acyclic graph. Care to share the distinction? NOTE: \u6307\u7684\u662fgit petertodd on Apr 25, 2015 The terminology used in #bitcoin-wizards is to call it a merkelized dag , or merkle-dag. Similarly, we also refer to \"Merkelized Abstract Syntax Trees\", a way of hashing code originally proposed by Pieter Wuille and Russel O'connor(1) that will probably be added to Bitcoin's scripting system eventually. Pretty much any data structure can have hash functions added to it to \"merkelize\" it, producing an authenticated data structure: http://www.cs.umd.edu/~amiller/gpads/ 1) https://download.wpsoftware.net/bitcoin/wizards/2014-12-16.h... jscottmiller on Apr 25, 2015 \"Blockchain\" is generally used to refer to systems that either 1) use the sequence of blocks to model changes in custodianship or 2) (more generally) enforce a set of rules governing the correctness of a given block . That is, a block in the bitcoin blockchain is valid not only if its hash matches what one would expect given the included transactions, but that those transactions adhere to the rules of bitcoin. (No double-spends, no dust transactions, etc). While there are data structures in a git commit that must be present and/or follow a particular set of semantics, git does not enforce anything about the contents of those commits. Another key distinction: blockchains seek consensus , whereas divergent forks in git repos are by design. EDIT: I should probably not distinguish too much between consensus and rule enforcement, as those two are obviously intertwined. :) wmf on Apr 25, 2015 Consensus seems like an essential property that distinguishes a blockchain from a Merkle tree, and git does not provide consensus. oleganza on Apr 25, 2015 I'll quote myself ( http://blog.oleganza.com/post/85111558553/bitcoin-is-like ): Bitcoin is like Git: in Git (a distributed version control system) all your changes(commit) are organized in a chain protected by cryptographic hashes. If you trust the latest hash , you can get all the previous information (or any part of it) from any source and still verify that it is what you expect. Similarly, in Bitcoin, all transactions are organized in a chain (the blockchain) and once validated, no matter where they are stored, you can always trust any piece of blockchain by checking a chain of hashes that link to a hash you already trust. This naturally enables distributed storage and easy integrity checks. Bitcoin is unlike Git in a way that everyone strives to work on a single branch. In Git everyone may have several branches and fork and merge them all day long. In Bitcoin one cannot \u201cmerge\u201d forks. Blockchain is a actually a tree of transaction histories, but there is always one biggest branch (which has the value) and some accidental mini-branches (no more than one-two blocks long) that have no value at all. In Git content matters (regardless of the branch), in Bitcoin consensus matters (regardless of the content).","title":"Introduction"},{"location":"Application/Blockchain/Is-git-a-block-chain/#ycombinator#is#git#a#block#chain","text":"\u200b ntonozzi on Apr 25, 2015 Both git repositories and bitcoin are specialized Merkle trees. Merkle trees are incredibly useful and general; they are used in many kinds of verification, especially of large chunks of data. http://en.wikipedia.org/wiki/Merkle_tree ejholmes on Apr 25, 2015 The power of the Merkle tree is pretty amazing. IPFS is a good example ngoldbaum on Apr 25, 2015 I thought it was a directed acyclic graph. Care to share the distinction? NOTE: \u6307\u7684\u662fgit petertodd on Apr 25, 2015 The terminology used in #bitcoin-wizards is to call it a merkelized dag , or merkle-dag. Similarly, we also refer to \"Merkelized Abstract Syntax Trees\", a way of hashing code originally proposed by Pieter Wuille and Russel O'connor(1) that will probably be added to Bitcoin's scripting system eventually. Pretty much any data structure can have hash functions added to it to \"merkelize\" it, producing an authenticated data structure: http://www.cs.umd.edu/~amiller/gpads/ 1) https://download.wpsoftware.net/bitcoin/wizards/2014-12-16.h... jscottmiller on Apr 25, 2015 \"Blockchain\" is generally used to refer to systems that either 1) use the sequence of blocks to model changes in custodianship or 2) (more generally) enforce a set of rules governing the correctness of a given block . That is, a block in the bitcoin blockchain is valid not only if its hash matches what one would expect given the included transactions, but that those transactions adhere to the rules of bitcoin. (No double-spends, no dust transactions, etc). While there are data structures in a git commit that must be present and/or follow a particular set of semantics, git does not enforce anything about the contents of those commits. Another key distinction: blockchains seek consensus , whereas divergent forks in git repos are by design. EDIT: I should probably not distinguish too much between consensus and rule enforcement, as those two are obviously intertwined. :) wmf on Apr 25, 2015 Consensus seems like an essential property that distinguishes a blockchain from a Merkle tree, and git does not provide consensus. oleganza on Apr 25, 2015 I'll quote myself ( http://blog.oleganza.com/post/85111558553/bitcoin-is-like ): Bitcoin is like Git: in Git (a distributed version control system) all your changes(commit) are organized in a chain protected by cryptographic hashes. If you trust the latest hash , you can get all the previous information (or any part of it) from any source and still verify that it is what you expect. Similarly, in Bitcoin, all transactions are organized in a chain (the blockchain) and once validated, no matter where they are stored, you can always trust any piece of blockchain by checking a chain of hashes that link to a hash you already trust. This naturally enables distributed storage and easy integrity checks. Bitcoin is unlike Git in a way that everyone strives to work on a single branch. In Git everyone may have several branches and fork and merge them all day long. In Bitcoin one cannot \u201cmerge\u201d forks. Blockchain is a actually a tree of transaction histories, but there is always one biggest branch (which has the value) and some accidental mini-branches (no more than one-two blocks long) that have no value at all. In Git content matters (regardless of the branch), in Bitcoin consensus matters (regardless of the content).","title":"ycombinator Is Git a Block Chain?"},{"location":"Application/IPFS/","text":"IPFS","title":"Introduction"},{"location":"Application/IPFS/#ipfs","text":"","title":"IPFS"},{"location":"Application/IPFS/InterPlanetary-File-System/","text":"\u7ef4\u57fa\u767e\u79d1 InterPlanetary File System","title":"InterPlanetary File System"},{"location":"Application/NFS/Network-File-System/","text":"\u7ef4\u57fa\u767e\u79d1 Network File System","title":"Network-File-System"},{"location":"Application/NFS/Network-File-System/#network#file#system","text":"","title":"\u7ef4\u57fa\u767e\u79d1Network File System"},{"location":"Application/Parallel-numeric-processing-system/","text":"Parallel-numeric-processing-system \u5728\u5b66\u4e60TensorFlow\u7684\u5b9e\u73b0\u7684\u65f6\u5019\uff0c\u60f3\u5230\u4e86\u8fd9\u4e2a\u4e3b\u9898\uff0c\u89c9\u5f97\u6709\u5fc5\u8981\u603b\u7ed3\u4e00\u4e0b\u3002 \u65f6\u4ee3\u80cc\u666f 1) \u7b97\u529b\u589e\u5f3a 2) \u7f51\u7edc\u589e\u5f3a 3) big data 4) AI \u5bf9big data\u7684computation\uff0c\u80fd\u591f\u5b9e\u73b0\u975e\u5e38\u5927\u7684\u4ef7\u503c\uff0c\u672c\u7ae0\u5c31\u662f\u5bf9\u8fd9\u4e2a\u8bdd\u9898\u7684\u63a2\u8ba8\uff0c\u4f5c\u4e3asoftware engineer\uff0c\u6211\u4eec\u9700\u8981\u8003\u8651\u5982\u4e0b\u95ee\u9898: 1) \u5982\u4f55\u5b9e\u73b0? \u5982\u4f55\u67b6\u6784? Implementation Dataflow-programming\uff0c\u53c2\u89c1 ./Dataflow-programming \u3002 Front end and back end NOTE: \u5728numeric processing system\u4e2d\uff0c\u4f1a\u6d89\u53caexpression-oriented programming\u7684\uff0c\u56e0\u4e3a\u6211\u4eec\u9700\u8981\u63cf\u8ff0expression\u3002 Programmer\u5b9e\u73b0\u9700\u8981\u63cf\u8ff0computation\uff0c\u4e00\u822c\u4f7f\u7528expression tree/computational graph\u7684\u5f62\u5f0f\uff0c\u7136\u540e\u7531front end\u5c06expression tree/computational graph\u8f6c\u6362\u4e3ainstruction/command\uff0c\u7136\u540e\u7531backend\u6765\u8fdb\u884c\u6267\u884c\uff1b \u5173\u4e8efront end\uff0c\u53c2\u89c1\u5de5\u7a0bLanguage\u7684 Formal-language-processing \u7ae0\u8282\u3002 Case: Microsoft Naiad The Naiad project is an investigation of data-parallel dataflow computation , like Dryad and DryadLINQ, but with a focus on low-latency streaming and cyclic computations. Naiad introduces a new computational model , timely dataflow, which combines low-latency asynchronous message flow with lightweight coordination when required. These primitives (\u539f\u8bed) allow the efficient implementation of many dataflow patterns , from bulk and streaming computation to iterative graph processing and machine learning. \"timely dataflow\"\u7684\u542b\u4e49\u662f\"\u53ca\u65f6\u6570\u636e\u6d41\"\u3002 \"primitives\"\u5373\"\u539f\u8bed\"\uff0c\u5b83\u544a\u8bc9\u6211\u4eecNaiad\u63d0\u4f9b\u4e86\u4e00\u4e9bAPI\u6765\u4f9buser\u5bf9computation\u8fdb\u884c\u63cf\u8ff0\u3002 \u4ece\u6700\u540e\u4e00\u6bb5\u8bdd\u53ef\u4ee5\u770b\u51fa\uff0cNaiad\u662f\u975e\u5e38\u7075\u6d3b\u7684\u3002 \u5b9e\u73b0\u601d\u8def: 1) stream model \u539f\u6587\u4e2d\u7684dataflow\u5176\u5b9e\u5c31\u662fstream\u3002 2) data-parallel dataflow computation 3) distributed/parallel computing 4) symbolic programming Case: Google TensorFlow \u5b9e\u73b0\u601d\u8def: \u4e3b\u8981\u662f\u501f\u9274Microsoft Naiad\u7684\u5b9e\u73b0\u601d\u8def\u3002","title":"Introduction"},{"location":"Application/Parallel-numeric-processing-system/#parallel-numeric-processing-system","text":"\u5728\u5b66\u4e60TensorFlow\u7684\u5b9e\u73b0\u7684\u65f6\u5019\uff0c\u60f3\u5230\u4e86\u8fd9\u4e2a\u4e3b\u9898\uff0c\u89c9\u5f97\u6709\u5fc5\u8981\u603b\u7ed3\u4e00\u4e0b\u3002","title":"Parallel-numeric-processing-system"},{"location":"Application/Parallel-numeric-processing-system/#_1","text":"1) \u7b97\u529b\u589e\u5f3a 2) \u7f51\u7edc\u589e\u5f3a 3) big data 4) AI \u5bf9big data\u7684computation\uff0c\u80fd\u591f\u5b9e\u73b0\u975e\u5e38\u5927\u7684\u4ef7\u503c\uff0c\u672c\u7ae0\u5c31\u662f\u5bf9\u8fd9\u4e2a\u8bdd\u9898\u7684\u63a2\u8ba8\uff0c\u4f5c\u4e3asoftware engineer\uff0c\u6211\u4eec\u9700\u8981\u8003\u8651\u5982\u4e0b\u95ee\u9898: 1) \u5982\u4f55\u5b9e\u73b0? \u5982\u4f55\u67b6\u6784?","title":"\u65f6\u4ee3\u80cc\u666f"},{"location":"Application/Parallel-numeric-processing-system/#implementation","text":"Dataflow-programming\uff0c\u53c2\u89c1 ./Dataflow-programming \u3002","title":"Implementation"},{"location":"Application/Parallel-numeric-processing-system/#front#end#and#back#end","text":"NOTE: \u5728numeric processing system\u4e2d\uff0c\u4f1a\u6d89\u53caexpression-oriented programming\u7684\uff0c\u56e0\u4e3a\u6211\u4eec\u9700\u8981\u63cf\u8ff0expression\u3002 Programmer\u5b9e\u73b0\u9700\u8981\u63cf\u8ff0computation\uff0c\u4e00\u822c\u4f7f\u7528expression tree/computational graph\u7684\u5f62\u5f0f\uff0c\u7136\u540e\u7531front end\u5c06expression tree/computational graph\u8f6c\u6362\u4e3ainstruction/command\uff0c\u7136\u540e\u7531backend\u6765\u8fdb\u884c\u6267\u884c\uff1b \u5173\u4e8efront end\uff0c\u53c2\u89c1\u5de5\u7a0bLanguage\u7684 Formal-language-processing \u7ae0\u8282\u3002","title":"Front end and back end"},{"location":"Application/Parallel-numeric-processing-system/#case#microsoft#naiad","text":"The Naiad project is an investigation of data-parallel dataflow computation , like Dryad and DryadLINQ, but with a focus on low-latency streaming and cyclic computations. Naiad introduces a new computational model , timely dataflow, which combines low-latency asynchronous message flow with lightweight coordination when required. These primitives (\u539f\u8bed) allow the efficient implementation of many dataflow patterns , from bulk and streaming computation to iterative graph processing and machine learning. \"timely dataflow\"\u7684\u542b\u4e49\u662f\"\u53ca\u65f6\u6570\u636e\u6d41\"\u3002 \"primitives\"\u5373\"\u539f\u8bed\"\uff0c\u5b83\u544a\u8bc9\u6211\u4eecNaiad\u63d0\u4f9b\u4e86\u4e00\u4e9bAPI\u6765\u4f9buser\u5bf9computation\u8fdb\u884c\u63cf\u8ff0\u3002 \u4ece\u6700\u540e\u4e00\u6bb5\u8bdd\u53ef\u4ee5\u770b\u51fa\uff0cNaiad\u662f\u975e\u5e38\u7075\u6d3b\u7684\u3002","title":"Case: Microsoft Naiad"},{"location":"Application/Parallel-numeric-processing-system/#_2","text":"1) stream model \u539f\u6587\u4e2d\u7684dataflow\u5176\u5b9e\u5c31\u662fstream\u3002 2) data-parallel dataflow computation 3) distributed/parallel computing 4) symbolic programming","title":"\u5b9e\u73b0\u601d\u8def:"},{"location":"Application/Parallel-numeric-processing-system/#case#google#tensorflow","text":"","title":"Case: Google TensorFlow"},{"location":"Application/Parallel-numeric-processing-system/#_3","text":"\u4e3b\u8981\u662f\u501f\u9274Microsoft Naiad\u7684\u5b9e\u73b0\u601d\u8def\u3002","title":"\u5b9e\u73b0\u601d\u8def:"},{"location":"Application/Parallel-numeric-processing-system/Dataflow-programming/","text":"Dataflow programming \u5728\u5b66\u4e60TensorFlow\u65f6\uff0c\u77e5\u9053\u4ed6\u662f\u57fa\u4e8edataflow architecture\u7684\uff0c\u6240\u4ee5\u5bf9Dataflow programming\u8fdb\u884c\u4e86\u4e00\u4e9b\u4e86\u89e3\u3002 wikipedia Dataflow programming In computer programming , dataflow programming is a programming paradigm that models a program as a directed graph of the data flowing between operations, thus implementing dataflow principles and architecture. NOTE: TensorFlow\u5c31\u662f\u5178\u578b\u7684\u57fa\u4e8edataflow\u7684\u3002TensorFlow\u4e2d\u7684directed graph\u88ab\u79f0\u4e3acomputational graph\u3002 Dataflow programming languages share some features of functional languages , and were generally developed in order to bring some functional concepts to a language more suitable for numeric processing . NOTE: \u672c\u6bb5\u6240\u8ba8\u8bba\u7684\u662fdataflow language \u548c functional language\u3002 Some authors use the term Datastream instead of Dataflow to avoid confusion with Dataflow Computing or Dataflow architecture , based on an indeterministic machine paradigm. NOTE: Dataflow architecture \u662fCPU architecture\u3002 Dataflow programming was pioneered by Jack Dennis and his graduate students at MIT in the 1960s. Properties of dataflow programming languages Traditionally, a program is modeled as a series of operations happening in a specific order; this may be referred to as sequential,[ 1] :p.3 procedural,[ 2] Control flow [ 2] (indicating that the program chooses a specific path), or imperative programming (\u547d\u4ee4\u5f0f\u7f16\u7a0b). The program focuses on commands, in line with the von Neumann [ 1] :p.3 vision of sequential programming, where data is normally \"at rest\"[ 2] :p.7 In contrast, dataflow programming emphasizes the movement of data and models programs as a series of connections. Explicitly defined inputs and outputs connect operations, which function like black boxes .[ 2] :p.2 An operation runs as soon as all of its inputs become valid.[ 3] Thus, dataflow languages are inherently parallel and can work well in large, decentralized systems.[ 1] :p.3[ 4] [ 5] NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u5bf9dataflow programming language\u548c\u5bfb\u5e38\u7684imperative programming\u7684\u5bf9\u6bd4\u662f\u975e\u5e38\u597d\u7684\u3002\u5173\u4e8e\u5b83\u4eec\u7684\u5dee\u5f02\uff0c\u5728\u5de5\u7a0bmachine-learning\u7684 Programming\\TensorFlow\\Implementation \u7684 TensorFlow VS compiler \u7ae0\u8282\u4e2d\u4e5f\u8fdb\u884c\u4e86\u8ba8\u8bba\u3002 wisegeek What Is Dataflow Programming? Dataflow and stream dataflow\u5176\u5b9e\u5c31\u662f\u4e00\u79cdstream\uff0cTensorFlow\u7684\u4e2d\uff0c\u663e\u7136\u5b83\u7684stream\u7684\u5355\u4f4d\u662ftensor\u3002\u5173\u4e8estream\uff0c\u53c2\u89c1\u5de5\u7a0bdiscrete\u7684 Relation-structure-computation\\Model\\Stream \u7ae0\u8282\u3002 github topics dataflow-programming","title":"Introduction"},{"location":"Application/Parallel-numeric-processing-system/Dataflow-programming/#dataflow#programming","text":"\u5728\u5b66\u4e60TensorFlow\u65f6\uff0c\u77e5\u9053\u4ed6\u662f\u57fa\u4e8edataflow architecture\u7684\uff0c\u6240\u4ee5\u5bf9Dataflow programming\u8fdb\u884c\u4e86\u4e00\u4e9b\u4e86\u89e3\u3002","title":"Dataflow programming"},{"location":"Application/Parallel-numeric-processing-system/Dataflow-programming/#wikipedia#dataflow#programming","text":"In computer programming , dataflow programming is a programming paradigm that models a program as a directed graph of the data flowing between operations, thus implementing dataflow principles and architecture. NOTE: TensorFlow\u5c31\u662f\u5178\u578b\u7684\u57fa\u4e8edataflow\u7684\u3002TensorFlow\u4e2d\u7684directed graph\u88ab\u79f0\u4e3acomputational graph\u3002 Dataflow programming languages share some features of functional languages , and were generally developed in order to bring some functional concepts to a language more suitable for numeric processing . NOTE: \u672c\u6bb5\u6240\u8ba8\u8bba\u7684\u662fdataflow language \u548c functional language\u3002 Some authors use the term Datastream instead of Dataflow to avoid confusion with Dataflow Computing or Dataflow architecture , based on an indeterministic machine paradigm. NOTE: Dataflow architecture \u662fCPU architecture\u3002 Dataflow programming was pioneered by Jack Dennis and his graduate students at MIT in the 1960s.","title":"wikipedia Dataflow programming"},{"location":"Application/Parallel-numeric-processing-system/Dataflow-programming/#properties#of#dataflow#programming#languages","text":"Traditionally, a program is modeled as a series of operations happening in a specific order; this may be referred to as sequential,[ 1] :p.3 procedural,[ 2] Control flow [ 2] (indicating that the program chooses a specific path), or imperative programming (\u547d\u4ee4\u5f0f\u7f16\u7a0b). The program focuses on commands, in line with the von Neumann [ 1] :p.3 vision of sequential programming, where data is normally \"at rest\"[ 2] :p.7 In contrast, dataflow programming emphasizes the movement of data and models programs as a series of connections. Explicitly defined inputs and outputs connect operations, which function like black boxes .[ 2] :p.2 An operation runs as soon as all of its inputs become valid.[ 3] Thus, dataflow languages are inherently parallel and can work well in large, decentralized systems.[ 1] :p.3[ 4] [ 5] NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u5bf9dataflow programming language\u548c\u5bfb\u5e38\u7684imperative programming\u7684\u5bf9\u6bd4\u662f\u975e\u5e38\u597d\u7684\u3002\u5173\u4e8e\u5b83\u4eec\u7684\u5dee\u5f02\uff0c\u5728\u5de5\u7a0bmachine-learning\u7684 Programming\\TensorFlow\\Implementation \u7684 TensorFlow VS compiler \u7ae0\u8282\u4e2d\u4e5f\u8fdb\u884c\u4e86\u8ba8\u8bba\u3002","title":"Properties of dataflow programming languages"},{"location":"Application/Parallel-numeric-processing-system/Dataflow-programming/#wisegeek#what#is#dataflow#programming","text":"","title":"wisegeek What Is Dataflow Programming?"},{"location":"Application/Parallel-numeric-processing-system/Dataflow-programming/#dataflow#and#stream","text":"dataflow\u5176\u5b9e\u5c31\u662f\u4e00\u79cdstream\uff0cTensorFlow\u7684\u4e2d\uff0c\u663e\u7136\u5b83\u7684stream\u7684\u5355\u4f4d\u662ftensor\u3002\u5173\u4e8estream\uff0c\u53c2\u89c1\u5de5\u7a0bdiscrete\u7684 Relation-structure-computation\\Model\\Stream \u7ae0\u8282\u3002","title":"Dataflow and stream"},{"location":"Application/Parallel-numeric-processing-system/Dataflow-programming/#github#topics#dataflow-programming","text":"","title":"github topics dataflow-programming"},{"location":"Concurrent-computing/","text":"Concurrent computing \u672c\u7ae0\u8ba8\u8bbaconcurrent computing\u3002 \u6211\u7684\u5b66\u4e60\u4e4b\u8def \u4e00\u3001APUE \u4ece\u4e2d\u5b66\u4e60\u57fa\u672c\u7684pthread API\uff0c\u57fa\u672c\u7684thread concurrency control\uff0c\u57fa\u672c\u7684inter-thread communication\u3001event notification\u7b49\uff0c\u4e3b\u8981\u662fC\u3001pthread\u3002 \u4e8c\u3001C++ thread library\u3001asynchronous programming library \u4e09\u3001\u4e00\u4e9bdesign pattern 1\u3001\u53c2\u89c1 Event-driven-concurrent-server\\Design-pattern 2\u3001thread pool \u56db\u3001\u5b66\u4e60lock free 1\u3001C++ memory model\u3001atomic library 2\u3001 CPU-cache-memory \u3001cache coherence\u3001cache optimization 3\u3001custom spinning lock 4\u3001\u53c2\u89c1 Concurrent-computing\\Concurrency-control\\Non-blocking \u7ae0\u8282 \u4e94\u3001Expert-Herb-Sutter\u7684Effective-Concurrency-Serial \u524d\u9762\u3001\u7cfb\u7edf\u6027\u7684\u3001\u6982\u62ec\u3001\u5c45\u9ad8\u4e34\u4e0b\u3001\u975e\u5e38\u597d\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u7cfb\u7edf\u6027\u7684\u5b66\u4e60concurrency\u7684guide\u3002 \u540e\u7eed\u5f88\u591a\u5185\u5bb9\uff0c\u53ef\u80fd\u662f\u57fa\u4e8e\u8fd9\u4e2a\u7cfb\u5217\u4e2d\u7684\u5185\u5bb9\u5c55\u5f00\u3001\u7ec4\u7ec7\u3002 \u4e3b\u8981topic \u4e0b\u9762\u603b\u7ed3\u4e86\u5b66\u4e60concurrent programming\u65f6\uff0c\u6d89\u53ca\u7684\u4e00\u4e9btopic\u3002 Concurrency control \u5728 wikipedia Concurrent computing#Coordinating access to shared resources \u4e2d\u8bf4\u660e\u4e86: \u4e3a\u4ec0\u4e48\u5728concurrent computing\u4e2d\u9700\u8981concurrency control: The main challenge in designing concurrent programs is concurrency control : ensuring the correct sequencing of the interactions or communications between different computational executions, and coordinating access to resources that are shared among executions. Unit \u201cconcurrent computing\u201d\u5373\u5e76\u53d1\uff0c\u4e00\u4e2a\u6211\u4eec\u5e73\u65f6\u7ecf\u5e38\u542c\u5230\u7684\u8bcd\u8bed\u3002\u5728\u8c08\u53caconcurrency\u7684\u65f6\u5019\uff0c\u6211\u4eec\u8981\u601d\u8003\uff1a 1\u3001\u5e76\u53d1\u7684entity/unit\u662f\u4ec0\u4e48\uff1f Concurrent-data-structure \u5e76\u53d1\u7684\u6570\u636e\u7ed3\u6784\u3002 wikipedia Concurrent computing Concurrent computing is a form of computing in which several computations are executed concurrently \u2014during overlapping time periods\u2014instead of sequentially , with one completing before the next starts. NOTE: \u4e00\u3001\u5e76\u884c VS \u4e32\u884c","title":"Concurrent-computing"},{"location":"Concurrent-computing/#concurrent#computing","text":"\u672c\u7ae0\u8ba8\u8bbaconcurrent computing\u3002","title":"Concurrent computing"},{"location":"Concurrent-computing/#_1","text":"\u4e00\u3001APUE \u4ece\u4e2d\u5b66\u4e60\u57fa\u672c\u7684pthread API\uff0c\u57fa\u672c\u7684thread concurrency control\uff0c\u57fa\u672c\u7684inter-thread communication\u3001event notification\u7b49\uff0c\u4e3b\u8981\u662fC\u3001pthread\u3002 \u4e8c\u3001C++ thread library\u3001asynchronous programming library \u4e09\u3001\u4e00\u4e9bdesign pattern 1\u3001\u53c2\u89c1 Event-driven-concurrent-server\\Design-pattern 2\u3001thread pool \u56db\u3001\u5b66\u4e60lock free 1\u3001C++ memory model\u3001atomic library 2\u3001 CPU-cache-memory \u3001cache coherence\u3001cache optimization 3\u3001custom spinning lock 4\u3001\u53c2\u89c1 Concurrent-computing\\Concurrency-control\\Non-blocking \u7ae0\u8282 \u4e94\u3001Expert-Herb-Sutter\u7684Effective-Concurrency-Serial \u524d\u9762\u3001\u7cfb\u7edf\u6027\u7684\u3001\u6982\u62ec\u3001\u5c45\u9ad8\u4e34\u4e0b\u3001\u975e\u5e38\u597d\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u7cfb\u7edf\u6027\u7684\u5b66\u4e60concurrency\u7684guide\u3002 \u540e\u7eed\u5f88\u591a\u5185\u5bb9\uff0c\u53ef\u80fd\u662f\u57fa\u4e8e\u8fd9\u4e2a\u7cfb\u5217\u4e2d\u7684\u5185\u5bb9\u5c55\u5f00\u3001\u7ec4\u7ec7\u3002","title":"\u6211\u7684\u5b66\u4e60\u4e4b\u8def"},{"location":"Concurrent-computing/#topic","text":"\u4e0b\u9762\u603b\u7ed3\u4e86\u5b66\u4e60concurrent programming\u65f6\uff0c\u6d89\u53ca\u7684\u4e00\u4e9btopic\u3002","title":"\u4e3b\u8981topic"},{"location":"Concurrent-computing/#concurrency#control","text":"\u5728 wikipedia Concurrent computing#Coordinating access to shared resources \u4e2d\u8bf4\u660e\u4e86: \u4e3a\u4ec0\u4e48\u5728concurrent computing\u4e2d\u9700\u8981concurrency control: The main challenge in designing concurrent programs is concurrency control : ensuring the correct sequencing of the interactions or communications between different computational executions, and coordinating access to resources that are shared among executions.","title":"Concurrency control"},{"location":"Concurrent-computing/#unit","text":"\u201cconcurrent computing\u201d\u5373\u5e76\u53d1\uff0c\u4e00\u4e2a\u6211\u4eec\u5e73\u65f6\u7ecf\u5e38\u542c\u5230\u7684\u8bcd\u8bed\u3002\u5728\u8c08\u53caconcurrency\u7684\u65f6\u5019\uff0c\u6211\u4eec\u8981\u601d\u8003\uff1a 1\u3001\u5e76\u53d1\u7684entity/unit\u662f\u4ec0\u4e48\uff1f","title":"Unit"},{"location":"Concurrent-computing/#concurrent-data-structure","text":"\u5e76\u53d1\u7684\u6570\u636e\u7ed3\u6784\u3002","title":"Concurrent-data-structure"},{"location":"Concurrent-computing/#wikipedia#concurrent#computing","text":"Concurrent computing is a form of computing in which several computations are executed concurrently \u2014during overlapping time periods\u2014instead of sequentially , with one completing before the next starts. NOTE: \u4e00\u3001\u5e76\u884c VS \u4e32\u884c","title":"wikipedia Concurrent computing"},{"location":"Concurrent-computing/Books/","text":"Books concurrencyfreaks http://www.concurrencyfreaks.com","title":"Books"},{"location":"Concurrent-computing/Books/#books","text":"","title":"Books"},{"location":"Concurrent-computing/Books/#concurrencyfreaks","text":"http://www.concurrencyfreaks.com","title":"concurrencyfreaks"},{"location":"Concurrent-computing/Classic-problem/","text":"Classic problem \u672c\u7ae0\u603b\u7ed3\u5728concurrent programming\u4e2d\u7684\u7ecf\u5178\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u7ecf\u5178\u95ee\u9898\uff0c\u662f\u7531\u8ba1\u7b97\u673a\u79d1\u5b66\u7684\u5148\u9a71\u4eec\u63d0\u51fa\u7684\uff0c\u5e76\u8fdb\u884c\u4e86\u5f62\u5f0f\u5316\u5730\u5b9a\u4e49\uff0c\u6bcf\u4e00\u4e2a\u7ecf\u5178\u7684\u95ee\u9898\uff0c\u5176\u5b9e\u90fd\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u4e2a\u6a21\u578b\uff0c\u80fd\u591f\u63cf\u8ff0\u6211\u4eec\u6240\u78b0\u5230\u7684\u5f88\u591a\u95ee\u9898\u3002 \u672c\u7ae0\u4e3b\u8981\u53c2\u8003\u7ef4\u57fa\u767e\u79d1 Concurrent computing \u6240\u603b\u7ed3\u7684**Classic problems**\u3002 TODO golangprograms Golang Concurrency","title":"Introduction"},{"location":"Concurrent-computing/Classic-problem/#classic#problem","text":"\u672c\u7ae0\u603b\u7ed3\u5728concurrent programming\u4e2d\u7684\u7ecf\u5178\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u7ecf\u5178\u95ee\u9898\uff0c\u662f\u7531\u8ba1\u7b97\u673a\u79d1\u5b66\u7684\u5148\u9a71\u4eec\u63d0\u51fa\u7684\uff0c\u5e76\u8fdb\u884c\u4e86\u5f62\u5f0f\u5316\u5730\u5b9a\u4e49\uff0c\u6bcf\u4e00\u4e2a\u7ecf\u5178\u7684\u95ee\u9898\uff0c\u5176\u5b9e\u90fd\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u4e2a\u6a21\u578b\uff0c\u80fd\u591f\u63cf\u8ff0\u6211\u4eec\u6240\u78b0\u5230\u7684\u5f88\u591a\u95ee\u9898\u3002 \u672c\u7ae0\u4e3b\u8981\u53c2\u8003\u7ef4\u57fa\u767e\u79d1 Concurrent computing \u6240\u603b\u7ed3\u7684**Classic problems**\u3002","title":"Classic problem"},{"location":"Concurrent-computing/Classic-problem/#todo","text":"golangprograms Golang Concurrency","title":"TODO"},{"location":"Concurrent-computing/Classic-problem/Checkpoint-Synchronization/","text":"Checkpoint Synchronization \u662f\u5728\u9605\u8bfb golangprograms Golang Concurrency \u65f6\uff0c\u53d1\u73b0\u7684\u8fd9\u4e2atopic\u3002 golangprograms Golang Concurrency # Illustration of Checkpoint Synchronization in Golang rosettacode Checkpoint synchronization The checkpoint synchronization is a problem of synchronizing multiple tasks . Consider a workshop where several workers ( tasks ) assembly details of some mechanism. When each of them completes his work they put the details together. There is no store, so a worker who finished its part first must wait for others before starting another one. Putting details together is the checkpoint at which tasks synchronize themselves before going their paths apart.","title":"Introduction"},{"location":"Concurrent-computing/Classic-problem/Checkpoint-Synchronization/#checkpoint#synchronization","text":"\u662f\u5728\u9605\u8bfb golangprograms Golang Concurrency \u65f6\uff0c\u53d1\u73b0\u7684\u8fd9\u4e2atopic\u3002","title":"Checkpoint Synchronization"},{"location":"Concurrent-computing/Classic-problem/Checkpoint-Synchronization/#golangprograms#golang#concurrency#illustration#of#checkpoint#synchronization#in#golang","text":"","title":"golangprograms Golang Concurrency # Illustration of Checkpoint Synchronization in Golang"},{"location":"Concurrent-computing/Classic-problem/Checkpoint-Synchronization/#rosettacode#checkpoint#synchronization","text":"The checkpoint synchronization is a problem of synchronizing multiple tasks . Consider a workshop where several workers ( tasks ) assembly details of some mechanism. When each of them completes his work they put the details together. There is no store, so a worker who finished its part first must wait for others before starting another one. Putting details together is the checkpoint at which tasks synchronize themselves before going their paths apart.","title":"rosettacode Checkpoint synchronization"},{"location":"Concurrent-computing/Classic-problem/Cigarette-smokers-problem/","text":"Cigarette smokers problem wikipedia Cigarette smokers problem TODO golangprograms Golang Concurrency","title":"Introduction"},{"location":"Concurrent-computing/Classic-problem/Cigarette-smokers-problem/#cigarette#smokers#problem","text":"","title":"Cigarette smokers problem"},{"location":"Concurrent-computing/Classic-problem/Cigarette-smokers-problem/#wikipedia#cigarette#smokers#problem","text":"","title":"wikipedia Cigarette smokers problem"},{"location":"Concurrent-computing/Classic-problem/Cigarette-smokers-problem/#todo","text":"golangprograms Golang Concurrency","title":"TODO"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/","text":"Dining philosophers problem \u4e00\u3001 1\u3001\u8fd9\u4e2a\u95ee\u9898\u662f\u6700\u6700\u80fd\u591f\u4f53\u73b0deadlock\u3001livelock\u7684 2\u3001\u5b83\u6700\u6700\u80fd\u591f\u4f53\u73b0circular dependency \u4e8c\u3001\u8fd9\u4e2a\u95ee\u9898\u672c\u8eab\u5c31\u5305\u542b\u4e86circle: \"\u5706\u684c\" --\u300b circle\uff1b\u56e0\u6b64\u5b83\u5929\u751f\u5c31\u6709circular dependency\u95ee\u9898\uff1b \u56e0\u6b64\u89e3\u51b3\u65b9\u6848\uff0c\u5c31\u662f\u8981\"\u7834\u73af\"\uff0c\u53ef\u4ee5: 1\u3001\u5f15\u5165Arbitrator\uff0c\u5bf9\u5e94 \"\u670d\u52a1\u751f\u89e3\u6cd5/Arbitrator solution\" 2\u3001\u5f15\u5165hierarchy \u4e09\u3001\u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\u7ed9\u51fa\u4e86demo code mariusbancila Dining Philosophers in C++11 mariusbancila Dining philosophers in C++11: Chandy-Misra algorithm \u5728\u4e0b\u9762\u7684\u662f\u4e00\u4e9b\u5f00\u6e90\u5b9e\u73b0: 1\u3001 mtking2 / dining-philosophers 2\u3001 graninas / cpp_philosophers_stm \u56db\u3001\u8fd9\u4e2a\u95ee\u9898\u53ef\u4ee5\u8fdb\u4e00\u6b65\u5730\u62bd\u8c61\u4e3a\u4e00\u4e2a: \u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u5b83\u7684\u4e00\u4e2a\u8981\u6c42: 1\u3001\u8981\u4fdd\u8bc1\u516c\u5e73\u6027\uff0c\u5176\u5b9estarvation\u5c31\u662f\u4e00\u79cd\u6781\u7aef\u7684\u4e0d\u516c\u5e73 2\u3001\u4e0d\u4f1a\u6b7b\u9501 biancheng \u54f2\u5b66\u5bb6\u5c31\u9910\u95ee\u9898\u5206\u6790\uff08\u542b\u89e3\u51b3\u65b9\u6848\uff09 NOTE: 1\u3001\u95ee\u9898\u63cf\u8ff0\u975e\u5e38\u597d \u5047\u8bbe\u6709 5 \u4e2a\u54f2\u5b66\u5bb6\uff0c\u4ed6\u4eec\u7684\u751f\u6d3b\u53ea\u662f\u601d\u8003\u548c\u5403\u996d\u3002\u8fd9\u4e9b\u54f2\u5b66\u5bb6\u5171\u7528\u4e00\u4e2a**\u5706\u684c**\uff0c\u6bcf\u4f4d\u90fd\u6709\u4e00\u628a\u6905\u5b50\u3002\u5728\u684c\u5b50\u4e2d\u592e\u6709\u4e00\u7897\u7c73\u996d\uff0c\u5728\u684c\u5b50\u4e0a\u653e\u7740 5 \u6839\u7b77\u5b50\uff08\u56fe 1 )\u3002 \u5f53\u4e00\u4f4d\u54f2\u5b66\u5bb6\u601d\u8003\u65f6\uff0c\u4ed6\u4e0e\u5176\u4ed6\u540c\u4e8b\u4e0d\u4ea4\u6d41\u3002\u65f6\u800c\uff0c\u4ed6\u4f1a\u611f\u5230\u9965\u997f\uff0c\u5e76\u8bd5\u56fe\u62ff\u8d77\u4e0e\u4ed6\u76f8\u8fd1\u7684\u4e24\u6839\u7b77\u5b50\uff08\u7b77\u5b50\u5728\u4ed6\u548c\u4ed6\u7684\u5de6\u6216\u53f3\u90bb\u5c45\u4e4b\u95f4\uff09\u3002\u4e00\u4e2a\u54f2\u5b66\u5bb6\u4e00\u6b21\u53ea\u80fd\u62ff\u8d77\u4e00\u6839\u7b77\u5b50\u3002\u663e\u7136\uff0c\u4ed6\u4e0d\u80fd\u4ece\u5176\u4ed6\u54f2\u5b66\u5bb6\u624b\u91cc\u62ff\u8d70\u7b77\u5b50\u3002\u5f53\u4e00\u4e2a\u9965\u997f\u7684\u54f2\u5b66\u5bb6\u540c\u65f6\u62e5\u6709\u4e24\u6839\u7b77\u5b50\u65f6\uff0c\u4ed6\u5c31\u80fd\u5403\u3002\u5728\u5403\u5b8c\u540e\uff0c\u4ed6\u4f1a\u653e\u4e0b\u4e24\u6839\u7b77\u5b50\uff0c\u5e76\u5f00\u59cb\u601d\u8003\u3002 \u54f2\u5b66\u5bb6\u5c31\u9910\u95ee\u9898\u662f\u4e00\u4e2a\u7ecf\u5178\u7684\u540c\u6b65\u95ee\u9898\uff0c\u8fd9\u4e0d\u662f\u56e0\u4e3a\u5176\u672c\u8eab\u7684\u5b9e\u9645\u91cd\u8981\u6027\uff0c\u4e5f\u4e0d\u662f\u56e0\u4e3a\u8ba1\u7b97\u673a\u79d1\u5b66\u5bb6\u4e0d\u559c\u6b22\u54f2\u5b66\u5bb6\uff0c\u800c\u662f\u56e0\u4e3a\u5b83\u662f\u5927\u91cf\u5e76\u53d1\u63a7\u5236\u95ee\u9898\u7684\u4e00\u4e2a\u4f8b\u5b50\u3002\u8fd9\u4e2a\u4ee3\u8868\u578b\u7684\u4f8b\u5b50\u6ee1\u8db3\uff1a\u5728\u591a\u4e2a\u8fdb\u7a0b\u4e4b\u95f4\u5206\u914d\u591a\u4e2a\u8d44\u6e90\uff0c\u800c\u4e14\u4e0d\u4f1a\u51fa\u73b0\u6b7b\u9501\u548c\u9965\u997f\u3002 NOTE: 1\u3001\u9965\u997f\u6307\u7684\u662f \"resource starvation\" \u4fe1\u53f7\u91cf \u4e00\u79cd\u7b80\u5355\u7684\u89e3\u51b3\u65b9\u6cd5\u662f\u6bcf\u53ea\u7b77\u5b50\u90fd\u7528\u4e00\u4e2a\u4fe1\u53f7\u91cf\u6765\u8868\u793a\u3002\u4e00\u4e2a\u54f2\u5b66\u5bb6\u901a\u8fc7\u6267\u884c\u64cd\u4f5c wait() \u8bd5\u56fe\u83b7\u53d6\u76f8\u5e94\u7684\u7b77\u5b50\uff0c\u4ed6\u4f1a\u901a\u8fc7\u6267\u884c\u64cd\u4f5c signal() \u4ee5\u91ca\u653e\u76f8\u5e94\u7684\u7b77\u5b50\u3002 \u56e0\u6b64\uff0c\u5171\u4eab\u6570\u636e\u4e3a: semaphore chopstick [ 5 ]; \u5176\u4e2d\uff0c chopstick \u7684\u6240\u6709\u5143\u7d20\u90fd\u521d\u59cb\u5316\u4e3a 1\u3002\u54f2\u5b66\u5bb6 i \u7684\u7ed3\u6784\u5982\u4e0b\u6240\u793a\uff1a do { wait ( chopstick [ i ]); wait ( chopstick [( i + 1 ) % 5 ]); // \u53d6\u4e0b\u4e00\u4e2a\u7b77\u5b50 /* eat for awhile */ signal ( chopstick [ i ]); signal ( chopstick [( i + 1 ) % 5 ]); /* think for awhile */ } while ( true ); \u867d\u7136\u8fd9\u4e00\u89e3\u51b3\u65b9\u6848\u4fdd\u8bc1\u4e24\u4e2a\u90bb\u5c45\u4e0d\u80fd\u540c\u65f6\u8fdb\u98df\uff0c\u4f46\u662f\u5b83\u53ef\u80fd\u5bfc\u81f4\u6b7b\u9501\uff0c\u56e0\u6b64\u8fd8\u662f\u5e94\u88ab\u62d2\u7edd\u7684\u3002\u5047\u82e5\u6240\u6709 5 \u4e2a\u54f2\u5b66\u5bb6\u540c\u65f6\u9965\u997f\u5e76\u62ff\u8d77\u5de6\u8fb9\u7684\u7b77\u5b50\u3002\u6240\u6709\u7b77\u5b50\u7684\u4fe1\u53f7\u91cf\u73b0\u5728\u5747\u4e3a 0\u3002\u5f53\u6bcf\u4e2a\u54f2\u5b66\u5bb6\u8bd5\u56fe\u62ff\u53f3\u8fb9\u7684\u7b77\u5b50\u65f6\uff0c\u4ed6\u4f1a\u88ab\u6c38\u8fdc\u63a8\u8fdf\u3002 \u6b7b\u9501\u95ee\u9898\u6709\u591a\u79cd\u53ef\u80fd\u7684\u8865\u6551\u63aa\u65bd\uff1a 1\u3001\u5141\u8bb8\u6700\u591a 4 \u4e2a\u54f2\u5b66\u5bb6\u540c\u65f6\u5750\u5728\u684c\u5b50\u4e0a\u3002 2\u3001\u53ea\u6709\u4e00\u4e2a\u54f2\u5b66\u5bb6\u7684\u4e24\u6839\u7b77\u5b50\u90fd\u53ef\u7528\u65f6\uff0c\u4ed6\u624d\u80fd\u62ff\u8d77\u5b83\u4eec\uff08\u4ed6\u5fc5\u987b\u5728\u4e34\u754c\u533a\u5185\u62ff\u8d77\u4e24\u6839 \u8f95\u5b50)\u3002 3\u3001\u4f7f\u7528\u975e\u5bf9\u79f0\u89e3\u51b3\u65b9\u6848\u3002\u5373\u5355\u53f7\u7684\u54f2\u5b66\u5bb6\u5148\u62ff\u8d77\u5de6\u8fb9\u7684\u7b77\u5b50\uff0c\u63a5\u7740\u53f3\u8fb9\u7684\u7b77\u5b50\uff1b\u800c\u53cc \u53f7\u7684\u54f2\u5b66\u5bb6\u5148\u62ff\u8d77\u53f3\u8fb9\u7684\u7b77\u5b50\uff0c\u63a5\u7740\u5de6\u8fb9\u7684\u7b77\u5b50\u3002 baike \u54f2\u5b66\u5bb6\u5c31\u9910\u95ee\u9898 \u95ee\u9898\u63cf\u8ff0 \u6b7b\u9501 \u548c \u6d3b\u9501 \u54f2\u5b66\u5bb6\u4ece\u6765\u4e0d\u4ea4\u8c08\uff0c\u8fd9\u5c31\u5f88\u5371\u9669\uff0c\u53ef\u80fd\u4ea7\u751f\u6b7b\u9501\uff0c\u6bcf\u4e2a\u54f2\u5b66\u5bb6\u90fd\u62ff\u7740\u5de6\u624b\u7684\u9910\u53c9\uff0c\u6c38\u8fdc\u90fd\u5728\u7b49\u53f3\u8fb9\u7684\u9910\u53c9\uff08\u6216\u8005\u76f8\u53cd\uff09\u3002\u5373\u4f7f\u6ca1\u6709\u6b7b\u9501\uff0c\u4e5f\u6709\u53ef\u80fd\u53d1\u751f\u8d44\u6e90\u8017\u5c3d\u3002\u4f8b\u5982\uff0c\u5047\u8bbe\u89c4\u5b9a\u5f53\u54f2\u5b66\u5bb6\u7b49\u5f85\u53e6\u4e00\u53ea\u9910\u53c9\u8d85\u8fc7\u4e94\u5206\u949f\u540e\u5c31\u653e\u4e0b\u81ea\u5df1\u624b\u91cc\u7684\u90a3\u4e00\u53ea\u9910\u53c9\uff0c\u5e76\u4e14\u518d\u7b49\u4e94\u5206\u949f\u540e\u8fdb\u884c\u4e0b\u4e00\u6b21\u5c1d\u8bd5\u3002\u8fd9\u4e2a\u7b56\u7565\u6d88\u9664\u4e86\u6b7b\u9501\uff08\u7cfb\u7edf\u603b\u4f1a\u8fdb\u5165\u5230\u4e0b\u4e00\u4e2a\u72b6\u6001\uff09\uff0c\u4f46\u4ecd\u7136\u6709\u53ef\u80fd\u53d1\u751f\u201c \u6d3b\u9501 \u201d\u3002\u5982\u679c\u4e94\u4f4d\u54f2\u5b66\u5bb6\u5728\u5b8c\u5168\u76f8\u540c\u7684\u65f6\u523b\u8fdb\u5165\u9910\u5385\uff0c\u5e76\u540c\u65f6\u62ff\u8d77\u5de6\u8fb9\u7684\u9910\u53c9\uff0c\u90a3\u4e48\u8fd9\u4e9b\u54f2\u5b66\u5bb6\u5c31\u4f1a\u7b49\u5f85\u4e94\u5206\u949f\uff0c\u540c\u65f6\u653e\u4e0b\u624b\u4e2d\u7684\u9910\u53c9\uff0c\u518d\u7b49\u4e94\u5206\u949f\uff0c\u53c8\u540c\u65f6\u62ff\u8d77\u8fd9\u4e9b\u9910\u53c9\u3002 \u95ee\u9898\u89e3\u6cd5 \u670d\u52a1\u751f\u89e3\u6cd5/Arbitrator solution Chandy/Misra\u89e3\u6cd5 1984\u5e74\uff0cK. Mani Chandy\u548cJ. Misra\u63d0\u51fa\u4e86\u54f2\u5b66\u5bb6\u5c31\u9910\u95ee\u9898\u7684\u53e6\u4e00\u4e2a\u89e3\u6cd5\uff0c\u5141\u8bb8\u4efb\u610f\u7684\u7528\u6237\uff08\u7f16\u53f7P1, ..., Pn\uff09\u4e89\u7528\u4efb\u610f\u6570\u91cf\u7684\u8d44\u6e90\u3002\u4e0e\u8fea\u79d1\u65af\u5f7b\u7684\u89e3\u6cd5\u4e0d\u540c\u7684\u662f\uff0c\u8fd9\u91cc\u7f16\u53f7\u53ef\u4ee5\u662f\u4efb\u610f\u7684\u3002 1.\u5bf9\u6bcf\u4e00\u5bf9\u7ade\u4e89\u4e00\u4e2a\u8d44\u6e90\u7684\u54f2\u5b66\u5bb6\uff0c\u65b0\u62ff\u4e00\u4e2a\u9910\u53c9\uff0c\u7ed9\u7f16\u53f7\u8f83\u4f4e\u7684\u54f2\u5b66\u5bb6\u3002\u6bcf\u53ea\u9910\u53c9\u90fd\u662f\u201c\u5e72\u51c0\u7684\u201d\u6216\u8005\u201c\u810f\u7684\u201d\u3002\u6700\u521d\uff0c\u6240\u6709\u7684\u9910\u53c9\u90fd\u662f\u810f\u7684\u3002 2.\u5f53\u4e00\u4f4d\u54f2\u5b66\u5bb6\u8981\u4f7f\u7528\u8d44\u6e90\uff08\u4e5f\u5c31\u662f\u8981\u5403\u4e1c\u897f\uff09\u65f6\uff0c\u4ed6\u5fc5\u987b\u4ece\u4e0e\u4ed6\u7ade\u4e89\u7684\u90bb\u5c45\u90a3\u91cc\u5f97\u5230\u3002\u5bf9\u6bcf\u53ea\u4ed6\u5f53\u524d\u6ca1\u6709\u7684\u9910\u53c9\uff0c\u4ed6\u90fd\u53d1\u9001\u4e00\u4e2a\u8bf7\u6c42\u3002 3.\u5f53\u62e5\u6709\u9910\u53c9\u7684\u54f2\u5b66\u5bb6\u6536\u5230\u8bf7\u6c42\u65f6\uff0c\u5982\u679c\u9910\u53c9\u662f\u5e72\u51c0\u7684\uff0c\u90a3\u4e48\u4ed6\u7ee7\u7eed\u7559\u7740\uff0c\u5426\u5219\u5c31\u64e6\u5e72\u51c0\u5e76\u4ea4\u51fa\u9910\u53c9\u3002 4.\u5f53\u67d0\u4e2a\u54f2\u5b66\u5bb6\u5403\u4e1c\u897f\u540e\uff0c\u4ed6\u7684\u9910\u53c9\u5c31\u53d8\u810f\u4e86\u3002\u5982\u679c\u53e6\u4e00\u4e2a\u54f2\u5b66\u5bb6\u4e4b\u524d\u8bf7\u6c42\u8fc7\u5176\u4e2d\u7684\u9910\u53c9\uff0c\u90a3\u4ed6\u5c31\u64e6\u5e72\u51c0\u5e76\u4ea4\u51fa\u9910\u53c9\u3002 \u8fd9\u4e2a\u89e3\u6cd5\u5141\u8bb8\u5f88\u5927\u7684 \u5e76\u884c\u6027 \uff0c\u9002\u7528\u4e8e\u4efb\u610f\u5927\u591a\u95ee\u9898\u3002 wikipedia Dining philosophers problem Resource hierarchy solution NOTE: 1\u3001lock hierarchy \u5c31\u662f\u9075\u5faa\u8fd9\u79cd\u601d\u8def\uff0c\u53c2\u89c1 drdobbs Use Lock Hierarchies to Avoid Deadlock \uff0c\u5176\u4e2d\u6709\u7740\u975e\u5e38\u597d\u7684\u63cf\u8ff0\u3002 2\u3001\u8fd9\u79cd\u65b9\u6848\u662f\u4e00\u79cd \"Autonomy \u81ea\u6cbb\u7684-and-decentralization \u53bb\u4e2d\u5fc3\u5316\" This solution to the problem is the one originally proposed by Dijkstra . It assigns a partial order to the resources (the forks, in this case), and establishes the convention that all resources will be requested in order, and that no two resources unrelated by order will ever be used by a single unit of work at the same time. Here, the resources (forks) will be numbered 1 through 5 and each unit of work (philosopher) will always pick up the lower-numbered fork first, and then the higher-numbered fork, from among the two forks they plan to use. The order in which each philosopher puts down the forks does not matter. In this case, if four of the five philosophers simultaneously pick up their lower-numbered fork, only the highest-numbered fork will remain on the table, so the fifth philosopher will not be able to pick up any fork. Moreover, only one philosopher will have access to that highest-numbered fork, so he will be able to eat using two forks. NOTE: \u4e00\u3001\u6309\u7167\u4e0a\u8ff0\u65b9\u5f0f\u63a8\u6f14:\uff0c\u5b58\u5728\u4e0b\u9762\u7684\u4e00\u79cd\u60c5\u51b5(\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u7531\u4e8ethread\u7684\u6267\u884c\u662f\u7531OS\u6765\u8c03\u5ea6\u7684\uff0c\u56e0\u6b64\u5b9e\u9645\u7684\u6267\u884c\u60c5\u51b5\u662f\u975e\u5e38\u591a\u7684\uff0c\u4e0b\u9762\u662f\u5176\u4e2d\u7684\u4e00\u79cd\u60c5\u51b5) \u7b2c\u4e00\u4f4d\u54f2\u5b66\u5bb6\u5728\u7b2c\u4e00\u8f6e\u80fd\u591f\u62ff\u5230\u4e24\u4e2afork \u7b2c\u4e94\u4f4d\u54f2\u5b66\u5bb6\u5728\u7b2c\u4e00\u8f6e\u662f\u4e0d\u4f1a\u62ff\u8d77fork\u7684\uff0c\u56e0\u4e3a\u8f6e\u5230\u4ed6\u7684\u65f6\u5019\u53ea\u5269\u4e0b\u4e86\"highest-numbered fork\" While the resource hierarchy solution avoids deadlocks, it is not always practical, especially when the list of required resources is not completely known in advance. For example, if a unit of work holds resources 3 and 5 and then determines it needs resource 2, it must release 5, then 3 before acquiring 2, and then it must re-acquire 3 and 5 in that order. Computer programs that access large numbers of database records would not run efficiently if they were required to release all higher-numbered records before accessing a new record, making the method impractical for that purpose.[ 2] NOTE: 1\u3001\u4e0a\u8ff0\u662f\u975e\u5e38\u597d\u7684\u5206\u6790\uff0c\u4ece\u4e0a\u8ff0\u7684\u5206\u6790\u6765\u770b\uff0clock hierarchy\u4e0d\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u5f0f The resource hierarchy solution is not fair . If philosopher 1 is slow to take a fork, and if philosopher 2 is quick to think and pick its forks back up, then philosopher 1 will never get to pick up both forks. A fair solution must guarantee that each philosopher will eventually eat, no matter how slowly that philosopher moves relative to the others. NOTE: 1\u3001\u516c\u5e73\u6027 Arbitrator solution NOTE: \u8fd9\u79cd\u65b9\u6848\u662f\"Arbitrator\u4ef2\u88c1\u8005-\u4e2d\u5fc3\u5316\" Chandy/Misra solution NOTE: 1\u3001\u8fd9\u79cd\u65b9\u6848\u662f\u4e00\u79cd \"Autonomy \u81ea\u6cbb\u7684-and-decentralization \u53bb\u4e2d\u5fc3\u5316\" 2\u3001\u524d\u9762\u7684 baike \u54f2\u5b66\u5bb6\u5c31\u9910\u95ee\u9898 \u4e2d\u6709\u7ffb\u8bd1 In 1984, K. Mani Chandy and J. Misra [ 5] proposed a different solution to the dining philosophers problem to allow for arbitrary agents (numbered P*1, ..., *Pn ) to contend for an arbitrary number of resources, unlike Dijkstra's solution. It is also completely distributed and requires no central authority after initialization. However, it violates the requirement that \"the philosophers do not speak to each other\" (due to the request messages). 1\u3001For every pair of philosophers contending for a resource, create a fork and give it to the philosopher with the lower ID ( n for agent Pn ). Each fork can either be dirty or clean. Initially, all forks are dirty. NOTE: 1\u3001\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7684\"create a fork \"\u662f\u4ec0\u4e48\u542b\u4e49\uff1f\"\u65b0\u62ff\u4e00\u4e2a\u9910\u53c9\" 2\u3001When a philosopher wants to use a set of resources ( i.e. , eat), said philosopher must obtain the forks from their contending neighbors. For all such forks the philosopher does not have, they send a request message. 3\u3001When a philosopher with a fork receives a request message, they keep the fork if it is clean, but give it up when it is dirty. If the philosopher sends the fork over(\u9001\u51fa\u53bb), they clean the fork before doing so. 4\u3001After a philosopher is done eating, all their forks become dirty. If another philosopher had previously requested one of the forks, the philosopher that has just finished eating cleans the fork and sends it. NOTE: \u4e00\u3001\u540e\u9762\u4ecb\u7ecd\u4e86\u5b83\u7684\u4f18\u52bf: 1\u3001\u5141\u8bb8\u5f88\u5927\u7684 \u5e76\u884c\u6027 \uff0c\u9002\u7528\u4e8e\u4efb\u610f\u5927\u591a\u95ee\u9898 2\u3001solves the starvation problem \u540e\u9762\u8fdb\u884c\u4e86\u4e00\u4e9b\u5206\u6790\u6765\u8bba\u8ff0\u4e3a\u4ec0\u4e48\u8fd9\u79cd\u65b9\u6848\u662f\u53ef\u884c\u7684 \u63d0\u4f9b\u5e76\u53d1 This solution also allows for a large degree of concurrency, and will solve an arbitrarily large problem. solves the starvation problem NOTE: \u4e00\u3001\u4ece\u4e0b\u9762\u7684\u63cf\u8ff0\u6765\u770b\uff0c\u5b83\u4f7f\u7528\"clean/dirty labels\"\u5b9e\u73b0\u4e86\u4e00\u79cd\u5747\u8861\u7684\u7ef4\u6301\u516c\u5e73\u7684\u7b56\u7565\uff0c\u867d\u7136\u6ca1\u6709\u5bf9\u5b83\u8fdb\u884c\u5f62\u5f0f\u5316\u7684\u5206\u6790\uff0c\u4f46\u662f\u4ece\u4e0b\u9762\u7684\u63cf\u8ff0\u6765\u770b\uff0c\u5b83\u662f\u975e\u5e38\u7075\u6d3b\u3001\u5de7\u5999\u7684: 1\u3001\u5f53\u5403\u5b8c\u540e\uff0c\u5c31\u5c06\u81ea\u5df1\u7684fork\u7f6e\u4e3a**dirty**: giving a disadvantage to processes that have just \"eaten\" 2\u3001\u5f53\u6536\u5230\u8bf7\u6c42\u540e\uff0c\u5c06\u81ea\u5df1\u7684fork\u7f6e\u4f4d**clean**\uff0c\u7136\u540e\u4ea4\u7ed9\u8bf7\u6c42\u65b9: giving preference(\u504f\u5411\u4e8e) to the most \"starved\" processes It also solves the starvation problem . The clean/dirty labels act as a way of giving preference(\u504f\u5411\u4e8e\u3001\u63d0\u4f9b\u4f18\u5148\u7ea7) to the most \"starved\" processes, and a disadvantage(\u964d\u4f4e\u4f18\u5148\u7ea7) to processes that have just \"eaten\". One could compare their solution to one where philosophers are not allowed to eat twice in a row without letting others use the forks in between. Chandy and Misra's solution is more flexible than that, but has an element tending in that direction. Formal analysis NOTE: \u4e00\u3001\u601d\u8003: 1\u3001\u5b83\u662f\u5982\u4f55\u5f97\u51fa directed acyclic graph \u7684\uff1f 2\u3001\u4e3a\u4ec0\u4e48\"Initializing the system so that philosophers with lower IDs have dirty forks ensures the graph is initially acyclic\"\uff1f In their analysis, they derive a system of preference levels from the distribution of the forks and their clean/dirty states. They show that this system may describe a directed acyclic graph , and if so, the operations in their protocol cannot turn that graph into a cyclic(\u6709\u73af\u7684) one. This guarantees that deadlock cannot occur. However, if the system is initialized to a perfectly symmetric state, like all philosophers holding their left side forks, then the graph is cyclic at the outset(\u5f00\u59cb), and their solution cannot prevent a deadlock. Initializing the system so that philosophers with lower IDs have dirty forks ensures the graph is initially acyclic. TODO golangprograms Golang Concurrency # Illustration of the dining philosophers problem in Golang","title":"Introduction"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/#dining#philosophers#problem","text":"\u4e00\u3001 1\u3001\u8fd9\u4e2a\u95ee\u9898\u662f\u6700\u6700\u80fd\u591f\u4f53\u73b0deadlock\u3001livelock\u7684 2\u3001\u5b83\u6700\u6700\u80fd\u591f\u4f53\u73b0circular dependency \u4e8c\u3001\u8fd9\u4e2a\u95ee\u9898\u672c\u8eab\u5c31\u5305\u542b\u4e86circle: \"\u5706\u684c\" --\u300b circle\uff1b\u56e0\u6b64\u5b83\u5929\u751f\u5c31\u6709circular dependency\u95ee\u9898\uff1b \u56e0\u6b64\u89e3\u51b3\u65b9\u6848\uff0c\u5c31\u662f\u8981\"\u7834\u73af\"\uff0c\u53ef\u4ee5: 1\u3001\u5f15\u5165Arbitrator\uff0c\u5bf9\u5e94 \"\u670d\u52a1\u751f\u89e3\u6cd5/Arbitrator solution\" 2\u3001\u5f15\u5165hierarchy \u4e09\u3001\u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\u7ed9\u51fa\u4e86demo code mariusbancila Dining Philosophers in C++11 mariusbancila Dining philosophers in C++11: Chandy-Misra algorithm \u5728\u4e0b\u9762\u7684\u662f\u4e00\u4e9b\u5f00\u6e90\u5b9e\u73b0: 1\u3001 mtking2 / dining-philosophers 2\u3001 graninas / cpp_philosophers_stm \u56db\u3001\u8fd9\u4e2a\u95ee\u9898\u53ef\u4ee5\u8fdb\u4e00\u6b65\u5730\u62bd\u8c61\u4e3a\u4e00\u4e2a: \u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u5b83\u7684\u4e00\u4e2a\u8981\u6c42: 1\u3001\u8981\u4fdd\u8bc1\u516c\u5e73\u6027\uff0c\u5176\u5b9estarvation\u5c31\u662f\u4e00\u79cd\u6781\u7aef\u7684\u4e0d\u516c\u5e73 2\u3001\u4e0d\u4f1a\u6b7b\u9501","title":"Dining philosophers problem"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/#biancheng","text":"NOTE: 1\u3001\u95ee\u9898\u63cf\u8ff0\u975e\u5e38\u597d \u5047\u8bbe\u6709 5 \u4e2a\u54f2\u5b66\u5bb6\uff0c\u4ed6\u4eec\u7684\u751f\u6d3b\u53ea\u662f\u601d\u8003\u548c\u5403\u996d\u3002\u8fd9\u4e9b\u54f2\u5b66\u5bb6\u5171\u7528\u4e00\u4e2a**\u5706\u684c**\uff0c\u6bcf\u4f4d\u90fd\u6709\u4e00\u628a\u6905\u5b50\u3002\u5728\u684c\u5b50\u4e2d\u592e\u6709\u4e00\u7897\u7c73\u996d\uff0c\u5728\u684c\u5b50\u4e0a\u653e\u7740 5 \u6839\u7b77\u5b50\uff08\u56fe 1 )\u3002 \u5f53\u4e00\u4f4d\u54f2\u5b66\u5bb6\u601d\u8003\u65f6\uff0c\u4ed6\u4e0e\u5176\u4ed6\u540c\u4e8b\u4e0d\u4ea4\u6d41\u3002\u65f6\u800c\uff0c\u4ed6\u4f1a\u611f\u5230\u9965\u997f\uff0c\u5e76\u8bd5\u56fe\u62ff\u8d77\u4e0e\u4ed6\u76f8\u8fd1\u7684\u4e24\u6839\u7b77\u5b50\uff08\u7b77\u5b50\u5728\u4ed6\u548c\u4ed6\u7684\u5de6\u6216\u53f3\u90bb\u5c45\u4e4b\u95f4\uff09\u3002\u4e00\u4e2a\u54f2\u5b66\u5bb6\u4e00\u6b21\u53ea\u80fd\u62ff\u8d77\u4e00\u6839\u7b77\u5b50\u3002\u663e\u7136\uff0c\u4ed6\u4e0d\u80fd\u4ece\u5176\u4ed6\u54f2\u5b66\u5bb6\u624b\u91cc\u62ff\u8d70\u7b77\u5b50\u3002\u5f53\u4e00\u4e2a\u9965\u997f\u7684\u54f2\u5b66\u5bb6\u540c\u65f6\u62e5\u6709\u4e24\u6839\u7b77\u5b50\u65f6\uff0c\u4ed6\u5c31\u80fd\u5403\u3002\u5728\u5403\u5b8c\u540e\uff0c\u4ed6\u4f1a\u653e\u4e0b\u4e24\u6839\u7b77\u5b50\uff0c\u5e76\u5f00\u59cb\u601d\u8003\u3002 \u54f2\u5b66\u5bb6\u5c31\u9910\u95ee\u9898\u662f\u4e00\u4e2a\u7ecf\u5178\u7684\u540c\u6b65\u95ee\u9898\uff0c\u8fd9\u4e0d\u662f\u56e0\u4e3a\u5176\u672c\u8eab\u7684\u5b9e\u9645\u91cd\u8981\u6027\uff0c\u4e5f\u4e0d\u662f\u56e0\u4e3a\u8ba1\u7b97\u673a\u79d1\u5b66\u5bb6\u4e0d\u559c\u6b22\u54f2\u5b66\u5bb6\uff0c\u800c\u662f\u56e0\u4e3a\u5b83\u662f\u5927\u91cf\u5e76\u53d1\u63a7\u5236\u95ee\u9898\u7684\u4e00\u4e2a\u4f8b\u5b50\u3002\u8fd9\u4e2a\u4ee3\u8868\u578b\u7684\u4f8b\u5b50\u6ee1\u8db3\uff1a\u5728\u591a\u4e2a\u8fdb\u7a0b\u4e4b\u95f4\u5206\u914d\u591a\u4e2a\u8d44\u6e90\uff0c\u800c\u4e14\u4e0d\u4f1a\u51fa\u73b0\u6b7b\u9501\u548c\u9965\u997f\u3002 NOTE: 1\u3001\u9965\u997f\u6307\u7684\u662f \"resource starvation\"","title":"biancheng \u54f2\u5b66\u5bb6\u5c31\u9910\u95ee\u9898\u5206\u6790\uff08\u542b\u89e3\u51b3\u65b9\u6848\uff09"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/#_1","text":"\u4e00\u79cd\u7b80\u5355\u7684\u89e3\u51b3\u65b9\u6cd5\u662f\u6bcf\u53ea\u7b77\u5b50\u90fd\u7528\u4e00\u4e2a\u4fe1\u53f7\u91cf\u6765\u8868\u793a\u3002\u4e00\u4e2a\u54f2\u5b66\u5bb6\u901a\u8fc7\u6267\u884c\u64cd\u4f5c wait() \u8bd5\u56fe\u83b7\u53d6\u76f8\u5e94\u7684\u7b77\u5b50\uff0c\u4ed6\u4f1a\u901a\u8fc7\u6267\u884c\u64cd\u4f5c signal() \u4ee5\u91ca\u653e\u76f8\u5e94\u7684\u7b77\u5b50\u3002 \u56e0\u6b64\uff0c\u5171\u4eab\u6570\u636e\u4e3a: semaphore chopstick [ 5 ]; \u5176\u4e2d\uff0c chopstick \u7684\u6240\u6709\u5143\u7d20\u90fd\u521d\u59cb\u5316\u4e3a 1\u3002\u54f2\u5b66\u5bb6 i \u7684\u7ed3\u6784\u5982\u4e0b\u6240\u793a\uff1a do { wait ( chopstick [ i ]); wait ( chopstick [( i + 1 ) % 5 ]); // \u53d6\u4e0b\u4e00\u4e2a\u7b77\u5b50 /* eat for awhile */ signal ( chopstick [ i ]); signal ( chopstick [( i + 1 ) % 5 ]); /* think for awhile */ } while ( true ); \u867d\u7136\u8fd9\u4e00\u89e3\u51b3\u65b9\u6848\u4fdd\u8bc1\u4e24\u4e2a\u90bb\u5c45\u4e0d\u80fd\u540c\u65f6\u8fdb\u98df\uff0c\u4f46\u662f\u5b83\u53ef\u80fd\u5bfc\u81f4\u6b7b\u9501\uff0c\u56e0\u6b64\u8fd8\u662f\u5e94\u88ab\u62d2\u7edd\u7684\u3002\u5047\u82e5\u6240\u6709 5 \u4e2a\u54f2\u5b66\u5bb6\u540c\u65f6\u9965\u997f\u5e76\u62ff\u8d77\u5de6\u8fb9\u7684\u7b77\u5b50\u3002\u6240\u6709\u7b77\u5b50\u7684\u4fe1\u53f7\u91cf\u73b0\u5728\u5747\u4e3a 0\u3002\u5f53\u6bcf\u4e2a\u54f2\u5b66\u5bb6\u8bd5\u56fe\u62ff\u53f3\u8fb9\u7684\u7b77\u5b50\u65f6\uff0c\u4ed6\u4f1a\u88ab\u6c38\u8fdc\u63a8\u8fdf\u3002 \u6b7b\u9501\u95ee\u9898\u6709\u591a\u79cd\u53ef\u80fd\u7684\u8865\u6551\u63aa\u65bd\uff1a 1\u3001\u5141\u8bb8\u6700\u591a 4 \u4e2a\u54f2\u5b66\u5bb6\u540c\u65f6\u5750\u5728\u684c\u5b50\u4e0a\u3002 2\u3001\u53ea\u6709\u4e00\u4e2a\u54f2\u5b66\u5bb6\u7684\u4e24\u6839\u7b77\u5b50\u90fd\u53ef\u7528\u65f6\uff0c\u4ed6\u624d\u80fd\u62ff\u8d77\u5b83\u4eec\uff08\u4ed6\u5fc5\u987b\u5728\u4e34\u754c\u533a\u5185\u62ff\u8d77\u4e24\u6839 \u8f95\u5b50)\u3002 3\u3001\u4f7f\u7528\u975e\u5bf9\u79f0\u89e3\u51b3\u65b9\u6848\u3002\u5373\u5355\u53f7\u7684\u54f2\u5b66\u5bb6\u5148\u62ff\u8d77\u5de6\u8fb9\u7684\u7b77\u5b50\uff0c\u63a5\u7740\u53f3\u8fb9\u7684\u7b77\u5b50\uff1b\u800c\u53cc \u53f7\u7684\u54f2\u5b66\u5bb6\u5148\u62ff\u8d77\u53f3\u8fb9\u7684\u7b77\u5b50\uff0c\u63a5\u7740\u5de6\u8fb9\u7684\u7b77\u5b50\u3002","title":"\u4fe1\u53f7\u91cf"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/#baike","text":"","title":"baike \u54f2\u5b66\u5bb6\u5c31\u9910\u95ee\u9898"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/#_2","text":"","title":"\u95ee\u9898\u63cf\u8ff0"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/#_3","text":"\u54f2\u5b66\u5bb6\u4ece\u6765\u4e0d\u4ea4\u8c08\uff0c\u8fd9\u5c31\u5f88\u5371\u9669\uff0c\u53ef\u80fd\u4ea7\u751f\u6b7b\u9501\uff0c\u6bcf\u4e2a\u54f2\u5b66\u5bb6\u90fd\u62ff\u7740\u5de6\u624b\u7684\u9910\u53c9\uff0c\u6c38\u8fdc\u90fd\u5728\u7b49\u53f3\u8fb9\u7684\u9910\u53c9\uff08\u6216\u8005\u76f8\u53cd\uff09\u3002\u5373\u4f7f\u6ca1\u6709\u6b7b\u9501\uff0c\u4e5f\u6709\u53ef\u80fd\u53d1\u751f\u8d44\u6e90\u8017\u5c3d\u3002\u4f8b\u5982\uff0c\u5047\u8bbe\u89c4\u5b9a\u5f53\u54f2\u5b66\u5bb6\u7b49\u5f85\u53e6\u4e00\u53ea\u9910\u53c9\u8d85\u8fc7\u4e94\u5206\u949f\u540e\u5c31\u653e\u4e0b\u81ea\u5df1\u624b\u91cc\u7684\u90a3\u4e00\u53ea\u9910\u53c9\uff0c\u5e76\u4e14\u518d\u7b49\u4e94\u5206\u949f\u540e\u8fdb\u884c\u4e0b\u4e00\u6b21\u5c1d\u8bd5\u3002\u8fd9\u4e2a\u7b56\u7565\u6d88\u9664\u4e86\u6b7b\u9501\uff08\u7cfb\u7edf\u603b\u4f1a\u8fdb\u5165\u5230\u4e0b\u4e00\u4e2a\u72b6\u6001\uff09\uff0c\u4f46\u4ecd\u7136\u6709\u53ef\u80fd\u53d1\u751f\u201c \u6d3b\u9501 \u201d\u3002\u5982\u679c\u4e94\u4f4d\u54f2\u5b66\u5bb6\u5728\u5b8c\u5168\u76f8\u540c\u7684\u65f6\u523b\u8fdb\u5165\u9910\u5385\uff0c\u5e76\u540c\u65f6\u62ff\u8d77\u5de6\u8fb9\u7684\u9910\u53c9\uff0c\u90a3\u4e48\u8fd9\u4e9b\u54f2\u5b66\u5bb6\u5c31\u4f1a\u7b49\u5f85\u4e94\u5206\u949f\uff0c\u540c\u65f6\u653e\u4e0b\u624b\u4e2d\u7684\u9910\u53c9\uff0c\u518d\u7b49\u4e94\u5206\u949f\uff0c\u53c8\u540c\u65f6\u62ff\u8d77\u8fd9\u4e9b\u9910\u53c9\u3002","title":"\u6b7b\u9501 \u548c \u6d3b\u9501"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/#_4","text":"","title":"\u95ee\u9898\u89e3\u6cd5"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/#arbitrator#solution","text":"","title":"\u670d\u52a1\u751f\u89e3\u6cd5/Arbitrator solution"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/#chandymisra","text":"1984\u5e74\uff0cK. Mani Chandy\u548cJ. Misra\u63d0\u51fa\u4e86\u54f2\u5b66\u5bb6\u5c31\u9910\u95ee\u9898\u7684\u53e6\u4e00\u4e2a\u89e3\u6cd5\uff0c\u5141\u8bb8\u4efb\u610f\u7684\u7528\u6237\uff08\u7f16\u53f7P1, ..., Pn\uff09\u4e89\u7528\u4efb\u610f\u6570\u91cf\u7684\u8d44\u6e90\u3002\u4e0e\u8fea\u79d1\u65af\u5f7b\u7684\u89e3\u6cd5\u4e0d\u540c\u7684\u662f\uff0c\u8fd9\u91cc\u7f16\u53f7\u53ef\u4ee5\u662f\u4efb\u610f\u7684\u3002 1.\u5bf9\u6bcf\u4e00\u5bf9\u7ade\u4e89\u4e00\u4e2a\u8d44\u6e90\u7684\u54f2\u5b66\u5bb6\uff0c\u65b0\u62ff\u4e00\u4e2a\u9910\u53c9\uff0c\u7ed9\u7f16\u53f7\u8f83\u4f4e\u7684\u54f2\u5b66\u5bb6\u3002\u6bcf\u53ea\u9910\u53c9\u90fd\u662f\u201c\u5e72\u51c0\u7684\u201d\u6216\u8005\u201c\u810f\u7684\u201d\u3002\u6700\u521d\uff0c\u6240\u6709\u7684\u9910\u53c9\u90fd\u662f\u810f\u7684\u3002 2.\u5f53\u4e00\u4f4d\u54f2\u5b66\u5bb6\u8981\u4f7f\u7528\u8d44\u6e90\uff08\u4e5f\u5c31\u662f\u8981\u5403\u4e1c\u897f\uff09\u65f6\uff0c\u4ed6\u5fc5\u987b\u4ece\u4e0e\u4ed6\u7ade\u4e89\u7684\u90bb\u5c45\u90a3\u91cc\u5f97\u5230\u3002\u5bf9\u6bcf\u53ea\u4ed6\u5f53\u524d\u6ca1\u6709\u7684\u9910\u53c9\uff0c\u4ed6\u90fd\u53d1\u9001\u4e00\u4e2a\u8bf7\u6c42\u3002 3.\u5f53\u62e5\u6709\u9910\u53c9\u7684\u54f2\u5b66\u5bb6\u6536\u5230\u8bf7\u6c42\u65f6\uff0c\u5982\u679c\u9910\u53c9\u662f\u5e72\u51c0\u7684\uff0c\u90a3\u4e48\u4ed6\u7ee7\u7eed\u7559\u7740\uff0c\u5426\u5219\u5c31\u64e6\u5e72\u51c0\u5e76\u4ea4\u51fa\u9910\u53c9\u3002 4.\u5f53\u67d0\u4e2a\u54f2\u5b66\u5bb6\u5403\u4e1c\u897f\u540e\uff0c\u4ed6\u7684\u9910\u53c9\u5c31\u53d8\u810f\u4e86\u3002\u5982\u679c\u53e6\u4e00\u4e2a\u54f2\u5b66\u5bb6\u4e4b\u524d\u8bf7\u6c42\u8fc7\u5176\u4e2d\u7684\u9910\u53c9\uff0c\u90a3\u4ed6\u5c31\u64e6\u5e72\u51c0\u5e76\u4ea4\u51fa\u9910\u53c9\u3002 \u8fd9\u4e2a\u89e3\u6cd5\u5141\u8bb8\u5f88\u5927\u7684 \u5e76\u884c\u6027 \uff0c\u9002\u7528\u4e8e\u4efb\u610f\u5927\u591a\u95ee\u9898\u3002","title":"Chandy/Misra\u89e3\u6cd5"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/#wikipedia#dining#philosophers#problem","text":"","title":"wikipedia Dining philosophers problem"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/#resource#hierarchy#solution","text":"NOTE: 1\u3001lock hierarchy \u5c31\u662f\u9075\u5faa\u8fd9\u79cd\u601d\u8def\uff0c\u53c2\u89c1 drdobbs Use Lock Hierarchies to Avoid Deadlock \uff0c\u5176\u4e2d\u6709\u7740\u975e\u5e38\u597d\u7684\u63cf\u8ff0\u3002 2\u3001\u8fd9\u79cd\u65b9\u6848\u662f\u4e00\u79cd \"Autonomy \u81ea\u6cbb\u7684-and-decentralization \u53bb\u4e2d\u5fc3\u5316\" This solution to the problem is the one originally proposed by Dijkstra . It assigns a partial order to the resources (the forks, in this case), and establishes the convention that all resources will be requested in order, and that no two resources unrelated by order will ever be used by a single unit of work at the same time. Here, the resources (forks) will be numbered 1 through 5 and each unit of work (philosopher) will always pick up the lower-numbered fork first, and then the higher-numbered fork, from among the two forks they plan to use. The order in which each philosopher puts down the forks does not matter. In this case, if four of the five philosophers simultaneously pick up their lower-numbered fork, only the highest-numbered fork will remain on the table, so the fifth philosopher will not be able to pick up any fork. Moreover, only one philosopher will have access to that highest-numbered fork, so he will be able to eat using two forks. NOTE: \u4e00\u3001\u6309\u7167\u4e0a\u8ff0\u65b9\u5f0f\u63a8\u6f14:\uff0c\u5b58\u5728\u4e0b\u9762\u7684\u4e00\u79cd\u60c5\u51b5(\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u7531\u4e8ethread\u7684\u6267\u884c\u662f\u7531OS\u6765\u8c03\u5ea6\u7684\uff0c\u56e0\u6b64\u5b9e\u9645\u7684\u6267\u884c\u60c5\u51b5\u662f\u975e\u5e38\u591a\u7684\uff0c\u4e0b\u9762\u662f\u5176\u4e2d\u7684\u4e00\u79cd\u60c5\u51b5) \u7b2c\u4e00\u4f4d\u54f2\u5b66\u5bb6\u5728\u7b2c\u4e00\u8f6e\u80fd\u591f\u62ff\u5230\u4e24\u4e2afork \u7b2c\u4e94\u4f4d\u54f2\u5b66\u5bb6\u5728\u7b2c\u4e00\u8f6e\u662f\u4e0d\u4f1a\u62ff\u8d77fork\u7684\uff0c\u56e0\u4e3a\u8f6e\u5230\u4ed6\u7684\u65f6\u5019\u53ea\u5269\u4e0b\u4e86\"highest-numbered fork\" While the resource hierarchy solution avoids deadlocks, it is not always practical, especially when the list of required resources is not completely known in advance. For example, if a unit of work holds resources 3 and 5 and then determines it needs resource 2, it must release 5, then 3 before acquiring 2, and then it must re-acquire 3 and 5 in that order. Computer programs that access large numbers of database records would not run efficiently if they were required to release all higher-numbered records before accessing a new record, making the method impractical for that purpose.[ 2] NOTE: 1\u3001\u4e0a\u8ff0\u662f\u975e\u5e38\u597d\u7684\u5206\u6790\uff0c\u4ece\u4e0a\u8ff0\u7684\u5206\u6790\u6765\u770b\uff0clock hierarchy\u4e0d\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u5f0f The resource hierarchy solution is not fair . If philosopher 1 is slow to take a fork, and if philosopher 2 is quick to think and pick its forks back up, then philosopher 1 will never get to pick up both forks. A fair solution must guarantee that each philosopher will eventually eat, no matter how slowly that philosopher moves relative to the others. NOTE: 1\u3001\u516c\u5e73\u6027","title":"Resource hierarchy solution"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/#arbitrator#solution_1","text":"NOTE: \u8fd9\u79cd\u65b9\u6848\u662f\"Arbitrator\u4ef2\u88c1\u8005-\u4e2d\u5fc3\u5316\"","title":"Arbitrator solution"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/#chandymisra#solution","text":"NOTE: 1\u3001\u8fd9\u79cd\u65b9\u6848\u662f\u4e00\u79cd \"Autonomy \u81ea\u6cbb\u7684-and-decentralization \u53bb\u4e2d\u5fc3\u5316\" 2\u3001\u524d\u9762\u7684 baike \u54f2\u5b66\u5bb6\u5c31\u9910\u95ee\u9898 \u4e2d\u6709\u7ffb\u8bd1 In 1984, K. Mani Chandy and J. Misra [ 5] proposed a different solution to the dining philosophers problem to allow for arbitrary agents (numbered P*1, ..., *Pn ) to contend for an arbitrary number of resources, unlike Dijkstra's solution. It is also completely distributed and requires no central authority after initialization. However, it violates the requirement that \"the philosophers do not speak to each other\" (due to the request messages). 1\u3001For every pair of philosophers contending for a resource, create a fork and give it to the philosopher with the lower ID ( n for agent Pn ). Each fork can either be dirty or clean. Initially, all forks are dirty. NOTE: 1\u3001\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7684\"create a fork \"\u662f\u4ec0\u4e48\u542b\u4e49\uff1f\"\u65b0\u62ff\u4e00\u4e2a\u9910\u53c9\" 2\u3001When a philosopher wants to use a set of resources ( i.e. , eat), said philosopher must obtain the forks from their contending neighbors. For all such forks the philosopher does not have, they send a request message. 3\u3001When a philosopher with a fork receives a request message, they keep the fork if it is clean, but give it up when it is dirty. If the philosopher sends the fork over(\u9001\u51fa\u53bb), they clean the fork before doing so. 4\u3001After a philosopher is done eating, all their forks become dirty. If another philosopher had previously requested one of the forks, the philosopher that has just finished eating cleans the fork and sends it. NOTE: \u4e00\u3001\u540e\u9762\u4ecb\u7ecd\u4e86\u5b83\u7684\u4f18\u52bf: 1\u3001\u5141\u8bb8\u5f88\u5927\u7684 \u5e76\u884c\u6027 \uff0c\u9002\u7528\u4e8e\u4efb\u610f\u5927\u591a\u95ee\u9898 2\u3001solves the starvation problem \u540e\u9762\u8fdb\u884c\u4e86\u4e00\u4e9b\u5206\u6790\u6765\u8bba\u8ff0\u4e3a\u4ec0\u4e48\u8fd9\u79cd\u65b9\u6848\u662f\u53ef\u884c\u7684 \u63d0\u4f9b\u5e76\u53d1 This solution also allows for a large degree of concurrency, and will solve an arbitrarily large problem. solves the starvation problem NOTE: \u4e00\u3001\u4ece\u4e0b\u9762\u7684\u63cf\u8ff0\u6765\u770b\uff0c\u5b83\u4f7f\u7528\"clean/dirty labels\"\u5b9e\u73b0\u4e86\u4e00\u79cd\u5747\u8861\u7684\u7ef4\u6301\u516c\u5e73\u7684\u7b56\u7565\uff0c\u867d\u7136\u6ca1\u6709\u5bf9\u5b83\u8fdb\u884c\u5f62\u5f0f\u5316\u7684\u5206\u6790\uff0c\u4f46\u662f\u4ece\u4e0b\u9762\u7684\u63cf\u8ff0\u6765\u770b\uff0c\u5b83\u662f\u975e\u5e38\u7075\u6d3b\u3001\u5de7\u5999\u7684: 1\u3001\u5f53\u5403\u5b8c\u540e\uff0c\u5c31\u5c06\u81ea\u5df1\u7684fork\u7f6e\u4e3a**dirty**: giving a disadvantage to processes that have just \"eaten\" 2\u3001\u5f53\u6536\u5230\u8bf7\u6c42\u540e\uff0c\u5c06\u81ea\u5df1\u7684fork\u7f6e\u4f4d**clean**\uff0c\u7136\u540e\u4ea4\u7ed9\u8bf7\u6c42\u65b9: giving preference(\u504f\u5411\u4e8e) to the most \"starved\" processes It also solves the starvation problem . The clean/dirty labels act as a way of giving preference(\u504f\u5411\u4e8e\u3001\u63d0\u4f9b\u4f18\u5148\u7ea7) to the most \"starved\" processes, and a disadvantage(\u964d\u4f4e\u4f18\u5148\u7ea7) to processes that have just \"eaten\". One could compare their solution to one where philosophers are not allowed to eat twice in a row without letting others use the forks in between. Chandy and Misra's solution is more flexible than that, but has an element tending in that direction. Formal analysis NOTE: \u4e00\u3001\u601d\u8003: 1\u3001\u5b83\u662f\u5982\u4f55\u5f97\u51fa directed acyclic graph \u7684\uff1f 2\u3001\u4e3a\u4ec0\u4e48\"Initializing the system so that philosophers with lower IDs have dirty forks ensures the graph is initially acyclic\"\uff1f In their analysis, they derive a system of preference levels from the distribution of the forks and their clean/dirty states. They show that this system may describe a directed acyclic graph , and if so, the operations in their protocol cannot turn that graph into a cyclic(\u6709\u73af\u7684) one. This guarantees that deadlock cannot occur. However, if the system is initialized to a perfectly symmetric state, like all philosophers holding their left side forks, then the graph is cyclic at the outset(\u5f00\u59cb), and their solution cannot prevent a deadlock. Initializing the system so that philosophers with lower IDs have dirty forks ensures the graph is initially acyclic.","title":"Chandy/Misra solution"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/#todo","text":"golangprograms Golang Concurrency # Illustration of the dining philosophers problem in Golang","title":"TODO"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/TODO-howardhinnant-Dining-Philosophers-Rebooted/","text":"howardhinnant Dining Philosophers Rebooted The Solutions Ordered NOTE: 1\u3001\u5176\u5b9e\u5c31\u662f\"tag-Dijkstra-Resource-lock hierarchy-partial order-avoid deadlock-address order\"","title":"Introduction"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/TODO-howardhinnant-Dining-Philosophers-Rebooted/#howardhinnant#dining#philosophers#rebooted","text":"","title":"howardhinnant Dining Philosophers Rebooted"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/TODO-howardhinnant-Dining-Philosophers-Rebooted/#the#solutions","text":"","title":"The Solutions"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/TODO-howardhinnant-Dining-Philosophers-Rebooted/#ordered","text":"NOTE: 1\u3001\u5176\u5b9e\u5c31\u662f\"tag-Dijkstra-Resource-lock hierarchy-partial order-avoid deadlock-address order\"","title":"Ordered"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/mariusbancila-Dining-philosopher/","text":"mariusbancila Dining Philosophers in C++11 NOTE: 1\u3001\u5b9e\u9645\u4e0a\u662f\u9700\u8981C++17\u652f\u6301\u7684 UPDATE : for an implementation of the Chandy/Misra solution see Dining philosophers in C++11: Chandy-Misra algorithm The idea is to find a solution so that none of the philosophers would starve, i.e. never have the chance to acquire the forks necessary for him to eat. Below I propose a simple implementation to this problem using C++11 language and library features. The following classes are defined: fork fork represents a fork at the table; the only member of this structure is a std::mutex that will be locked when the philosopher picks up the fork and unlocked when he puts it down. struct fork { std :: mutex mutex ; }; table table represents the round table where the philosophers are dining. It has an array of forks, but also an atomic boolean that indicates that the table is ready for the philosophers to start thinking and eating. struct table { std :: atomic < bool > ready { false }; std :: array < fork , no_of_philosophers > forks ; }; philosopher philosopher represents a philosopher dining at the table. It has a name and a reference to the forks on his left and right. struct philosopher { private : std :: string const name ; table const & dinnertable ; fork & left_fork ; // \u5de6\u8fb9\u7684\u53c9\u5b50 fork & right_fork ; // \u53f3\u8fb9\u7684\u53c9\u5b50 std :: thread lifethread ; std :: mt19937 rng { std :: random_device { }() }; }; Most of the implementation of the solution is part of the philosopher class. When an object of this class is instantiated, a thread is started. This thread is joined when the object is destroyed. The thread runs a loop of thinking and eating until the dinner is signaled to end by setting the ready member of the table to false. There are three main methods in the philosopher class: dine() NOTE: 1\u3001\u7ebf\u7a0b\u6267\u884c\u51fd\u6570 dine() is the thread function; this is implemented as a simple loop of thinking and eating. void dine () { while ( ! dinnertable . ready ); do { think (); eat (); } while ( dinnertable . ready ); } think() think() is the method that represents the thinking period. To model this the thread sleeps for a random period of time. void think () { static thread_local std :: uniform_int_distribution <> wait ( 1 , 6 ); std :: this_thread :: sleep_for ( std :: chrono :: milliseconds ( wait ( rng ) * 150 )); print ( \" is thinking \" ); } eat() NOTE: \u4e00\u3001\u901a\u8fc7\u67e5\u9605\u6587\u6863\u53ef\u77e5\uff0c std::lock \u662f\u5df2\u7ecf\u5b9e\u73b0\u4e86\"deadlock avoidance algorithm\"\uff0c\u56e0\u6b64\u76f4\u63a5\u4f7f\u7528\u5b83\u5c31\u53ef\u4ee5\u89e3\u51b3starvation\u7684\u95ee\u9898\u4e86\u3002 eat() is the method that models the eating. The left and right forks are acquired in a deadlock free manner using std::lock . After the forks, i.e. mutexes, are acquired, their ownership is transfered to a std::lock_guard object, so that the mutexes are correctly released when the function returns. Eating is simulated with a sleep. void eat () { std :: lock ( left_fork . mutex , right_fork . mutex ); std :: lock_guard < std :: mutex > left_lock ( left_fork . mutex , std :: adopt_lock ); std :: lock_guard < std :: mutex > right_lock ( right_fork . mutex , std :: adopt_lock ); print ( \" started eating.\" ); static thread_local std :: uniform_int_distribution <> dist ( 1 , 6 ); std :: this_thread :: sleep_for ( std :: chrono :: milliseconds ( dist ( rng ) * 50 )); print ( \" finished eating.\" ); } \u5b8c\u6574\u7684\u7a0b\u5e8f The whole implementation is show below: #include <array> #include <mutex> #include <thread> #include <atomic> #include <chrono> #include <iostream> #include <string> #include <random> #include <iomanip> #include <string_view> std :: mutex g_lockprint ; constexpr int no_of_philosophers = 5 ; struct fork { std :: mutex mutex ; }; struct table { std :: atomic < bool > ready { false }; std :: array < fork , no_of_philosophers > forks ; }; struct philosopher { private : std :: string const name ; table const & dinnertable ; fork & left_fork ; fork & right_fork ; std :: thread lifethread ; std :: mt19937 rng { std :: random_device { }() }; public : philosopher ( std :: string_view n , table const & t , fork & l , fork & r ) : name ( n ), dinnertable ( t ), left_fork ( l ), right_fork ( r ), lifethread ( & philosopher :: dine , this ) { } ~ philosopher () { lifethread . join (); } void dine () { while ( ! dinnertable . ready ) ; do { think (); eat (); } while ( dinnertable . ready ); } void print ( std :: string_view text ) { std :: lock_guard < std :: mutex > cout_lock ( g_lockprint ); std :: cout << std :: left << std :: setw ( 10 ) << std :: setfill ( ' ' ) << name << text << std :: endl ; } void eat () { std :: lock ( left_fork . mutex , right_fork . mutex ); std :: lock_guard < std :: mutex > left_lock ( left_fork . mutex , std :: adopt_lock ); std :: lock_guard < std :: mutex > right_lock ( right_fork . mutex , std :: adopt_lock ); print ( \" started eating.\" ); static thread_local std :: uniform_int_distribution <> dist ( 1 , 6 ); std :: this_thread :: sleep_for ( std :: chrono :: milliseconds ( dist ( rng ) * 50 )); print ( \" finished eating.\" ); } void think () { static thread_local std :: uniform_int_distribution <> wait ( 1 , 6 ); std :: this_thread :: sleep_for ( std :: chrono :: milliseconds ( wait ( rng ) * 150 )); print ( \" is thinking \" ); } }; void dine () { std :: this_thread :: sleep_for ( std :: chrono :: seconds ( 1 )); std :: cout << \"Dinner started!\" << std :: endl ; { table table ; std :: array < philosopher , no_of_philosophers > philosophers { { { \"Aristotle\" , table , table . forks [ 0 ], table . forks [ 1 ] }, { \"Platon\" , table , table . forks [ 1 ], table . forks [ 2 ] }, { \"Descartes\" , table , table . forks [ 2 ], table . forks [ 3 ] }, { \"Kant\" , table , table . forks [ 3 ], table . forks [ 4 ] }, { \"Nietzsche\" , table , table . forks [ 4 ], table . forks [ 0 ] }, } }; table . ready = true ; std :: this_thread :: sleep_for ( std :: chrono :: seconds ( 5 )); table . ready = false ; } std :: cout << \"Dinner done!\" << std :: endl ; } int main () { dine (); return 0 ; } // g++ test.cpp -pedantic -Wall -Wextra --std=c++17 -lpthread \u4ece\u5b9e\u9645\u7684\u8fd0\u884c\u7ed3\u6784\u6765\u770b\uff0c\u5e76\u6ca1\u6709\u53d1\u751fdeadlock\u800c\u5bfc\u81f4starve\u3002","title":"Introduction"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/mariusbancila-Dining-philosopher/#mariusbancila#dining#philosophers#in#c11","text":"NOTE: 1\u3001\u5b9e\u9645\u4e0a\u662f\u9700\u8981C++17\u652f\u6301\u7684 UPDATE : for an implementation of the Chandy/Misra solution see Dining philosophers in C++11: Chandy-Misra algorithm The idea is to find a solution so that none of the philosophers would starve, i.e. never have the chance to acquire the forks necessary for him to eat. Below I propose a simple implementation to this problem using C++11 language and library features. The following classes are defined:","title":"mariusbancila Dining Philosophers in C++11"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/mariusbancila-Dining-philosopher/#fork","text":"fork represents a fork at the table; the only member of this structure is a std::mutex that will be locked when the philosopher picks up the fork and unlocked when he puts it down. struct fork { std :: mutex mutex ; };","title":"fork"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/mariusbancila-Dining-philosopher/#table","text":"table represents the round table where the philosophers are dining. It has an array of forks, but also an atomic boolean that indicates that the table is ready for the philosophers to start thinking and eating. struct table { std :: atomic < bool > ready { false }; std :: array < fork , no_of_philosophers > forks ; };","title":"table"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/mariusbancila-Dining-philosopher/#philosopher","text":"philosopher represents a philosopher dining at the table. It has a name and a reference to the forks on his left and right. struct philosopher { private : std :: string const name ; table const & dinnertable ; fork & left_fork ; // \u5de6\u8fb9\u7684\u53c9\u5b50 fork & right_fork ; // \u53f3\u8fb9\u7684\u53c9\u5b50 std :: thread lifethread ; std :: mt19937 rng { std :: random_device { }() }; }; Most of the implementation of the solution is part of the philosopher class. When an object of this class is instantiated, a thread is started. This thread is joined when the object is destroyed. The thread runs a loop of thinking and eating until the dinner is signaled to end by setting the ready member of the table to false. There are three main methods in the philosopher class:","title":"philosopher"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/mariusbancila-Dining-philosopher/#dine","text":"NOTE: 1\u3001\u7ebf\u7a0b\u6267\u884c\u51fd\u6570 dine() is the thread function; this is implemented as a simple loop of thinking and eating. void dine () { while ( ! dinnertable . ready ); do { think (); eat (); } while ( dinnertable . ready ); }","title":"dine()"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/mariusbancila-Dining-philosopher/#think","text":"think() is the method that represents the thinking period. To model this the thread sleeps for a random period of time. void think () { static thread_local std :: uniform_int_distribution <> wait ( 1 , 6 ); std :: this_thread :: sleep_for ( std :: chrono :: milliseconds ( wait ( rng ) * 150 )); print ( \" is thinking \" ); }","title":"think()"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/mariusbancila-Dining-philosopher/#eat","text":"NOTE: \u4e00\u3001\u901a\u8fc7\u67e5\u9605\u6587\u6863\u53ef\u77e5\uff0c std::lock \u662f\u5df2\u7ecf\u5b9e\u73b0\u4e86\"deadlock avoidance algorithm\"\uff0c\u56e0\u6b64\u76f4\u63a5\u4f7f\u7528\u5b83\u5c31\u53ef\u4ee5\u89e3\u51b3starvation\u7684\u95ee\u9898\u4e86\u3002 eat() is the method that models the eating. The left and right forks are acquired in a deadlock free manner using std::lock . After the forks, i.e. mutexes, are acquired, their ownership is transfered to a std::lock_guard object, so that the mutexes are correctly released when the function returns. Eating is simulated with a sleep. void eat () { std :: lock ( left_fork . mutex , right_fork . mutex ); std :: lock_guard < std :: mutex > left_lock ( left_fork . mutex , std :: adopt_lock ); std :: lock_guard < std :: mutex > right_lock ( right_fork . mutex , std :: adopt_lock ); print ( \" started eating.\" ); static thread_local std :: uniform_int_distribution <> dist ( 1 , 6 ); std :: this_thread :: sleep_for ( std :: chrono :: milliseconds ( dist ( rng ) * 50 )); print ( \" finished eating.\" ); }","title":"eat()"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/mariusbancila-Dining-philosopher/#_1","text":"The whole implementation is show below: #include <array> #include <mutex> #include <thread> #include <atomic> #include <chrono> #include <iostream> #include <string> #include <random> #include <iomanip> #include <string_view> std :: mutex g_lockprint ; constexpr int no_of_philosophers = 5 ; struct fork { std :: mutex mutex ; }; struct table { std :: atomic < bool > ready { false }; std :: array < fork , no_of_philosophers > forks ; }; struct philosopher { private : std :: string const name ; table const & dinnertable ; fork & left_fork ; fork & right_fork ; std :: thread lifethread ; std :: mt19937 rng { std :: random_device { }() }; public : philosopher ( std :: string_view n , table const & t , fork & l , fork & r ) : name ( n ), dinnertable ( t ), left_fork ( l ), right_fork ( r ), lifethread ( & philosopher :: dine , this ) { } ~ philosopher () { lifethread . join (); } void dine () { while ( ! dinnertable . ready ) ; do { think (); eat (); } while ( dinnertable . ready ); } void print ( std :: string_view text ) { std :: lock_guard < std :: mutex > cout_lock ( g_lockprint ); std :: cout << std :: left << std :: setw ( 10 ) << std :: setfill ( ' ' ) << name << text << std :: endl ; } void eat () { std :: lock ( left_fork . mutex , right_fork . mutex ); std :: lock_guard < std :: mutex > left_lock ( left_fork . mutex , std :: adopt_lock ); std :: lock_guard < std :: mutex > right_lock ( right_fork . mutex , std :: adopt_lock ); print ( \" started eating.\" ); static thread_local std :: uniform_int_distribution <> dist ( 1 , 6 ); std :: this_thread :: sleep_for ( std :: chrono :: milliseconds ( dist ( rng ) * 50 )); print ( \" finished eating.\" ); } void think () { static thread_local std :: uniform_int_distribution <> wait ( 1 , 6 ); std :: this_thread :: sleep_for ( std :: chrono :: milliseconds ( wait ( rng ) * 150 )); print ( \" is thinking \" ); } }; void dine () { std :: this_thread :: sleep_for ( std :: chrono :: seconds ( 1 )); std :: cout << \"Dinner started!\" << std :: endl ; { table table ; std :: array < philosopher , no_of_philosophers > philosophers { { { \"Aristotle\" , table , table . forks [ 0 ], table . forks [ 1 ] }, { \"Platon\" , table , table . forks [ 1 ], table . forks [ 2 ] }, { \"Descartes\" , table , table . forks [ 2 ], table . forks [ 3 ] }, { \"Kant\" , table , table . forks [ 3 ], table . forks [ 4 ] }, { \"Nietzsche\" , table , table . forks [ 4 ], table . forks [ 0 ] }, } }; table . ready = true ; std :: this_thread :: sleep_for ( std :: chrono :: seconds ( 5 )); table . ready = false ; } std :: cout << \"Dinner done!\" << std :: endl ; } int main () { dine (); return 0 ; } // g++ test.cpp -pedantic -Wall -Wextra --std=c++17 -lpthread \u4ece\u5b9e\u9645\u7684\u8fd0\u884c\u7ed3\u6784\u6765\u770b\uff0c\u5e76\u6ca1\u6709\u53d1\u751fdeadlock\u800c\u5bfc\u81f4starve\u3002","title":"\u5b8c\u6574\u7684\u7a0b\u5e8f"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/mariusbancila-Dining-philosopher/Chandy-Misra-algorithm/","text":"mariusbancila Dining philosophers in C++11: Chandy-Misra algorithm In my previous post, Dining Philosophers in C++11 , I have provided an implementation for the dining philosophers problem using modern C++ features, such as threads and mutexes. However, it was noted in the comments that the implementation did not prevent the philosophers starving to death when you remove the waiting times. An algorithm that prevents the philosophers from starving was proposed by Mani Chandy and J. Misra and is known as the Chandy/Misra solution . This is a bit different than the original problem because it requires the philosophers to communicate with each other. The algorithm, as described on Wikipedia, is the following: For every pair of philosophers contending for a resource, create a fork and give it to the philosopher with the lower ID (n for agent Pn). Each fork can either be dirty or clean. Initially, all forks are dirty. When a philosopher wants to use a set of resources (i.e. eat), said philosopher must obtain the forks from their contending neighbors. For all such forks the philosopher does not have, they send a request message. When a philosopher with a fork receives a request message, they keep the fork if it is clean, but give it up when it is dirty. If the philosopher sends the fork over, they clean the fork before doing so. After a philosopher is done eating, all their forks become dirty. If another philosopher had previously requested one of the forks, the philosopher that has just finished eating cleans the fork and sends it. In order to implement this, we must make several changes to the solution proposed in the previous post: 1\u3001forks and philosophers must have identifiers NOTE: 1\u3001\u521d\u59cb\u5316\u7684\u65f6\u5019\uff0cID\u5c0f\u7684philosophers \u5f97\u5230fork 2\u3001there is an initial setup of both forks and philosophers 3\u3001use std::condition_variable to communicate between threads NOTE: 1\u3001channel 4\u3001increase the number of philosophers Because it has been also argued that string_view is only available in C++17 and this implementation is supposed to work in C++11, I have replaced that with std::string const& . sync_channel In this implementation, philosophers, i.e. threads, need to communicate with each other to request the forks, i.e. resources. For this, we will use a std::condition_variable , which is a synchronization primitive that enables the blocking of one or more threads until another thread notifies it. A std::condition_variable requires a std::mutex to protect access to a shared variable. The following class, sync_channel , contains both a condition variable and a mutex and provides two methods: one that waits on the condition variable, blocking the calling thread(s), and one that notifies the condition variable, unblocking all the threads that are waiting for a signal. NOTE: 1\u3001tag-std condition variable channel block notify_all\u5524\u9192\u901a\u77e5multiple waiting thread \u8fd9\u79cd\u505a\u6cd5\u662f\u503c\u5f97\u501f\u9274\u7684 class sync_channel { std :: mutex mutex ; std :: condition_variable cv ; public : void wait () { std :: unique_lock < std :: mutex > lock ( mutex ); cv . wait ( lock ); } void notifyall () { std :: unique_lock < std :: mutex > lock ( mutex ); cv . notify_all (); } }; table_setup The table class from the previous implementation is modified: the forks are no longer defined here, but a sync_channel is used to prevent philosophers start dining until the table setup is completed. Its name has been changed to table_setup . struct table_setup { std::atomic<bool> done{ false }; sync_channel channel; }; fork The fork class is no longer a wrapper for a mutex. It has an identifier, an owner, a flag to indicate whether it is dirty or clean, a mutex , and a sync_channel that enables owners to request used forks. It has two methods: request() method request() that enables a philosopher to request the fork. If the fork is dirty, it is set to clean, and the ownership is given to the philosopher that asked for it. If the fork is clean (i.e. the current owner is eating), than the philosopher that asked for it will block, waiting for it to become dirty (i.e. the current owner has finished eating). void request ( int const ownerId ) { while ( owner != ownerId ) { if ( dirty ) { std :: lock_guard < std :: mutex > lock ( mutex ); dirty = false ; owner = ownerId ; } else { channel . wait (); } } } done_using() method done_using() a philosopher indicates that has finished eating and notifies other philosopher that is waiting for the fork that it can have it. void done_using () { dirty = true ; channel . notifyall (); } philosopher There are less changes to the philosopher class: it has an identifier, and there are no more waiting times to simulate eating and thinking. There are some small changes to the following methods: dine() method each philosopher only starts eating after the entire table has been setup. A condition variable, from the table_setup object is used for this. void dine () { setup . channel . wait (); do { think (); eat (); } while ( ! setup . done ); } eat() method each philosopher first requests the left and right fork. When they are available, they are locked using std::lock() to avoid possible deadlocks, and then their ownership is transfered to a std::lock_guard object, so they are properly released when done. After eating, the fork is set as dirty and other philosophers waiting for it are notified of this. void eat () { left_fork . request ( id ); right_fork . request ( id ); std :: lock ( left_fork . getmutex (), right_fork . getmutex ()); std :: lock_guard < std :: mutex > left_lock ( left_fork . getmutex (), std :: adopt_lock ); std :: lock_guard < std :: mutex > right_lock ( right_fork . getmutex (), std :: adopt_lock ); print ( \" started eating.\" ); print ( \" finished eating.\" ); left_fork . done_using (); right_fork . done_using (); } According to the initial setup, each fork is given to the philosopher with the lower ID. That means fork 1, placed between philosopher 1 and N, goes to philosopher 1. Fork 2, placed between philosophers 2 and 3 is given to philosopher 2. Eventually, fork N, placed between philosophers N and 1, is given to philosopher 1. Overall, this means all philosophers have initially 1 fork, except for the first one that has two, and the last philosopher, that has none. \u5b8c\u6574\u7a0b\u5e8f Put all together, the code looks like this: #include <array> #include <mutex> #include <thread> #include <atomic> #include <chrono> #include <iostream> #include <string> #include <iomanip> #include <condition_variable> std :: mutex g_lockprint ; constexpr int no_of_philosophers = 7 ; class sync_channel { std :: mutex mutex ; std :: condition_variable cv ; public : void wait () { std :: unique_lock < std :: mutex > lock ( mutex ); cv . wait ( lock ); } void notifyall () { std :: unique_lock < std :: mutex > lock ( mutex ); cv . notify_all (); } }; struct table_setup { std :: atomic < bool > done { false }; sync_channel channel ; }; class fork { int id ; int owner ; bool dirty ; std :: mutex mutex ; sync_channel channel ; public : fork ( int const forkId , int const ownerId ) : id ( forkId ), owner ( ownerId ), dirty ( true ) { } void request ( int const ownerId ) { while ( owner != ownerId ) { if ( dirty ) { std :: lock_guard < std :: mutex > lock ( mutex ); dirty = false ; owner = ownerId ; } else { channel . wait (); } } } void done_using () { dirty = true ; channel . notifyall (); } std :: mutex & getmutex () { return mutex ; } }; struct philosopher { private : int id ; std :: string const name ; table_setup & setup ; fork & left_fork ; fork & right_fork ; std :: thread lifethread ; public : philosopher ( int const id , std :: string const & n , table_setup & s , fork & l , fork & r ) : id ( id ), name ( n ), setup ( s ), left_fork ( l ), right_fork ( r ), lifethread ( & philosopher :: dine , this ) { } ~ philosopher () { lifethread . join (); } void dine () { setup . channel . wait (); do { think (); eat (); } while ( ! setup . done ); } void print ( std :: string const & text ) { std :: lock_guard < std :: mutex > cout_lock ( g_lockprint ); std :: cout << std :: left << std :: setw ( 10 ) << std :: setfill ( ' ' ) << name << text << std :: endl ; } void eat () { print ( \" get left fork \" ); left_fork . request ( id ); print ( \" get right fork \" ); right_fork . request ( id ); std :: lock ( left_fork . getmutex (), right_fork . getmutex ()); std :: lock_guard < std :: mutex > left_lock ( left_fork . getmutex (), std :: adopt_lock ); std :: lock_guard < std :: mutex > right_lock ( right_fork . getmutex (), std :: adopt_lock ); print ( \" started eating.\" ); print ( \" finished eating.\" ); left_fork . done_using (); right_fork . done_using (); } void think () { print ( \" is thinking \" ); } }; class table { table_setup setup ; /** * \u6700\u540e\u4e00\u4e2a\u53c9\u5b50\u662f\u8981\u7ed9\u7b2c\u4e00\u4e2aphilosopher\u7684 */ std :: array < fork , no_of_philosophers > forks { { { 1 , 1 }, { 2 , 2 }, { 3 , 3 }, { 4 , 4 }, { 5 , 5 }, { 6 , 6 }, { 7 , 1 }, } }; std :: array < philosopher , no_of_philosophers > philosophers { { { 1 , \"Aristotle\" , setup , forks [ 0 ], forks [ 1 ] }, { 2 , \"Platon\" , setup , forks [ 1 ], forks [ 2 ] }, { 3 , \"Descartes\" , setup , forks [ 2 ], forks [ 3 ] }, { 4 , \"Kant\" , setup , forks [ 3 ], forks [ 4 ] }, { 5 , \"Nietzsche\" , setup , forks [ 4 ], forks [ 5 ] }, { 6 , \"Hume\" , setup , forks [ 5 ], forks [ 6 ] }, { 7 , \"Russell\" , setup , forks [ 6 ], forks [ 0 ] }, } }; public : void start () { setup . channel . notifyall (); } void stop () { setup . done = true ; } }; void dine () { std :: cout << \"Dinner started!\" << std :: endl ; { table table ; table . start (); std :: this_thread :: sleep_for ( std :: chrono :: seconds ( 60 )); table . stop (); } std :: cout << \"Dinner done!\" << std :: endl ; } int main () { dine (); return 0 ; } // g++ test.cpp -pedantic -Wall -Wextra --std=c++17 -lpthread \u7f3a\u9677\u5206\u6790 \u4f5c\u8005\u7ed9\u51fa\u7684\u539f\u7248\u7a0b\u5e8f\u662f\u5b58\u5728\u95ee\u9898\u7684: \u5b83\u5c1d\u8bd5\u53bbstop a blocked thread\uff0c\u663e\u7136\u5b83\u662f\u65e0\u6cd5\u88abjoin\u7684\uff0c\u56e0\u6b64\u88ab\u963b\u585e\u5728\u4e0b\u9762\u7684\u51fd\u6570\u4e2d: ~ philosopher () { lifethread . join (); } \u663e\u7136\uff0c\u5728\u505c\u6b62\u7684\u65f6\u5019\uff0c\u9700\u8981\u5c06\u963b\u585e\u7684thread\u5524\u9192\uff0c\u4e0b\u9762\u662f\u6211\u4fee\u6539\u7684\u4e00\u4e2a\u7248\u672c\uff0c\u5b83\u6ca1\u6709\u6539\u5f7b\u5e95\uff0c\u8fd8\u662f\u5b58\u5728\u65e0\u6cd5\u505c\u6b62\u7684\u95ee\u9898\uff0c\u5173\u4e8e\u5b8c\u6574\u7684\u4fee\u6539\u65b9\u6cd5\uff0c\u53ef\u4ee5\u53c2\u8003: 1\u3001 Stop-a-blocked-thread 2\u3001 Stop-a-sleeping-thread \u4e00\u4e2a\u4e0d\u5f7b\u5e95\u7684\u4fee\u6539 #include <array> #include <mutex> #include <thread> #include <atomic> #include <chrono> #include <iostream> #include <string> #include <iomanip> #include <condition_variable> std :: mutex g_lockprint ; constexpr int no_of_philosophers = 7 ; class sync_channel { std :: mutex mutex ; std :: condition_variable cv ; public : void wait () { std :: unique_lock < std :: mutex > lock ( mutex ); cv . wait ( lock ); } void notifyall () { std :: unique_lock < std :: mutex > lock ( mutex ); cv . notify_all (); } }; struct table_setup { std :: atomic < bool > done { false }; sync_channel channel ; }; class fork { int id ; int owner ; bool dirty ; std :: mutex mutex ; public : sync_channel channel ; fork ( int const forkId , int const ownerId ) : id ( forkId ), owner ( ownerId ), dirty ( true ) { } void request ( int const ownerId ) { while ( owner != ownerId ) { if ( dirty ) { std :: lock_guard < std :: mutex > lock ( mutex ); dirty = false ; owner = ownerId ; } else { channel . wait (); } } } void done_using () { dirty = true ; channel . notifyall (); } std :: mutex & getmutex () { return mutex ; } }; struct philosopher { private : int id ; std :: string const name ; table_setup & setup ; fork & left_fork ; fork & right_fork ; std :: thread lifethread ; public : philosopher ( int const id , std :: string const & n , table_setup & s , fork & l , fork & r ) : id ( id ), name ( n ), setup ( s ), left_fork ( l ), right_fork ( r ), lifethread ( & philosopher :: dine , this ) { } ~ philosopher () { lifethread . join (); } void dine () { setup . channel . wait (); do { think (); eat (); } while ( ! setup . done ); } void print ( std :: string const & text ) { std :: lock_guard < std :: mutex > cout_lock ( g_lockprint ); std :: cout << std :: left << std :: setw ( 10 ) << std :: setfill ( ' ' ) << name << text << std :: endl ; } void eat () { left_fork . request ( id ); right_fork . request ( id ); std :: lock ( left_fork . getmutex (), right_fork . getmutex ()); std :: lock_guard < std :: mutex > left_lock ( left_fork . getmutex (), std :: adopt_lock ); std :: lock_guard < std :: mutex > right_lock ( right_fork . getmutex (), std :: adopt_lock ); print ( \" started eating.\" ); print ( \" finished eating.\" ); left_fork . done_using (); right_fork . done_using (); } void think () { print ( \" is thinking \" ); } }; class table { table_setup setup ; /** * \u6700\u540e\u4e00\u4e2a\u53c9\u5b50\u662f\u8981\u7ed9\u7b2c\u4e00\u4e2aphilosopher\u7684 */ std :: array < fork , no_of_philosophers > forks { { { 1 , 1 }, { 2 , 2 }, { 3 , 3 }, { 4 , 4 }, { 5 , 5 }, { 6 , 6 }, { 7 , 1 }, } }; std :: array < philosopher , no_of_philosophers > philosophers { { { 1 , \"Aristotle\" , setup , forks [ 0 ], forks [ 1 ] }, { 2 , \"Platon\" , setup , forks [ 1 ], forks [ 2 ] }, { 3 , \"Descartes\" , setup , forks [ 2 ], forks [ 3 ] }, { 4 , \"Kant\" , setup , forks [ 3 ], forks [ 4 ] }, { 5 , \"Nietzsche\" , setup , forks [ 4 ], forks [ 5 ] }, { 6 , \"Hume\" , setup , forks [ 5 ], forks [ 6 ] }, { 7 , \"Russell\" , setup , forks [ 6 ], forks [ 0 ] }, } }; public : void start () { setup . channel . notifyall (); } void stop () { setup . done = true ; for ( auto && fork : forks ) { fork . channel . notifyall (); } } }; void dine () { std :: cout << \"Dinner started!\" << std :: endl ; { table table ; table . start (); std :: this_thread :: sleep_for ( std :: chrono :: seconds ( 30 )); std :: cout << \"\u505c\u6b62\" << std :: endl ; table . stop (); } std :: cout << \"Dinner done!\" << std :: endl ; } int main () { dine (); return 0 ; } // g++ test.cpp -pedantic -Wall -Wextra --std=c++11 -lpthread \u603b\u7ed3\u5206\u6790 \u7531\u4e8e\u4e00\u4e2afork\u53ea\u6709\u4e24\u4e2aphilosopher\u4f7f\u7528\uff0c\u56e0\u6b64\u4e0a\u8ff0implementation\u4e2d\uff0c\u6bcf\u4e2afork\u4f7f\u7528\u4e00\u4e2acondition variable\u5373\u53ef\uff0c\u5982\u679c\u8bf4\uff0c\u4e09\u4e2a\u751a\u81f3\u66f4\u591a\u7684philosopher\u4f7f\u7528\u540c\u4e00\u4e2afork\uff0c\u90a3\u4e48\u5728\u8fd9\u4e9bphilosopher\u4e4b\u95f4\u5982\u4f55\u6765\u8fdb\u884c\u534f\u8c03\u5206\u914d\u5462\uff1f\u663e\u7136\uff0c\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528\u4e00\u4e2acondition variable\u662f\u4e0d\u884c\u7684\u3002","title":"Introduction"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/mariusbancila-Dining-philosopher/Chandy-Misra-algorithm/#mariusbancila#dining#philosophers#in#c11#chandy-misra#algorithm","text":"In my previous post, Dining Philosophers in C++11 , I have provided an implementation for the dining philosophers problem using modern C++ features, such as threads and mutexes. However, it was noted in the comments that the implementation did not prevent the philosophers starving to death when you remove the waiting times. An algorithm that prevents the philosophers from starving was proposed by Mani Chandy and J. Misra and is known as the Chandy/Misra solution . This is a bit different than the original problem because it requires the philosophers to communicate with each other. The algorithm, as described on Wikipedia, is the following: For every pair of philosophers contending for a resource, create a fork and give it to the philosopher with the lower ID (n for agent Pn). Each fork can either be dirty or clean. Initially, all forks are dirty. When a philosopher wants to use a set of resources (i.e. eat), said philosopher must obtain the forks from their contending neighbors. For all such forks the philosopher does not have, they send a request message. When a philosopher with a fork receives a request message, they keep the fork if it is clean, but give it up when it is dirty. If the philosopher sends the fork over, they clean the fork before doing so. After a philosopher is done eating, all their forks become dirty. If another philosopher had previously requested one of the forks, the philosopher that has just finished eating cleans the fork and sends it. In order to implement this, we must make several changes to the solution proposed in the previous post: 1\u3001forks and philosophers must have identifiers NOTE: 1\u3001\u521d\u59cb\u5316\u7684\u65f6\u5019\uff0cID\u5c0f\u7684philosophers \u5f97\u5230fork 2\u3001there is an initial setup of both forks and philosophers 3\u3001use std::condition_variable to communicate between threads NOTE: 1\u3001channel 4\u3001increase the number of philosophers Because it has been also argued that string_view is only available in C++17 and this implementation is supposed to work in C++11, I have replaced that with std::string const& .","title":"mariusbancila Dining philosophers in C++11: Chandy-Misra algorithm"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/mariusbancila-Dining-philosopher/Chandy-Misra-algorithm/#sync_channel","text":"In this implementation, philosophers, i.e. threads, need to communicate with each other to request the forks, i.e. resources. For this, we will use a std::condition_variable , which is a synchronization primitive that enables the blocking of one or more threads until another thread notifies it. A std::condition_variable requires a std::mutex to protect access to a shared variable. The following class, sync_channel , contains both a condition variable and a mutex and provides two methods: one that waits on the condition variable, blocking the calling thread(s), and one that notifies the condition variable, unblocking all the threads that are waiting for a signal. NOTE: 1\u3001tag-std condition variable channel block notify_all\u5524\u9192\u901a\u77e5multiple waiting thread \u8fd9\u79cd\u505a\u6cd5\u662f\u503c\u5f97\u501f\u9274\u7684 class sync_channel { std :: mutex mutex ; std :: condition_variable cv ; public : void wait () { std :: unique_lock < std :: mutex > lock ( mutex ); cv . wait ( lock ); } void notifyall () { std :: unique_lock < std :: mutex > lock ( mutex ); cv . notify_all (); } };","title":"sync_channel"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/mariusbancila-Dining-philosopher/Chandy-Misra-algorithm/#table_setup","text":"The table class from the previous implementation is modified: the forks are no longer defined here, but a sync_channel is used to prevent philosophers start dining until the table setup is completed. Its name has been changed to table_setup . struct table_setup { std::atomic<bool> done{ false }; sync_channel channel; };","title":"table_setup"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/mariusbancila-Dining-philosopher/Chandy-Misra-algorithm/#fork","text":"The fork class is no longer a wrapper for a mutex. It has an identifier, an owner, a flag to indicate whether it is dirty or clean, a mutex , and a sync_channel that enables owners to request used forks. It has two methods:","title":"fork"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/mariusbancila-Dining-philosopher/Chandy-Misra-algorithm/#request#method","text":"request() that enables a philosopher to request the fork. If the fork is dirty, it is set to clean, and the ownership is given to the philosopher that asked for it. If the fork is clean (i.e. the current owner is eating), than the philosopher that asked for it will block, waiting for it to become dirty (i.e. the current owner has finished eating). void request ( int const ownerId ) { while ( owner != ownerId ) { if ( dirty ) { std :: lock_guard < std :: mutex > lock ( mutex ); dirty = false ; owner = ownerId ; } else { channel . wait (); } } }","title":"request() method"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/mariusbancila-Dining-philosopher/Chandy-Misra-algorithm/#done_using#method","text":"done_using() a philosopher indicates that has finished eating and notifies other philosopher that is waiting for the fork that it can have it. void done_using () { dirty = true ; channel . notifyall (); }","title":"done_using() method"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/mariusbancila-Dining-philosopher/Chandy-Misra-algorithm/#philosopher","text":"There are less changes to the philosopher class: it has an identifier, and there are no more waiting times to simulate eating and thinking. There are some small changes to the following methods:","title":"philosopher"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/mariusbancila-Dining-philosopher/Chandy-Misra-algorithm/#dine#method","text":"each philosopher only starts eating after the entire table has been setup. A condition variable, from the table_setup object is used for this. void dine () { setup . channel . wait (); do { think (); eat (); } while ( ! setup . done ); }","title":"dine() method"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/mariusbancila-Dining-philosopher/Chandy-Misra-algorithm/#eat#method","text":"each philosopher first requests the left and right fork. When they are available, they are locked using std::lock() to avoid possible deadlocks, and then their ownership is transfered to a std::lock_guard object, so they are properly released when done. After eating, the fork is set as dirty and other philosophers waiting for it are notified of this. void eat () { left_fork . request ( id ); right_fork . request ( id ); std :: lock ( left_fork . getmutex (), right_fork . getmutex ()); std :: lock_guard < std :: mutex > left_lock ( left_fork . getmutex (), std :: adopt_lock ); std :: lock_guard < std :: mutex > right_lock ( right_fork . getmutex (), std :: adopt_lock ); print ( \" started eating.\" ); print ( \" finished eating.\" ); left_fork . done_using (); right_fork . done_using (); } According to the initial setup, each fork is given to the philosopher with the lower ID. That means fork 1, placed between philosopher 1 and N, goes to philosopher 1. Fork 2, placed between philosophers 2 and 3 is given to philosopher 2. Eventually, fork N, placed between philosophers N and 1, is given to philosopher 1. Overall, this means all philosophers have initially 1 fork, except for the first one that has two, and the last philosopher, that has none.","title":"eat() method"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/mariusbancila-Dining-philosopher/Chandy-Misra-algorithm/#_1","text":"Put all together, the code looks like this: #include <array> #include <mutex> #include <thread> #include <atomic> #include <chrono> #include <iostream> #include <string> #include <iomanip> #include <condition_variable> std :: mutex g_lockprint ; constexpr int no_of_philosophers = 7 ; class sync_channel { std :: mutex mutex ; std :: condition_variable cv ; public : void wait () { std :: unique_lock < std :: mutex > lock ( mutex ); cv . wait ( lock ); } void notifyall () { std :: unique_lock < std :: mutex > lock ( mutex ); cv . notify_all (); } }; struct table_setup { std :: atomic < bool > done { false }; sync_channel channel ; }; class fork { int id ; int owner ; bool dirty ; std :: mutex mutex ; sync_channel channel ; public : fork ( int const forkId , int const ownerId ) : id ( forkId ), owner ( ownerId ), dirty ( true ) { } void request ( int const ownerId ) { while ( owner != ownerId ) { if ( dirty ) { std :: lock_guard < std :: mutex > lock ( mutex ); dirty = false ; owner = ownerId ; } else { channel . wait (); } } } void done_using () { dirty = true ; channel . notifyall (); } std :: mutex & getmutex () { return mutex ; } }; struct philosopher { private : int id ; std :: string const name ; table_setup & setup ; fork & left_fork ; fork & right_fork ; std :: thread lifethread ; public : philosopher ( int const id , std :: string const & n , table_setup & s , fork & l , fork & r ) : id ( id ), name ( n ), setup ( s ), left_fork ( l ), right_fork ( r ), lifethread ( & philosopher :: dine , this ) { } ~ philosopher () { lifethread . join (); } void dine () { setup . channel . wait (); do { think (); eat (); } while ( ! setup . done ); } void print ( std :: string const & text ) { std :: lock_guard < std :: mutex > cout_lock ( g_lockprint ); std :: cout << std :: left << std :: setw ( 10 ) << std :: setfill ( ' ' ) << name << text << std :: endl ; } void eat () { print ( \" get left fork \" ); left_fork . request ( id ); print ( \" get right fork \" ); right_fork . request ( id ); std :: lock ( left_fork . getmutex (), right_fork . getmutex ()); std :: lock_guard < std :: mutex > left_lock ( left_fork . getmutex (), std :: adopt_lock ); std :: lock_guard < std :: mutex > right_lock ( right_fork . getmutex (), std :: adopt_lock ); print ( \" started eating.\" ); print ( \" finished eating.\" ); left_fork . done_using (); right_fork . done_using (); } void think () { print ( \" is thinking \" ); } }; class table { table_setup setup ; /** * \u6700\u540e\u4e00\u4e2a\u53c9\u5b50\u662f\u8981\u7ed9\u7b2c\u4e00\u4e2aphilosopher\u7684 */ std :: array < fork , no_of_philosophers > forks { { { 1 , 1 }, { 2 , 2 }, { 3 , 3 }, { 4 , 4 }, { 5 , 5 }, { 6 , 6 }, { 7 , 1 }, } }; std :: array < philosopher , no_of_philosophers > philosophers { { { 1 , \"Aristotle\" , setup , forks [ 0 ], forks [ 1 ] }, { 2 , \"Platon\" , setup , forks [ 1 ], forks [ 2 ] }, { 3 , \"Descartes\" , setup , forks [ 2 ], forks [ 3 ] }, { 4 , \"Kant\" , setup , forks [ 3 ], forks [ 4 ] }, { 5 , \"Nietzsche\" , setup , forks [ 4 ], forks [ 5 ] }, { 6 , \"Hume\" , setup , forks [ 5 ], forks [ 6 ] }, { 7 , \"Russell\" , setup , forks [ 6 ], forks [ 0 ] }, } }; public : void start () { setup . channel . notifyall (); } void stop () { setup . done = true ; } }; void dine () { std :: cout << \"Dinner started!\" << std :: endl ; { table table ; table . start (); std :: this_thread :: sleep_for ( std :: chrono :: seconds ( 60 )); table . stop (); } std :: cout << \"Dinner done!\" << std :: endl ; } int main () { dine (); return 0 ; } // g++ test.cpp -pedantic -Wall -Wextra --std=c++17 -lpthread","title":"\u5b8c\u6574\u7a0b\u5e8f"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/mariusbancila-Dining-philosopher/Chandy-Misra-algorithm/#_2","text":"\u4f5c\u8005\u7ed9\u51fa\u7684\u539f\u7248\u7a0b\u5e8f\u662f\u5b58\u5728\u95ee\u9898\u7684: \u5b83\u5c1d\u8bd5\u53bbstop a blocked thread\uff0c\u663e\u7136\u5b83\u662f\u65e0\u6cd5\u88abjoin\u7684\uff0c\u56e0\u6b64\u88ab\u963b\u585e\u5728\u4e0b\u9762\u7684\u51fd\u6570\u4e2d: ~ philosopher () { lifethread . join (); } \u663e\u7136\uff0c\u5728\u505c\u6b62\u7684\u65f6\u5019\uff0c\u9700\u8981\u5c06\u963b\u585e\u7684thread\u5524\u9192\uff0c\u4e0b\u9762\u662f\u6211\u4fee\u6539\u7684\u4e00\u4e2a\u7248\u672c\uff0c\u5b83\u6ca1\u6709\u6539\u5f7b\u5e95\uff0c\u8fd8\u662f\u5b58\u5728\u65e0\u6cd5\u505c\u6b62\u7684\u95ee\u9898\uff0c\u5173\u4e8e\u5b8c\u6574\u7684\u4fee\u6539\u65b9\u6cd5\uff0c\u53ef\u4ee5\u53c2\u8003: 1\u3001 Stop-a-blocked-thread 2\u3001 Stop-a-sleeping-thread","title":"\u7f3a\u9677\u5206\u6790"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/mariusbancila-Dining-philosopher/Chandy-Misra-algorithm/#_3","text":"#include <array> #include <mutex> #include <thread> #include <atomic> #include <chrono> #include <iostream> #include <string> #include <iomanip> #include <condition_variable> std :: mutex g_lockprint ; constexpr int no_of_philosophers = 7 ; class sync_channel { std :: mutex mutex ; std :: condition_variable cv ; public : void wait () { std :: unique_lock < std :: mutex > lock ( mutex ); cv . wait ( lock ); } void notifyall () { std :: unique_lock < std :: mutex > lock ( mutex ); cv . notify_all (); } }; struct table_setup { std :: atomic < bool > done { false }; sync_channel channel ; }; class fork { int id ; int owner ; bool dirty ; std :: mutex mutex ; public : sync_channel channel ; fork ( int const forkId , int const ownerId ) : id ( forkId ), owner ( ownerId ), dirty ( true ) { } void request ( int const ownerId ) { while ( owner != ownerId ) { if ( dirty ) { std :: lock_guard < std :: mutex > lock ( mutex ); dirty = false ; owner = ownerId ; } else { channel . wait (); } } } void done_using () { dirty = true ; channel . notifyall (); } std :: mutex & getmutex () { return mutex ; } }; struct philosopher { private : int id ; std :: string const name ; table_setup & setup ; fork & left_fork ; fork & right_fork ; std :: thread lifethread ; public : philosopher ( int const id , std :: string const & n , table_setup & s , fork & l , fork & r ) : id ( id ), name ( n ), setup ( s ), left_fork ( l ), right_fork ( r ), lifethread ( & philosopher :: dine , this ) { } ~ philosopher () { lifethread . join (); } void dine () { setup . channel . wait (); do { think (); eat (); } while ( ! setup . done ); } void print ( std :: string const & text ) { std :: lock_guard < std :: mutex > cout_lock ( g_lockprint ); std :: cout << std :: left << std :: setw ( 10 ) << std :: setfill ( ' ' ) << name << text << std :: endl ; } void eat () { left_fork . request ( id ); right_fork . request ( id ); std :: lock ( left_fork . getmutex (), right_fork . getmutex ()); std :: lock_guard < std :: mutex > left_lock ( left_fork . getmutex (), std :: adopt_lock ); std :: lock_guard < std :: mutex > right_lock ( right_fork . getmutex (), std :: adopt_lock ); print ( \" started eating.\" ); print ( \" finished eating.\" ); left_fork . done_using (); right_fork . done_using (); } void think () { print ( \" is thinking \" ); } }; class table { table_setup setup ; /** * \u6700\u540e\u4e00\u4e2a\u53c9\u5b50\u662f\u8981\u7ed9\u7b2c\u4e00\u4e2aphilosopher\u7684 */ std :: array < fork , no_of_philosophers > forks { { { 1 , 1 }, { 2 , 2 }, { 3 , 3 }, { 4 , 4 }, { 5 , 5 }, { 6 , 6 }, { 7 , 1 }, } }; std :: array < philosopher , no_of_philosophers > philosophers { { { 1 , \"Aristotle\" , setup , forks [ 0 ], forks [ 1 ] }, { 2 , \"Platon\" , setup , forks [ 1 ], forks [ 2 ] }, { 3 , \"Descartes\" , setup , forks [ 2 ], forks [ 3 ] }, { 4 , \"Kant\" , setup , forks [ 3 ], forks [ 4 ] }, { 5 , \"Nietzsche\" , setup , forks [ 4 ], forks [ 5 ] }, { 6 , \"Hume\" , setup , forks [ 5 ], forks [ 6 ] }, { 7 , \"Russell\" , setup , forks [ 6 ], forks [ 0 ] }, } }; public : void start () { setup . channel . notifyall (); } void stop () { setup . done = true ; for ( auto && fork : forks ) { fork . channel . notifyall (); } } }; void dine () { std :: cout << \"Dinner started!\" << std :: endl ; { table table ; table . start (); std :: this_thread :: sleep_for ( std :: chrono :: seconds ( 30 )); std :: cout << \"\u505c\u6b62\" << std :: endl ; table . stop (); } std :: cout << \"Dinner done!\" << std :: endl ; } int main () { dine (); return 0 ; } // g++ test.cpp -pedantic -Wall -Wextra --std=c++11 -lpthread","title":"\u4e00\u4e2a\u4e0d\u5f7b\u5e95\u7684\u4fee\u6539"},{"location":"Concurrent-computing/Classic-problem/Dining-philosopher/mariusbancila-Dining-philosopher/Chandy-Misra-algorithm/#_4","text":"\u7531\u4e8e\u4e00\u4e2afork\u53ea\u6709\u4e24\u4e2aphilosopher\u4f7f\u7528\uff0c\u56e0\u6b64\u4e0a\u8ff0implementation\u4e2d\uff0c\u6bcf\u4e2afork\u4f7f\u7528\u4e00\u4e2acondition variable\u5373\u53ef\uff0c\u5982\u679c\u8bf4\uff0c\u4e09\u4e2a\u751a\u81f3\u66f4\u591a\u7684philosopher\u4f7f\u7528\u540c\u4e00\u4e2afork\uff0c\u90a3\u4e48\u5728\u8fd9\u4e9bphilosopher\u4e4b\u95f4\u5982\u4f55\u6765\u8fdb\u884c\u534f\u8c03\u5206\u914d\u5462\uff1f\u663e\u7136\uff0c\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528\u4e00\u4e2acondition variable\u662f\u4e0d\u884c\u7684\u3002","title":"\u603b\u7ed3\u5206\u6790"},{"location":"Concurrent-computing/Classic-problem/Producer%E2%80%93consumer-problem/","text":"Producer\u2013consumer problem wikipedia Producer\u2013consumer problem Without semaphores or monitors The producer\u2013consumer problem, particularly in the case of a single producer and single consumer, strongly relates to implementing a FIFO or a channel . The producer\u2013consumer pattern can provide highly efficient data communication without relying on semaphores, mutexes, or monitors for data transfer . The use of those primitives can be expansive in terms of performance, in comparison to basic read/write atomic operation. Channels and FIFOs are popular just because they avoid the need for end-to-end atomic synchronization. A basic example coded in C is shown below. NOTE: lock-free 1\u3001Atomic read-modify-write access to shared variables is avoided, as each of the two Count variables is updated only by a single thread. Also, these variables support an unbounded number of increment operations; the relation remains correct when their values wrap around on an integer overflow . 2\u3001This example does not put threads to sleep, which may be acceptable depending on the system context. The schedulerYield() is inserted as an attempt to improve performance, and may be omitted. Thread libraries typically require semaphores or condition variables to control the sleep/wakeup of threads. In a multi-processor environment, thread sleep/wakeup would occur much less frequently than passing of data tokens, so avoiding atomic operations on data passing is beneficial. cornell cs3110 Lecture 18: Concurrency\u2014Producer/Consumer Pattern and Thread Pools Implementation C++ A C++ Producer-Consumer Concurrency Template Library Eiffel https://www.eiffel.org/doc/solutions/Producer-consumer Java https://www.geeksforgeeks.org/producer-consumer-solution-using-threads-java/","title":"Introduction"},{"location":"Concurrent-computing/Classic-problem/Producer%E2%80%93consumer-problem/#producerconsumer#problem","text":"","title":"Producer\u2013consumer problem"},{"location":"Concurrent-computing/Classic-problem/Producer%E2%80%93consumer-problem/#wikipedia#producerconsumer#problem","text":"","title":"wikipedia Producer\u2013consumer problem"},{"location":"Concurrent-computing/Classic-problem/Producer%E2%80%93consumer-problem/#without#semaphores#or#monitors","text":"The producer\u2013consumer problem, particularly in the case of a single producer and single consumer, strongly relates to implementing a FIFO or a channel . The producer\u2013consumer pattern can provide highly efficient data communication without relying on semaphores, mutexes, or monitors for data transfer . The use of those primitives can be expansive in terms of performance, in comparison to basic read/write atomic operation. Channels and FIFOs are popular just because they avoid the need for end-to-end atomic synchronization. A basic example coded in C is shown below. NOTE: lock-free 1\u3001Atomic read-modify-write access to shared variables is avoided, as each of the two Count variables is updated only by a single thread. Also, these variables support an unbounded number of increment operations; the relation remains correct when their values wrap around on an integer overflow . 2\u3001This example does not put threads to sleep, which may be acceptable depending on the system context. The schedulerYield() is inserted as an attempt to improve performance, and may be omitted. Thread libraries typically require semaphores or condition variables to control the sleep/wakeup of threads. In a multi-processor environment, thread sleep/wakeup would occur much less frequently than passing of data tokens, so avoiding atomic operations on data passing is beneficial.","title":"Without semaphores or monitors"},{"location":"Concurrent-computing/Classic-problem/Producer%E2%80%93consumer-problem/#cornell#cs3110#lecture#18#concurrencyproducerconsumer#pattern#and#thread#pools","text":"","title":"cornell cs3110 Lecture 18: Concurrency\u2014Producer/Consumer Pattern and Thread Pools"},{"location":"Concurrent-computing/Classic-problem/Producer%E2%80%93consumer-problem/#implementation","text":"","title":"Implementation"},{"location":"Concurrent-computing/Classic-problem/Producer%E2%80%93consumer-problem/#c","text":"A C++ Producer-Consumer Concurrency Template Library","title":"C++"},{"location":"Concurrent-computing/Classic-problem/Producer%E2%80%93consumer-problem/#eiffel","text":"https://www.eiffel.org/doc/solutions/Producer-consumer","title":"Eiffel"},{"location":"Concurrent-computing/Classic-problem/Producer%E2%80%93consumer-problem/#java","text":"https://www.geeksforgeeks.org/producer-consumer-solution-using-threads-java/","title":"Java"},{"location":"Concurrent-computing/Classic-problem/Producer%E2%80%93consumer-problem/TODO/","text":"\u751f\u4ea7\u8005\u4e00\u4e2athread\uff0c\u6d88\u8d39\u8005\u4e00\u4e2athread\uff0c\u4e24\u8005\u4e4b\u95f4\u7684\u961f\u5217\u4f7f\u7528list\u6765\u5b9e\u73b0\uff0c\u4e00\u79cd\u6bd4\u8f83\u7406\u60f3\u7684\u6a21\u5f0f\u662f\uff0c\u6d88\u8d39\u8005\u4e0d\u65ad\u5730\u4ece\u961f\u5217\u7684\u4e00\u6bb5pop\uff0c\u751f\u4ea7\u8005\u4e0d\u65ad\u5730\u5411\u961f\u5217\u7684\u4e00\u6bb5append\uff0c\u53ef\u4ee5\u8ba9\u6d88\u8d39\u8005\u6301list\u7684end\uff0c\u8ba9\u751f\u4ea7\u8005\u6301list\u7684front\uff0c\u5f53end\u7b49\u4e8efront\u7684\u65f6\u5019\uff0c\u5219\u961f\u5217\u4e3a\u7a7a\uff0c\u5219\u6d88\u8d39\u8005\u963b\u585e\u3002","title":"TODO"},{"location":"Concurrent-computing/Classic-problem/Producer%E2%80%93consumer-problem/Unbounded-producer-and-consumer-problem/","text":"unbounded producer and consumer Implementation C++ concurrentqueue A Fast General Purpose Lock-Free Queue for C++ Detailed Design of a Lock-Free Queue Unbounded linked-list based MPMC Queue \u9879\u76ee\u6e90\u7801\uff1a ConcurrentQueues Lock-Free Multiple Producer/Consumer Queue in C++11","title":"Unbounded-producer-and-consumer-problem"},{"location":"Concurrent-computing/Classic-problem/Producer%E2%80%93consumer-problem/Unbounded-producer-and-consumer-problem/#unbounded#producer#and#consumer","text":"","title":"unbounded producer and consumer"},{"location":"Concurrent-computing/Classic-problem/Producer%E2%80%93consumer-problem/Unbounded-producer-and-consumer-problem/#implementation","text":"","title":"Implementation"},{"location":"Concurrent-computing/Classic-problem/Producer%E2%80%93consumer-problem/Unbounded-producer-and-consumer-problem/#c","text":"","title":"C++"},{"location":"Concurrent-computing/Classic-problem/Producer%E2%80%93consumer-problem/Unbounded-producer-and-consumer-problem/#concurrentqueue","text":"A Fast General Purpose Lock-Free Queue for C++ Detailed Design of a Lock-Free Queue","title":"concurrentqueue"},{"location":"Concurrent-computing/Classic-problem/Producer%E2%80%93consumer-problem/Unbounded-producer-and-consumer-problem/#unbounded#linked-list#based#mpmc#queue","text":"\u9879\u76ee\u6e90\u7801\uff1a ConcurrentQueues","title":"Unbounded linked-list based MPMC Queue"},{"location":"Concurrent-computing/Classic-problem/Producer%E2%80%93consumer-problem/Unbounded-producer-and-consumer-problem/#lock-free#multiple#producerconsumer#queue#in#c11","text":"","title":"Lock-Free Multiple Producer/Consumer Queue in C++11"},{"location":"Concurrent-computing/Classic-problem/Readers%E2%80%93writers-problem/","text":"Readers\u2013writers problem Wikipedia Readers\u2013writers problem","title":"Introduction"},{"location":"Concurrent-computing/Classic-problem/Readers%E2%80%93writers-problem/#readerswriters#problem","text":"","title":"Readers\u2013writers problem"},{"location":"Concurrent-computing/Classic-problem/Readers%E2%80%93writers-problem/#wikipedia#readerswriters#problem","text":"","title":"Wikipedia Readers\u2013writers problem"},{"location":"Concurrent-computing/Classic-problem/Sleeping-barber-problem/","text":"Sleeping barber problem wikipedia Sleeping barber problem TODO golangprograms Golang Concurrency","title":"Introduction"},{"location":"Concurrent-computing/Classic-problem/Sleeping-barber-problem/#sleeping#barber#problem","text":"","title":"Sleeping barber problem"},{"location":"Concurrent-computing/Classic-problem/Sleeping-barber-problem/#wikipedia#sleeping#barber#problem","text":"","title":"wikipedia Sleeping barber problem"},{"location":"Concurrent-computing/Classic-problem/Sleeping-barber-problem/#todo","text":"golangprograms Golang Concurrency","title":"TODO"},{"location":"Concurrent-computing/Concurrency-control/","text":"Concurrency control Concurrency control\u65e0\u5904\u4e0d\u5728\u3002 Concurrency control\u7684\u76ee\u7684 \u6700\u6700\u57fa\u672c\u7684\u76ee\u7684: \u4fdd\u8bc1shared data\u7684correction\uff1b \u5176\u4ed6\u7684\u7cfb\u7edf\uff0c\u53ef\u80fd\u6709\u7740\u5176\u4ed6\u7684\u76ee\u7684\uff1b wikipedia Concurrency control In information technology and computer science, especially in the fields of computer programming, operating systems, multiprocessors, and databases, concurrency control ensures that correct results for concurrent operations are generated, while getting those results as quickly as possible. NOTE: concurrency control\u5b58\u5728\u4e8ecomputer science\u7684\u5404\u4e2a\u9886\u57df: 1) programming language 2) operating system 3) multiprocessor 4) database \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u6700\u540e\u4e00\u53e5\u603b\u7ed3\u4e86concurrency control\u7684\u76ee\u6807: 1) \u4fdd\u8bc1correct 2) \u4fdd\u8bc1performance Computer systems, both software and hardware, consist of modules, or components. Each component is designed to operate correctly, i.e., to obey or to meet certain consistency rules . When components that operate concurrently interact by messaging or by sharing accessed data (in memory or storage), a certain component's consistency may be violated by another component. The general area of concurrency control provides rules, methods, design methodologies, and theories to maintain the consistency of components operating concurrently while interacting, and thus the consistency and correctness of the whole system. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u524d\u9762\u90e8\u5206\u5176\u5b9e\u5c31\u662fmultiple model(\u53c2\u89c1 Model\\Multiple-model \u7ae0\u8282)\u3002 Introducing concurrency control into a system means applying operation constraints which typically result in some performance reduction. Operation consistency and correctness should be achieved with as good as possible efficiency, without reducing performance below reasonable levels. Concurrency control can require significant additional complexity and overhead in a concurrent algorithm compared to the simpler sequential algorithm. Concurrency control can require significant additional complexity and overhead in a concurrent algorithm compared to the simpler sequential algorithm . NOTE: concurrent algorithm VS sequential algorithm Concurrency control in databases NOTE: \u8fd9\u90e8\u5206\u5185\u5bb9\u653e\u5230\u4e86\u5de5\u7a0bDB\u4e2d wikipedia Distributed concurrency control wikipedia Category:Concurrency control wikipedia Category:Concurrency control algorithms \u4ecemultiple model\u6765\u770b\u5f85concurrency control \u7531\u4e8e\u5bf9shared data\u7684operation\u53ea\u6709read\u3001write\u4e24\u5927\u7c7b\uff0c\u56e0\u6b64\uff0c \u4e00\u3001\u5927\u591a\u6570concurrency control\u90fd\u662f\u57fa\u4e8eread\u3001write\u6765\u505a\u7684 \u6bd4\u5982: 1\u3001atomic instruction\uff0c\u5c24\u5176\u662fread-modify-write\u7cfb\u5217instruction\uff0c\u53c2\u89c1 Concurrent-computing\\Concurrency-control\\Non-blocking\\Atomic-instruction \u7ae0\u8282 2\u3001read-write lock \u4e8c\u3001\u53ef\u4ee5\u57fa\u4e8eread and write\u6765\u8fdb\u884coptimize \u5728 ./How-to-choose-technique \u4e2d\u8fdb\u884c\u4e86\u603b\u7ed3\u3002 How to optimize\uff1f NOTE: \u663e\u7136\uff0c\u9700\u8981\u9075\u5faaoptimization principle \u5728 ./How to choose technique \u4e2d\u8fdb\u884c\u4e86\u603b\u7ed3\u3002 TODO Test and test-and-set Consistency in Distributed Systems","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/#concurrency#control","text":"Concurrency control\u65e0\u5904\u4e0d\u5728\u3002","title":"Concurrency control"},{"location":"Concurrent-computing/Concurrency-control/#concurrency#control_1","text":"\u6700\u6700\u57fa\u672c\u7684\u76ee\u7684: \u4fdd\u8bc1shared data\u7684correction\uff1b \u5176\u4ed6\u7684\u7cfb\u7edf\uff0c\u53ef\u80fd\u6709\u7740\u5176\u4ed6\u7684\u76ee\u7684\uff1b","title":"Concurrency control\u7684\u76ee\u7684"},{"location":"Concurrent-computing/Concurrency-control/#wikipedia#concurrency#control","text":"In information technology and computer science, especially in the fields of computer programming, operating systems, multiprocessors, and databases, concurrency control ensures that correct results for concurrent operations are generated, while getting those results as quickly as possible. NOTE: concurrency control\u5b58\u5728\u4e8ecomputer science\u7684\u5404\u4e2a\u9886\u57df: 1) programming language 2) operating system 3) multiprocessor 4) database \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u6700\u540e\u4e00\u53e5\u603b\u7ed3\u4e86concurrency control\u7684\u76ee\u6807: 1) \u4fdd\u8bc1correct 2) \u4fdd\u8bc1performance Computer systems, both software and hardware, consist of modules, or components. Each component is designed to operate correctly, i.e., to obey or to meet certain consistency rules . When components that operate concurrently interact by messaging or by sharing accessed data (in memory or storage), a certain component's consistency may be violated by another component. The general area of concurrency control provides rules, methods, design methodologies, and theories to maintain the consistency of components operating concurrently while interacting, and thus the consistency and correctness of the whole system. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u524d\u9762\u90e8\u5206\u5176\u5b9e\u5c31\u662fmultiple model(\u53c2\u89c1 Model\\Multiple-model \u7ae0\u8282)\u3002 Introducing concurrency control into a system means applying operation constraints which typically result in some performance reduction. Operation consistency and correctness should be achieved with as good as possible efficiency, without reducing performance below reasonable levels. Concurrency control can require significant additional complexity and overhead in a concurrent algorithm compared to the simpler sequential algorithm. Concurrency control can require significant additional complexity and overhead in a concurrent algorithm compared to the simpler sequential algorithm . NOTE: concurrent algorithm VS sequential algorithm","title":"wikipedia Concurrency control"},{"location":"Concurrent-computing/Concurrency-control/#concurrency#control#in#databases","text":"NOTE: \u8fd9\u90e8\u5206\u5185\u5bb9\u653e\u5230\u4e86\u5de5\u7a0bDB\u4e2d","title":"Concurrency control in databases"},{"location":"Concurrent-computing/Concurrency-control/#wikipedia#distributed#concurrency#control","text":"","title":"wikipedia Distributed concurrency control"},{"location":"Concurrent-computing/Concurrency-control/#wikipedia#categoryconcurrency#control","text":"","title":"wikipedia Category:Concurrency control"},{"location":"Concurrent-computing/Concurrency-control/#wikipedia#categoryconcurrency#control#algorithms","text":"","title":"wikipedia Category:Concurrency control algorithms"},{"location":"Concurrent-computing/Concurrency-control/#multiple#modelconcurrency#control","text":"\u7531\u4e8e\u5bf9shared data\u7684operation\u53ea\u6709read\u3001write\u4e24\u5927\u7c7b\uff0c\u56e0\u6b64\uff0c \u4e00\u3001\u5927\u591a\u6570concurrency control\u90fd\u662f\u57fa\u4e8eread\u3001write\u6765\u505a\u7684 \u6bd4\u5982: 1\u3001atomic instruction\uff0c\u5c24\u5176\u662fread-modify-write\u7cfb\u5217instruction\uff0c\u53c2\u89c1 Concurrent-computing\\Concurrency-control\\Non-blocking\\Atomic-instruction \u7ae0\u8282 2\u3001read-write lock \u4e8c\u3001\u53ef\u4ee5\u57fa\u4e8eread and write\u6765\u8fdb\u884coptimize \u5728 ./How-to-choose-technique \u4e2d\u8fdb\u884c\u4e86\u603b\u7ed3\u3002","title":"\u4ecemultiple model\u6765\u770b\u5f85concurrency control"},{"location":"Concurrent-computing/Concurrency-control/#how#to#optimize","text":"NOTE: \u663e\u7136\uff0c\u9700\u8981\u9075\u5faaoptimization principle \u5728 ./How to choose technique \u4e2d\u8fdb\u884c\u4e86\u603b\u7ed3\u3002","title":"How to optimize\uff1f"},{"location":"Concurrent-computing/Concurrency-control/#todo","text":"Test and test-and-set Consistency in Distributed Systems","title":"TODO"},{"location":"Concurrent-computing/Concurrency-control/Barrier/","text":"Barrier wikipedia Barrier (computer science) Classification instruction level Memory barrier thread level pthread_barrier process level","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Barrier/#barrier","text":"","title":"Barrier"},{"location":"Concurrent-computing/Concurrency-control/Barrier/#wikipedia#barrier#computer#science","text":"","title":"wikipedia Barrier (computer science)"},{"location":"Concurrent-computing/Concurrency-control/Barrier/#classification","text":"instruction level Memory barrier thread level pthread_barrier process level","title":"Classification"},{"location":"Concurrent-computing/Concurrency-control/Lock/","text":"Lock 1\u3001Lock\u6700\u6700\u5e38\u89c1\u7684Mutual exclusion\uff0c\u672c\u6587\u5bf9\u5b83\u8fdb\u884c\u8be6\u7ec6\u8bf4\u660e\u3002 2\u3001Lock\u662fpessimistic\u7684\uff0c\u5173\u4e8e\u6b64\uff0c\u53c2\u89c1 wikipedia Mutual exclusion In computer science , mutual exclusion is a property of concurrency control , which is instituted for the purpose of preventing race conditions . It is the requirement that one thread of execution never enters its critical section at the same time that another concurrent thread of execution enters its own critical section, which refers to an interval of time during which a thread of execution accesses a shared resource, such as shared memory . wikipedia Lock (computer science) TODO https://stackoverflow.com/questions/2252452/does-a-getter-function-need-a-mutex https://www.informit.com/articles/article.aspx?p=1750198&seqNum=3 https://stackoverflow.com/questions/17217268/c-sharp-is-locking-within-getters-and-setters-necessary https://rules.sonarsource.com/cpp/RSPEC-1912 https://help.semmle.com/wiki/display/CSHARP/Inconsistently+synchronized+property https://books.google.com/books?id=IgAICAAAQBAJ&pg=PA29&lpg=PA29&dq=should+getter+and+setter+be+locked&source=bl&ots=cSQCoTywkd&sig=ACfU3U03vlcYgKl29kmWcubdqe8q2sk-gQ&hl=en&sa=X&ved=2ahUKEwjnh4jtqrDoAhX-yosBHeKkAlEQ6AEwA3oECAgQAQ#v=onepage&q=should%20getter%20and%20setter%20be%20locked&f=false","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Lock/#lock","text":"1\u3001Lock\u6700\u6700\u5e38\u89c1\u7684Mutual exclusion\uff0c\u672c\u6587\u5bf9\u5b83\u8fdb\u884c\u8be6\u7ec6\u8bf4\u660e\u3002 2\u3001Lock\u662fpessimistic\u7684\uff0c\u5173\u4e8e\u6b64\uff0c\u53c2\u89c1","title":"Lock"},{"location":"Concurrent-computing/Concurrency-control/Lock/#wikipedia#mutual#exclusion","text":"In computer science , mutual exclusion is a property of concurrency control , which is instituted for the purpose of preventing race conditions . It is the requirement that one thread of execution never enters its critical section at the same time that another concurrent thread of execution enters its own critical section, which refers to an interval of time during which a thread of execution accesses a shared resource, such as shared memory .","title":"wikipedia Mutual exclusion"},{"location":"Concurrent-computing/Concurrency-control/Lock/#wikipedia#lock#computer#science","text":"","title":"wikipedia Lock (computer science)"},{"location":"Concurrent-computing/Concurrency-control/Lock/#todo","text":"https://stackoverflow.com/questions/2252452/does-a-getter-function-need-a-mutex https://www.informit.com/articles/article.aspx?p=1750198&seqNum=3 https://stackoverflow.com/questions/17217268/c-sharp-is-locking-within-getters-and-setters-necessary https://rules.sonarsource.com/cpp/RSPEC-1912 https://help.semmle.com/wiki/display/CSHARP/Inconsistently+synchronized+property https://books.google.com/books?id=IgAICAAAQBAJ&pg=PA29&lpg=PA29&dq=should+getter+and+setter+be+locked&source=bl&ots=cSQCoTywkd&sig=ACfU3U03vlcYgKl29kmWcubdqe8q2sk-gQ&hl=en&sa=X&ved=2ahUKEwjnh4jtqrDoAhX-yosBHeKkAlEQ6AEwA3oECAgQAQ#v=onepage&q=should%20getter%20and%20setter%20be%20locked&f=false","title":"TODO"},{"location":"Concurrent-computing/Concurrency-control/Lock/Disadvantage/","text":"Disadvantages 1\u3001\u52a0\u9501\uff0c\u5219\u5b58\u5728 \"\u7528\u6237\u6001\" \u5230 \"\u5185\u6838\u6001\"\u3001blocking\u3001context switch\uff0c\u76f8\u5bf9\u800c\u8a00\uff0c\u8fd9\u662f\u6210\u672c\u8f83\u9ad8\u7684 2\u3001\u5e76\u884c \u53d8 \u4e32\u884c 3\u3001\u591a\u6838\u4e4b\u95f4\u8fdb\u884c\u6570\u636e\u540c\u6b65\u3001cache coherence\u3001bus traffic \u5c24\u5176\u662f\u5f53\u591a\u4e2athread\u90fd\u4f7f\u7528\u540c\u4e00\u4e2alock\u7684\u65f6\u5019\u3002 wikipedia Lock (computer science) # Disadvantages wikipedia Non-blocking algorithm # Motivation","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Lock/Disadvantage/#disadvantages","text":"1\u3001\u52a0\u9501\uff0c\u5219\u5b58\u5728 \"\u7528\u6237\u6001\" \u5230 \"\u5185\u6838\u6001\"\u3001blocking\u3001context switch\uff0c\u76f8\u5bf9\u800c\u8a00\uff0c\u8fd9\u662f\u6210\u672c\u8f83\u9ad8\u7684 2\u3001\u5e76\u884c \u53d8 \u4e32\u884c 3\u3001\u591a\u6838\u4e4b\u95f4\u8fdb\u884c\u6570\u636e\u540c\u6b65\u3001cache coherence\u3001bus traffic \u5c24\u5176\u662f\u5f53\u591a\u4e2athread\u90fd\u4f7f\u7528\u540c\u4e00\u4e2alock\u7684\u65f6\u5019\u3002","title":"Disadvantages"},{"location":"Concurrent-computing/Concurrency-control/Lock/Disadvantage/#wikipedia#lock#computer#science#disadvantages","text":"","title":"wikipedia Lock (computer science) # Disadvantages"},{"location":"Concurrent-computing/Concurrency-control/Lock/Disadvantage/#wikipedia#non-blocking#algorithm#motivation","text":"","title":"wikipedia Non-blocking algorithm # Motivation"},{"location":"Concurrent-computing/Concurrency-control/Lock/Disadvantage/Deadlock-and-livelock/","text":"Deadlock and livelock wikipedia Deadlock Livelock baike \u6d3b\u9501 Example \u5355\u4e00\u5b9e\u4f53\u7684\u6d3b\u9501 \u4f8b\u5982 \u7ebf\u7a0b \u4ece\u961f\u5217\u4e2d\u62ff\u51fa\u4e00\u4e2a\u4efb\u52a1\u6765\u6267\u884c\uff0c\u5982\u679c\u4efb\u52a1\u6267\u884c\u5931\u8d25\uff0c\u90a3\u4e48\u5c06\u4efb\u52a1\u91cd\u65b0\u52a0\u5165\u961f\u5217\uff0c\u7ee7\u7eed\u6267\u884c\u3002\u5047\u8bbe\u4efb\u52a1\u603b\u662f\u6267\u884c\u5931\u8d25\uff0c\u6216\u8005\u67d0\u79cd\u4f9d\u8d56\u7684\u6761\u4ef6\u603b\u662f\u4e0d\u6ee1\u8db3\uff0c\u90a3\u4e48\u7ebf\u7a0b\u4e00\u76f4\u5728\u7e41\u5fd9\u5374\u6ca1\u6709\u4efb\u4f55\u7ed3\u679c\u3002 \u534f\u540c\u5bfc\u81f4\u7684\u6d3b\u9501 NOTE: 1\u3001\"\u534f\u540c\"\u662f\u6307\u6709\u591a\u4eba\u540c\u65f6\u53c2\u4e0e \u751f\u6d3b\u4e2d\u7684\u5178\u578b\u4f8b\u5b50\uff1a \u4e24\u4e2a\u4eba\u5728\u7a84\u8def\u76f8\u9047\uff0c\u540c\u65f6\u5411\u4e00\u4e2a\u65b9\u5411\u907f\u8ba9\uff0c\u7136\u540e\u53c8\u5411\u53e6\u4e00\u4e2a\u65b9\u5411\u907f\u8ba9\uff0c\u5982\u6b64\u53cd\u590d\u3002 \u901a\u4fe1\u4e2d\u4e5f\u6709\u7c7b\u4f3c\u7684\u4f8b\u5b50\uff0c\u591a\u4e2a\u7528\u6237\u5171\u4eab\u4fe1\u9053\uff08\u6700\u7b80\u5355\u7684\u4f8b\u5b50\u662f\u5927\u5bb6\u90fd\u7528\u5bf9\u8bb2\u673a\uff09\uff0c\u540c\u4e00\u65f6\u523b\u53ea\u80fd\u6709\u4e00\u65b9\u53d1\u9001\u4fe1\u606f\u3002\u53d1\u9001\u4fe1\u53f7\u7684\u7528\u6237\u4f1a\u8fdb\u884c \u51b2\u7a81\u68c0\u6d4b \uff0c \u5982\u679c\u53d1\u751f\u51b2\u7a81\uff0c\u5c31\u9009\u62e9\u907f\u8ba9\uff0c\u7136\u540e\u518d\u53d1\u9001\u3002 \u5047\u8bbe\u907f\u8ba9\u7b97\u6cd5\u4e0d\u5408\u7406\uff0c\u5c31\u5bfc\u81f4\u6bcf\u6b21\u53d1\u9001\uff0c\u90fd\u51b2\u7a81\uff0c\u907f\u8ba9\u540e\u518d\u53d1\u9001\uff0c\u8fd8\u662f\u51b2\u7a81\u3002 \u8ba1\u7b97\u673a\u4e2d\u7684\u4f8b\u5b50\uff1a\u4e24\u4e2a\u7ebf\u7a0b\u53d1\u751f\u4e86\u67d0\u4e9b\u6761\u4ef6\u7684\u78b0\u649e\u540e\u91cd\u65b0\u6267\u884c\uff0c\u90a3\u4e48\u5982\u679c\u518d\u6b21\u5c1d\u8bd5\u540e\u4f9d\u7136\u53d1\u751f\u4e86\u78b0\u649e\uff0c\u957f\u6b64\u4e0b\u53bb\u5c31\u6709\u53ef\u80fd\u53d1\u751f\u6d3b\u9501\u3002 NOTE: 1\u3001\u4ece\u4e0a\u9762\u7684\u63cf\u8ff0\u53ef\u4ee5\u770b\u51fa\uff0clivelock\u672c\u8d28\u4e0a\u5c31\u662f\u4e0d\u65ad\u5730\u78b0\u649e \u89e3\u51b3\u65b9\u6cd5 \u89e3\u51b3\u534f\u540c\u6d3b\u9501\u7684\u4e00\u79cd\u65b9\u6848\u662f\u8c03\u6574\u91cd\u8bd5\u673a\u5236\u3002 **\u6bd4\u5982\u5f15\u5165\u4e00\u4e9b\u968f\u673a\u6027\u3002**\u4f8b\u5982\u5982\u679c\u68c0\u6d4b\u5230\u51b2\u7a81\uff0c\u90a3\u4e48\u5c31\u6682\u505c\u968f\u673a\u7684\u4e00\u5b9a\u65f6\u95f4\u8fdb\u884c\u91cd\u8bd5\u3002\u8fd9\u56de\u5927\u5927\u51cf\u5c11\u78b0\u649e\u7684\u53ef\u80fd\u6027\u3002 \u5178\u578b\u7684\u4f8b\u5b50\u662f\u4ee5\u592a\u7f51\u7684 CSMA/CD \u68c0\u6d4b\u673a\u5236\u3002 \u53e6\u5916\u4e3a\u4e86\u907f\u514d\u53ef\u80fd\u7684 \u6b7b\u9501 \uff0c\u9002\u5f53\u52a0\u5165\u4e00\u5b9a\u7684\u91cd\u8bd5\u6b21\u6570\u4e5f\u662f\u6709\u6548\u7684\u89e3\u51b3\u529e\u6cd5\u3002\u5c3d\u7ba1\u8fd9\u5728\u4e1a\u52a1\u4e0a\u4f1a\u5f15\u8d77\u4e00\u4e9b\u590d\u6742\u7684\u903b\u8f91\u5904\u7406\u3002 \u6bd4\u5982\u7ea6\u5b9a\u91cd\u8bd5\u673a\u5236\u907f\u514d\u518d\u6b21\u51b2\u7a81\u3002 \u4f8b\u5982\u81ea\u52a8\u9a7e\u9a76\u7684\u9632\u78b0\u649e\u7cfb\u7edf\uff08\u5047\u60f3\u7684\u4f8b\u5b50\uff09\uff0c\u53ef\u4ee5\u6839\u636e\u5e8f\u5217\u53f7\u7ea6\u5b9a\u68c0\u6d4b\u5230\u76f8\u649e\u98ce\u9669\u65f6\uff0c\u5e8f\u5217\u53f7\u5c0f\u7684\u98de\u673a\u671d\u4e0a\u98de\uff0c \u5e8f\u5217\u53f7\u5927\u7684\u98de\u673a\u671d\u4e0b\u98de\u3002 [2] \u8865\u5145\u6848\u4f8b dining philosopher \u6240\u6709\u7684\u54f2\u5b66\u5bb6\uff0c\u540c\u65f6\u62ff\u8d77\u5de6\u8fb9\u7684\u53c9\u5b50\uff0c\u53c8\u540c\u65f6\u65b9\u5411\uff0c\u8fd9\u662f\u975e\u5e38\u7ecf\u5178\u7684\u6848\u4f8b\u3002 TODO https://www.guru99.com/what-is-livelock-example.html https://www.geeksforgeeks.org/deadlock-starvation-and-livelock/ https://stackoverflow.com/questions/6155951/whats-the-difference-between-deadlock-and-livelock https://stackoverflow.com/questions/1036364/good-example-of-livelock https://www.quora.com/What-is-the-difference-between-deadlock-and-livelock-deadlock-infinite-recursion-and-starvation/answer/Akash-Kava","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Lock/Disadvantage/Deadlock-and-livelock/#deadlock#and#livelock","text":"","title":"Deadlock and livelock"},{"location":"Concurrent-computing/Concurrency-control/Lock/Disadvantage/Deadlock-and-livelock/#wikipedia#deadlock","text":"","title":"wikipedia Deadlock"},{"location":"Concurrent-computing/Concurrency-control/Lock/Disadvantage/Deadlock-and-livelock/#livelock","text":"","title":"Livelock"},{"location":"Concurrent-computing/Concurrency-control/Lock/Disadvantage/Deadlock-and-livelock/#baike","text":"","title":"baike \u6d3b\u9501"},{"location":"Concurrent-computing/Concurrency-control/Lock/Disadvantage/Deadlock-and-livelock/#example","text":"\u5355\u4e00\u5b9e\u4f53\u7684\u6d3b\u9501 \u4f8b\u5982 \u7ebf\u7a0b \u4ece\u961f\u5217\u4e2d\u62ff\u51fa\u4e00\u4e2a\u4efb\u52a1\u6765\u6267\u884c\uff0c\u5982\u679c\u4efb\u52a1\u6267\u884c\u5931\u8d25\uff0c\u90a3\u4e48\u5c06\u4efb\u52a1\u91cd\u65b0\u52a0\u5165\u961f\u5217\uff0c\u7ee7\u7eed\u6267\u884c\u3002\u5047\u8bbe\u4efb\u52a1\u603b\u662f\u6267\u884c\u5931\u8d25\uff0c\u6216\u8005\u67d0\u79cd\u4f9d\u8d56\u7684\u6761\u4ef6\u603b\u662f\u4e0d\u6ee1\u8db3\uff0c\u90a3\u4e48\u7ebf\u7a0b\u4e00\u76f4\u5728\u7e41\u5fd9\u5374\u6ca1\u6709\u4efb\u4f55\u7ed3\u679c\u3002 \u534f\u540c\u5bfc\u81f4\u7684\u6d3b\u9501 NOTE: 1\u3001\"\u534f\u540c\"\u662f\u6307\u6709\u591a\u4eba\u540c\u65f6\u53c2\u4e0e \u751f\u6d3b\u4e2d\u7684\u5178\u578b\u4f8b\u5b50\uff1a \u4e24\u4e2a\u4eba\u5728\u7a84\u8def\u76f8\u9047\uff0c\u540c\u65f6\u5411\u4e00\u4e2a\u65b9\u5411\u907f\u8ba9\uff0c\u7136\u540e\u53c8\u5411\u53e6\u4e00\u4e2a\u65b9\u5411\u907f\u8ba9\uff0c\u5982\u6b64\u53cd\u590d\u3002 \u901a\u4fe1\u4e2d\u4e5f\u6709\u7c7b\u4f3c\u7684\u4f8b\u5b50\uff0c\u591a\u4e2a\u7528\u6237\u5171\u4eab\u4fe1\u9053\uff08\u6700\u7b80\u5355\u7684\u4f8b\u5b50\u662f\u5927\u5bb6\u90fd\u7528\u5bf9\u8bb2\u673a\uff09\uff0c\u540c\u4e00\u65f6\u523b\u53ea\u80fd\u6709\u4e00\u65b9\u53d1\u9001\u4fe1\u606f\u3002\u53d1\u9001\u4fe1\u53f7\u7684\u7528\u6237\u4f1a\u8fdb\u884c \u51b2\u7a81\u68c0\u6d4b \uff0c \u5982\u679c\u53d1\u751f\u51b2\u7a81\uff0c\u5c31\u9009\u62e9\u907f\u8ba9\uff0c\u7136\u540e\u518d\u53d1\u9001\u3002 \u5047\u8bbe\u907f\u8ba9\u7b97\u6cd5\u4e0d\u5408\u7406\uff0c\u5c31\u5bfc\u81f4\u6bcf\u6b21\u53d1\u9001\uff0c\u90fd\u51b2\u7a81\uff0c\u907f\u8ba9\u540e\u518d\u53d1\u9001\uff0c\u8fd8\u662f\u51b2\u7a81\u3002 \u8ba1\u7b97\u673a\u4e2d\u7684\u4f8b\u5b50\uff1a\u4e24\u4e2a\u7ebf\u7a0b\u53d1\u751f\u4e86\u67d0\u4e9b\u6761\u4ef6\u7684\u78b0\u649e\u540e\u91cd\u65b0\u6267\u884c\uff0c\u90a3\u4e48\u5982\u679c\u518d\u6b21\u5c1d\u8bd5\u540e\u4f9d\u7136\u53d1\u751f\u4e86\u78b0\u649e\uff0c\u957f\u6b64\u4e0b\u53bb\u5c31\u6709\u53ef\u80fd\u53d1\u751f\u6d3b\u9501\u3002 NOTE: 1\u3001\u4ece\u4e0a\u9762\u7684\u63cf\u8ff0\u53ef\u4ee5\u770b\u51fa\uff0clivelock\u672c\u8d28\u4e0a\u5c31\u662f\u4e0d\u65ad\u5730\u78b0\u649e","title":"Example"},{"location":"Concurrent-computing/Concurrency-control/Lock/Disadvantage/Deadlock-and-livelock/#_1","text":"\u89e3\u51b3\u534f\u540c\u6d3b\u9501\u7684\u4e00\u79cd\u65b9\u6848\u662f\u8c03\u6574\u91cd\u8bd5\u673a\u5236\u3002 **\u6bd4\u5982\u5f15\u5165\u4e00\u4e9b\u968f\u673a\u6027\u3002**\u4f8b\u5982\u5982\u679c\u68c0\u6d4b\u5230\u51b2\u7a81\uff0c\u90a3\u4e48\u5c31\u6682\u505c\u968f\u673a\u7684\u4e00\u5b9a\u65f6\u95f4\u8fdb\u884c\u91cd\u8bd5\u3002\u8fd9\u56de\u5927\u5927\u51cf\u5c11\u78b0\u649e\u7684\u53ef\u80fd\u6027\u3002 \u5178\u578b\u7684\u4f8b\u5b50\u662f\u4ee5\u592a\u7f51\u7684 CSMA/CD \u68c0\u6d4b\u673a\u5236\u3002 \u53e6\u5916\u4e3a\u4e86\u907f\u514d\u53ef\u80fd\u7684 \u6b7b\u9501 \uff0c\u9002\u5f53\u52a0\u5165\u4e00\u5b9a\u7684\u91cd\u8bd5\u6b21\u6570\u4e5f\u662f\u6709\u6548\u7684\u89e3\u51b3\u529e\u6cd5\u3002\u5c3d\u7ba1\u8fd9\u5728\u4e1a\u52a1\u4e0a\u4f1a\u5f15\u8d77\u4e00\u4e9b\u590d\u6742\u7684\u903b\u8f91\u5904\u7406\u3002 \u6bd4\u5982\u7ea6\u5b9a\u91cd\u8bd5\u673a\u5236\u907f\u514d\u518d\u6b21\u51b2\u7a81\u3002 \u4f8b\u5982\u81ea\u52a8\u9a7e\u9a76\u7684\u9632\u78b0\u649e\u7cfb\u7edf\uff08\u5047\u60f3\u7684\u4f8b\u5b50\uff09\uff0c\u53ef\u4ee5\u6839\u636e\u5e8f\u5217\u53f7\u7ea6\u5b9a\u68c0\u6d4b\u5230\u76f8\u649e\u98ce\u9669\u65f6\uff0c\u5e8f\u5217\u53f7\u5c0f\u7684\u98de\u673a\u671d\u4e0a\u98de\uff0c \u5e8f\u5217\u53f7\u5927\u7684\u98de\u673a\u671d\u4e0b\u98de\u3002 [2]","title":"\u89e3\u51b3\u65b9\u6cd5"},{"location":"Concurrent-computing/Concurrency-control/Lock/Disadvantage/Deadlock-and-livelock/#_2","text":"","title":"\u8865\u5145\u6848\u4f8b"},{"location":"Concurrent-computing/Concurrency-control/Lock/Disadvantage/Deadlock-and-livelock/#dining#philosopher","text":"\u6240\u6709\u7684\u54f2\u5b66\u5bb6\uff0c\u540c\u65f6\u62ff\u8d77\u5de6\u8fb9\u7684\u53c9\u5b50\uff0c\u53c8\u540c\u65f6\u65b9\u5411\uff0c\u8fd9\u662f\u975e\u5e38\u7ecf\u5178\u7684\u6848\u4f8b\u3002","title":"dining philosopher"},{"location":"Concurrent-computing/Concurrency-control/Lock/Disadvantage/Deadlock-and-livelock/#todo","text":"https://www.guru99.com/what-is-livelock-example.html https://www.geeksforgeeks.org/deadlock-starvation-and-livelock/ https://stackoverflow.com/questions/6155951/whats-the-difference-between-deadlock-and-livelock https://stackoverflow.com/questions/1036364/good-example-of-livelock https://www.quora.com/What-is-the-difference-between-deadlock-and-livelock-deadlock-infinite-recursion-and-starvation/answer/Akash-Kava","title":"TODO"},{"location":"Concurrent-computing/Concurrency-control/Lock/Disadvantage/Starvation/","text":"Challenge in concurrency control \u672c\u6587\u603b\u7ed3\u5728concurrency control\u4e2d\u7684\u4e00\u4e9bchallenge\u3002 wikipedia Starvation In computer science , resource starvation is a problem encountered in concurrent computing where a process is perpetually denied necessary resources to process its work.[ 1] Starvation may be caused by errors in a scheduling or mutual exclusion algorithm, but can also be caused by resource leaks , and can be intentionally caused via a denial-of-service attack such as a fork bomb . Fairness and starvation 1\u3001\u5982\u679c\u80fd\u591f\u4fdd\u8bc1\"fairness\" \uff0c\u90a3\u4e48\u5c31\u80fd\u591f\u514d\u4e8estarvation \u7d20\u6750: wikipedia Test-and-set # Performance evaluation of test-and-set locks When we consider fairness, we consider if a processor gets a fair chance of acquiring the lock when it is set free. In an extreme situation the processor might starve i.e. it might not be able to acquire the lock for an extended period of time even though it has become free during that time.","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Lock/Disadvantage/Starvation/#challenge#in#concurrency#control","text":"\u672c\u6587\u603b\u7ed3\u5728concurrency control\u4e2d\u7684\u4e00\u4e9bchallenge\u3002","title":"Challenge in concurrency control"},{"location":"Concurrent-computing/Concurrency-control/Lock/Disadvantage/Starvation/#wikipedia#starvation","text":"In computer science , resource starvation is a problem encountered in concurrent computing where a process is perpetually denied necessary resources to process its work.[ 1] Starvation may be caused by errors in a scheduling or mutual exclusion algorithm, but can also be caused by resource leaks , and can be intentionally caused via a denial-of-service attack such as a fork bomb .","title":"wikipedia Starvation"},{"location":"Concurrent-computing/Concurrency-control/Lock/Disadvantage/Starvation/#fairness#and#starvation","text":"1\u3001\u5982\u679c\u80fd\u591f\u4fdd\u8bc1\"fairness\" \uff0c\u90a3\u4e48\u5c31\u80fd\u591f\u514d\u4e8estarvation","title":"Fairness and starvation"},{"location":"Concurrent-computing/Concurrency-control/Lock/Disadvantage/Starvation/#wikipedia#test-and-set#performance#evaluation#of#test-and-set#locks","text":"When we consider fairness, we consider if a processor gets a fair chance of acquiring the lock when it is set free. In an extreme situation the processor might starve i.e. it might not be able to acquire the lock for an extended period of time even though it has become free during that time.","title":"\u7d20\u6750: wikipedia Test-and-set # Performance evaluation of test-and-set locks"},{"location":"Concurrent-computing/Concurrency-control/Lock/Double-checked-locking/","text":"Double-checked locking Application 1\u3001execute once \u4fdd\u8bc1\u6307\u5b9a\u7684code\u3001logic\u5373\u4f7f\u5728multi thread\u3001multi process\u7b49\u7b49\u60c5\u51b5\u4e0b\u53ea\"execute once\"\u662f\"double-checked locking\"\u7684\u4e3b\u8981application\uff0c\u540e\u7eed\u7684\u5f88\u591aapplication\u90fd\u662f\u57fa\u4e8e\u5b83\u7684\u8fd9\u4e2a\u7279\u6027\u7684\u3002 2\u3001singleton \u4fdd\u8bc1constructor\u53ea\u80fd\u602a\u88ab\u6267\u884c\u4e00\u6b21\u3001\u53ea\u6709\u4e00\u4e2aobject 3\u3001lazy initialization \u53c2\u89c1\u4e0b\u9762\u7684\u5185\u5bb9 Implementation 1\u3001justsoftwaresolutions Multithreading in C++0x part 6: Lazy initialization and double-checked locking with atomics 2\u3001preshing Double-Checked Locking is Fixed In C++11 3\u3001 preshing / cpp11-on-multicore inmemorylogger wikipedia Double-checked locking In software engineering , double-checked locking (also known as \"double-checked locking optimization\"[ 1] ) is a software design pattern used to reduce the overhead of acquiring a lock by testing the locking criterion (the \"lock hint\") before acquiring the lock. Locking occurs only if the locking criterion check indicates that locking is required. NOTE: 1\u3001\"Double-checked locking\"\u662f\u4e00\u79cd\u52a0\u9501\u7684\u65b9\u5f0f\uff0c\u800c\u4e0d\u662f\u67d0\u79cdlock The pattern, when implemented in some language/hardware combinations, can be unsafe. At times, it can be considered an anti-pattern .[ 2] NOTE: 1\u3001\u867d\u7136\u539f\u6587\u7ed9\u51fa\u4e86reference\uff0c\u6211\u76ee\u524d\u9605\u8bfb\u7684\u6700\u597d\u7684\u6587\u7ae0\u662f: aristeia C++ and the Perils of Double-Checked Locking It is typically used to reduce locking overhead when implementing \" lazy initialization \" in a multi-threaded environment, especially as part of the Singleton pattern . Lazy initialization avoids initializing a value until the first time it is accessed. NOTE: 1\u3001\u8fd9\u6bb5\u8bdd\uff0c\u5bf9lazy initialization\u7684\u89e3\u91ca\u975e\u5e38\u597d Usage in C++11 For the singleton pattern, double-checked locking is not needed: If control enters the declaration concurrently while the variable is being initialized, the concurrent execution shall wait for completion of the initialization. \u2014\u2009\u00a7 6.7 [stmt.dcl] p4 Singleton & GetInstance () { static Singleton s ; return s ; } C++11 and beyond also provide a built-in double-checked locking pattern in the form of std::once_flag and std::call_once : #include <mutex> #include <optional> // Since C++17 // Singleton.h class Singleton { public : static Singleton * GetInstance (); private : Singleton () = default ; static std :: optional < Singleton > s_instance ; static std :: once_flag s_flag ; }; // Singleton.cpp std :: optional < Singleton > Singleton :: s_instance ; std :: once_flag Singleton :: s_flag { }; Singleton * Singleton :: GetInstance () { std :: call_once ( Singleton :: s_flag , []() { s_instance . emplace ( Singleton {});}); return &* s_instance ; } If one truly wishes to use the double-checked idiom instead of the trivially working example above (for instance because Visual Studio before the 2015 release did not implement the C++11 standard's language about concurrent initialization quoted above [ 3] ), one needs to use acquire and release fences:[ 4] #include <atomic> #include <mutex> class Singleton { public : static Singleton * GetInstance (); private : Singleton () = default ; static std :: atomic < Singleton *> s_instance ; static std :: mutex s_mutex ; }; Singleton * Singleton :: GetInstance () { Singleton * p = s_instance . load ( std :: memory_order_acquire ); if ( p == nullptr ) { // 1st check std :: lock_guard < std :: mutex > lock ( s_mutex ); p = s_instance . load ( std :: memory_order_relaxed ); if ( p == nullptr ) { // 2nd (double) check p = new Singleton (); s_instance . store ( p , std :: memory_order_release ); } } return p ; } \u5176\u4ed6\u8d44\u6e90 \u5728\u5de5\u7a0bprogramming-language\u7684 C-family-language\\C++\\Pattern\\Singleton \u7ae0\u8282\u4e5f\u5bf9\u5b83\u8fdb\u884c\u4e86\u8ba8\u8bba\uff0c\u5176\u4e2d\u4e3b\u8981\u6536\u5f55\u4e86\u5982\u4e0b\u6587\u7ae0\uff1a 1\u3001aristeia C++ and the Perils of Double-Checked Locking","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Lock/Double-checked-locking/#double-checked#locking","text":"","title":"Double-checked locking"},{"location":"Concurrent-computing/Concurrency-control/Lock/Double-checked-locking/#application","text":"1\u3001execute once \u4fdd\u8bc1\u6307\u5b9a\u7684code\u3001logic\u5373\u4f7f\u5728multi thread\u3001multi process\u7b49\u7b49\u60c5\u51b5\u4e0b\u53ea\"execute once\"\u662f\"double-checked locking\"\u7684\u4e3b\u8981application\uff0c\u540e\u7eed\u7684\u5f88\u591aapplication\u90fd\u662f\u57fa\u4e8e\u5b83\u7684\u8fd9\u4e2a\u7279\u6027\u7684\u3002 2\u3001singleton \u4fdd\u8bc1constructor\u53ea\u80fd\u602a\u88ab\u6267\u884c\u4e00\u6b21\u3001\u53ea\u6709\u4e00\u4e2aobject 3\u3001lazy initialization \u53c2\u89c1\u4e0b\u9762\u7684\u5185\u5bb9","title":"Application"},{"location":"Concurrent-computing/Concurrency-control/Lock/Double-checked-locking/#implementation","text":"1\u3001justsoftwaresolutions Multithreading in C++0x part 6: Lazy initialization and double-checked locking with atomics 2\u3001preshing Double-Checked Locking is Fixed In C++11 3\u3001 preshing / cpp11-on-multicore inmemorylogger","title":"Implementation"},{"location":"Concurrent-computing/Concurrency-control/Lock/Double-checked-locking/#wikipedia#double-checked#locking","text":"In software engineering , double-checked locking (also known as \"double-checked locking optimization\"[ 1] ) is a software design pattern used to reduce the overhead of acquiring a lock by testing the locking criterion (the \"lock hint\") before acquiring the lock. Locking occurs only if the locking criterion check indicates that locking is required. NOTE: 1\u3001\"Double-checked locking\"\u662f\u4e00\u79cd\u52a0\u9501\u7684\u65b9\u5f0f\uff0c\u800c\u4e0d\u662f\u67d0\u79cdlock The pattern, when implemented in some language/hardware combinations, can be unsafe. At times, it can be considered an anti-pattern .[ 2] NOTE: 1\u3001\u867d\u7136\u539f\u6587\u7ed9\u51fa\u4e86reference\uff0c\u6211\u76ee\u524d\u9605\u8bfb\u7684\u6700\u597d\u7684\u6587\u7ae0\u662f: aristeia C++ and the Perils of Double-Checked Locking It is typically used to reduce locking overhead when implementing \" lazy initialization \" in a multi-threaded environment, especially as part of the Singleton pattern . Lazy initialization avoids initializing a value until the first time it is accessed. NOTE: 1\u3001\u8fd9\u6bb5\u8bdd\uff0c\u5bf9lazy initialization\u7684\u89e3\u91ca\u975e\u5e38\u597d","title":"wikipedia Double-checked locking"},{"location":"Concurrent-computing/Concurrency-control/Lock/Double-checked-locking/#usage#in#c11","text":"For the singleton pattern, double-checked locking is not needed: If control enters the declaration concurrently while the variable is being initialized, the concurrent execution shall wait for completion of the initialization. \u2014\u2009\u00a7 6.7 [stmt.dcl] p4 Singleton & GetInstance () { static Singleton s ; return s ; } C++11 and beyond also provide a built-in double-checked locking pattern in the form of std::once_flag and std::call_once : #include <mutex> #include <optional> // Since C++17 // Singleton.h class Singleton { public : static Singleton * GetInstance (); private : Singleton () = default ; static std :: optional < Singleton > s_instance ; static std :: once_flag s_flag ; }; // Singleton.cpp std :: optional < Singleton > Singleton :: s_instance ; std :: once_flag Singleton :: s_flag { }; Singleton * Singleton :: GetInstance () { std :: call_once ( Singleton :: s_flag , []() { s_instance . emplace ( Singleton {});}); return &* s_instance ; } If one truly wishes to use the double-checked idiom instead of the trivially working example above (for instance because Visual Studio before the 2015 release did not implement the C++11 standard's language about concurrent initialization quoted above [ 3] ), one needs to use acquire and release fences:[ 4] #include <atomic> #include <mutex> class Singleton { public : static Singleton * GetInstance (); private : Singleton () = default ; static std :: atomic < Singleton *> s_instance ; static std :: mutex s_mutex ; }; Singleton * Singleton :: GetInstance () { Singleton * p = s_instance . load ( std :: memory_order_acquire ); if ( p == nullptr ) { // 1st check std :: lock_guard < std :: mutex > lock ( s_mutex ); p = s_instance . load ( std :: memory_order_relaxed ); if ( p == nullptr ) { // 2nd (double) check p = new Singleton (); s_instance . store ( p , std :: memory_order_release ); } } return p ; }","title":"Usage in C++11"},{"location":"Concurrent-computing/Concurrency-control/Lock/Double-checked-locking/#_1","text":"\u5728\u5de5\u7a0bprogramming-language\u7684 C-family-language\\C++\\Pattern\\Singleton \u7ae0\u8282\u4e5f\u5bf9\u5b83\u8fdb\u884c\u4e86\u8ba8\u8bba\uff0c\u5176\u4e2d\u4e3b\u8981\u6536\u5f55\u4e86\u5982\u4e0b\u6587\u7ae0\uff1a 1\u3001aristeia C++ and the Perils of Double-Checked Locking","title":"\u5176\u4ed6\u8d44\u6e90"},{"location":"Concurrent-computing/Concurrency-control/Lock/Evaluation-metric/","text":"Evaluation metrics for lock \u9501\u7684\u8bc4\u4f30\u6307\u6807\u3002 wikipedia Test-and-set # Performance evaluation of test-and-set locks The four major evaluation metrics for locks in general are : 1\u3001uncontended lock-acquisition latency NOTE: 1\u3001\u5728\u6ca1\u6709contention\u7684\u60c5\u51b5\u4e0b\uff0c\u83b7\u5f97\u6240\u7684\u5ef6\u65f6 bus traffic, NOTE: \u4e00\u3001\u6700\u6700\u5178\u578b\u7684\u662fTAS spin lock->TTAS spin lock->MCS Lock\u3001CLH Lock \u5b83\u4eec\u7684bus traffic\u90fd\u5728\u964d\u4f4e \u4e8c\u3001\u5173\u4e8ebus traffic\uff0c\u53c2\u89c1 1\u3001csdn \u804a\u804a\u9ad8\u5e76\u53d1\uff08\u4e94\uff09\u7406\u89e3\u7f13\u5b58\u4e00\u81f4\u6027\u534f\u8bae\u4ee5\u53ca\u5bf9\u5e76\u53d1\u7f16\u7a0b\u7684\u5f71\u54cd \u4e09\u3001bus traffic\u662f\u5f71\u54cd\"scalability\"\u7684\u91cd\u8981\u56e0\u7d20 fairness, and NOTE: 1\u3001\u516c\u5e73\u6027\uff0c\u4e00\u822c\u4f7f\u7528FIFO ordering \"tag-fairness-\u516c\u5e73\u6027-starvation-free-fair to lock acquisition\" storage.[ 7] NOTE: \u8865\u5145\u7684\u8bc4\u5224\u6307\u6807 \u4e00\u3001user space VS kernel space \u4e8c\u3001block VS non blocking Scalability \"scalability\"\u662f\u5728spin lock\u7684\u4f18\u5316\u8bba\u6587\u4e2d\u63d0\u7684\u975e\u5e38\u591a\u7684\u4e00\u4e2a\u5185\u5bb9\uff0c\u5b83\u548c\u5982\u4e0b\u5185\u5bb9\u6709\u5173: 1\u3001bus traffic","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Lock/Evaluation-metric/#evaluation#metrics#for#lock","text":"\u9501\u7684\u8bc4\u4f30\u6307\u6807\u3002","title":"Evaluation metrics for lock"},{"location":"Concurrent-computing/Concurrency-control/Lock/Evaluation-metric/#wikipedia#test-and-set#performance#evaluation#of#test-and-set#locks","text":"The four major evaluation metrics for locks in general are : 1\u3001uncontended lock-acquisition latency NOTE: 1\u3001\u5728\u6ca1\u6709contention\u7684\u60c5\u51b5\u4e0b\uff0c\u83b7\u5f97\u6240\u7684\u5ef6\u65f6 bus traffic, NOTE: \u4e00\u3001\u6700\u6700\u5178\u578b\u7684\u662fTAS spin lock->TTAS spin lock->MCS Lock\u3001CLH Lock \u5b83\u4eec\u7684bus traffic\u90fd\u5728\u964d\u4f4e \u4e8c\u3001\u5173\u4e8ebus traffic\uff0c\u53c2\u89c1 1\u3001csdn \u804a\u804a\u9ad8\u5e76\u53d1\uff08\u4e94\uff09\u7406\u89e3\u7f13\u5b58\u4e00\u81f4\u6027\u534f\u8bae\u4ee5\u53ca\u5bf9\u5e76\u53d1\u7f16\u7a0b\u7684\u5f71\u54cd \u4e09\u3001bus traffic\u662f\u5f71\u54cd\"scalability\"\u7684\u91cd\u8981\u56e0\u7d20 fairness, and NOTE: 1\u3001\u516c\u5e73\u6027\uff0c\u4e00\u822c\u4f7f\u7528FIFO ordering \"tag-fairness-\u516c\u5e73\u6027-starvation-free-fair to lock acquisition\" storage.[ 7] NOTE:","title":"wikipedia Test-and-set # Performance evaluation of test-and-set locks"},{"location":"Concurrent-computing/Concurrency-control/Lock/Evaluation-metric/#_1","text":"\u4e00\u3001user space VS kernel space \u4e8c\u3001block VS non blocking","title":"\u8865\u5145\u7684\u8bc4\u5224\u6307\u6807"},{"location":"Concurrent-computing/Concurrency-control/Lock/Evaluation-metric/#scalability","text":"\"scalability\"\u662f\u5728spin lock\u7684\u4f18\u5316\u8bba\u6587\u4e2d\u63d0\u7684\u975e\u5e38\u591a\u7684\u4e00\u4e2a\u5185\u5bb9\uff0c\u5b83\u548c\u5982\u4e0b\u5185\u5bb9\u6709\u5173: 1\u3001bus traffic","title":"Scalability"},{"location":"Concurrent-computing/Concurrency-control/Lock/Granularity-of-lock/","text":"Granularity of lock \u5f15\u53d1\u6211\u5bf9\u8fd9\u4e2a\u95ee\u9898\u601d\u8003\u7684\u662f\u5bf9\u4e0b\u9762\u4ee3\u7801\u7684\u9605\u8bfb\uff1a std :: map < int , SubRequestMap >:: const_iterator citerSubRequestMap = CHqAccessFlow :: GetInstance () -> m_SubRequestMapByMkt . find ( iMktType ); if ( citerSubRequestMap != CHqAccessFlow :: GetInstance () -> m_SubRequestMapByMkt . end ()) { // \u672a\u4f7f\u7528\u51fd\u6570\u65b9\u5f0f\u8c03\u7528\u6dfb\u52a0\u672a\u5c31\u7eea\u72b6\u6001\u7684\u539f\u56e0(\u8be5\u5904\u60c5\u666f\uff1a\u6b63\u5e38\u60c5\u51b5\u5728\u6b64\u5904\u4f1a\u6709\u4e0a\u5343\u4e2a\u5238\u88ab\u7f6e\u4f4d\u3002) // * \u51cf\u5c11\u52a0\u51cf\u9501\u7684\u6b21\u6570\u3002 // * \u9632\u6b62\u5728\u7f6e\u4f4d\u65f6\uff0c\u5176\u4ed6\u5730\u65b9\u89e6\u53d1\u8bfb\u9501\u3002\u5bfc\u81f4\u6570\u636e\u4e0d\u4e00\u81f4\u548c\u5f53\u524d\u51fd\u6570\u54cd\u5e94\u8fc7\u6162\u3002 CReadWriteLock_AUTO_WRLock WRLock ( & CHqAccessFlow :: GetInstance () -> m_hUnreadyStock ); for ( SubRequestMap :: const_iterator citerSubRequest = citerSubRequestMap -> second . begin (); citerSubRequest != citerSubRequestMap -> second . end (); ++ citerSubRequest ) { CHqAccessFlow :: GetInstance () -> m_UnreadyStock [ iMktType ][ citerSubRequest -> first . c_str ()] = citerSubRequest -> second ; } } \u4e0a\u8ff0\u4ee3\u7801\u7684\u6ce8\u91ca\u6240\u8ba8\u8bba\u7684\u95ee\u9898\u662f\u548cgranularity of lock\u76f8\u5173\u7684\uff0c\u663e\u7136\uff0c\u4e0a\u8ff0\u4ee3\u7801\u4e2dlock\u7684\u7c92\u5ea6\u76f8\u8f83\u4e8e\u5bf9\u5206\u522b\u6bcf\u4e2a\u5238\u52a0\u9501\u3001\u64cd\u4f5c\u3001\u89e3\u9501\u7684\u7c92\u5ea6\u8981\u66f4\u5927\u4e00\u4e9b","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Lock/Granularity-of-lock/#granularity#of#lock","text":"\u5f15\u53d1\u6211\u5bf9\u8fd9\u4e2a\u95ee\u9898\u601d\u8003\u7684\u662f\u5bf9\u4e0b\u9762\u4ee3\u7801\u7684\u9605\u8bfb\uff1a std :: map < int , SubRequestMap >:: const_iterator citerSubRequestMap = CHqAccessFlow :: GetInstance () -> m_SubRequestMapByMkt . find ( iMktType ); if ( citerSubRequestMap != CHqAccessFlow :: GetInstance () -> m_SubRequestMapByMkt . end ()) { // \u672a\u4f7f\u7528\u51fd\u6570\u65b9\u5f0f\u8c03\u7528\u6dfb\u52a0\u672a\u5c31\u7eea\u72b6\u6001\u7684\u539f\u56e0(\u8be5\u5904\u60c5\u666f\uff1a\u6b63\u5e38\u60c5\u51b5\u5728\u6b64\u5904\u4f1a\u6709\u4e0a\u5343\u4e2a\u5238\u88ab\u7f6e\u4f4d\u3002) // * \u51cf\u5c11\u52a0\u51cf\u9501\u7684\u6b21\u6570\u3002 // * \u9632\u6b62\u5728\u7f6e\u4f4d\u65f6\uff0c\u5176\u4ed6\u5730\u65b9\u89e6\u53d1\u8bfb\u9501\u3002\u5bfc\u81f4\u6570\u636e\u4e0d\u4e00\u81f4\u548c\u5f53\u524d\u51fd\u6570\u54cd\u5e94\u8fc7\u6162\u3002 CReadWriteLock_AUTO_WRLock WRLock ( & CHqAccessFlow :: GetInstance () -> m_hUnreadyStock ); for ( SubRequestMap :: const_iterator citerSubRequest = citerSubRequestMap -> second . begin (); citerSubRequest != citerSubRequestMap -> second . end (); ++ citerSubRequest ) { CHqAccessFlow :: GetInstance () -> m_UnreadyStock [ iMktType ][ citerSubRequest -> first . c_str ()] = citerSubRequest -> second ; } } \u4e0a\u8ff0\u4ee3\u7801\u7684\u6ce8\u91ca\u6240\u8ba8\u8bba\u7684\u95ee\u9898\u662f\u548cgranularity of lock\u76f8\u5173\u7684\uff0c\u663e\u7136\uff0c\u4e0a\u8ff0\u4ee3\u7801\u4e2dlock\u7684\u7c92\u5ea6\u76f8\u8f83\u4e8e\u5bf9\u5206\u522b\u6bcf\u4e2a\u5238\u52a0\u9501\u3001\u64cd\u4f5c\u3001\u89e3\u9501\u7684\u7c92\u5ea6\u8981\u66f4\u5927\u4e00\u4e9b","title":"Granularity of lock"},{"location":"Concurrent-computing/Concurrency-control/Lock/Linux-Seqlock/","text":"Seqlock \u662f\u5728\u9605\u8bfb P1478R1: Byte-wise atomic memcpy \u53d1\u73b0\u7684\u5b83\u3002 wowotech Linux\u5185\u6838\u540c\u6b65\u673a\u5236\u4e4b\uff08\u516d\uff09\uff1aSeqlock \u666e\u901a\u7684spin lock\u5bf9\u5f85reader\u548cwriter\u662f\u4e00\u89c6\u540c\u4ec1\uff0cRW spin lock\u7ed9reader\u8d4b\u4e88\u4e86\u66f4\u9ad8\u7684\u4f18\u5148\u7ea7\uff0c\u90a3\u4e48\u6709\u6ca1\u6709\u8ba9writer\u4f18\u5148\u7684\u9501\u7684\u673a\u5236\u5462\uff1f\u7b54\u6848\u5c31\u662fseqlock\u3002 wikipedia Seqlock A seqlock (short for sequence lock ) is a special locking mechanism used in Linux for supporting fast writes of shared variables between two parallel operating system routines. The semantics stabilized as of version 2.5.59, and they are present in the 2.6.x stable kernel series. stackoverflow Why rwlock is more popular than seqlock in linux kernel? [closed] A A seqlock has a strong limitation , that readers should correctly work with inconsistent data . Not every processing algorithm allows inconstistent data. In most cases, such data can only be numbers : integers, booleans, etc. They rarely can be pointers, because a stale pointer may point to the memory which is already freed, so dereferencing such pointer is no-no. Locks (and rw-locks among them) doesn't have \"inconsitent data\" limitations, so they can be used in much more cases. Example of inconstisten data under seqlock Assume there are two structure's fields protected by the single seqlock. The first field, a is incremented by each \"write\", the second field, b is decremented by each \"write\". Both fields initially are 0 . On may assume, that a reader will always find a + b to be 0 . But in case of seqlock, this is not true. E.g., between reading a and b it could a \"write\", so a value will be old, and b value will be new, and a + b gives -1 .","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Lock/Linux-Seqlock/#seqlock","text":"\u662f\u5728\u9605\u8bfb P1478R1: Byte-wise atomic memcpy \u53d1\u73b0\u7684\u5b83\u3002","title":"Seqlock"},{"location":"Concurrent-computing/Concurrency-control/Lock/Linux-Seqlock/#wowotech#linuxseqlock","text":"\u666e\u901a\u7684spin lock\u5bf9\u5f85reader\u548cwriter\u662f\u4e00\u89c6\u540c\u4ec1\uff0cRW spin lock\u7ed9reader\u8d4b\u4e88\u4e86\u66f4\u9ad8\u7684\u4f18\u5148\u7ea7\uff0c\u90a3\u4e48\u6709\u6ca1\u6709\u8ba9writer\u4f18\u5148\u7684\u9501\u7684\u673a\u5236\u5462\uff1f\u7b54\u6848\u5c31\u662fseqlock\u3002","title":"wowotech Linux\u5185\u6838\u540c\u6b65\u673a\u5236\u4e4b\uff08\u516d\uff09\uff1aSeqlock"},{"location":"Concurrent-computing/Concurrency-control/Lock/Linux-Seqlock/#wikipedia#seqlock","text":"A seqlock (short for sequence lock ) is a special locking mechanism used in Linux for supporting fast writes of shared variables between two parallel operating system routines. The semantics stabilized as of version 2.5.59, and they are present in the 2.6.x stable kernel series.","title":"wikipedia Seqlock"},{"location":"Concurrent-computing/Concurrency-control/Lock/Linux-Seqlock/#stackoverflow#why#rwlock#is#more#popular#than#seqlock#in#linux#kernel#closed","text":"A A seqlock has a strong limitation , that readers should correctly work with inconsistent data . Not every processing algorithm allows inconstistent data. In most cases, such data can only be numbers : integers, booleans, etc. They rarely can be pointers, because a stale pointer may point to the memory which is already freed, so dereferencing such pointer is no-no. Locks (and rw-locks among them) doesn't have \"inconsitent data\" limitations, so they can be used in much more cases.","title":"stackoverflow Why rwlock is more popular than seqlock in linux kernel? [closed]"},{"location":"Concurrent-computing/Concurrency-control/Lock/Linux-Seqlock/#example#of#inconstisten#data#under#seqlock","text":"Assume there are two structure's fields protected by the single seqlock. The first field, a is incremented by each \"write\", the second field, b is decremented by each \"write\". Both fields initially are 0 . On may assume, that a reader will always find a + b to be 0 . But in case of seqlock, this is not true. E.g., between reading a and b it could a \"write\", so a value will be old, and b value will be new, and a + b gives -1 .","title":"Example of inconstisten data under seqlock"},{"location":"Concurrent-computing/Concurrency-control/Lock/Read%E2%80%93write-lock/","text":"Readers\u2013writer lock wikipedia Readers\u2013writer lock In computer science , a readers\u2013writer ( single-writer lock,[ 1] a multi-reader lock,[ 2] a push lock ,[ 3] or an MRSW lock ) is a synchronization primitive that solves one of the readers\u2013writers problems . An RW lock allows concurrent access for read-only operations, while write operations require exclusive access. This means that multiple threads can read the data in parallel but an exclusive lock is needed for writing or modifying data. When a writer is writing the data, all other writers or readers will be blocked until the writer is finished writing. A common use might be to control access to a data structure in memory that cannot be updated atomically and is invalid (and should not be read by another thread) until the update is complete. Priority policies NOTE: \u53ef\u4ee5\u6839\u636eread and write\u6765\u9009\u62e9priority\u3002","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Lock/Read%E2%80%93write-lock/#readerswriter#lock","text":"","title":"Readers\u2013writer lock"},{"location":"Concurrent-computing/Concurrency-control/Lock/Read%E2%80%93write-lock/#wikipedia#readerswriter#lock","text":"In computer science , a readers\u2013writer ( single-writer lock,[ 1] a multi-reader lock,[ 2] a push lock ,[ 3] or an MRSW lock ) is a synchronization primitive that solves one of the readers\u2013writers problems . An RW lock allows concurrent access for read-only operations, while write operations require exclusive access. This means that multiple threads can read the data in parallel but an exclusive lock is needed for writing or modifying data. When a writer is writing the data, all other writers or readers will be blocked until the writer is finished writing. A common use might be to control access to a data structure in memory that cannot be updated atomically and is invalid (and should not be read by another thread) until the update is complete.","title":"wikipedia Readers\u2013writer lock"},{"location":"Concurrent-computing/Concurrency-control/Lock/Read%E2%80%93write-lock/#priority#policies","text":"NOTE: \u53ef\u4ee5\u6839\u636eread and write\u6765\u9009\u62e9priority\u3002","title":"Priority policies"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/","text":"Spinlock wikipedia Spinlock In software engineering , a spinlock is a lock which causes a thread trying to acquire it to simply wait in a loop (\"spin\") while repeatedly checking if the lock is available. Since the thread remains active but is not performing a useful task, the use of such a lock is a kind of busy waiting . Once acquired, spinlocks will usually be held until they are explicitly released, although in some implementations they may be automatically released if the thread being waited on (the one which holds the lock) blocks, or \"goes to sleep\". stackoverflow What exactly are \u201cspin-locks\u201d? A NOTE: \u89e3\u91ca\u5f97\u975e\u5e38\u597d When you use regular locks (mutexes, critical sections etc), operating system puts your thread in the WAIT state and preempts it by scheduling other threads on the same core. This has a performance penalty if the wait time is really short, because your thread now has to wait for a preemption to receive CPU time again. Besides, kernel objects are not available in every state of the kernel, such as in an interrupt handler or when paging is not available etc. Spinlocks don't cause preemption but wait in a loop (\"spin\") till the other core releases the lock. This prevents the thread from losing its quantum and continue as soon as the lock gets released. The simple mechanism of spinlocks allows a kernel to utilize it in almost any state. That's why on a single core machine a spinlock is simply a \"disable interrupts\" or \"raise IRQL\" which prevents thread scheduling completely. Spinlocks ultimately allow kernels to avoid \"Big Kernel Lock\"s (a lock acquired when core enters kernel and released at the exit) and have granular locking over kernel primitives, causing better multi-processing on multi-core machines thus better performance. EDIT : A question came up: \"Does that mean I should use spinlocks wherever possible?\" and I'll try to answer it: As I mentioned, Spinlocks are only useful in places where anticipated waiting time is shorter than a quantum (read: milliseconds) and preemption doesn't make much sense (e.g. kernel objects aren't available). If waiting time is unknown, or if you're in user mode Spinlocks aren't efficient. You consume 100% CPU time on the waiting core while checking if a spinlock is available. You prevent other threads from running on that core till your quantum expires. This scenario is only feasible for short bursts at kernel level and unlikely an option for a user-mode application. Here is a question on SO addressing that: Spinlocks, How Useful Are They? stackoverflow Spinlocks, How Useful Are They? \u5bf9spinning lock\u7684optimization \u53c2\u89c1 Spinning-lock-optimization \u7ae0\u8282\u3002 Implementation stackoverflow x86 spinlock using cmpxchg bogachevdmitry / CLHLock \u63cf\u8ff0\u4e86 CHL-lock algorithm \u7684\u539f\u7406\u3002 MCS-lock honkiko / Multi-Core-Toolbox \u5b9e\u73b0\u4e86 MCS-lock \u3002 ntrivix / MCSLock-C MCS locking algorithm with timeouts avionipevaju / MCSTimeoutLock MCS Lock algorithm implementation in C NOTE: with timeout per-framework / mcs.cpp geidav / spinlocks-bench Implementation and benchmark of different spin lock types \u5176\u4e2d\u5c31\u7ed9\u51fa\u4e86MCS spinning lock\u7684\u5b9e\u73b0\uff0c\u53c2\u89c1 library-geidav-spinlocks-bench \u7ae0\u8282\u3002 NoiseEHC / spinpool tick lock https://github.com/search?l=C&q=ticket+lock&type=Repositories jason741852 / c_locks Implementations of spin lock, exponential backoff lock and queue lock luapvu / lock-algorithms \u5176\u4e2d\u6709benchmark\u3002 shines77 / RingQueue TODO 1\u30010xax.gitbooks Synchronization primitives in the Linux kernel. Part 2.","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/#spinlock","text":"","title":"Spinlock"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/#wikipedia#spinlock","text":"In software engineering , a spinlock is a lock which causes a thread trying to acquire it to simply wait in a loop (\"spin\") while repeatedly checking if the lock is available. Since the thread remains active but is not performing a useful task, the use of such a lock is a kind of busy waiting . Once acquired, spinlocks will usually be held until they are explicitly released, although in some implementations they may be automatically released if the thread being waited on (the one which holds the lock) blocks, or \"goes to sleep\".","title":"wikipedia Spinlock"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/#stackoverflow#what#exactly#are#spin-locks","text":"A NOTE: \u89e3\u91ca\u5f97\u975e\u5e38\u597d When you use regular locks (mutexes, critical sections etc), operating system puts your thread in the WAIT state and preempts it by scheduling other threads on the same core. This has a performance penalty if the wait time is really short, because your thread now has to wait for a preemption to receive CPU time again. Besides, kernel objects are not available in every state of the kernel, such as in an interrupt handler or when paging is not available etc. Spinlocks don't cause preemption but wait in a loop (\"spin\") till the other core releases the lock. This prevents the thread from losing its quantum and continue as soon as the lock gets released. The simple mechanism of spinlocks allows a kernel to utilize it in almost any state. That's why on a single core machine a spinlock is simply a \"disable interrupts\" or \"raise IRQL\" which prevents thread scheduling completely. Spinlocks ultimately allow kernels to avoid \"Big Kernel Lock\"s (a lock acquired when core enters kernel and released at the exit) and have granular locking over kernel primitives, causing better multi-processing on multi-core machines thus better performance. EDIT : A question came up: \"Does that mean I should use spinlocks wherever possible?\" and I'll try to answer it: As I mentioned, Spinlocks are only useful in places where anticipated waiting time is shorter than a quantum (read: milliseconds) and preemption doesn't make much sense (e.g. kernel objects aren't available). If waiting time is unknown, or if you're in user mode Spinlocks aren't efficient. You consume 100% CPU time on the waiting core while checking if a spinlock is available. You prevent other threads from running on that core till your quantum expires. This scenario is only feasible for short bursts at kernel level and unlikely an option for a user-mode application. Here is a question on SO addressing that: Spinlocks, How Useful Are They?","title":"stackoverflow What exactly are \u201cspin-locks\u201d?"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/#stackoverflow#spinlocks#how#useful#are#they","text":"","title":"stackoverflow Spinlocks, How Useful Are They?"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/#spinning#lockoptimization","text":"\u53c2\u89c1 Spinning-lock-optimization \u7ae0\u8282\u3002","title":"\u5bf9spinning lock\u7684optimization"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/#implementation","text":"stackoverflow x86 spinlock using cmpxchg","title":"Implementation"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/#bogachevdmitryclhlock","text":"\u63cf\u8ff0\u4e86 CHL-lock algorithm \u7684\u539f\u7406\u3002","title":"bogachevdmitry/CLHLock"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/#mcs-lock","text":"","title":"MCS-lock"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/#honkikomulti-core-toolbox","text":"\u5b9e\u73b0\u4e86 MCS-lock \u3002","title":"honkiko/Multi-Core-Toolbox"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/#ntrivixmcslock-c","text":"MCS locking algorithm with timeouts","title":"ntrivix/MCSLock-C"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/#avionipevajumcstimeoutlock","text":"MCS Lock algorithm implementation in C NOTE: with timeout","title":"avionipevaju/MCSTimeoutLock"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/#per-frameworkmcscpp","text":"","title":"per-framework/mcs.cpp"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/#geidavspinlocks-bench","text":"Implementation and benchmark of different spin lock types \u5176\u4e2d\u5c31\u7ed9\u51fa\u4e86MCS spinning lock\u7684\u5b9e\u73b0\uff0c\u53c2\u89c1 library-geidav-spinlocks-bench \u7ae0\u8282\u3002","title":"geidav/spinlocks-bench"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/#noiseehcspinpool","text":"","title":"NoiseEHC/spinpool"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/#tick#lock","text":"https://github.com/search?l=C&q=ticket+lock&type=Repositories","title":"tick lock"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/#jason741852c_locks","text":"Implementations of spin lock, exponential backoff lock and queue lock","title":"jason741852/c_locks"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/#luapvulock-algorithms","text":"\u5176\u4e2d\u6709benchmark\u3002","title":"luapvu/lock-algorithms"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/#shines77ringqueue","text":"","title":"shines77/RingQueue"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/#todo","text":"1\u30010xax.gitbooks Synchronization primitives in the Linux kernel. Part 2.","title":"TODO"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/Spinning-lock-optimization/","text":"\u5bf9spinning lock\u7684optimization 1\u3001\u6211\u5728\u601d\u8003\"spinning lock with timeout\"\u7684\u65f6\u5019\uff0c\u901a\u8fc7Google \"spinning lock with timeout\"\u53d1\u73b0\u7684\u8fd9\u4e2atopic 2\u3001\u540e\u6765\u53d1\u73b0\u4e86\u5f88\u591a\u7684\u8d44\u6e90\uff0c\u6211\u89c9\u5f97\u5e94\u8be5\u4ece\u9010\u6b65\u4f18\u5316\u7684\u89d2\u5ea6\u6765\u7ec4\u7ec7\u5185\u5bb9 \u7efc\u8ff0\u7684\u3001\u68b3\u7406\u6027\u7684\u3001\u5bfc\u8bfb\u6027\u7684\u6559\u7a0b cs.tau Multiprocessor Programming # Lecture 7: Spin Locks and Contention Management NOTE: 1\u3001\u53c2\u89c1 Course-tau-Multiprocessor-Programming-CS-0368-4061-01 \u7ae0\u8282 \u5176\u4e2d\u5185\u5bb9\u6bd4\u8f83\u4e0d\u9519\uff0c\u68b3\u7406\u4e86: TASLock ->TTASLock->TTASLock with exponential delay->Queue-based Locks: a\u3001An Array Based Queue Lock(A-lock) b\u3001The MCS queue lock(CLH lock of Craig and Landin and Hagersten) c\u3001The CLH queue lock(Mellor-Crummey and Scott provided the popular list based MCS Queue lock) 7.10 Chapter Notes The TTASLock is due to Kruskal and Rudolph and Snir. Exponential backoff is a well known technique used in Ethernet routing. Anderson was one of the first to empirically(\u4ee5\u7ecf\u9a8c\u4e3a\u4e3b\u7684) study contention in shared memory multiprocessors, introducing the A-lock, the first queue lock which was array based. Many of the graphs in this document are idealizations of his actual empirical results on the Sequent Symmetry. Mellor-Crummey and Scott provided the popular list based MCS Queue lock, later improved upon by the CLH lock of Craig and Landin and Hagersten. The NUMA machine graphs we show are idealizations of the experiments of Mellor-Crummey and Scott on the BBN butterfly machine. The modern machine graphs are due to Scott. \u603b\u7ed3\u5730\u6bd4\u8f83\u597d\u3002 \u5e76\u4e14\u5176\u4e2d\u8fd8\u63d0\u4f9b\u4e86C++ source code\u3002 Expert-iter_zc \u53c2\u89c1\u76f8\u5e94\u7ae0\u8282\u3002 rochester Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors 1\u3001\u68b3\u7406\u5f97\u975e\u5e38\u597d disco.ethz Locking Part 2, Chapter 11 1\u3001\u53c2\u89c1 Course-disco.ethz-Distributed-Systems \u7ae0\u8282\u3002 Optimization: TASLock ->TTASLock 1\u3001spinning lock\u7684\u52a3\u52bf\u3001\u5bf9spinning lock\u7684\u7b2c\u4e00\u6b21\u4f18\u5316 2\u3001TASLock ->TTASLock \u53c2\u8003\u5185\u5bb9: 1\u3001csdn iter_zc \u804a\u804a\u9ad8\u5e76\u53d1\uff08\u516d\uff09\u5b9e\u73b0\u51e0\u79cd\u81ea\u65cb\u9501\uff08\u4e00\uff09 TAS vs TTAS stackoverflow Better explanations of TAS Vs. TTAS in the context of \u201cThe Art Of Multiprocessor Programming\u201d Herlihy & Shavits Optimization: TTASLock with exponential delay Optimization: Queue-based Locks wikipedia Array_Based_Queuing_Locks Optimization: Queue-based Locks with timeout rochester Scalable Queue-Based Spin Locks with Timeout \u8fd9\u662f\u975e\u5e38\u597d\u7684\u8d44\u6e90: \u4e00\u3001\u5b83\u603b\u7ed3\u4e86\u524d\u4eba\u7684\u5de5\u4f5c \u4e8c\u3001\u5b83\u6dfb\u52a0\u4e0a\u4e86timeout\uff0c\u8fd9\u53c8\u662f\u4e00\u6b21\u4f18\u5316\uff0c\u5e76\u4e14\u8fd8\u7ed9\u51fa\u4e86\u4ee3\u7801 1\u3001CLH queue-based spin lock with timeout 2\u3001MCS queue-based spin lock with timeout 2\u3001 https://patents.google.com/patent/US6965961B1/en \u8c37\u6b4c\u7684\u4e13\u5229","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/Spinning-lock-optimization/#spinning#lockoptimization","text":"1\u3001\u6211\u5728\u601d\u8003\"spinning lock with timeout\"\u7684\u65f6\u5019\uff0c\u901a\u8fc7Google \"spinning lock with timeout\"\u53d1\u73b0\u7684\u8fd9\u4e2atopic 2\u3001\u540e\u6765\u53d1\u73b0\u4e86\u5f88\u591a\u7684\u8d44\u6e90\uff0c\u6211\u89c9\u5f97\u5e94\u8be5\u4ece\u9010\u6b65\u4f18\u5316\u7684\u89d2\u5ea6\u6765\u7ec4\u7ec7\u5185\u5bb9","title":"\u5bf9spinning lock\u7684optimization"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/Spinning-lock-optimization/#_1","text":"","title":"\u7efc\u8ff0\u7684\u3001\u68b3\u7406\u6027\u7684\u3001\u5bfc\u8bfb\u6027\u7684\u6559\u7a0b"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/Spinning-lock-optimization/#cstau#multiprocessor#programming#lecture#7#spin#locks#and#contention#management","text":"NOTE: 1\u3001\u53c2\u89c1 Course-tau-Multiprocessor-Programming-CS-0368-4061-01 \u7ae0\u8282 \u5176\u4e2d\u5185\u5bb9\u6bd4\u8f83\u4e0d\u9519\uff0c\u68b3\u7406\u4e86: TASLock ->TTASLock->TTASLock with exponential delay->Queue-based Locks: a\u3001An Array Based Queue Lock(A-lock) b\u3001The MCS queue lock(CLH lock of Craig and Landin and Hagersten) c\u3001The CLH queue lock(Mellor-Crummey and Scott provided the popular list based MCS Queue lock) 7.10 Chapter Notes The TTASLock is due to Kruskal and Rudolph and Snir. Exponential backoff is a well known technique used in Ethernet routing. Anderson was one of the first to empirically(\u4ee5\u7ecf\u9a8c\u4e3a\u4e3b\u7684) study contention in shared memory multiprocessors, introducing the A-lock, the first queue lock which was array based. Many of the graphs in this document are idealizations of his actual empirical results on the Sequent Symmetry. Mellor-Crummey and Scott provided the popular list based MCS Queue lock, later improved upon by the CLH lock of Craig and Landin and Hagersten. The NUMA machine graphs we show are idealizations of the experiments of Mellor-Crummey and Scott on the BBN butterfly machine. The modern machine graphs are due to Scott. \u603b\u7ed3\u5730\u6bd4\u8f83\u597d\u3002 \u5e76\u4e14\u5176\u4e2d\u8fd8\u63d0\u4f9b\u4e86C++ source code\u3002","title":"cs.tau Multiprocessor Programming # Lecture 7: Spin Locks and Contention Management"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/Spinning-lock-optimization/#expert-iter_zc","text":"\u53c2\u89c1\u76f8\u5e94\u7ae0\u8282\u3002","title":"Expert-iter_zc"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/Spinning-lock-optimization/#rochester#algorithms#for#scalable#synchronization#on#shared-memory#multiprocessors","text":"1\u3001\u68b3\u7406\u5f97\u975e\u5e38\u597d","title":"rochester Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/Spinning-lock-optimization/#discoethz#locking#part#2#chapter#11","text":"1\u3001\u53c2\u89c1 Course-disco.ethz-Distributed-Systems \u7ae0\u8282\u3002","title":"disco.ethz Locking Part 2, Chapter 11"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/Spinning-lock-optimization/#optimization#taslock#-ttaslock","text":"1\u3001spinning lock\u7684\u52a3\u52bf\u3001\u5bf9spinning lock\u7684\u7b2c\u4e00\u6b21\u4f18\u5316 2\u3001TASLock ->TTASLock \u53c2\u8003\u5185\u5bb9: 1\u3001csdn iter_zc \u804a\u804a\u9ad8\u5e76\u53d1\uff08\u516d\uff09\u5b9e\u73b0\u51e0\u79cd\u81ea\u65cb\u9501\uff08\u4e00\uff09","title":"Optimization: TASLock -&gt;TTASLock"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/Spinning-lock-optimization/#tas#vs#ttas","text":"stackoverflow Better explanations of TAS Vs. TTAS in the context of \u201cThe Art Of Multiprocessor Programming\u201d Herlihy & Shavits","title":"TAS vs TTAS"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/Spinning-lock-optimization/#optimization#ttaslock#with#exponential#delay","text":"","title":"Optimization: TTASLock with exponential delay"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/Spinning-lock-optimization/#optimization#queue-based#locks","text":"wikipedia Array_Based_Queuing_Locks","title":"Optimization: Queue-based Locks"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/Spinning-lock-optimization/#optimization#queue-based#locks#with#timeout","text":"","title":"Optimization: Queue-based Locks with timeout"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/Spinning-lock-optimization/#rochester#scalable#queue-based#spin#locks#with#timeout","text":"\u8fd9\u662f\u975e\u5e38\u597d\u7684\u8d44\u6e90: \u4e00\u3001\u5b83\u603b\u7ed3\u4e86\u524d\u4eba\u7684\u5de5\u4f5c \u4e8c\u3001\u5b83\u6dfb\u52a0\u4e0a\u4e86timeout\uff0c\u8fd9\u53c8\u662f\u4e00\u6b21\u4f18\u5316\uff0c\u5e76\u4e14\u8fd8\u7ed9\u51fa\u4e86\u4ee3\u7801 1\u3001CLH queue-based spin lock with timeout 2\u3001MCS queue-based spin lock with timeout 2\u3001 https://patents.google.com/patent/US6965961B1/en \u8c37\u6b4c\u7684\u4e13\u5229","title":"rochester Scalable Queue-Based Spin Locks with Timeout"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/Ticket-lock/","text":"Ticket lock \u662f\u5728\u601d\u8003 \"spinning lock with timeout\" \u65f6\uff0c\u60f3\u5230\u7684\u8fd9\u4e2a\u95ee\u9898 wikipedia Ticket lock NOTE: 1\u3001\u975e\u5e38\u7c7b\u4f3c\u4e8e\u5728\u94f6\u884c\u91cc\u9762\"\u53d6\u53f7\u6392\u961f\u3001\u53eb\u53f7\u670d\u52a1\"\u7684\u65b9\u5f0f\uff0c\u8fd9\u662f\u5178\u578b\u7684one to many 2\u3001\u539f\u7406\u975e\u5e38\u7b80\u5355\u3001\u5b9e\u73b0\u4e5f\u975e\u5e38\u7b80\u5355\uff0c\u540c\u65f6\u6709\u7740\u975e\u5e38\u597d\u7684\u7279\u6027 In computer science , a ticket lock is a synchronization mechanism, or locking algorithm , that is a type of spinlock that uses \"tickets\" to control which thread of execution is allowed to enter a critical section . Overview The basic concept of a ticket lock is similar to the ticket queue management system . This is the method that many bakeries(\u9762\u5305\u5e97) and delis(\u719f\u98df\u5e97) use to serve customers in the order that they arrive, without making them stand in a line. Generally, there is some type of dispenser(\u5206\u914d\u5668) from which customers pull sequentially numbered tickets upon arrival. The dispenser usually has a sign above or near it stating something like \"Please take a number\". There is also typically a dynamic sign, usually digital, that displays the ticket number that is now being served. Each time the next ticket number (customer) is ready to be served, the \"Now Serving\" sign is incremented and the number called out. This allows all of the waiting customers to know how many people are still ahead of them in the queue or line. NOTE: 1\u3001\" ticket queue management system \"\u5373\"\u53d6\u53f7\u6392\u961f\" 2\u3001\u76f8\u6bd4\u4e8e\u666e\u901a\u7684lock\uff0c\u5b83\u6709\"queue\" Like this system, a ticket lock is a first in first out (FIFO) queue-based mechanism. It adds the benefit of fairness of lock acquisition and works as follows; there are two integer values which begin at 0. The first value is the queue ticket, the second is the dequeue ticket. The queue ticket is the thread's position in the queue, and the dequeue ticket is the ticket, or queue position, that now has the lock (Now Serving). NOTE: 1\u3001\"fairness\" \u5373 \u516c\u5e73 \u8fd0\u884c\u903b\u8f91 NOTE: 1\u3001\u4ece\u4e0b\u9762\u7684\u63cf\u8ff0\u5e93\u770b\u51fa\uff0c\u5b83\u57fa\u672c\u4e0a\u662f\u6a21\u62df\u4e86\"\u53d6\u53f7\u6392\u961f\u3001\u53eb\u53f7\u670d\u52a1\" When a thread arrives, it atomically obtains and then increments the queue ticket. The atomicity of this operation is required to prevent two threads from simultaneously being able to obtain the same ticket number. It then compares its ticket value, before the increment, with the dequeue ticket's value. If they are the same, the thread is permitted to enter the critical section. If they are not the same, then another thread must already be in the critical section and this thread must busy-wait or yield. When a thread leaves the critical section controlled by the lock, it atomically increments the dequeue ticket. This permits the next waiting thread, the one with the next sequential ticket number, to enter the critical section.[ 1]","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/Ticket-lock/#ticket#lock","text":"\u662f\u5728\u601d\u8003 \"spinning lock with timeout\" \u65f6\uff0c\u60f3\u5230\u7684\u8fd9\u4e2a\u95ee\u9898","title":"Ticket lock"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/Ticket-lock/#wikipedia#ticket#lock","text":"NOTE: 1\u3001\u975e\u5e38\u7c7b\u4f3c\u4e8e\u5728\u94f6\u884c\u91cc\u9762\"\u53d6\u53f7\u6392\u961f\u3001\u53eb\u53f7\u670d\u52a1\"\u7684\u65b9\u5f0f\uff0c\u8fd9\u662f\u5178\u578b\u7684one to many 2\u3001\u539f\u7406\u975e\u5e38\u7b80\u5355\u3001\u5b9e\u73b0\u4e5f\u975e\u5e38\u7b80\u5355\uff0c\u540c\u65f6\u6709\u7740\u975e\u5e38\u597d\u7684\u7279\u6027 In computer science , a ticket lock is a synchronization mechanism, or locking algorithm , that is a type of spinlock that uses \"tickets\" to control which thread of execution is allowed to enter a critical section .","title":"wikipedia Ticket lock"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/Ticket-lock/#overview","text":"The basic concept of a ticket lock is similar to the ticket queue management system . This is the method that many bakeries(\u9762\u5305\u5e97) and delis(\u719f\u98df\u5e97) use to serve customers in the order that they arrive, without making them stand in a line. Generally, there is some type of dispenser(\u5206\u914d\u5668) from which customers pull sequentially numbered tickets upon arrival. The dispenser usually has a sign above or near it stating something like \"Please take a number\". There is also typically a dynamic sign, usually digital, that displays the ticket number that is now being served. Each time the next ticket number (customer) is ready to be served, the \"Now Serving\" sign is incremented and the number called out. This allows all of the waiting customers to know how many people are still ahead of them in the queue or line. NOTE: 1\u3001\" ticket queue management system \"\u5373\"\u53d6\u53f7\u6392\u961f\" 2\u3001\u76f8\u6bd4\u4e8e\u666e\u901a\u7684lock\uff0c\u5b83\u6709\"queue\" Like this system, a ticket lock is a first in first out (FIFO) queue-based mechanism. It adds the benefit of fairness of lock acquisition and works as follows; there are two integer values which begin at 0. The first value is the queue ticket, the second is the dequeue ticket. The queue ticket is the thread's position in the queue, and the dequeue ticket is the ticket, or queue position, that now has the lock (Now Serving). NOTE: 1\u3001\"fairness\" \u5373 \u516c\u5e73","title":"Overview"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/Ticket-lock/#_1","text":"NOTE: 1\u3001\u4ece\u4e0b\u9762\u7684\u63cf\u8ff0\u5e93\u770b\u51fa\uff0c\u5b83\u57fa\u672c\u4e0a\u662f\u6a21\u62df\u4e86\"\u53d6\u53f7\u6392\u961f\u3001\u53eb\u53f7\u670d\u52a1\" When a thread arrives, it atomically obtains and then increments the queue ticket. The atomicity of this operation is required to prevent two threads from simultaneously being able to obtain the same ticket number. It then compares its ticket value, before the increment, with the dequeue ticket's value. If they are the same, the thread is permitted to enter the critical section. If they are not the same, then another thread must already be in the critical section and this thread must busy-wait or yield. When a thread leaves the critical section controlled by the lock, it atomically increments the dequeue ticket. This permits the next waiting thread, the one with the next sequential ticket number, to enter the critical section.[ 1]","title":"\u8fd0\u884c\u903b\u8f91"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/library-bogachevdmitry-CLHLock/","text":"bogachevdmitry / CLHLock CLH locks (due to Travis Craig, Erik Hagersten, and Anders Landin), it is a spin lock based on a linked list, it constantly polls the state of the precursor, if the precursor releases the lock, it ends the spin.","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/library-bogachevdmitry-CLHLock/#bogachevdmitryclhlock","text":"CLH locks (due to Travis Craig, Erik Hagersten, and Anders Landin), it is a spin lock based on a linked list, it constantly polls the state of the precursor, if the precursor releases the lock, it ends the spin.","title":"bogachevdmitry/CLHLock"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/library-geidav-spinlocks-bench/","text":"github geidav / spinlocks-bench 1\u3001\u63d0\u4f9b\u4e86\u591a\u79cdspin lock\u7684implementation\u3002 Read the code 1\u3001source code: spinlocks-bench / excllocks.hpp 2\u3001\u5178\u578b\u7684 write-release-flag-notify-read-acquire-model pthread spinlock #include <pthread.h> class SpinLockPThread { public : ALWAYS_INLINE SpinLockPThread () { pthread_spin_init ( & Lock , 0 ); } ALWAYS_INLINE void Enter () { pthread_spin_lock ( & Lock ); } ALWAYS_INLINE void Leave () { pthread_spin_unlock ( & Lock ); } private : pthread_spinlock_t Lock ; }; Sequential consistent test-and-set spin lock #include <atomic> class ScTasSpinLock { public : ALWAYS_INLINE void Enter () { while ( Locked . exchange ( true )); } ALWAYS_INLINE void Leave () { Locked . store ( false ); } private : std :: atomic_bool Locked = { false }; }; Acquire-release test-and-set spin lock #include <atomic> class TasSpinLock { public : ALWAYS_INLINE void Enter () { while ( Locked . exchange ( true , std :: memory_order_acquire )); } ALWAYS_INLINE void Leave () { Locked . store ( false , std :: memory_order_release ); } private : std :: atomic_bool Locked = { false }; }; Acquire-release test-test-and-set spin lock #include <atomic> class TTasSpinLock { public : ALWAYS_INLINE void Enter () { do { while ( Locked . load ( std :: memory_order_relaxed )); } while ( Locked . exchange ( true , std :: memory_order_acquire )); } ALWAYS_INLINE void Leave () { Locked . store ( false , std :: memory_order_release ); } private : std :: atomic_bool Locked = { false }; }; Acquire-release test-test-and-set spin lock with CpuRelax #include <atomic> class RelaxTTasSpinLock { public : ALWAYS_INLINE void Enter () { do { while ( Locked . load ( std :: memory_order_relaxed )) CpuRelax (); } while ( Locked . exchange ( true , std :: memory_order_acquire )); } ALWAYS_INLINE void Leave () { Locked . store ( false , std :: memory_order_release ); } private : std :: atomic_bool Locked = { false }; }; Acquire-release test-test-and-set spin lock with Exponential backoff CpuRelax #include <atomic> class ExpBoRelaxTTasSpinLock { public : ALWAYS_INLINE void Enter () { size_t curMaxDelay = MIN_BACKOFF_ITERS ; while ( true ) { WaitUntilLockIsFree (); if ( Locked . exchange ( true , std :: memory_order_acquire )) BackoffExp ( curMaxDelay ); else break ; } } ALWAYS_INLINE void Leave () { Locked . store ( false , std :: memory_order_release ); } private : ALWAYS_INLINE void WaitUntilLockIsFree () const { size_t numIters = 0 ; while ( Locked . load ( std :: memory_order_relaxed )) { if ( numIters < MAX_WAIT_ITERS ) { numIters ++ ; CpuRelax (); } else YieldSleep (); } } public : std :: atomic_bool Locked = { false }; private : static const size_t MAX_WAIT_ITERS = 0x10000 ; static const size_t MIN_BACKOFF_ITERS = 32 ; }; Ticket Spin Lock \u5b83\u4f1a\u5b58\u5728\u548c TTasSpinLock \u76f8\u540c\u7684\u95ee\u9898\u3002 class TicketSpinLock { public : ALWAYS_INLINE void Enter () { const auto myTicketNo = NextTicketNo . fetch_add ( 1 , std :: memory_order_relaxed ); while ( ServingTicketNo . load ( std :: memory_order_acquire ) != myTicketNo ) CpuRelax (); } ALWAYS_INLINE void Leave () { // We can get around a more expensive read-modify-write operation // (std::atomic_size_t::fetch_add()), because noone can modify // ServingTicketNo while we're in the critical section. const auto newNo = ServingTicketNo . load ( std :: memory_order_relaxed ) + 1 ; ServingTicketNo . store ( newNo , std :: memory_order_release ); } private : alignas ( CACHELINE_SIZE ) std :: atomic_size_t ServingTicketNo = { 0 }; alignas ( CACHELINE_SIZE ) std :: atomic_size_t NextTicketNo = { 0 }; }; Anderson Spin Lock\u3001A Lock #include <atomic> #include <vector> #include <thread> #include <iostream> #include <stddef.h> /** * @brief * \u5b9e\u73b0\u539f\u7406\u6709\u70b9\u7c7b\u4f3c\u4e8e ticket spin lock\uff0cNextServingIdx \u8868\u793a \u4e0b\u4e00\u4e2a\u5f97\u5230lock\u3001\u80fd\u591f\u8fdb\u5165critical region\u7684\uff0c\u5b83\u5c31\u76f8\u5f53\u4e8eticket * \u4e0b\u9762\u7684\u4ee3\u7801\uff0c\u6709\u4e9b\u5730\u65b9\u6ca1\u6709\u8bfb\u61c2: * 1\u3001`Enter()` * ``` * // Ensure overflow never happens if (index == 0) { NextFreeIdx -= LockedFlags.size(); std::cout<<NextFreeIdx<<std::endl; } * ``` *\u4e0a\u8ff0\u662f\u4ec0\u4e48\u539f\u7406\uff0c\u663e\u7136\uff0c\u7b2c\u4e00\u6b21\u7684\u65f6\u5019\uff0c\u5b83\u4f1a\u53d1\u751funderflow *2\u3001\u4e3a\u4ec0\u4e48\u7528 NextFreeIdx\u3001NextServingIdx *3\u3001\u5b83\u53ea\u80fd\u591f\u7528\u4e8e\u56fa\u5b9a\u6570\u91cf\u7684thread */ class AndersonSpinLock { public : AndersonSpinLock ( size_t maxThreads = std :: thread :: hardware_concurrency ()) : LockedFlags ( maxThreads ) { for ( auto & flag : LockedFlags ) flag . first = true ; LockedFlags [ 0 ]. first = false ; } ALWAYS_INLINE void Enter () { /** * `fetch_add` \u8fd4\u56de\u7684\u662f\u4e4b\u524d\u7684\u503c */ const size_t index = NextFreeIdx . fetch_add ( 1 ) % LockedFlags . size (); std :: cout << index << std :: endl ; auto & flag = LockedFlags [ index ]. first ; // Ensure overflow never happens if ( index == 0 ) { NextFreeIdx -= LockedFlags . size (); std :: cout << NextFreeIdx << std :: endl ; } while ( flag ) CpuRelax (); flag = true ; } ALWAYS_INLINE void Leave () { const size_t idx = NextServingIdx . fetch_add ( 1 ); LockedFlags [ idx % LockedFlags . size ()]. first = false ; } private : using PaddedFlag = std :: pair < std :: atomic_bool , uint8_t [ CACHELINE_SIZE - sizeof ( std :: atomic_bool )] > ; static_assert ( sizeof ( PaddedFlag ) == CACHELINE_SIZE , \"\" ); alignas ( CACHELINE_SIZE ) std :: vector < PaddedFlag > LockedFlags ; alignas ( CACHELINE_SIZE ) std :: atomic_size_t NextFreeIdx = { 0 }; alignas ( CACHELINE_SIZE ) std :: atomic_size_t NextServingIdx = { 1 }; }; McsLock #include <atomic> #include <vector> #include <thread> #include <iostream> #include <stddef.h> class McsLock { public : struct QNode { std :: atomic < QNode *> Next = { nullptr }; std :: atomic_bool Locked = { false }; }; public : ALWAYS_INLINE void Enter ( QNode & node ) { node . Next = nullptr ; node . Locked = true ; QNode * oldTail = Tail . exchange ( & node ); // \u5c06node\u4f5c\u4e3atail if ( oldTail != nullptr ) { oldTail -> Next = & node ; while ( node . Locked == true ) CpuRelax (); } } ALWAYS_INLINE void Leave ( QNode & node ) { if ( node . Next . load () == nullptr ) { QNode * tailWasMe = & node ; if ( Tail . compare_exchange_strong ( tailWasMe , nullptr )) return ; while ( node . Next . load () == nullptr ) CpuRelax (); } node . Next . load () -> Locked = false ; } private : std :: atomic < QNode *> Tail = { nullptr }; }; \u4ee5RAII\u7684\u65b9\u5f0f\u6765\u8fdb\u884c\u5c01\u88c5 1\u3001\u4e0a\u9762\u7684\u8fd9\u4e9b\u76f8\u5f53\u4e8e\u662f mutex \uff0c\u7136\u540e\u4f7f\u7528\u7c7b\u4f3c\u4e8eSTL\u7684 std::unique_lock \u6765\u8fdb\u884c\u5c01\u88c5\u3002 \u6d4b\u8bd5\u7a0b\u5e8f #include <atomic> #include <vector> #include <thread> #include <iostream> #include <stddef.h> #if defined(__SSE2__) #include <xmmintrin.h> // _mm_pause #endif constexpr size_t CACHELINE_SIZE = 64 ; #define WIN 0 #define UNIX 1 #define OS UNIX #if (OS == WIN) #define WIN32_LEAN_AND_MEAN #define NOMINMAX #include <windows.h> #define ALWAYS_INLINE __forceinline #elif (OS == UNIX) #include <pthread.h> #define ALWAYS_INLINE inline __attribute__((__always_inline__)) #endif ALWAYS_INLINE static void CpuRelax () { #if (OS == WIN) _mm_pause (); #elif defined(__SSE2__) // AMD and Intel _mm_pause (); #elif defined(__i386__) || defined(__x86_64__) asm volatile ( \"pause\" ); #elif defined(__aarch64__) asm volatile ( \"wfe\" ); #elif defined(__armel__) || defined(__ARMEL__) asm volatile ( \"nop\" ::: \"memory\" ); // default operation - does nothing => Might lead to passive spinning. #elif defined(__arm__) || defined(__aarch64__) // arm big endian / arm64 __asm__ __volatile__ ( \"yield\" ::: \"memory\" ); #elif defined(__ia64__) // IA64 __asm__ __volatile__ ( \"hint @pause\" ); #elif defined(__powerpc__) || defined(__ppc__) || defined(__PPC__) // PowerPC __asm__ __volatile__ ( \"or 27,27,27\" ::: \"memory\" ); #else // everything else. asm volatile ( \"nop\" ::: \"memory\" ); // default operation - does nothing => Might lead to passive spinning. #endif } /** * @brief * \u5b9e\u73b0\u539f\u7406\u6709\u70b9\u7c7b\u4f3c\u4e8e ticket spin lock\uff0cNextServingIdx \u8868\u793a \u4e0b\u4e00\u4e2a\u5f97\u5230lock\u3001\u80fd\u591f\u8fdb\u5165critical region\u7684\uff0c\u5b83\u5c31\u76f8\u5f53\u4e8eticket * * */ class AndersonSpinLock { public : AndersonSpinLock ( size_t maxThreads = std :: thread :: hardware_concurrency ()) : LockedFlags ( maxThreads ) { for ( auto & flag : LockedFlags ) flag . first = true ; LockedFlags [ 0 ]. first = false ; } ALWAYS_INLINE void Enter () { /** * `fetch_add` \u8fd4\u56de\u7684\u662f\u4e4b\u524d\u7684\u503c */ const size_t index = NextFreeIdx . fetch_add ( 1 ) % LockedFlags . size (); std :: cout << index << std :: endl ; auto & flag = LockedFlags [ index ]. first ; // Ensure overflow never happens if ( index == 0 ) { NextFreeIdx -= LockedFlags . size (); std :: cout << NextFreeIdx << std :: endl ; } while ( flag ) CpuRelax (); flag = true ; } ALWAYS_INLINE void Leave () { const size_t idx = NextServingIdx . fetch_add ( 1 ); LockedFlags [ idx % LockedFlags . size ()]. first = false ; } private : using PaddedFlag = std :: pair < std :: atomic_bool , uint8_t [ CACHELINE_SIZE - sizeof ( std :: atomic_bool )] > ; static_assert ( sizeof ( PaddedFlag ) == CACHELINE_SIZE , \"\" ); alignas ( CACHELINE_SIZE ) std :: vector < PaddedFlag > LockedFlags ; alignas ( CACHELINE_SIZE ) std :: atomic_size_t NextFreeIdx = { 0 }; alignas ( CACHELINE_SIZE ) std :: atomic_size_t NextServingIdx = { 1 }; }; int main () { AndersonSpinLock Lock { 3 }; Lock . Enter (); Lock . Leave (); } // g++ test.cpp -pedantic -Wall -Wextra --std=c++11","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/library-geidav-spinlocks-bench/#github#geidavspinlocks-bench","text":"1\u3001\u63d0\u4f9b\u4e86\u591a\u79cdspin lock\u7684implementation\u3002","title":"github geidav/spinlocks-bench"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/library-geidav-spinlocks-bench/#read#the#code","text":"1\u3001source code: spinlocks-bench / excllocks.hpp 2\u3001\u5178\u578b\u7684 write-release-flag-notify-read-acquire-model","title":"Read the code"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/library-geidav-spinlocks-bench/#pthread#spinlock","text":"#include <pthread.h> class SpinLockPThread { public : ALWAYS_INLINE SpinLockPThread () { pthread_spin_init ( & Lock , 0 ); } ALWAYS_INLINE void Enter () { pthread_spin_lock ( & Lock ); } ALWAYS_INLINE void Leave () { pthread_spin_unlock ( & Lock ); } private : pthread_spinlock_t Lock ; };","title":"pthread spinlock"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/library-geidav-spinlocks-bench/#sequential#consistent#test-and-set#spin#lock","text":"#include <atomic> class ScTasSpinLock { public : ALWAYS_INLINE void Enter () { while ( Locked . exchange ( true )); } ALWAYS_INLINE void Leave () { Locked . store ( false ); } private : std :: atomic_bool Locked = { false }; };","title":"Sequential consistent test-and-set spin lock"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/library-geidav-spinlocks-bench/#acquire-release#test-and-set#spin#lock","text":"#include <atomic> class TasSpinLock { public : ALWAYS_INLINE void Enter () { while ( Locked . exchange ( true , std :: memory_order_acquire )); } ALWAYS_INLINE void Leave () { Locked . store ( false , std :: memory_order_release ); } private : std :: atomic_bool Locked = { false }; };","title":"Acquire-release test-and-set spin lock"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/library-geidav-spinlocks-bench/#acquire-release#test-test-and-set#spin#lock","text":"#include <atomic> class TTasSpinLock { public : ALWAYS_INLINE void Enter () { do { while ( Locked . load ( std :: memory_order_relaxed )); } while ( Locked . exchange ( true , std :: memory_order_acquire )); } ALWAYS_INLINE void Leave () { Locked . store ( false , std :: memory_order_release ); } private : std :: atomic_bool Locked = { false }; };","title":"Acquire-release test-test-and-set spin lock"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/library-geidav-spinlocks-bench/#acquire-release#test-test-and-set#spin#lock#with#cpurelax","text":"#include <atomic> class RelaxTTasSpinLock { public : ALWAYS_INLINE void Enter () { do { while ( Locked . load ( std :: memory_order_relaxed )) CpuRelax (); } while ( Locked . exchange ( true , std :: memory_order_acquire )); } ALWAYS_INLINE void Leave () { Locked . store ( false , std :: memory_order_release ); } private : std :: atomic_bool Locked = { false }; };","title":"Acquire-release test-test-and-set spin lock with CpuRelax"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/library-geidav-spinlocks-bench/#acquire-release#test-test-and-set#spin#lock#with#exponential#backoff#cpurelax","text":"#include <atomic> class ExpBoRelaxTTasSpinLock { public : ALWAYS_INLINE void Enter () { size_t curMaxDelay = MIN_BACKOFF_ITERS ; while ( true ) { WaitUntilLockIsFree (); if ( Locked . exchange ( true , std :: memory_order_acquire )) BackoffExp ( curMaxDelay ); else break ; } } ALWAYS_INLINE void Leave () { Locked . store ( false , std :: memory_order_release ); } private : ALWAYS_INLINE void WaitUntilLockIsFree () const { size_t numIters = 0 ; while ( Locked . load ( std :: memory_order_relaxed )) { if ( numIters < MAX_WAIT_ITERS ) { numIters ++ ; CpuRelax (); } else YieldSleep (); } } public : std :: atomic_bool Locked = { false }; private : static const size_t MAX_WAIT_ITERS = 0x10000 ; static const size_t MIN_BACKOFF_ITERS = 32 ; };","title":"Acquire-release test-test-and-set spin lock with Exponential backoff CpuRelax"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/library-geidav-spinlocks-bench/#ticket#spin#lock","text":"\u5b83\u4f1a\u5b58\u5728\u548c TTasSpinLock \u76f8\u540c\u7684\u95ee\u9898\u3002 class TicketSpinLock { public : ALWAYS_INLINE void Enter () { const auto myTicketNo = NextTicketNo . fetch_add ( 1 , std :: memory_order_relaxed ); while ( ServingTicketNo . load ( std :: memory_order_acquire ) != myTicketNo ) CpuRelax (); } ALWAYS_INLINE void Leave () { // We can get around a more expensive read-modify-write operation // (std::atomic_size_t::fetch_add()), because noone can modify // ServingTicketNo while we're in the critical section. const auto newNo = ServingTicketNo . load ( std :: memory_order_relaxed ) + 1 ; ServingTicketNo . store ( newNo , std :: memory_order_release ); } private : alignas ( CACHELINE_SIZE ) std :: atomic_size_t ServingTicketNo = { 0 }; alignas ( CACHELINE_SIZE ) std :: atomic_size_t NextTicketNo = { 0 }; };","title":"Ticket Spin Lock"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/library-geidav-spinlocks-bench/#anderson#spin#locka#lock","text":"#include <atomic> #include <vector> #include <thread> #include <iostream> #include <stddef.h> /** * @brief * \u5b9e\u73b0\u539f\u7406\u6709\u70b9\u7c7b\u4f3c\u4e8e ticket spin lock\uff0cNextServingIdx \u8868\u793a \u4e0b\u4e00\u4e2a\u5f97\u5230lock\u3001\u80fd\u591f\u8fdb\u5165critical region\u7684\uff0c\u5b83\u5c31\u76f8\u5f53\u4e8eticket * \u4e0b\u9762\u7684\u4ee3\u7801\uff0c\u6709\u4e9b\u5730\u65b9\u6ca1\u6709\u8bfb\u61c2: * 1\u3001`Enter()` * ``` * // Ensure overflow never happens if (index == 0) { NextFreeIdx -= LockedFlags.size(); std::cout<<NextFreeIdx<<std::endl; } * ``` *\u4e0a\u8ff0\u662f\u4ec0\u4e48\u539f\u7406\uff0c\u663e\u7136\uff0c\u7b2c\u4e00\u6b21\u7684\u65f6\u5019\uff0c\u5b83\u4f1a\u53d1\u751funderflow *2\u3001\u4e3a\u4ec0\u4e48\u7528 NextFreeIdx\u3001NextServingIdx *3\u3001\u5b83\u53ea\u80fd\u591f\u7528\u4e8e\u56fa\u5b9a\u6570\u91cf\u7684thread */ class AndersonSpinLock { public : AndersonSpinLock ( size_t maxThreads = std :: thread :: hardware_concurrency ()) : LockedFlags ( maxThreads ) { for ( auto & flag : LockedFlags ) flag . first = true ; LockedFlags [ 0 ]. first = false ; } ALWAYS_INLINE void Enter () { /** * `fetch_add` \u8fd4\u56de\u7684\u662f\u4e4b\u524d\u7684\u503c */ const size_t index = NextFreeIdx . fetch_add ( 1 ) % LockedFlags . size (); std :: cout << index << std :: endl ; auto & flag = LockedFlags [ index ]. first ; // Ensure overflow never happens if ( index == 0 ) { NextFreeIdx -= LockedFlags . size (); std :: cout << NextFreeIdx << std :: endl ; } while ( flag ) CpuRelax (); flag = true ; } ALWAYS_INLINE void Leave () { const size_t idx = NextServingIdx . fetch_add ( 1 ); LockedFlags [ idx % LockedFlags . size ()]. first = false ; } private : using PaddedFlag = std :: pair < std :: atomic_bool , uint8_t [ CACHELINE_SIZE - sizeof ( std :: atomic_bool )] > ; static_assert ( sizeof ( PaddedFlag ) == CACHELINE_SIZE , \"\" ); alignas ( CACHELINE_SIZE ) std :: vector < PaddedFlag > LockedFlags ; alignas ( CACHELINE_SIZE ) std :: atomic_size_t NextFreeIdx = { 0 }; alignas ( CACHELINE_SIZE ) std :: atomic_size_t NextServingIdx = { 1 }; };","title":"Anderson Spin Lock\u3001A Lock"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/library-geidav-spinlocks-bench/#mcslock","text":"#include <atomic> #include <vector> #include <thread> #include <iostream> #include <stddef.h> class McsLock { public : struct QNode { std :: atomic < QNode *> Next = { nullptr }; std :: atomic_bool Locked = { false }; }; public : ALWAYS_INLINE void Enter ( QNode & node ) { node . Next = nullptr ; node . Locked = true ; QNode * oldTail = Tail . exchange ( & node ); // \u5c06node\u4f5c\u4e3atail if ( oldTail != nullptr ) { oldTail -> Next = & node ; while ( node . Locked == true ) CpuRelax (); } } ALWAYS_INLINE void Leave ( QNode & node ) { if ( node . Next . load () == nullptr ) { QNode * tailWasMe = & node ; if ( Tail . compare_exchange_strong ( tailWasMe , nullptr )) return ; while ( node . Next . load () == nullptr ) CpuRelax (); } node . Next . load () -> Locked = false ; } private : std :: atomic < QNode *> Tail = { nullptr }; };","title":"McsLock"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/library-geidav-spinlocks-bench/#raii","text":"1\u3001\u4e0a\u9762\u7684\u8fd9\u4e9b\u76f8\u5f53\u4e8e\u662f mutex \uff0c\u7136\u540e\u4f7f\u7528\u7c7b\u4f3c\u4e8eSTL\u7684 std::unique_lock \u6765\u8fdb\u884c\u5c01\u88c5\u3002","title":"\u4ee5RAII\u7684\u65b9\u5f0f\u6765\u8fdb\u884c\u5c01\u88c5"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/library-geidav-spinlocks-bench/#_1","text":"#include <atomic> #include <vector> #include <thread> #include <iostream> #include <stddef.h> #if defined(__SSE2__) #include <xmmintrin.h> // _mm_pause #endif constexpr size_t CACHELINE_SIZE = 64 ; #define WIN 0 #define UNIX 1 #define OS UNIX #if (OS == WIN) #define WIN32_LEAN_AND_MEAN #define NOMINMAX #include <windows.h> #define ALWAYS_INLINE __forceinline #elif (OS == UNIX) #include <pthread.h> #define ALWAYS_INLINE inline __attribute__((__always_inline__)) #endif ALWAYS_INLINE static void CpuRelax () { #if (OS == WIN) _mm_pause (); #elif defined(__SSE2__) // AMD and Intel _mm_pause (); #elif defined(__i386__) || defined(__x86_64__) asm volatile ( \"pause\" ); #elif defined(__aarch64__) asm volatile ( \"wfe\" ); #elif defined(__armel__) || defined(__ARMEL__) asm volatile ( \"nop\" ::: \"memory\" ); // default operation - does nothing => Might lead to passive spinning. #elif defined(__arm__) || defined(__aarch64__) // arm big endian / arm64 __asm__ __volatile__ ( \"yield\" ::: \"memory\" ); #elif defined(__ia64__) // IA64 __asm__ __volatile__ ( \"hint @pause\" ); #elif defined(__powerpc__) || defined(__ppc__) || defined(__PPC__) // PowerPC __asm__ __volatile__ ( \"or 27,27,27\" ::: \"memory\" ); #else // everything else. asm volatile ( \"nop\" ::: \"memory\" ); // default operation - does nothing => Might lead to passive spinning. #endif } /** * @brief * \u5b9e\u73b0\u539f\u7406\u6709\u70b9\u7c7b\u4f3c\u4e8e ticket spin lock\uff0cNextServingIdx \u8868\u793a \u4e0b\u4e00\u4e2a\u5f97\u5230lock\u3001\u80fd\u591f\u8fdb\u5165critical region\u7684\uff0c\u5b83\u5c31\u76f8\u5f53\u4e8eticket * * */ class AndersonSpinLock { public : AndersonSpinLock ( size_t maxThreads = std :: thread :: hardware_concurrency ()) : LockedFlags ( maxThreads ) { for ( auto & flag : LockedFlags ) flag . first = true ; LockedFlags [ 0 ]. first = false ; } ALWAYS_INLINE void Enter () { /** * `fetch_add` \u8fd4\u56de\u7684\u662f\u4e4b\u524d\u7684\u503c */ const size_t index = NextFreeIdx . fetch_add ( 1 ) % LockedFlags . size (); std :: cout << index << std :: endl ; auto & flag = LockedFlags [ index ]. first ; // Ensure overflow never happens if ( index == 0 ) { NextFreeIdx -= LockedFlags . size (); std :: cout << NextFreeIdx << std :: endl ; } while ( flag ) CpuRelax (); flag = true ; } ALWAYS_INLINE void Leave () { const size_t idx = NextServingIdx . fetch_add ( 1 ); LockedFlags [ idx % LockedFlags . size ()]. first = false ; } private : using PaddedFlag = std :: pair < std :: atomic_bool , uint8_t [ CACHELINE_SIZE - sizeof ( std :: atomic_bool )] > ; static_assert ( sizeof ( PaddedFlag ) == CACHELINE_SIZE , \"\" ); alignas ( CACHELINE_SIZE ) std :: vector < PaddedFlag > LockedFlags ; alignas ( CACHELINE_SIZE ) std :: atomic_size_t NextFreeIdx = { 0 }; alignas ( CACHELINE_SIZE ) std :: atomic_size_t NextServingIdx = { 1 }; }; int main () { AndersonSpinLock Lock { 3 }; Lock . Enter (); Lock . Leave (); } // g++ test.cpp -pedantic -Wall -Wextra --std=c++11","title":"\u6d4b\u8bd5\u7a0b\u5e8f"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/library-honkiko-Multi-Core-Toolbox/","text":"honkiko / Multi-Core-Toolbox Multi-Core-Toolbox / mcs_lock.h Algorithm published on ACM TOCS, February 1991, by John M. Mellor-Crummey and Michael L. Scott. http://www.cs.rochester.edu/u/scott/papers/1991_TOCS_synch.pdf guarantees FIFO ordering of lock acquisitions; spins on locally-accessible flag variables only; requires a small constant amount of space per lock; and works equally well (requiring only O(1) network transactions per lock acquisition) on machines with and without coherent caches.","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/library-honkiko-Multi-Core-Toolbox/#honkikomulti-core-toolbox","text":"","title":"honkiko/Multi-Core-Toolbox"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/library-honkiko-Multi-Core-Toolbox/#multi-core-toolboxmcs_lockh","text":"Algorithm published on ACM TOCS, February 1991, by John M. Mellor-Crummey and Michael L. Scott. http://www.cs.rochester.edu/u/scott/papers/1991_TOCS_synch.pdf guarantees FIFO ordering of lock acquisitions; spins on locally-accessible flag variables only; requires a small constant amount of space per lock; and works equally well (requiring only O(1) network transactions per lock acquisition) on machines with and without coherent caches.","title":"Multi-Core-Toolbox/mcs_lock.h"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/paper-Algorithms-for-Scalable-Synchronization/","text":"rochester Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors Pseudocode from article of the above name , ACM TOCS , February 1991. John M. Mellor-Crummey and Michael L. Scott , with later additions due to (a) Craig, Landin, and Hagersten, and (b) Auslander, Edelsohn, Krieger, Rosenburg, and Wisniewski. All of these algorithms (except for the non-scalable centralized barrier) perform well in tests on machines with scores of processors. NOTE: 1\u3001\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u610f\u601d\u662f: \u672c\u6587\u4e0b\u9762\u7684Pseudocode\uff0c\u662f\u6e90\u81ea\u5982\u4e0b\u8bba\u6587: Algorithm for Scalable Synchronization on Shared-memory Multiprocessors Spinlocks Barriers NOTE: 1\u3001\u76ee\u524d\u8fd8\u4e0d\u77e5\u9053\u4e0b\u9762\u7684\u7a0b\u5e8f\u662f\u4f7f\u7528\u4ec0\u4e48\u8bed\u8a00\u5f00\u53d1\u7684 2\u3001 ^lock \u8868\u793a\u7684\u662fpointer to lock\uff0c\u663e\u7136 ^ \u7c7b\u4f3c\u4e8eC\u7684 * Simple test_and_set lock with exponential backoff type lock = ( unlocked , locked ) // lock\u7c7b\u578b\uff0c\u53ef\u9009\u503c\u4e3a unlocked\u3001locked procedure acquire_lock ( L : ^ lock ) // \u51fd\u6570acquire_lock\uff0cL\u662f\u5165\u53c2\uff0c\u5b83\u7684\u7c7b\u578b\u4e3a lock delay : integer := 1 while test_and_set ( L ) = locked // returns old value pause ( delay ) // consume this many units of time delay := delay * 2 procedure release_lock ( L : ^ lock ) lock ^ := unlocked Ticket lock with proportional backoff NOTE: 1\u3001\"proportional\"\u7684\u610f\u601d\u662f\"\u6bd4\u4f8b\u7684\uff0c\u6210\u6bd4\u4f8b\u7684\"\uff0c\u7ed3\u5408\u4e0b\u9762\u7684code\u6765\u770b\uff0c\u5b83\u5bf9\u5e94\u7684\u662f pause ( my_ticket - L -> now_serving ) // \u6b64\u5904\u8868\u793a\u7684\u662f\u6682\u505c\uff0c\u663e\u7136\u5c31\u662fsleep\uff0c\"my_ticket - L->now_serving\"\u8868\u793a\u7684\u662f\u5728\u6211\u524d\u9762\u6709\u51e0\u4e2a\u4eba\u5728\u7b49\u5f85 type lock = record next_ticket : unsigned integer := 0 // \u4e0b\u4e00\u4e2a\u8981\u670d\u52a1\u7684\u53f7\u5b50 now_serving : unsigned integer := 0 // \u5f53\u524d\u670d\u52a1\u7684\u53f7\u5b50 procedure acquire_lock ( L : ^ lock ) my_ticket : unsigned integer := fetch_and_increment ( & L -> next_ticket ) // \u6bcf\u4e2athread\u8fdb\u6765\u90fd\u9700\u8981\u53d6\u53f7 // returns old value; arithmetic overflow is harmless loop pause ( my_ticket - L -> now_serving ) // \u6b64\u5904\u8868\u793a\u7684\u662f\u6682\u505c\uff0c\u663e\u7136\u5c31\u662fsleep\uff0c\"my_ticket - L->now_serving\"\u8868\u793a\u7684\u662f\u5728\u6211\u524d\u9762\u6709\u51e0\u4e2a\u4eba\u5728\u7b49\u5f85 // consume this many units of time // on most machines, subtraction works correctly despite overflow if L -> now_serving = my_ticket // \u88ab\u53eb\u5230\u53f7\u4e86 return procedure release_lock ( L : ^ lock ) L -> now_serving := L -> now_serving + 1 // \u5f53\u524d\u7528\u6237\u5df2\u7ecf\u670d\u52a1\u5b8c\u4e86\uff0c\u53ef\u4ee5\u53eb\u4e0b\u4e00\u4e2a\u53f7\u4e86 Anderson's array-based queue lock NOTE: 1\u3001A-lock type lock = record // slots \u7684\u957f\u5ea6\u5bf9\u4e8e processor\u7684\u4e2a\u6570 // has_lock \u8868\u793a\u53ef\u4ee5\u83b7\u5f97lock slots : array [ 0 .. numprocs - 1 ] of ( has_lock , must_wait ) := ( has_lock , must_wait , must_wait , ..., must_wait ) // each element of slots should lie in a different memory module // or cache line next_slot : integer := 0 // \u4e0b\u4e00\u4e2aslot\u7684index // parameter my_place, below, points to a private variable // in an enclosing scope procedure acquire_lock ( L : ^ lock , my_place : ^ integer ) my_place ^ := fetch_and_increment ( & L -> next_slot ) // returns old value if my_place ^ mod numprocs = 0 atomic_add ( & L -> next_slot , - numprocs ) // avoid problems with overflow; return value ignored my_place ^ := my_place ^ mod numprocs repeat while L -> slots [ my_place ^ ] = must_wait // spin L -> slots [ my_place ^ ] := must_wait // init for next time procedure release_lock ( L : ^ lock , my_place : ^ integer ) L -> slots [( my_place ^ + 1 ) mod numprocs ] := has_lock // \u91ca\u653e\u9501 Graunke and Thakkar's array-based queue lock NOTE: 1\u3001\u672a\u9605\u8bfb The MCS list-based queue lock type qnode = record next : ^ qnode // pointer to qnode locked : Boolean type lock = ^ qnode // initialized to nil // parameter I, below, points to a qnode record allocated // (in an enclosing scope) in shared memory locally-accessible // to the invoking processor procedure acquire_lock ( L : ^ lock , I : ^ qnode ) I -> next := nil predecessor : ^ qnode := fetch_and_store ( L , I ) if predecessor ! = nil // queue was non-empty I -> locked := true predecessor -> next := I repeat while I -> locked // spin procedure release_lock ( L : ^ lock , I : ^ qnode ) if I -> next = nil // no known successor if compare_and_store ( L , I , nil ) return // compare_and_store returns true iff it stored repeat while I -> next = nil // spin I -> next -> locked := false","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/paper-Algorithms-for-Scalable-Synchronization/#rochester#algorithms#for#scalable#synchronization#on#shared-memory#multiprocessors","text":"Pseudocode from article of the above name , ACM TOCS , February 1991. John M. Mellor-Crummey and Michael L. Scott , with later additions due to (a) Craig, Landin, and Hagersten, and (b) Auslander, Edelsohn, Krieger, Rosenburg, and Wisniewski. All of these algorithms (except for the non-scalable centralized barrier) perform well in tests on machines with scores of processors. NOTE: 1\u3001\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u610f\u601d\u662f: \u672c\u6587\u4e0b\u9762\u7684Pseudocode\uff0c\u662f\u6e90\u81ea\u5982\u4e0b\u8bba\u6587: Algorithm for Scalable Synchronization on Shared-memory Multiprocessors","title":"rochester Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/paper-Algorithms-for-Scalable-Synchronization/#spinlocks","text":"","title":"Spinlocks"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/paper-Algorithms-for-Scalable-Synchronization/#barriers","text":"NOTE: 1\u3001\u76ee\u524d\u8fd8\u4e0d\u77e5\u9053\u4e0b\u9762\u7684\u7a0b\u5e8f\u662f\u4f7f\u7528\u4ec0\u4e48\u8bed\u8a00\u5f00\u53d1\u7684 2\u3001 ^lock \u8868\u793a\u7684\u662fpointer to lock\uff0c\u663e\u7136 ^ \u7c7b\u4f3c\u4e8eC\u7684 *","title":"Barriers"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/paper-Algorithms-for-Scalable-Synchronization/#simple#test_and_set#lock#with#exponential#backoff","text":"type lock = ( unlocked , locked ) // lock\u7c7b\u578b\uff0c\u53ef\u9009\u503c\u4e3a unlocked\u3001locked procedure acquire_lock ( L : ^ lock ) // \u51fd\u6570acquire_lock\uff0cL\u662f\u5165\u53c2\uff0c\u5b83\u7684\u7c7b\u578b\u4e3a lock delay : integer := 1 while test_and_set ( L ) = locked // returns old value pause ( delay ) // consume this many units of time delay := delay * 2 procedure release_lock ( L : ^ lock ) lock ^ := unlocked","title":"Simple test_and_set lock with exponential backoff"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/paper-Algorithms-for-Scalable-Synchronization/#ticket#lock#with#proportional#backoff","text":"NOTE: 1\u3001\"proportional\"\u7684\u610f\u601d\u662f\"\u6bd4\u4f8b\u7684\uff0c\u6210\u6bd4\u4f8b\u7684\"\uff0c\u7ed3\u5408\u4e0b\u9762\u7684code\u6765\u770b\uff0c\u5b83\u5bf9\u5e94\u7684\u662f pause ( my_ticket - L -> now_serving ) // \u6b64\u5904\u8868\u793a\u7684\u662f\u6682\u505c\uff0c\u663e\u7136\u5c31\u662fsleep\uff0c\"my_ticket - L->now_serving\"\u8868\u793a\u7684\u662f\u5728\u6211\u524d\u9762\u6709\u51e0\u4e2a\u4eba\u5728\u7b49\u5f85 type lock = record next_ticket : unsigned integer := 0 // \u4e0b\u4e00\u4e2a\u8981\u670d\u52a1\u7684\u53f7\u5b50 now_serving : unsigned integer := 0 // \u5f53\u524d\u670d\u52a1\u7684\u53f7\u5b50 procedure acquire_lock ( L : ^ lock ) my_ticket : unsigned integer := fetch_and_increment ( & L -> next_ticket ) // \u6bcf\u4e2athread\u8fdb\u6765\u90fd\u9700\u8981\u53d6\u53f7 // returns old value; arithmetic overflow is harmless loop pause ( my_ticket - L -> now_serving ) // \u6b64\u5904\u8868\u793a\u7684\u662f\u6682\u505c\uff0c\u663e\u7136\u5c31\u662fsleep\uff0c\"my_ticket - L->now_serving\"\u8868\u793a\u7684\u662f\u5728\u6211\u524d\u9762\u6709\u51e0\u4e2a\u4eba\u5728\u7b49\u5f85 // consume this many units of time // on most machines, subtraction works correctly despite overflow if L -> now_serving = my_ticket // \u88ab\u53eb\u5230\u53f7\u4e86 return procedure release_lock ( L : ^ lock ) L -> now_serving := L -> now_serving + 1 // \u5f53\u524d\u7528\u6237\u5df2\u7ecf\u670d\u52a1\u5b8c\u4e86\uff0c\u53ef\u4ee5\u53eb\u4e0b\u4e00\u4e2a\u53f7\u4e86","title":"Ticket lock with proportional backoff"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/paper-Algorithms-for-Scalable-Synchronization/#andersons#array-based#queue#lock","text":"NOTE: 1\u3001A-lock type lock = record // slots \u7684\u957f\u5ea6\u5bf9\u4e8e processor\u7684\u4e2a\u6570 // has_lock \u8868\u793a\u53ef\u4ee5\u83b7\u5f97lock slots : array [ 0 .. numprocs - 1 ] of ( has_lock , must_wait ) := ( has_lock , must_wait , must_wait , ..., must_wait ) // each element of slots should lie in a different memory module // or cache line next_slot : integer := 0 // \u4e0b\u4e00\u4e2aslot\u7684index // parameter my_place, below, points to a private variable // in an enclosing scope procedure acquire_lock ( L : ^ lock , my_place : ^ integer ) my_place ^ := fetch_and_increment ( & L -> next_slot ) // returns old value if my_place ^ mod numprocs = 0 atomic_add ( & L -> next_slot , - numprocs ) // avoid problems with overflow; return value ignored my_place ^ := my_place ^ mod numprocs repeat while L -> slots [ my_place ^ ] = must_wait // spin L -> slots [ my_place ^ ] := must_wait // init for next time procedure release_lock ( L : ^ lock , my_place : ^ integer ) L -> slots [( my_place ^ + 1 ) mod numprocs ] := has_lock // \u91ca\u653e\u9501","title":"Anderson's array-based queue lock"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/paper-Algorithms-for-Scalable-Synchronization/#graunke#and#thakkars#array-based#queue#lock","text":"NOTE: 1\u3001\u672a\u9605\u8bfb","title":"Graunke and Thakkar's array-based queue lock"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/paper-Algorithms-for-Scalable-Synchronization/#the#mcs#list-based#queue#lock","text":"type qnode = record next : ^ qnode // pointer to qnode locked : Boolean type lock = ^ qnode // initialized to nil // parameter I, below, points to a qnode record allocated // (in an enclosing scope) in shared memory locally-accessible // to the invoking processor procedure acquire_lock ( L : ^ lock , I : ^ qnode ) I -> next := nil predecessor : ^ qnode := fetch_and_store ( L , I ) if predecessor ! = nil // queue was non-empty I -> locked := true predecessor -> next := I repeat while I -> locked // spin procedure release_lock ( L : ^ lock , I : ^ qnode ) if I -> next = nil // no known successor if compare_and_store ( L , I , nil ) return // compare_and_store returns true iff it stored repeat while I -> next = nil // spin I -> next -> locked := false","title":"The MCS list-based queue lock"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/paper-Spinlock-with-timeout/","text":"Spinning lock with timeout \u5e26timeout\u7684\u52a0\u9501\u65b9\u5f0f\u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u90a3spinning lock\u5982\u4f55\u5b9e\u73b0timeout\u5462\uff1f\u5728 rochester paper Scalable Queue-Based Spin Locks with Timeout \u4e2d\u63d0\u51fa\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u53c2\u89c1 paper \u7ae0\u8282\uff0c\u5176\u4e2d\u5bf9\u8fd9\u7bc7paper\u8fdb\u884c\u4e86\u6ce8\u89e3\u3002 \u8bba\u6587\u6709\u70b9\u590d\u6742\uff0c\u8fd8\u6ca1\u6709\u8bfb\u5b8c\u3002 pseudocode Scalable Queue-Based Spin Locks with Timeout NOTE: \u8fd9\u662f rochester paper Scalable Queue-Based Spin Locks with Timeout \u7684pseudocode","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/paper-Spinlock-with-timeout/#spinning#lock#with#timeout","text":"\u5e26timeout\u7684\u52a0\u9501\u65b9\u5f0f\u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u90a3spinning lock\u5982\u4f55\u5b9e\u73b0timeout\u5462\uff1f\u5728 rochester paper Scalable Queue-Based Spin Locks with Timeout \u4e2d\u63d0\u51fa\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u53c2\u89c1 paper \u7ae0\u8282\uff0c\u5176\u4e2d\u5bf9\u8fd9\u7bc7paper\u8fdb\u884c\u4e86\u6ce8\u89e3\u3002 \u8bba\u6587\u6709\u70b9\u590d\u6742\uff0c\u8fd8\u6ca1\u6709\u8bfb\u5b8c\u3002","title":"Spinning lock with timeout"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/paper-Spinlock-with-timeout/#pseudocode#scalable#queue-based#spin#locks#with#timeout","text":"NOTE: \u8fd9\u662f rochester paper Scalable Queue-Based Spin Locks with Timeout \u7684pseudocode","title":"pseudocode Scalable Queue-Based Spin Locks with Timeout"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/paper-Spinlock-with-timeout/paper/","text":"paper Scalable Queue-Based Spin Locks with Timeout ABSTRACT Queue-based spin locks allow programs with busy-wait synchronization to scale to very large multiprocessors, without fear of starvation or performance-destroying contention. So called try locks , traditionally based on non-scalable test-and-set locks, allow a process to abandon its attempt to acquire a lock after a given amount of time. The process can then pursue(\u7ee7\u7eed) an alternative code path, or yield the processor to some other process. NOTE: 1\u3001 Queue-based spin locks \u5177\u5907 scalability \u3001\"fairness\u516c\u5e73-starvation-free\" 2\u3001So called try locks , traditionally based on test-and-set locks \u4e0d\u5177\u5907 scalability \u3001\u4e0d\u5177\u5907\"fairness\u516c\u5e73-starvation-free\"\uff0c\u4f46\u662f\u5b83\u53ef\u4ee5\u5b9e\u73b0bounded waiting(timeout) We demonstrate that it is possible to obtain both scalability and bounded waiting, using variants of the queue-based locks of Craig, Landin, and Hagersten, and of Mellor-Crummey and Scott. A process that decides to stop waiting for one of these new locks can \u201clink itself out of line\u201d atomically. Single-processor experiments reveal performance penalties of 50\u2013100% for the CLH and MCS try locks in comparison to their standard versions; this marginal cost decreases with larger numbers of processors. NOTE: 1\u3001\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u7efc\u5408 scalability \u3001\"fairness\u516c\u5e73-starvation-free\"\u3001bounded waiting \u7684 spinning lock\u65b9\u6848 Comparison\u3001\u5b9e\u9a8c\u7ed3\u679c We have also compared our queue-based locks to a traditional test-and-test-and-set-lock with exponential backoff and timeout. At modest (non-zero) levels of contention, the queued locks sacrifice cache locality for fairness, resulting in a worst-case 3X performance penalty. At high levels of contention, however, they display a 1.5\u20132X performance advantage, with significantly more regular timings and significantly higher rates of acquisition prior to timeout. NOTE: 1\u3001tradeoff: sacrifice cache locality for fairness 1\u3001INTRODUCTION Spin locks are widely used for mutual exclusion on shared memory multiprocessors . Traditional test-and set-based spin locks are vulnerable(\u6613\u53d7\u2026\u2026\u7684\u653b\u51fb) to memory and interconnect contention , and do not scale well to large machines. Queue-based spin locks avoid contention by arranging for every waiting process to spin on a separate, local flag in memory. NOTE: 1\u3001\u5173\u4e8e \" memory and interconnect contention \"\uff0c\u53c2\u89c1 csdn \u804a\u804a\u9ad8\u5e76\u53d1\uff08\u4e94\uff09\u7406\u89e3\u7f13\u5b58\u4e00\u81f4\u6027\u534f\u8bae\u4ee5\u53ca\u5bf9\u5e76\u53d1\u7f16\u7a0b\u7684\u5f71\u54cd Lock contention and scalability Several researchers (ourselves among them) have conjectured(\u63a8\u6d4b\uff1b\u731c\u60f3\uff1b\u5047\u8bbe) that any program that routinely encounters high levels of lock contention is unlikely to scale well to large machines(\u6240\u8c13large machine\uff0c\u662f\u6307\u5177\u5907\u66f4\u591acore\u7684). Conversely, in any program that scales well, the overhead of spin locks may not be a serious concern. The problem with this argument is that infrequent pathological(\u75c5\u6001) events can be a serious problem to users. An improvement in worst case performance from, say, 2 to 10% efficiency may be enormously(\u6781\u5176\u7684) valuable, even if typical efficiency is more like 80%. With the increasing popularity of medium-scale (20\u2013100 processor) server-class machines, we believe that scalable spin locks will prove increasingly important. NOTE: \u4e00\u3001\u7ffb\u8bd1\u5982\u4e0b: \"\u8fd9\u79cd\u89c2\u70b9\u7684\u95ee\u9898\u5728\u4e8e\uff0c\u5076\u5c14\u53d1\u751f\u7684\u75c5\u6001\u4e8b\u4ef6\u5bf9\u7528\u6237\u6765\u8bf4\u53ef\u80fd\u662f\u4e00\u4e2a\u4e25\u91cd\u7684\u95ee\u9898\u3002 \u5728\u6700\u574f\u7684\u60c5\u51b5\u4e0b\uff0c\u4ece2\u523010%\u7684\u6548\u7387\u63d0\u9ad8\u53ef\u80fd\u662f\u975e\u5e38\u6709\u4ef7\u503c\u7684\uff0c\u5373\u4f7f\u5178\u578b\u7684\u6548\u7387\u66f4\u63a5\u8fd180%\u3002 \u968f\u7740\u4e2d\u7b49\u89c4\u6a21(20-100\u4e2a\u5904\u7406\u5668)\u670d\u52a1\u5668\u7ea7\u673a\u5668\u7684\u65e5\u76ca\u666e\u53ca\uff0c\u6211\u4eec\u76f8\u4fe1\u53ef\u4f38\u7f29\u81ea\u65cb\u9501\u5c06\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\" \u7406\u89e3\u4e0a\u9762\u8fd9\u6bb5\u8bdd\uff0c\u9700\u8981\u4e00\u70b9\u7edf\u8ba1\u5b66\u7684\u57fa\u7840: 1\u3001\"typical efficiency is more like 80%\"\u5176\u5b9e\u7c7b\u4f3c\u4e8e\u7edf\u8ba1\u5b66\u4e2d\u7684\"\u4f17\u6570\"\u3001\"\u5e73\u5747\u6570\"\u7684\u6982\u5ff5 2\u3001\"worst case performance\"\u5176\u5b9e\u5c31\u662f\u4e00\u4e9b\u6781\u7aef\u503c In a traditional test and set spin lock, processes are mutually anonymous. In a queue-based lock, they are not: each waiting process is linked to its predecessor and successor in the queue. In previous work [6], members of our group considered the possibility that a spinning process might be preempted, thereby unnecessarily delaying processes farther down the queue. In the current paper we consider the possibility that a spinning process may \u201cbecome impatient\u201d and wish to leave the queue before acquiring the lock. NOTE: \u4e00\u3001\u7ffb\u8bd1\u5982\u4e0b: \"\u5728\u4f20\u7edf\u7684test\u548cset spin lock\u4e2d\uff0c\u8fdb\u7a0b\u662f\u76f8\u4e92\u533f\u540d\u7684\u3002 \u5728\u57fa\u4e8e\u961f\u5217\u7684\u9501\u4e2d\uff0c\u5b83\u4eec\u4e0d\u662f:\u6bcf\u4e2a\u7b49\u5f85\u8fdb\u7a0b\u90fd\u94fe\u63a5\u5230\u961f\u5217\u4e2d\u7684\u524d\u4efb\u548c\u540e\u7ee7\u8fdb\u7a0b\u3002 \u5728\u4e4b\u524d\u7684\u5de5\u4f5c[6]\u4e2d\uff0c\u6211\u4eec\u5c0f\u7ec4\u7684\u6210\u5458\u8003\u8651\u4e86\u7eba\u4e1d\u8fc7\u7a0b\u53ef\u80fd\u662f \u62a2\u5360\uff0c\u56e0\u6b64\u4e0d\u5fc5\u8981\u5730\u5ef6\u8fdf\u961f\u5217\u540e\u9762\u7684\u8fdb\u7a0b\u3002 \u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u8003\u8651\u4e86\u4e00\u79cd\u53ef\u80fd\u6027\uff0c\u5373\u4e00\u4e2a\u6b63\u5728\u65cb\u8f6c\u7684\u8fdb\u7a0b\u53ef\u80fd\u4f1a\u201c\u5931\u53bb\u8010\u5fc3\u201d\uff0c\u5e76\u5e0c\u671b\u5728\u83b7\u5f97\u9501\u4e4b\u524d\u79bb\u5f00\u961f\u5217\u3002\" Timeout-capable spin locks\u3001try-locks NOTE: 1\u3001\u8fd9\u4e00\u8282\u5173\u4e8elock with timeout\u7684\u91cd\u8981\u6027 Timeout-capable spin locks, sometimes referred to as \u201ctry-locks\u201d, are important for several reasons: 1\u3001A process in a soft real-time application may need to bound the time it spends waiting for a lock. If the timeout expires, the process can choose to announce an error or to pursue(\u7ee7\u7eed) an alternative code path that does not require the lock. 2\u3001Absent special OS support, a user-level process in a multiprogrammed environment may be preempted while holding a lock. Timeout allows other processes waiting for the lock to give up, yield the processor, and try again when rescheduled. NOTE: 1\u3001\u5df2\u7ecf\u83b7\u5f97lock\u7684process\u88ab\u6302\u8d77\u4e86\uff0c\u90a3\u4e48\u7b49\u5f85lock\u7684process\u4f1a\u56e0\u4e3atimeout\u800c\u653e\u5f03 3\u3001In a parallel database system, timeout provides a viable strategy for deadlock recovery. A process that waits \u201ctoo long\u201d for a lock can assume that deadlock has occurred, abort the current transaction, and retry. We are aware of commercially significant signal processing applications that use timeout for reason (1), and parallel database servers that use timeout for reasons (2) and (3). In the latter case, timeout may be the deciding factor in making spin locks usable in user-level code. In contrast to the problem of preemption among spinning processes, timeout does not require interaction between the scheduler and the synchronization algorithm. It is complicated, however, by the fact that a timed-out process may attempt to acquire the lock again, possibly many times, before the point at which it would have acquired the lock if it had stayed in line. The possibility of multiple aborted acquisition attempts suggests that a timed-out process must remove itself from the queue entirely, leaving nothing behind; otherwise we would be unable to bound the space or time requirements of the algorithm. We have developed so-called \u201ctry lock\u201d (timeout capable) versions of our MCS queue-based lock [8] and of the CLH queue-based lock of Craig [2] and Landin and Hagersten [7]. After presenting additional background information in section 2, we describe our new locks in section 3. Both new locks employ swap and compare and swap instructions, and can be implemented on any shared-memory machine, with or without cache coherence, that provides these operations or their equivalent. In section 4 we present performance results obtained on a 56-processor Sun Wildfire machine. We conclude with a summary of recommendations in section 5. 2\u3001BACKGROUND Spinning and blocking NOTE: 1\u3001\u8fd9\u6bb5\u5173\u4e8e\"Spinning and blocking\"\u7684\u603b\u7ed3\u662f\u975e\u5e38\u597d\u7684 Programmers face several choices when synchronizing processes on a shared-memory multiprocessor. The most basic choice is between spinning (busy-waiting), in which processes actively poll for a desired condition, and blocking, in which processes yield the processor in expectation that they will be made runnable again when the desired condition holds. Spinning is the method of choice when the expected wait time is small, or when there is no other productive use for the processor. Spinning\u3001busy-wait The most basic busy-wait mechanism is a test-and-set lock, in which a process desiring entry to a critical section repeatedly attempts to change a \u201clocked\u201d flag from false to true, using an atomic hardware primitive. Unfortunately, test-and-set locks lead to increasing amounts of contention for memory and bus or interconnect bandwidth as the number of competing processors grows. This contention can be reduced somewhat by polling with ordinary read operations, rather than atomic test and set operations; polls are then satisfied from local caches during critical sections, with a burst of refill traffic whenever the lock is released. The cost of the burst can be further reduced using Ethernet-style exponential backoff [8]. Figure 1 shows code for a test-and-test-and-set (TATAS) lock with exponential backoff. NOTE: \u4e00\u3001\u540e\u534a\u6bb5\u7684\u7ffb\u8bd1\u5982\u4e0b: \"\u901a\u8fc7\u666e\u901a\u8bfb\u64cd\u4f5c\u7684\u8f6e\u8be2\uff0c\u800c\u4e0d\u662f\u539f\u5b50\u6d4b\u8bd5\u548c\u8bbe\u7f6e\u64cd\u4f5c\uff0c\u53ef\u4ee5\u591a\u5c11\u51cf\u5c11\u8fd9\u79cd\u4e89\u7528;\u7136\u540e\uff0c\u5728\u5173\u952e\u90e8\u5206\uff0c\u672c\u5730\u7f13\u5b58\u6ee1\u8db3\u8f6e\u8be2\uff0c\u5f53\u9501\u88ab\u91ca\u653e\u65f6\uff0c\u4f1a\u6709\u7206\u53d1\u6027\u7684\u91cd\u65b0\u586b\u5145\u6d41\u91cf\u3002\u4f7f\u7528\u4ee5\u592a\u7f51\u98ce\u683c\u7684\u6307\u6570\u56de\u9000[8]\u53ef\u4ee5\u8fdb\u4e00\u6b65\u964d\u4f4e\u7a81\u53d1\u7684\u6210\u672c\u3002\u56fe1\u663e\u793a\u4e86\u5e26\u6709\u6307\u6570\u540e\u9000\u7684\u6d4b\u8bd5-\u6d4b\u8bd5-\u8bbe\u7f6e(TATAS)\u9501\u7684\u4ee3\u7801\u3002\" \u4e8c\u3001read\u5e76\u4e0d\u4f1a\u5bfc\u81f4cache coherence flood\u3001high interconnect contention \u4e09\u3001\"polls are then satisfied from local caches during critical sections\"\u6307\u7684\u662f: if ( * L ) continue ; // spin with reads \u6b64\u65f6thread\u6267\u884c\u7684\u662fread\uff0c\u56e0\u6b64\u53ea\u9700\u8981\u548c\u81ea\u5df1\u7684cache\u4ea4\u4e92\u5373\u53ef\uff0c\u8fd9\u5c31\u662f\"polls are then satisfied from local caches during critical sections\"\u7684\u542b\u4e49 \u56db\u3001\"with a burst of refill traffic whenever the lock is released\"\u6307\u7684\u662f: \u5f53lock\u88abrelease\uff0c\u5219\u6240\u6709\u5176\u4ed6\u7684process\u4f1a\u5c1d\u8bd5\u8fdb\u884c\u5199\u5165\uff0c\u5219\u8fd9\u5c31\u5bfc\u81f4\u4e86\"burst of refill\"\uff0c\u73af\u8282\u5b83\u7684\u65b9\u6cd5\u662f\"Ethernet-style exponential backoff\" typedef unsigned long bool ; typedef volatile bool tatas_lock ; void tatas_acquire ( tatas_lock * L ) { if ( tas ( L )) { int b = BACKOFF_BASE ; do { for ( i = b ; i ; i -- ) ; // delay b = min ( b * BACKOFF_FACTOR , BACKOFF_CAP ); if ( * L ) continue ; // spin with reads } while ( tas ( L )); } } void tatas_release ( tatas_lock * L ) { * L = 0 ; } Figure 1: Test-and-test and set (TATAS) lock with exponential backoff. Parameters BACKOFF BASE , BACKOFF FACTOR , and BACKOFF CAP must be tuned by trial and error for each individual machine architecture. 2.1 Queue-based locks Even with exponential backoff, test-and-test and set locks still induce significant contention, leading to irregular timings and compromised memory performance on large machines. NOTE: 1\u3001\u5173\u4e8e\"irregular timings and compromised memory performance on large machines\"\u7684\u4ecb\u7ecd\uff0c\u53c2\u89c1csdn \u804a\u804a\u9ad8\u5e76\u53d1\uff08\u516d\uff09\u5b9e\u73b0\u51e0\u79cd\u81ea\u65cb\u9501\uff08\u4e00\uff09 \u7f3a\u70b9\u662f\u5728\u9501\u9ad8\u4e89\u7528\u7684\u60c5\u51b5\u4e0b\uff0c\u7ebf\u7a0b\u5f88\u96be\u4e00\u6b21\u5c31\u83b7\u53d6\u9501\uff0cCAS\u7684\u64cd\u4f5c\u4f1a\u5927\u5927\u589e\u52a0\u3002 Queue-based spin locks eliminate these problems by arranging for every competing process to spin on a different memory location. To ensure scalability, each location must be local to the spinning process, either by virtue of hardware cache coherence or as a result of explicit local allocation on a non-cache-coherent machine. NOTE: \u4e00\u3001\u4e3a\u4e86\u964d\u4f4econtention\uff0cr\u8ba9\u6bcf\u4e2acompeting process spin on a different memory location\uff0c\u4e3a\u4e86\u4fdd\u8bc1scalability\uff0c\u6bcf\u4e2alocation\u5fc5\u987blocal to the spinning process\uff0c\u90a3\u5982\u4f55\u5b9e\u73b0\u5462? 1\u3001\u5bf9\u4e8e\u652f\u6301cache coherence\u7684hardware\uff0c\u8fd9\u662f\u5929\u7136\u6ee1\u8db3\u7684 2\u3001\u5bf9\u4e8enon-cache-coherent machine\uff0c\u901a\u8fc7 explicit local allocation \u6765\u5b9e\u73b0 \u603b\u7684\u6765\u8bf4\uff0c\u601d\u8def\u662f: \u901a\u8fc7\u589e\u52a0locality\u6765\u964d\u4f4econtention\uff0c\u6765\u63d0\u9ad8 scalability Several researchers, working independently and roughly concurrently, developed queue-based spin locks in the late 1980s. Anderson [1] and Graunke and Thakkar [3] embed their queues in per-lock arrays, requiring space per lock proportional to the maximum number of processes that may compete for access concurrently. Anderson\u2019s lock uses atomic fetch and increment instructions to assign slots in the array to waiting processes; Graunke and Thakkar\u2019s lock uses swap instead. MCS lock Our MCS lock, co-designed with John Mellortypedef Crummey [8], employs a linked list with pointers from each process to its successor; it relies on swap and compare and swap , and has the advantage of requiring total space linear in the number of locks and the number of competing processes. (The lock can be re-written to use only swap , but at the expense of FIFO ordering: in the swap-only version of mcs release there is a timing window that may allow newly arriving processes to jump ahead of processes already in the queue.) The MCS lock is naturally suited to local-only spinning on both cache-coherent and non-cache coherent machines. The Anderson and Graunke/Thakkar locks must be modified to employ an extra level of indirection on non-cache-coherent machines. typedef struct mcs_qnode { volatile bool waiting ; volatile struct mcs_qnode * volatile next ; } mcs_qnode ; typedef volatile mcs_qnode * mcs_qnode_ptr ; typedef mcs_qnode_ptr mcs_lock ; // initialized to nil void mcs_acquire ( mcs_lock * L , mcs_qnode_ptr I ) { I -> next = nil ; mcs_qnode_ptr pred = swap ( L , I ); if ( pred == nil ) return ; // lock was free I -> waiting = true ; // word on which to spin pred -> next = I ; // make pred point to me while ( I -> waiting ) ; // spin } void mcs_release ( mcs_lock * L , mcs_qnode_ptr I ) { mcs_qnode_ptr succ ; if ( ! ( succ = I -> next )) { // I seem to have no succ. // try to fix global pointer if ( compare_and_store ( L , I , nil )) return ; do { succ = I -> next ; } while ( ! succ ); // wait for successor } succ -> waiting = false ; } Figure 2: The MCS queue-based spin lock. Parameter I points to a qnode record allocated (in an enclosing scope) in shared memory locally-accessible to the invoking processor. CLH lock The CLH lock, developed about three years later by Craig [2] and, independently, Landin and Hagersten [7], also employs a linked list, but with pointers from each process to its predecessor. The CLH lock relies on atomic swap, and may outperform the MCS lock on cache-coherent machines. Like the Anderson and Graunke/Thakkar locks, it requires an extra level of indirection to avoid spinning on remote locations on a non-cache-coherent machine [2]. Code for the MCS and CLH locks appears in Figures 2, 3, and 4. typedef struct clh_qnode { volatile bool waiting ; volatile struct clh_qnode * volatile prev ; } clh_qnode ; typedef volatile clh_qnode * clh_qnode_ptr ; typedef clh_qnode_ptr clh_lock ; // initialized to point to an unowned qnode void clh_acquire ( clh_lock * L , clh_qnode_ptr I ) { I -> waiting = true ; clh_qnode_ptr pred = I -> prev = swap ( L , I ); while ( pred -> waiting ) ; // spin } void clh_release ( clh_qnode_ptr * I ) { clh_qnode_ptr pred = ( * I ) -> prev ; ( * I ) -> waiting = false ; * I = pred ; // take pred\u2019s qnode } Figure 3: The CLH queue-based spin lock. Parameter I points to qnode record or, in clh release, to a pointer to a qnode record. The qnode \u201cbelongs\u201d to the calling process, but may be in main memory anywhere in the system, and will generally change identity as a result of releasing the lock. typedef struct clh_numa_qnode { volatile bool * w_ptr ; volatile struct clh_qnode * volatile prev ; } clh_numa_qnode ; typedef volatile clh_numa_qnode * clh_numa_qnode_ptr ; typedef clh_numa_qnode_ptr clh_numa_lock ; // initialized to point to an unowned qnode const bool * granted = 0x1 ; void clh_numa_acquire ( clh_numa_lock * L , clh_numa_qnode_ptr I ) { volatile bool waiting = true ; I -> w_ptr = nil ; clh_numa_qnode_ptr pred = I -> prev = swap ( L , I ); volatile bool * p = swap ( & pred -> w_ptr , & waiting ); if ( p == granted ) return ; while ( waiting ) ; // spin } void clh_numa_release ( clh_numa_qnode_ptr * I ) { clh_numa_qnode_ptr pred = ( * I ) -> prev ; volatile bool * p = swap ( & (( * I ) -> w_ptr ), granted ); if ( p ) * p = false ; * I = pred ; // take pred\u2019s qnode } Figure 4: Alternative version of the CLH lock, with an extra level of indirection to avoid remote spinning on a non-cache-coherent machine. // return value indicates whether lock was acquired bool tatas_try_acquire ( tatas_lock * L , hrtime_r T ) { if ( tas ( L )) { hrtime_t start = gethrtime (); int b = BACKOFF_BASE ; do { if ( gethrtime () - start > T ) // \u5df2\u7ecftimeout\u4e86 return false ; for ( i = b ; i ; i -- ) ; // delay b = min ( b * BACKOFF_FACTOR , BACKOFF_CAP ); if ( * L ) continue ; // spin with reads } while ( tas ( L )); } } Figure 5: The standard TATAS-try lock. Type definitions and release code are the same as in Figure 1. 2.2 Atomic primitives 3\u3001TRY LOCKS As noted in section 1, a process may wish to bound the time it may wait for a lock, in order to accommodate soft real-time constraints, to avoid waiting for a preempted peer, or to recover from transaction deadlock. NOTE: 1\u3001\u4e0a\u8ff0\"to avoid waiting for a preempted peer\"\u7684\u542b\u4e49\u662f: \u5f53\u524d\u6301\u6709lock\u7684process\u88ab\"preempted\"\uff0c\u90a3\u4e48\u5b83\u5c31\u4e0d\u4f1a\u88ab\u6267\u884c\uff0c\u90a3\u4e48\u5b83\u5c31\u4e00\u76f4\u6301\u6709\u9501\uff0c\u6240\u4ee5\u5176\u4ed6\u7684process\u5c06\u9677\u5165\u7b49\u5f85\u3002 Such a bound is easy to achieve with a test_and_set lock (see Figure 5): processes are anonymous and compete with one another chaotically(\u6df7\u4e71\u7684). Things are not so simple, however, in a queue-based lock: a waiting process is linked into a data structure on which other processes depend; it cannot simply leave. NOTE: 1\u3001\"Figure 5\"\u4e2d\uff0c\u6240\u5c55\u793a\u7684\u662f\"TATAS-try lock\" A similar problem occurs in multiprogrammed systems when a process stops spinning because it has been preempted. Our previous work in scheduler-conscious synchronization [6] arranged to mark the queue node of a preempted process so that the process releasing the lock would simply pass it over(\u4f9d\u6b21\u4f20\u9012\u4e0b\u53bb). Upon being rescheduled, a skipped-over process would have to reenter the queue. A process that had yet to reach the head of the queue when rescheduled would retain its original position. 3.1 The CLH-try lock","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/paper-Spinlock-with-timeout/paper/#paper#scalable#queue-based#spin#locks#with#timeout","text":"","title":"paper Scalable Queue-Based Spin Locks with Timeout"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/paper-Spinlock-with-timeout/paper/#abstract","text":"Queue-based spin locks allow programs with busy-wait synchronization to scale to very large multiprocessors, without fear of starvation or performance-destroying contention. So called try locks , traditionally based on non-scalable test-and-set locks, allow a process to abandon its attempt to acquire a lock after a given amount of time. The process can then pursue(\u7ee7\u7eed) an alternative code path, or yield the processor to some other process. NOTE: 1\u3001 Queue-based spin locks \u5177\u5907 scalability \u3001\"fairness\u516c\u5e73-starvation-free\" 2\u3001So called try locks , traditionally based on test-and-set locks \u4e0d\u5177\u5907 scalability \u3001\u4e0d\u5177\u5907\"fairness\u516c\u5e73-starvation-free\"\uff0c\u4f46\u662f\u5b83\u53ef\u4ee5\u5b9e\u73b0bounded waiting(timeout) We demonstrate that it is possible to obtain both scalability and bounded waiting, using variants of the queue-based locks of Craig, Landin, and Hagersten, and of Mellor-Crummey and Scott. A process that decides to stop waiting for one of these new locks can \u201clink itself out of line\u201d atomically. Single-processor experiments reveal performance penalties of 50\u2013100% for the CLH and MCS try locks in comparison to their standard versions; this marginal cost decreases with larger numbers of processors. NOTE: 1\u3001\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u7efc\u5408 scalability \u3001\"fairness\u516c\u5e73-starvation-free\"\u3001bounded waiting \u7684 spinning lock\u65b9\u6848","title":"ABSTRACT"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/paper-Spinlock-with-timeout/paper/#comparison","text":"We have also compared our queue-based locks to a traditional test-and-test-and-set-lock with exponential backoff and timeout. At modest (non-zero) levels of contention, the queued locks sacrifice cache locality for fairness, resulting in a worst-case 3X performance penalty. At high levels of contention, however, they display a 1.5\u20132X performance advantage, with significantly more regular timings and significantly higher rates of acquisition prior to timeout. NOTE: 1\u3001tradeoff: sacrifice cache locality for fairness","title":"Comparison\u3001\u5b9e\u9a8c\u7ed3\u679c"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/paper-Spinlock-with-timeout/paper/#1introduction","text":"Spin locks are widely used for mutual exclusion on shared memory multiprocessors . Traditional test-and set-based spin locks are vulnerable(\u6613\u53d7\u2026\u2026\u7684\u653b\u51fb) to memory and interconnect contention , and do not scale well to large machines. Queue-based spin locks avoid contention by arranging for every waiting process to spin on a separate, local flag in memory. NOTE: 1\u3001\u5173\u4e8e \" memory and interconnect contention \"\uff0c\u53c2\u89c1 csdn \u804a\u804a\u9ad8\u5e76\u53d1\uff08\u4e94\uff09\u7406\u89e3\u7f13\u5b58\u4e00\u81f4\u6027\u534f\u8bae\u4ee5\u53ca\u5bf9\u5e76\u53d1\u7f16\u7a0b\u7684\u5f71\u54cd","title":"1\u3001INTRODUCTION"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/paper-Spinlock-with-timeout/paper/#lock#contention#and#scalability","text":"Several researchers (ourselves among them) have conjectured(\u63a8\u6d4b\uff1b\u731c\u60f3\uff1b\u5047\u8bbe) that any program that routinely encounters high levels of lock contention is unlikely to scale well to large machines(\u6240\u8c13large machine\uff0c\u662f\u6307\u5177\u5907\u66f4\u591acore\u7684). Conversely, in any program that scales well, the overhead of spin locks may not be a serious concern. The problem with this argument is that infrequent pathological(\u75c5\u6001) events can be a serious problem to users. An improvement in worst case performance from, say, 2 to 10% efficiency may be enormously(\u6781\u5176\u7684) valuable, even if typical efficiency is more like 80%. With the increasing popularity of medium-scale (20\u2013100 processor) server-class machines, we believe that scalable spin locks will prove increasingly important. NOTE: \u4e00\u3001\u7ffb\u8bd1\u5982\u4e0b: \"\u8fd9\u79cd\u89c2\u70b9\u7684\u95ee\u9898\u5728\u4e8e\uff0c\u5076\u5c14\u53d1\u751f\u7684\u75c5\u6001\u4e8b\u4ef6\u5bf9\u7528\u6237\u6765\u8bf4\u53ef\u80fd\u662f\u4e00\u4e2a\u4e25\u91cd\u7684\u95ee\u9898\u3002 \u5728\u6700\u574f\u7684\u60c5\u51b5\u4e0b\uff0c\u4ece2\u523010%\u7684\u6548\u7387\u63d0\u9ad8\u53ef\u80fd\u662f\u975e\u5e38\u6709\u4ef7\u503c\u7684\uff0c\u5373\u4f7f\u5178\u578b\u7684\u6548\u7387\u66f4\u63a5\u8fd180%\u3002 \u968f\u7740\u4e2d\u7b49\u89c4\u6a21(20-100\u4e2a\u5904\u7406\u5668)\u670d\u52a1\u5668\u7ea7\u673a\u5668\u7684\u65e5\u76ca\u666e\u53ca\uff0c\u6211\u4eec\u76f8\u4fe1\u53ef\u4f38\u7f29\u81ea\u65cb\u9501\u5c06\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\" \u7406\u89e3\u4e0a\u9762\u8fd9\u6bb5\u8bdd\uff0c\u9700\u8981\u4e00\u70b9\u7edf\u8ba1\u5b66\u7684\u57fa\u7840: 1\u3001\"typical efficiency is more like 80%\"\u5176\u5b9e\u7c7b\u4f3c\u4e8e\u7edf\u8ba1\u5b66\u4e2d\u7684\"\u4f17\u6570\"\u3001\"\u5e73\u5747\u6570\"\u7684\u6982\u5ff5 2\u3001\"worst case performance\"\u5176\u5b9e\u5c31\u662f\u4e00\u4e9b\u6781\u7aef\u503c In a traditional test and set spin lock, processes are mutually anonymous. In a queue-based lock, they are not: each waiting process is linked to its predecessor and successor in the queue. In previous work [6], members of our group considered the possibility that a spinning process might be preempted, thereby unnecessarily delaying processes farther down the queue. In the current paper we consider the possibility that a spinning process may \u201cbecome impatient\u201d and wish to leave the queue before acquiring the lock. NOTE: \u4e00\u3001\u7ffb\u8bd1\u5982\u4e0b: \"\u5728\u4f20\u7edf\u7684test\u548cset spin lock\u4e2d\uff0c\u8fdb\u7a0b\u662f\u76f8\u4e92\u533f\u540d\u7684\u3002 \u5728\u57fa\u4e8e\u961f\u5217\u7684\u9501\u4e2d\uff0c\u5b83\u4eec\u4e0d\u662f:\u6bcf\u4e2a\u7b49\u5f85\u8fdb\u7a0b\u90fd\u94fe\u63a5\u5230\u961f\u5217\u4e2d\u7684\u524d\u4efb\u548c\u540e\u7ee7\u8fdb\u7a0b\u3002 \u5728\u4e4b\u524d\u7684\u5de5\u4f5c[6]\u4e2d\uff0c\u6211\u4eec\u5c0f\u7ec4\u7684\u6210\u5458\u8003\u8651\u4e86\u7eba\u4e1d\u8fc7\u7a0b\u53ef\u80fd\u662f \u62a2\u5360\uff0c\u56e0\u6b64\u4e0d\u5fc5\u8981\u5730\u5ef6\u8fdf\u961f\u5217\u540e\u9762\u7684\u8fdb\u7a0b\u3002 \u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u8003\u8651\u4e86\u4e00\u79cd\u53ef\u80fd\u6027\uff0c\u5373\u4e00\u4e2a\u6b63\u5728\u65cb\u8f6c\u7684\u8fdb\u7a0b\u53ef\u80fd\u4f1a\u201c\u5931\u53bb\u8010\u5fc3\u201d\uff0c\u5e76\u5e0c\u671b\u5728\u83b7\u5f97\u9501\u4e4b\u524d\u79bb\u5f00\u961f\u5217\u3002\"","title":"Lock contention and scalability"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/paper-Spinlock-with-timeout/paper/#timeout-capable#spin#lockstry-locks","text":"NOTE: 1\u3001\u8fd9\u4e00\u8282\u5173\u4e8elock with timeout\u7684\u91cd\u8981\u6027 Timeout-capable spin locks, sometimes referred to as \u201ctry-locks\u201d, are important for several reasons: 1\u3001A process in a soft real-time application may need to bound the time it spends waiting for a lock. If the timeout expires, the process can choose to announce an error or to pursue(\u7ee7\u7eed) an alternative code path that does not require the lock. 2\u3001Absent special OS support, a user-level process in a multiprogrammed environment may be preempted while holding a lock. Timeout allows other processes waiting for the lock to give up, yield the processor, and try again when rescheduled. NOTE: 1\u3001\u5df2\u7ecf\u83b7\u5f97lock\u7684process\u88ab\u6302\u8d77\u4e86\uff0c\u90a3\u4e48\u7b49\u5f85lock\u7684process\u4f1a\u56e0\u4e3atimeout\u800c\u653e\u5f03 3\u3001In a parallel database system, timeout provides a viable strategy for deadlock recovery. A process that waits \u201ctoo long\u201d for a lock can assume that deadlock has occurred, abort the current transaction, and retry. We are aware of commercially significant signal processing applications that use timeout for reason (1), and parallel database servers that use timeout for reasons (2) and (3). In the latter case, timeout may be the deciding factor in making spin locks usable in user-level code. In contrast to the problem of preemption among spinning processes, timeout does not require interaction between the scheduler and the synchronization algorithm. It is complicated, however, by the fact that a timed-out process may attempt to acquire the lock again, possibly many times, before the point at which it would have acquired the lock if it had stayed in line. The possibility of multiple aborted acquisition attempts suggests that a timed-out process must remove itself from the queue entirely, leaving nothing behind; otherwise we would be unable to bound the space or time requirements of the algorithm. We have developed so-called \u201ctry lock\u201d (timeout capable) versions of our MCS queue-based lock [8] and of the CLH queue-based lock of Craig [2] and Landin and Hagersten [7]. After presenting additional background information in section 2, we describe our new locks in section 3. Both new locks employ swap and compare and swap instructions, and can be implemented on any shared-memory machine, with or without cache coherence, that provides these operations or their equivalent. In section 4 we present performance results obtained on a 56-processor Sun Wildfire machine. We conclude with a summary of recommendations in section 5.","title":"Timeout-capable spin locks\u3001try-locks"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/paper-Spinlock-with-timeout/paper/#2background","text":"","title":"2\u3001BACKGROUND"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/paper-Spinlock-with-timeout/paper/#spinning#and#blocking","text":"NOTE: 1\u3001\u8fd9\u6bb5\u5173\u4e8e\"Spinning and blocking\"\u7684\u603b\u7ed3\u662f\u975e\u5e38\u597d\u7684 Programmers face several choices when synchronizing processes on a shared-memory multiprocessor. The most basic choice is between spinning (busy-waiting), in which processes actively poll for a desired condition, and blocking, in which processes yield the processor in expectation that they will be made runnable again when the desired condition holds. Spinning is the method of choice when the expected wait time is small, or when there is no other productive use for the processor.","title":"Spinning and blocking"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/paper-Spinlock-with-timeout/paper/#spinningbusy-wait","text":"The most basic busy-wait mechanism is a test-and-set lock, in which a process desiring entry to a critical section repeatedly attempts to change a \u201clocked\u201d flag from false to true, using an atomic hardware primitive. Unfortunately, test-and-set locks lead to increasing amounts of contention for memory and bus or interconnect bandwidth as the number of competing processors grows. This contention can be reduced somewhat by polling with ordinary read operations, rather than atomic test and set operations; polls are then satisfied from local caches during critical sections, with a burst of refill traffic whenever the lock is released. The cost of the burst can be further reduced using Ethernet-style exponential backoff [8]. Figure 1 shows code for a test-and-test-and-set (TATAS) lock with exponential backoff. NOTE: \u4e00\u3001\u540e\u534a\u6bb5\u7684\u7ffb\u8bd1\u5982\u4e0b: \"\u901a\u8fc7\u666e\u901a\u8bfb\u64cd\u4f5c\u7684\u8f6e\u8be2\uff0c\u800c\u4e0d\u662f\u539f\u5b50\u6d4b\u8bd5\u548c\u8bbe\u7f6e\u64cd\u4f5c\uff0c\u53ef\u4ee5\u591a\u5c11\u51cf\u5c11\u8fd9\u79cd\u4e89\u7528;\u7136\u540e\uff0c\u5728\u5173\u952e\u90e8\u5206\uff0c\u672c\u5730\u7f13\u5b58\u6ee1\u8db3\u8f6e\u8be2\uff0c\u5f53\u9501\u88ab\u91ca\u653e\u65f6\uff0c\u4f1a\u6709\u7206\u53d1\u6027\u7684\u91cd\u65b0\u586b\u5145\u6d41\u91cf\u3002\u4f7f\u7528\u4ee5\u592a\u7f51\u98ce\u683c\u7684\u6307\u6570\u56de\u9000[8]\u53ef\u4ee5\u8fdb\u4e00\u6b65\u964d\u4f4e\u7a81\u53d1\u7684\u6210\u672c\u3002\u56fe1\u663e\u793a\u4e86\u5e26\u6709\u6307\u6570\u540e\u9000\u7684\u6d4b\u8bd5-\u6d4b\u8bd5-\u8bbe\u7f6e(TATAS)\u9501\u7684\u4ee3\u7801\u3002\" \u4e8c\u3001read\u5e76\u4e0d\u4f1a\u5bfc\u81f4cache coherence flood\u3001high interconnect contention \u4e09\u3001\"polls are then satisfied from local caches during critical sections\"\u6307\u7684\u662f: if ( * L ) continue ; // spin with reads \u6b64\u65f6thread\u6267\u884c\u7684\u662fread\uff0c\u56e0\u6b64\u53ea\u9700\u8981\u548c\u81ea\u5df1\u7684cache\u4ea4\u4e92\u5373\u53ef\uff0c\u8fd9\u5c31\u662f\"polls are then satisfied from local caches during critical sections\"\u7684\u542b\u4e49 \u56db\u3001\"with a burst of refill traffic whenever the lock is released\"\u6307\u7684\u662f: \u5f53lock\u88abrelease\uff0c\u5219\u6240\u6709\u5176\u4ed6\u7684process\u4f1a\u5c1d\u8bd5\u8fdb\u884c\u5199\u5165\uff0c\u5219\u8fd9\u5c31\u5bfc\u81f4\u4e86\"burst of refill\"\uff0c\u73af\u8282\u5b83\u7684\u65b9\u6cd5\u662f\"Ethernet-style exponential backoff\" typedef unsigned long bool ; typedef volatile bool tatas_lock ; void tatas_acquire ( tatas_lock * L ) { if ( tas ( L )) { int b = BACKOFF_BASE ; do { for ( i = b ; i ; i -- ) ; // delay b = min ( b * BACKOFF_FACTOR , BACKOFF_CAP ); if ( * L ) continue ; // spin with reads } while ( tas ( L )); } } void tatas_release ( tatas_lock * L ) { * L = 0 ; } Figure 1: Test-and-test and set (TATAS) lock with exponential backoff. Parameters BACKOFF BASE , BACKOFF FACTOR , and BACKOFF CAP must be tuned by trial and error for each individual machine architecture.","title":"Spinning\u3001busy-wait"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/paper-Spinlock-with-timeout/paper/#21#queue-based#locks","text":"Even with exponential backoff, test-and-test and set locks still induce significant contention, leading to irregular timings and compromised memory performance on large machines. NOTE: 1\u3001\u5173\u4e8e\"irregular timings and compromised memory performance on large machines\"\u7684\u4ecb\u7ecd\uff0c\u53c2\u89c1csdn \u804a\u804a\u9ad8\u5e76\u53d1\uff08\u516d\uff09\u5b9e\u73b0\u51e0\u79cd\u81ea\u65cb\u9501\uff08\u4e00\uff09 \u7f3a\u70b9\u662f\u5728\u9501\u9ad8\u4e89\u7528\u7684\u60c5\u51b5\u4e0b\uff0c\u7ebf\u7a0b\u5f88\u96be\u4e00\u6b21\u5c31\u83b7\u53d6\u9501\uff0cCAS\u7684\u64cd\u4f5c\u4f1a\u5927\u5927\u589e\u52a0\u3002 Queue-based spin locks eliminate these problems by arranging for every competing process to spin on a different memory location. To ensure scalability, each location must be local to the spinning process, either by virtue of hardware cache coherence or as a result of explicit local allocation on a non-cache-coherent machine. NOTE: \u4e00\u3001\u4e3a\u4e86\u964d\u4f4econtention\uff0cr\u8ba9\u6bcf\u4e2acompeting process spin on a different memory location\uff0c\u4e3a\u4e86\u4fdd\u8bc1scalability\uff0c\u6bcf\u4e2alocation\u5fc5\u987blocal to the spinning process\uff0c\u90a3\u5982\u4f55\u5b9e\u73b0\u5462? 1\u3001\u5bf9\u4e8e\u652f\u6301cache coherence\u7684hardware\uff0c\u8fd9\u662f\u5929\u7136\u6ee1\u8db3\u7684 2\u3001\u5bf9\u4e8enon-cache-coherent machine\uff0c\u901a\u8fc7 explicit local allocation \u6765\u5b9e\u73b0 \u603b\u7684\u6765\u8bf4\uff0c\u601d\u8def\u662f: \u901a\u8fc7\u589e\u52a0locality\u6765\u964d\u4f4econtention\uff0c\u6765\u63d0\u9ad8 scalability Several researchers, working independently and roughly concurrently, developed queue-based spin locks in the late 1980s. Anderson [1] and Graunke and Thakkar [3] embed their queues in per-lock arrays, requiring space per lock proportional to the maximum number of processes that may compete for access concurrently. Anderson\u2019s lock uses atomic fetch and increment instructions to assign slots in the array to waiting processes; Graunke and Thakkar\u2019s lock uses swap instead.","title":"2.1 Queue-based locks"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/paper-Spinlock-with-timeout/paper/#mcs#lock","text":"Our MCS lock, co-designed with John Mellortypedef Crummey [8], employs a linked list with pointers from each process to its successor; it relies on swap and compare and swap , and has the advantage of requiring total space linear in the number of locks and the number of competing processes. (The lock can be re-written to use only swap , but at the expense of FIFO ordering: in the swap-only version of mcs release there is a timing window that may allow newly arriving processes to jump ahead of processes already in the queue.) The MCS lock is naturally suited to local-only spinning on both cache-coherent and non-cache coherent machines. The Anderson and Graunke/Thakkar locks must be modified to employ an extra level of indirection on non-cache-coherent machines. typedef struct mcs_qnode { volatile bool waiting ; volatile struct mcs_qnode * volatile next ; } mcs_qnode ; typedef volatile mcs_qnode * mcs_qnode_ptr ; typedef mcs_qnode_ptr mcs_lock ; // initialized to nil void mcs_acquire ( mcs_lock * L , mcs_qnode_ptr I ) { I -> next = nil ; mcs_qnode_ptr pred = swap ( L , I ); if ( pred == nil ) return ; // lock was free I -> waiting = true ; // word on which to spin pred -> next = I ; // make pred point to me while ( I -> waiting ) ; // spin } void mcs_release ( mcs_lock * L , mcs_qnode_ptr I ) { mcs_qnode_ptr succ ; if ( ! ( succ = I -> next )) { // I seem to have no succ. // try to fix global pointer if ( compare_and_store ( L , I , nil )) return ; do { succ = I -> next ; } while ( ! succ ); // wait for successor } succ -> waiting = false ; } Figure 2: The MCS queue-based spin lock. Parameter I points to a qnode record allocated (in an enclosing scope) in shared memory locally-accessible to the invoking processor.","title":"MCS lock"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/paper-Spinlock-with-timeout/paper/#clh#lock","text":"The CLH lock, developed about three years later by Craig [2] and, independently, Landin and Hagersten [7], also employs a linked list, but with pointers from each process to its predecessor. The CLH lock relies on atomic swap, and may outperform the MCS lock on cache-coherent machines. Like the Anderson and Graunke/Thakkar locks, it requires an extra level of indirection to avoid spinning on remote locations on a non-cache-coherent machine [2]. Code for the MCS and CLH locks appears in Figures 2, 3, and 4. typedef struct clh_qnode { volatile bool waiting ; volatile struct clh_qnode * volatile prev ; } clh_qnode ; typedef volatile clh_qnode * clh_qnode_ptr ; typedef clh_qnode_ptr clh_lock ; // initialized to point to an unowned qnode void clh_acquire ( clh_lock * L , clh_qnode_ptr I ) { I -> waiting = true ; clh_qnode_ptr pred = I -> prev = swap ( L , I ); while ( pred -> waiting ) ; // spin } void clh_release ( clh_qnode_ptr * I ) { clh_qnode_ptr pred = ( * I ) -> prev ; ( * I ) -> waiting = false ; * I = pred ; // take pred\u2019s qnode } Figure 3: The CLH queue-based spin lock. Parameter I points to qnode record or, in clh release, to a pointer to a qnode record. The qnode \u201cbelongs\u201d to the calling process, but may be in main memory anywhere in the system, and will generally change identity as a result of releasing the lock. typedef struct clh_numa_qnode { volatile bool * w_ptr ; volatile struct clh_qnode * volatile prev ; } clh_numa_qnode ; typedef volatile clh_numa_qnode * clh_numa_qnode_ptr ; typedef clh_numa_qnode_ptr clh_numa_lock ; // initialized to point to an unowned qnode const bool * granted = 0x1 ; void clh_numa_acquire ( clh_numa_lock * L , clh_numa_qnode_ptr I ) { volatile bool waiting = true ; I -> w_ptr = nil ; clh_numa_qnode_ptr pred = I -> prev = swap ( L , I ); volatile bool * p = swap ( & pred -> w_ptr , & waiting ); if ( p == granted ) return ; while ( waiting ) ; // spin } void clh_numa_release ( clh_numa_qnode_ptr * I ) { clh_numa_qnode_ptr pred = ( * I ) -> prev ; volatile bool * p = swap ( & (( * I ) -> w_ptr ), granted ); if ( p ) * p = false ; * I = pred ; // take pred\u2019s qnode } Figure 4: Alternative version of the CLH lock, with an extra level of indirection to avoid remote spinning on a non-cache-coherent machine. // return value indicates whether lock was acquired bool tatas_try_acquire ( tatas_lock * L , hrtime_r T ) { if ( tas ( L )) { hrtime_t start = gethrtime (); int b = BACKOFF_BASE ; do { if ( gethrtime () - start > T ) // \u5df2\u7ecftimeout\u4e86 return false ; for ( i = b ; i ; i -- ) ; // delay b = min ( b * BACKOFF_FACTOR , BACKOFF_CAP ); if ( * L ) continue ; // spin with reads } while ( tas ( L )); } } Figure 5: The standard TATAS-try lock. Type definitions and release code are the same as in Figure 1.","title":"CLH lock"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/paper-Spinlock-with-timeout/paper/#22#atomic#primitives","text":"","title":"2.2 Atomic primitives"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/paper-Spinlock-with-timeout/paper/#3try#locks","text":"As noted in section 1, a process may wish to bound the time it may wait for a lock, in order to accommodate soft real-time constraints, to avoid waiting for a preempted peer, or to recover from transaction deadlock. NOTE: 1\u3001\u4e0a\u8ff0\"to avoid waiting for a preempted peer\"\u7684\u542b\u4e49\u662f: \u5f53\u524d\u6301\u6709lock\u7684process\u88ab\"preempted\"\uff0c\u90a3\u4e48\u5b83\u5c31\u4e0d\u4f1a\u88ab\u6267\u884c\uff0c\u90a3\u4e48\u5b83\u5c31\u4e00\u76f4\u6301\u6709\u9501\uff0c\u6240\u4ee5\u5176\u4ed6\u7684process\u5c06\u9677\u5165\u7b49\u5f85\u3002 Such a bound is easy to achieve with a test_and_set lock (see Figure 5): processes are anonymous and compete with one another chaotically(\u6df7\u4e71\u7684). Things are not so simple, however, in a queue-based lock: a waiting process is linked into a data structure on which other processes depend; it cannot simply leave. NOTE: 1\u3001\"Figure 5\"\u4e2d\uff0c\u6240\u5c55\u793a\u7684\u662f\"TATAS-try lock\" A similar problem occurs in multiprogrammed systems when a process stops spinning because it has been preempted. Our previous work in scheduler-conscious synchronization [6] arranged to mark the queue node of a preempted process so that the process releasing the lock would simply pass it over(\u4f9d\u6b21\u4f20\u9012\u4e0b\u53bb). Upon being rescheduled, a skipped-over process would have to reenter the queue. A process that had yet to reach the head of the queue when rescheduled would retain its original position.","title":"3\u3001TRY LOCKS"},{"location":"Concurrent-computing/Concurrency-control/Lock/Spinlock/paper-Spinlock-with-timeout/paper/#31#the#clh-try#lock","text":"","title":"3.1 The CLH-try lock"},{"location":"Concurrent-computing/Concurrency-control/Lock/TODO-Hand-over-hand-lock/","text":"Hand-over-hand lock 1\u3001\u5728\u9605\u8bfb drdobbs Choose Concurrency-Friendly Data Structures \u65f6\uff0c\u5176\u4e2d\u4ecb\u7ecd\u4e86\u4e13\u7528\u4e8e linked list \u7684 hand-over-hand lock\u3002 2\u3001hand-over-hand-lock\u662f\u5178\u578b\u7684\u9700\u8981lock multiple lock\u7684 3\u3001tag-lock granularity-fine-grained-reduce contention-\u964d\u4f4e\u9501\u7c92\u5ea6-\u63d0\u9ad8\u5e76\u53d1\u6027-\u51cf\u5c11\u7ade\u4e89 4\u3001\u5b83\u7684\u4e2d\u6587\u7ffb\u8bd1\u662f: \u4ea4\u66ff\u9501(hand-over-hand locking) courses.csail.mit 6.852 Lecture 21 \u770b\u4e86\u4e00\u4e0b\uff0c\u5176\u4e2d\u5bf9\"hand-over-hand lock\"\u8fdb\u884c\u4e86\u6bd4\u8f83\u597d\u7684\u63cf\u8ff0\u3002 jgjin Hand-over-hand locking with the RAII pattern csdn \u4e03\u5468\u4e03\u5e76\u53d1\u4e4b\u7ebf\u7a0b\u4e0e\u9501","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Lock/TODO-Hand-over-hand-lock/#hand-over-hand#lock","text":"1\u3001\u5728\u9605\u8bfb drdobbs Choose Concurrency-Friendly Data Structures \u65f6\uff0c\u5176\u4e2d\u4ecb\u7ecd\u4e86\u4e13\u7528\u4e8e linked list \u7684 hand-over-hand lock\u3002 2\u3001hand-over-hand-lock\u662f\u5178\u578b\u7684\u9700\u8981lock multiple lock\u7684 3\u3001tag-lock granularity-fine-grained-reduce contention-\u964d\u4f4e\u9501\u7c92\u5ea6-\u63d0\u9ad8\u5e76\u53d1\u6027-\u51cf\u5c11\u7ade\u4e89 4\u3001\u5b83\u7684\u4e2d\u6587\u7ffb\u8bd1\u662f: \u4ea4\u66ff\u9501(hand-over-hand locking)","title":"Hand-over-hand lock"},{"location":"Concurrent-computing/Concurrency-control/Lock/TODO-Hand-over-hand-lock/#coursescsailmit#6852#lecture#21","text":"\u770b\u4e86\u4e00\u4e0b\uff0c\u5176\u4e2d\u5bf9\"hand-over-hand lock\"\u8fdb\u884c\u4e86\u6bd4\u8f83\u597d\u7684\u63cf\u8ff0\u3002","title":"courses.csail.mit 6.852 Lecture 21"},{"location":"Concurrent-computing/Concurrency-control/Lock/TODO-Hand-over-hand-lock/#jgjin#hand-over-hand#locking#with#the#raii#pattern","text":"","title":"jgjin Hand-over-hand locking with the RAII pattern"},{"location":"Concurrent-computing/Concurrency-control/Lock/TODO-Hand-over-hand-lock/#csdn","text":"","title":"csdn \u4e03\u5468\u4e03\u5e76\u53d1\u4e4b\u7ebf\u7a0b\u4e0e\u9501"},{"location":"Concurrent-computing/Concurrency-control/Lock/TODO-Mutex/","text":"","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u603b\u7ed3\u5b9e\u73b0non-blocking concurrency control\u7684\u4e00\u4e9b\u6280\u672f\u3002 wikipedia Non-lock concurrency control NOTE: \u539f\u6587\u4e3b\u8981\u4ecb\u7ecd\u7684\u662fDBMS\u4e2d\u7684non-lock concurrency control\uff0c\u5176\u5b9e\u8fd9\u4e9b\u6280\u672f\u4e0d\u4ec5\u4ec5\u5c40\u9650\u4e8eDBMS\uff0c\u5b83\u4eec\u8fd8\u53ef\u4ee5\u7528\u4e8e\u5176\u4ed6\u7684system\u4e2d 1\u3001 Optimistic concurrency control 2\u3001 Timestamp-based concurrency control 3\u3001 Multiversion concurrency control wikipedia Non-blocking algorithm In computer science , an algorithm is called non-blocking if failure or suspension of any thread cannot cause failure or suspension of another thread;[ 1] for some operations, these algorithms provide a useful alternative to traditional blocking implementations . NOTE: \u8868\u660e\u610f\u601d: \u5982\u679c\u4efb\u4f55\u7ebf\u7a0b\u7684\u5931\u8d25\u6216\u6682\u505c\u4e0d\u4f1a\u5bfc\u81f4\u53e6\u4e00\u4e2a\u7ebf\u7a0b\u7684\u5931\u8d25\u6216\u6682\u505c\uff0c\u5219\u79f0\u4e3a**\u975e\u963b\u585e\u7b97\u6cd5**\u3002 \u6ca1\u6709\u7406\u89e3\u5b83\u7684\u542b\u4e49\u3002 A non-blocking algorithm is 1) lock-free if there is guaranteed system-wide progress , and 2) wait-free if there is also guaranteed per-thread progress. 3\u3001obstruction-free \"Non-blocking\" was used as a synonym for \"lock-free\" in the literature until the introduction of obstruction-freedom in 2003.[ 2] NOTE: \u6ca1\u6709\u7406\u89e3\u5b83\u4eec\u7684\u542b\u4e49 Motivation NOTE: Main article: Disadvantages of locks non-blocking algorithm\u662f\u4e3a\u4e86\u89e3\u51b3lock\u7684\u5f0a\u7aef Implementation NOTE: \u4e0b\u9762\u63cf\u8ff0\u4e86\u51e0\u79cd\u5b9e\u73b0\u65b9\u5f0f With few exceptions, non-blocking algorithms use atomic read-modify-write primitives that the hardware must provide, the most notable of which is compare and swap (CAS) . Critical sections are almost always implemented using standard interfaces over these primitives (in the general case, critical sections will be blocking, even when implemented with these primitives). NOTE: atomic read-modify-write\u5728 ./Atomic-operation \u7ae0\u8282\u8fdb\u884c\u4e86\u63cf\u8ff0\u3002 In the 1990s all non-blocking algorithms had to be written \"natively\" with the underlying primitives to achieve acceptable performance. NOTE: \u5728application level\uff0c\u76f4\u63a5\u4f7f\u7528\u8fd9\u4e9batomic primitive\u3002 However, the emerging field of software transactional memory promises standard abstractions for writing efficient non-blocking code.[ 3] [ 4] NOTE: transactional memory Additionally, some non-blocking data structures are weak enough to be implemented without special atomic primitives. These exceptions include: 1\u3001a single-reader single-writer ring buffer FIFO , with a size which evenly divides the overflow of one of the available unsigned integer types, can unconditionally be implemented safely using only a memory barrier NOTE: \u4e0a\u8ff0 ring buffer FIFO \uff0c\u5176\u5b9e\u5c31\u662f\u4e00\u4e2a\u73af\u5f62\u961f\u5217 2\u3001 Read-copy-update with a single writer and any number of readers. (The readers are wait-free; the writer is usually lock-free, until it needs to reclaim memory). 3\u3001 Read-copy-update with multiple writers and any number of readers. (The readers are wait-free; multiple writers generally serialize with a lock and are not obstruction-free).","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/#_1","text":"\u672c\u7ae0\u603b\u7ed3\u5b9e\u73b0non-blocking concurrency control\u7684\u4e00\u4e9b\u6280\u672f\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/#wikipedia#non-lock#concurrency#control","text":"NOTE: \u539f\u6587\u4e3b\u8981\u4ecb\u7ecd\u7684\u662fDBMS\u4e2d\u7684non-lock concurrency control\uff0c\u5176\u5b9e\u8fd9\u4e9b\u6280\u672f\u4e0d\u4ec5\u4ec5\u5c40\u9650\u4e8eDBMS\uff0c\u5b83\u4eec\u8fd8\u53ef\u4ee5\u7528\u4e8e\u5176\u4ed6\u7684system\u4e2d 1\u3001 Optimistic concurrency control 2\u3001 Timestamp-based concurrency control 3\u3001 Multiversion concurrency control","title":"wikipedia Non-lock concurrency control"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/#wikipedia#non-blocking#algorithm","text":"In computer science , an algorithm is called non-blocking if failure or suspension of any thread cannot cause failure or suspension of another thread;[ 1] for some operations, these algorithms provide a useful alternative to traditional blocking implementations . NOTE: \u8868\u660e\u610f\u601d: \u5982\u679c\u4efb\u4f55\u7ebf\u7a0b\u7684\u5931\u8d25\u6216\u6682\u505c\u4e0d\u4f1a\u5bfc\u81f4\u53e6\u4e00\u4e2a\u7ebf\u7a0b\u7684\u5931\u8d25\u6216\u6682\u505c\uff0c\u5219\u79f0\u4e3a**\u975e\u963b\u585e\u7b97\u6cd5**\u3002 \u6ca1\u6709\u7406\u89e3\u5b83\u7684\u542b\u4e49\u3002 A non-blocking algorithm is 1) lock-free if there is guaranteed system-wide progress , and 2) wait-free if there is also guaranteed per-thread progress. 3\u3001obstruction-free \"Non-blocking\" was used as a synonym for \"lock-free\" in the literature until the introduction of obstruction-freedom in 2003.[ 2] NOTE: \u6ca1\u6709\u7406\u89e3\u5b83\u4eec\u7684\u542b\u4e49","title":"wikipedia Non-blocking algorithm"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/#motivation","text":"NOTE: Main article: Disadvantages of locks non-blocking algorithm\u662f\u4e3a\u4e86\u89e3\u51b3lock\u7684\u5f0a\u7aef","title":"Motivation"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/#implementation","text":"NOTE: \u4e0b\u9762\u63cf\u8ff0\u4e86\u51e0\u79cd\u5b9e\u73b0\u65b9\u5f0f With few exceptions, non-blocking algorithms use atomic read-modify-write primitives that the hardware must provide, the most notable of which is compare and swap (CAS) . Critical sections are almost always implemented using standard interfaces over these primitives (in the general case, critical sections will be blocking, even when implemented with these primitives). NOTE: atomic read-modify-write\u5728 ./Atomic-operation \u7ae0\u8282\u8fdb\u884c\u4e86\u63cf\u8ff0\u3002 In the 1990s all non-blocking algorithms had to be written \"natively\" with the underlying primitives to achieve acceptable performance. NOTE: \u5728application level\uff0c\u76f4\u63a5\u4f7f\u7528\u8fd9\u4e9batomic primitive\u3002 However, the emerging field of software transactional memory promises standard abstractions for writing efficient non-blocking code.[ 3] [ 4] NOTE: transactional memory Additionally, some non-blocking data structures are weak enough to be implemented without special atomic primitives. These exceptions include: 1\u3001a single-reader single-writer ring buffer FIFO , with a size which evenly divides the overflow of one of the available unsigned integer types, can unconditionally be implemented safely using only a memory barrier NOTE: \u4e0a\u8ff0 ring buffer FIFO \uff0c\u5176\u5b9e\u5c31\u662f\u4e00\u4e2a\u73af\u5f62\u961f\u5217 2\u3001 Read-copy-update with a single writer and any number of readers. (The readers are wait-free; the writer is usually lock-free, until it needs to reclaim memory). 3\u3001 Read-copy-update with multiple writers and any number of readers. (The readers are wait-free; multiple writers generally serialize with a lock and are not obstruction-free).","title":"Implementation"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Atomic-operation/","text":"\u5173\u4e8e\u672c\u7ae0 Atomic operation\u3002","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Atomic-operation/#_1","text":"Atomic operation\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Atomic-operation/Atomic-instruction/","text":"\u5173\u4e8e\u672c\u7ae0 \u7531\u4e8e\u8fd9\u90e8\u5206\u5185\u5bb9\u662f\u5173\u4e8einstruction\uff0c\u6240\u4ee5\u5c06\u5b83\u79fb\u5230\u4e86\u5de5\u7a0bhardware\u4e2d\uff0c\u53c2\u89c1 ./CPU-memory-access/Atomic \u3002","title":"Atomic-instruction"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Atomic-operation/Atomic-instruction/#_1","text":"\u7531\u4e8e\u8fd9\u90e8\u5206\u5185\u5bb9\u662f\u5173\u4e8einstruction\uff0c\u6240\u4ee5\u5c06\u5b83\u79fb\u5230\u4e86\u5de5\u7a0bhardware\u4e2d\uff0c\u53c2\u89c1 ./CPU-memory-access/Atomic \u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Copy-on-write/","text":"Copy on write in concurrent lock free data structure \u4e00\u3001Copy-on-write\u662f\u5b9e\u73b0concurrent lock free data structure\u7684\u4e00\u79cd\u5e38\u7528\u7684\u6280\u672f: 1\u3001\u5728Read-copy-update \u3001MVCC\u4e2d\uff0c\u90fd\u53ef\u4ee5\u770b\u5230\u5b83\u7684\u5f71\u5b50\uff1b 2\u3001\u7531\u4e8e\u4f1a\u5b58\u5728\u591a\u4efdcopy\uff0c\u56e0\u6b64\u5bf9\u4e8e\u8fd9\u4e9bcopy\u7684reclaimation\u5c31\u53d8\u5f97\u975e\u5e38\u91cd\u8981\uff0c\u5bf9\u4e8e\u6ca1\u6709GC\u7684programming language\uff0c\u9700\u8981\u7531programmer\u5b9e\u73b0memory reclamation\uff0c\u5728 Memory-reclaimation \u7ae0\u8282\u5bf9\u6b64\u8fdb\u884c\u4e86\u8ba8\u8bba\u3002 \u4e8c\u3001memory management \u901a\u8fc7copy on write\u3001memory reclamation\uff0c\u53ef\u4ee5\u53d1\u73b0\u5728lock-free programming\u4e2d\uff0cmemory management\u662f\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u4e3b\u9898\u3002 \u601d\u8003: \u662f\u5426\u4f1a\u5bfc\u81f4\u8bfb\u5230\u65e7\u6570\u636e\uff1f \u5728\u4f7f\u7528copy-on-write\u6765\u5b9e\u73b0concurrent lock free data structure\u7684\u65f6\u5019\uff0c\u662f\u5426\u4f1a\u5bfc\u81f4read\u5230\u65e7\u6570\u636e\uff1f\u7b54\u6848\u662f\u4e0d\u4f1a\u3002 \u4ee5RCU\u4e3a\u4f8b\u6765\u8fdb\u884c\u5206\u6790 \u5728RCU\u4e2d\uff0c\u5c31\u4f7f\u7528\u4e86copy-on-write\u6280\u672f\uff0c\u4e0b\u9762\u4ee5RCU\u4e3a\u4f8b\u6765\u8fdb\u884c\u5206\u6790: \u4f7f\u7528read-writer lock: \u5982\u679c\u4ee5read-writer lock\u6765\u8fdb\u884csynchronization\uff0c\u5219reader\u4f1a\u52a0\u4e0aread lock\uff0c\u5f53\u4e00\u4e2awriter\u5c1d\u8bd5\u53bbwrite\u7684\u65f6\u5019\uff0c\u663e\u7136\u5b83\u6b64\u65f6\u4f1a\u88ab\u963b\u585e\uff0c\u663e\u7136\u5b83\u6b64\u65f6\u5bf9shared data\u7684write\u65e0\u6cd5\u751f\u6548\uff0c\u6b64\u65f6reader\u6240\u8bfb\u5230\u7684\u8fd8\u662f\u539f\u6765\u7684\u6570\u636e\u3002 \u4f7f\u7528RCU: writer\u4f1a\u5199\u5230copy\u4e2d\uff0c\u7136\u540e\u8fdb\u884ccommit\u3002\u663e\u7136reader\u4e0b\u6b21\u8bfb\u7684\u65f6\u5019\uff0c\u80fd\u591f\u8bfb\u5230\u65b0\u6570\u636e\uff0c\u8fd9\u4e2a\u548cread-write lock\u7684\u6548\u679c\u662f\u4e00\u81f4\u7684\uff0c\u4f46\u662f\u5e76\u53d1\u6027\u66f4\u9ad8\u3002 In Java Java\u662f\u81ea\u5e26GC\u7684\uff0c\u56e0\u6b64\u5728\u4f7f\u7528copy on write\u6765\u5b9e\u73b0lock free data structure\u7684\u65f6\u5019\uff0c\u5c31\u7701\u53bb\u4e86\u7531programmer\u6765\u5b9e\u73b0memory reclamation\u7684\u8d1f\u62c5\u3002 openclassrooms Modify Arrays on Multiple Threads With CopyOnWriteArrayList \u663e\u7136\uff0cJava\u4e2d\u7684 CopyOnWriteArrayList \u5c31\u662f\u7528\u4e86copy-on-write\u6280\u672f\u6765\u5b9e\u73b0lock-free data structure\uff1b stackoverflow How can CopyOnWriteArrayList be thread-safe? A NOTE: Java\u662f\u5e26GC\u7684\uff0c\u56e0\u6b64\u65e0\u9700\u7531programmer\u8003\u8651\u5b83\u7684Memory-reclamation If you look at the underlying array reference you'll see it's marked as volatile . When a write operation occurs (such as in the above extract) this volatile reference is only updated in the final statement via setArray . Up until this point any read operations will return elements from the old copy of the array. The important point is that the array update is an atomic operation and hence reads will always see the array in a consistent state. NOTE: \u4e0d\u5b58\u5728half read\u3001half write\u95ee\u9898 The advantage of only taking out a lock for write operations is improved throughput for reads: This is because write operations for a CopyOnWriteArrayList can potentially be very slow as they involve copying the entire list. javamex Java copy-on-write collections QT copy on write doc.qt Implicit Sharing Many C++ classes in Qt use implicit data sharing to maximize resource usage and minimize copying. Implicitly shared classes are both safe and efficient when passed as arguments, because only a pointer to the data is passed around, and the data is copied only if and when a function writes to it, i.e., copy-on-write . stackoverflow What is implicit sharing? I am building a game engine library in C++. A little while back I was using Qt to build an application and was rather fascinated with its use of Implicit Sharing . I am wondering if anybody could explain this technique in greater detail or could offer a simple example of this in action. comments You gave a link to the docs providing an excellent explanation of how it works. It even has a reference to the thread docs explaining how atomic reference counting helps in multi-threaded apps . If something is still unclear, you should ask a more detailed question about that something. And don't forget you can always look at the Qt's sources. \u2013 Sergei Tachenov Jan 9 '11 at 7:38 The other name for implicit sharing is copy-on-write . You might want to take a look at questions mentioning this other name on SO like for example this one: stackoverflow.com/questions/628938/what-is-copy-on-write \u2013 Piotr Dobrogost Jan 9 '11 at 11:14 I believe implicit sharing is just Qt's implementation of copy-on-write . \u2013 HelloGoodbye Sep 15 '15 at 22:09 A The key idea behind implicit sharing seems to go around using the more common term copy-on-write . The idea behind copy-on-write is to have each object serve as a wrapper around a pointer to the actual implementation. Each implementation object keeps track of the number of pointers into it. Whenever an operation is performed on the wrapper object, it's just forwarded to the implementation object, which does the actual work. The advantage of this approach is that copying and destruction of these objects are cheap. To make a copy of the object, we just make a new instance of a wrapper, set its pointer to point at the implementation object, and then increment the count of the number of pointers to the object (this is sometimes called the reference count , by the way). Destruction is similar - we drop the reference count by one, then see if anyone else is pointing at the implementation. If not, we free its resources. Otherwise, we do nothing and just assume someone else will do the cleanup later. The challenge in this approach is that it means that multiple different objects will all be pointing at the same implementation. This means that if someone ends up making a change to the implementation, every object referencing that implementation will see the changes - a very serious problem. To fix this, every time an operation is performed that might potentially change the implementation, the operation checks to see if any other objects also reference the implementation by seeing if the reference count is identically 1. If no other objects reference the object, then the operation can just go ahead - there's no possibility of the changes propagating. If there is at least one other object referencing the data, then the wrapper first makes a deep-copy of the implementation for itself and changes its pointer to point to the new object. Now we know there can't be any sharing, and the changes can be made without a hassle. If you'd like to see some examples of this in action, take a look at lecture examples 15.0 and 16.0 from Stanford's introductory C++ programming course . It shows how to design an object to hold a list of words using this technique. Hope this helps! NOTE: \u4e0a\u8ff0 15.0\u300116.0\u4e2d\u7684code\u662f\u975e\u5e38\u597d\u7684 Qt-Widgets / thinker-qt-QtAsync","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Copy-on-write/#copy#on#write#in#concurrent#lock#free#data#structure","text":"\u4e00\u3001Copy-on-write\u662f\u5b9e\u73b0concurrent lock free data structure\u7684\u4e00\u79cd\u5e38\u7528\u7684\u6280\u672f: 1\u3001\u5728Read-copy-update \u3001MVCC\u4e2d\uff0c\u90fd\u53ef\u4ee5\u770b\u5230\u5b83\u7684\u5f71\u5b50\uff1b 2\u3001\u7531\u4e8e\u4f1a\u5b58\u5728\u591a\u4efdcopy\uff0c\u56e0\u6b64\u5bf9\u4e8e\u8fd9\u4e9bcopy\u7684reclaimation\u5c31\u53d8\u5f97\u975e\u5e38\u91cd\u8981\uff0c\u5bf9\u4e8e\u6ca1\u6709GC\u7684programming language\uff0c\u9700\u8981\u7531programmer\u5b9e\u73b0memory reclamation\uff0c\u5728 Memory-reclaimation \u7ae0\u8282\u5bf9\u6b64\u8fdb\u884c\u4e86\u8ba8\u8bba\u3002 \u4e8c\u3001memory management \u901a\u8fc7copy on write\u3001memory reclamation\uff0c\u53ef\u4ee5\u53d1\u73b0\u5728lock-free programming\u4e2d\uff0cmemory management\u662f\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u4e3b\u9898\u3002","title":"Copy on write in concurrent lock free data structure"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Copy-on-write/#_1","text":"\u5728\u4f7f\u7528copy-on-write\u6765\u5b9e\u73b0concurrent lock free data structure\u7684\u65f6\u5019\uff0c\u662f\u5426\u4f1a\u5bfc\u81f4read\u5230\u65e7\u6570\u636e\uff1f\u7b54\u6848\u662f\u4e0d\u4f1a\u3002","title":"\u601d\u8003: \u662f\u5426\u4f1a\u5bfc\u81f4\u8bfb\u5230\u65e7\u6570\u636e\uff1f"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Copy-on-write/#rcu","text":"\u5728RCU\u4e2d\uff0c\u5c31\u4f7f\u7528\u4e86copy-on-write\u6280\u672f\uff0c\u4e0b\u9762\u4ee5RCU\u4e3a\u4f8b\u6765\u8fdb\u884c\u5206\u6790: \u4f7f\u7528read-writer lock: \u5982\u679c\u4ee5read-writer lock\u6765\u8fdb\u884csynchronization\uff0c\u5219reader\u4f1a\u52a0\u4e0aread lock\uff0c\u5f53\u4e00\u4e2awriter\u5c1d\u8bd5\u53bbwrite\u7684\u65f6\u5019\uff0c\u663e\u7136\u5b83\u6b64\u65f6\u4f1a\u88ab\u963b\u585e\uff0c\u663e\u7136\u5b83\u6b64\u65f6\u5bf9shared data\u7684write\u65e0\u6cd5\u751f\u6548\uff0c\u6b64\u65f6reader\u6240\u8bfb\u5230\u7684\u8fd8\u662f\u539f\u6765\u7684\u6570\u636e\u3002 \u4f7f\u7528RCU: writer\u4f1a\u5199\u5230copy\u4e2d\uff0c\u7136\u540e\u8fdb\u884ccommit\u3002\u663e\u7136reader\u4e0b\u6b21\u8bfb\u7684\u65f6\u5019\uff0c\u80fd\u591f\u8bfb\u5230\u65b0\u6570\u636e\uff0c\u8fd9\u4e2a\u548cread-write lock\u7684\u6548\u679c\u662f\u4e00\u81f4\u7684\uff0c\u4f46\u662f\u5e76\u53d1\u6027\u66f4\u9ad8\u3002","title":"\u4ee5RCU\u4e3a\u4f8b\u6765\u8fdb\u884c\u5206\u6790"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Copy-on-write/#in#java","text":"Java\u662f\u81ea\u5e26GC\u7684\uff0c\u56e0\u6b64\u5728\u4f7f\u7528copy on write\u6765\u5b9e\u73b0lock free data structure\u7684\u65f6\u5019\uff0c\u5c31\u7701\u53bb\u4e86\u7531programmer\u6765\u5b9e\u73b0memory reclamation\u7684\u8d1f\u62c5\u3002","title":"In Java"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Copy-on-write/#openclassrooms#modify#arrays#on#multiple#threads#with#copyonwritearraylist","text":"\u663e\u7136\uff0cJava\u4e2d\u7684 CopyOnWriteArrayList \u5c31\u662f\u7528\u4e86copy-on-write\u6280\u672f\u6765\u5b9e\u73b0lock-free data structure\uff1b","title":"openclassrooms Modify Arrays on Multiple Threads With CopyOnWriteArrayList"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Copy-on-write/#stackoverflow#how#can#copyonwritearraylist#be#thread-safe","text":"A NOTE: Java\u662f\u5e26GC\u7684\uff0c\u56e0\u6b64\u65e0\u9700\u7531programmer\u8003\u8651\u5b83\u7684Memory-reclamation If you look at the underlying array reference you'll see it's marked as volatile . When a write operation occurs (such as in the above extract) this volatile reference is only updated in the final statement via setArray . Up until this point any read operations will return elements from the old copy of the array. The important point is that the array update is an atomic operation and hence reads will always see the array in a consistent state. NOTE: \u4e0d\u5b58\u5728half read\u3001half write\u95ee\u9898 The advantage of only taking out a lock for write operations is improved throughput for reads: This is because write operations for a CopyOnWriteArrayList can potentially be very slow as they involve copying the entire list.","title":"stackoverflow How can CopyOnWriteArrayList be thread-safe?"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Copy-on-write/#javamex#java#copy-on-write#collections","text":"","title":"javamex Java copy-on-write collections"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Copy-on-write/#qt#copy#on#write","text":"","title":"QT copy on write"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Copy-on-write/#docqt#implicit#sharing","text":"Many C++ classes in Qt use implicit data sharing to maximize resource usage and minimize copying. Implicitly shared classes are both safe and efficient when passed as arguments, because only a pointer to the data is passed around, and the data is copied only if and when a function writes to it, i.e., copy-on-write .","title":"doc.qt Implicit Sharing"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Copy-on-write/#stackoverflow#what#is#implicit#sharing","text":"I am building a game engine library in C++. A little while back I was using Qt to build an application and was rather fascinated with its use of Implicit Sharing . I am wondering if anybody could explain this technique in greater detail or could offer a simple example of this in action.","title":"stackoverflow What is implicit sharing?"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Copy-on-write/#comments","text":"You gave a link to the docs providing an excellent explanation of how it works. It even has a reference to the thread docs explaining how atomic reference counting helps in multi-threaded apps . If something is still unclear, you should ask a more detailed question about that something. And don't forget you can always look at the Qt's sources. \u2013 Sergei Tachenov Jan 9 '11 at 7:38 The other name for implicit sharing is copy-on-write . You might want to take a look at questions mentioning this other name on SO like for example this one: stackoverflow.com/questions/628938/what-is-copy-on-write \u2013 Piotr Dobrogost Jan 9 '11 at 11:14 I believe implicit sharing is just Qt's implementation of copy-on-write . \u2013 HelloGoodbye Sep 15 '15 at 22:09","title":"comments"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Copy-on-write/#a","text":"The key idea behind implicit sharing seems to go around using the more common term copy-on-write . The idea behind copy-on-write is to have each object serve as a wrapper around a pointer to the actual implementation. Each implementation object keeps track of the number of pointers into it. Whenever an operation is performed on the wrapper object, it's just forwarded to the implementation object, which does the actual work. The advantage of this approach is that copying and destruction of these objects are cheap. To make a copy of the object, we just make a new instance of a wrapper, set its pointer to point at the implementation object, and then increment the count of the number of pointers to the object (this is sometimes called the reference count , by the way). Destruction is similar - we drop the reference count by one, then see if anyone else is pointing at the implementation. If not, we free its resources. Otherwise, we do nothing and just assume someone else will do the cleanup later. The challenge in this approach is that it means that multiple different objects will all be pointing at the same implementation. This means that if someone ends up making a change to the implementation, every object referencing that implementation will see the changes - a very serious problem. To fix this, every time an operation is performed that might potentially change the implementation, the operation checks to see if any other objects also reference the implementation by seeing if the reference count is identically 1. If no other objects reference the object, then the operation can just go ahead - there's no possibility of the changes propagating. If there is at least one other object referencing the data, then the wrapper first makes a deep-copy of the implementation for itself and changes its pointer to point to the new object. Now we know there can't be any sharing, and the changes can be made without a hassle. If you'd like to see some examples of this in action, take a look at lecture examples 15.0 and 16.0 from Stanford's introductory C++ programming course . It shows how to design an object to hold a list of words using this technique. Hope this helps! NOTE: \u4e0a\u8ff0 15.0\u300116.0\u4e2d\u7684code\u662f\u975e\u5e38\u597d\u7684","title":"A"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Copy-on-write/#qt-widgetsthinker-qt-qtasync","text":"","title":"Qt-Widgets/thinker-qt-QtAsync"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Copy-on-write/wikipedia-Copy-on-write/","text":"wikipedia Copy-on-write Copy-on-write ( CoW or COW ), sometimes referred to as implicit sharing or shadowing , is a resource-management technique used in computer programming to efficiently implement a \"duplicate\" or \"copy\" operation on modifiable resources. If a resource is duplicated but not modified, it is not necessary to create a new resource; the resource can be shared between the copy and the original. Modifications must still create a copy, hence the technique: the copy operation is deferred to the first write \uff08\u8fd9\u53e5\u8bdd\u662f\u5bf9copy on write\u7684\u6700\u597d\u7684\u89e3\u91ca\uff09. By sharing resources in this way, it is possible to significantly reduce the resource consumption of unmodified copies, while adding a small overhead to resource-modifying operations. In virtual memory management Copy-on-write finds its main use in sharing the virtual memory of operating system processes , in the implementation of the fork system call . Typically, the process does not modify any memory and immediately executes a new process, replacing the address space entirely. Thus, it would be wasteful to copy all of the process's memory during a fork, and instead the copy-on-write technique is used. Copy-on-write can be implemented efficiently using the page table by marking\uff08\u6807\u5fd7\uff09 certain pages of memory as read-only and keeping a count of the number of references\uff08\u5f15\u7528\u8ba1\u6570\uff09 to the page. When data is written to these pages, the kernel intercepts\uff08\u62e6\u622a\uff09 the write attempt and allocates a new physical page, initialized with the copy-on-write data, although the allocation can be skipped if there is only one reference. The kernel then updates the page table with the new (writable) page, decrements the number of references, and performs the write. The new allocation ensures that a change in the memory of one process is not visible in another's. The copy-on-write technique can be extended to support efficient memory allocation by having a page of physical memory filled with zeros. When the memory is allocated, all the pages returned refer to the page of zeros and are all marked copy-on-write. This way, physical memory is not allocated for the process until data is written, allowing processes to reserve more virtual memory than physical memory and use memory sparsely, at the risk of running out of virtual address space. The combined algorithm is similar to demand paging . Copy-on-write pages are also used in the Linux kernel 's kernel same-page merging feature. Loading the libraries for an application is also a use of copy-on-write technique. The dynamic linker maps libraries as private like follows. Any writing action on the libraries will trigger a COW in virtual memory management. In software COW is also used in library , application and system code. In multithreaded systems, COW can be implemented without the use of traditional locking and instead use compare-and-swap to increment or decrement the internal reference counter. Since the original resource will never be altered, it can safely be copied by multiple threads (after the reference count was increased) without the need of performance-expensive locking such as mutexes . If the reference counter turns 0, then by definition only 1 thread was holding a reference so the resource can safely be de-allocated from memory, again without the use of performance-expensive locking mechanisms. The benefit of not having to copy the resource (and the resulting performance gain over traditional deep-copying) will therefore be valid in both single- and multithreaded systems.","title":"wikipedia-Copy-on-write"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Copy-on-write/wikipedia-Copy-on-write/#wikipedia#copy-on-write","text":"Copy-on-write ( CoW or COW ), sometimes referred to as implicit sharing or shadowing , is a resource-management technique used in computer programming to efficiently implement a \"duplicate\" or \"copy\" operation on modifiable resources. If a resource is duplicated but not modified, it is not necessary to create a new resource; the resource can be shared between the copy and the original. Modifications must still create a copy, hence the technique: the copy operation is deferred to the first write \uff08\u8fd9\u53e5\u8bdd\u662f\u5bf9copy on write\u7684\u6700\u597d\u7684\u89e3\u91ca\uff09. By sharing resources in this way, it is possible to significantly reduce the resource consumption of unmodified copies, while adding a small overhead to resource-modifying operations.","title":"wikipedia Copy-on-write"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Copy-on-write/wikipedia-Copy-on-write/#in#virtual#memory#management","text":"Copy-on-write finds its main use in sharing the virtual memory of operating system processes , in the implementation of the fork system call . Typically, the process does not modify any memory and immediately executes a new process, replacing the address space entirely. Thus, it would be wasteful to copy all of the process's memory during a fork, and instead the copy-on-write technique is used. Copy-on-write can be implemented efficiently using the page table by marking\uff08\u6807\u5fd7\uff09 certain pages of memory as read-only and keeping a count of the number of references\uff08\u5f15\u7528\u8ba1\u6570\uff09 to the page. When data is written to these pages, the kernel intercepts\uff08\u62e6\u622a\uff09 the write attempt and allocates a new physical page, initialized with the copy-on-write data, although the allocation can be skipped if there is only one reference. The kernel then updates the page table with the new (writable) page, decrements the number of references, and performs the write. The new allocation ensures that a change in the memory of one process is not visible in another's. The copy-on-write technique can be extended to support efficient memory allocation by having a page of physical memory filled with zeros. When the memory is allocated, all the pages returned refer to the page of zeros and are all marked copy-on-write. This way, physical memory is not allocated for the process until data is written, allowing processes to reserve more virtual memory than physical memory and use memory sparsely, at the risk of running out of virtual address space. The combined algorithm is similar to demand paging . Copy-on-write pages are also used in the Linux kernel 's kernel same-page merging feature. Loading the libraries for an application is also a use of copy-on-write technique. The dynamic linker maps libraries as private like follows. Any writing action on the libraries will trigger a COW in virtual memory management.","title":"In virtual memory management"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Copy-on-write/wikipedia-Copy-on-write/#in#software","text":"COW is also used in library , application and system code. In multithreaded systems, COW can be implemented without the use of traditional locking and instead use compare-and-swap to increment or decrement the internal reference counter. Since the original resource will never be altered, it can safely be copied by multiple threads (after the reference count was increased) without the need of performance-expensive locking such as mutexes . If the reference counter turns 0, then by definition only 1 thread was holding a reference so the resource can safely be de-allocated from memory, again without the use of performance-expensive locking mechanisms. The benefit of not having to copy the resource (and the resulting performance gain over traditional deep-copying) will therefore be valid in both single- and multithreaded systems.","title":"In software"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-barrier/","text":"Memory barrier \u7531\u4e8e\u8fd9\u90e8\u5206\u5185\u5bb9\u662f\u5173\u4e8einstruction\uff0c\u6240\u4ee5\u5c06\u5b83\u79fb\u5230\u4e86\u5de5\u7a0bhardware\u4e2d\uff0c\u53c2\u89c1 CPU-memory-access\\Memory-ordering\\Memory-barrier \u7ae0\u8282\u3002","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-barrier/#memory#barrier","text":"\u7531\u4e8e\u8fd9\u90e8\u5206\u5185\u5bb9\u662f\u5173\u4e8einstruction\uff0c\u6240\u4ee5\u5c06\u5b83\u79fb\u5230\u4e86\u5de5\u7a0bhardware\u4e2d\uff0c\u53c2\u89c1 CPU-memory-access\\Memory-ordering\\Memory-barrier \u7ae0\u8282\u3002","title":"Memory barrier"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-reclamation/","text":"Memory reclamation \u4e3a\u4ec0\u4e48\u9700\u8981memory reclamation? \u4e00\u3001\u5728 libcds \u7684readme\u4e2d\uff0c\u63d0\u53ca\u4e86memory reclamation: safe memory reclamation (SMR) algorithms like Hazard Pointer and user-space RCU that is used as an epoch-based SMR. \u4e8c\u3001\"reclamation\"\u7684\u610f\u601d\u662f\"\u56de\u6536\"\uff0c\"memory reclamation\"\u5373\"\u5185\u5b58\u56de\u6536\"\uff0c\u5b83\u548c\u6211\u4eec\u5e73\u65f6\u7ecf\u5e38\u6240\u8bf4\u7684\"garbage collection\"\u672c\u8d28\u662f\u6bd4\u8f83\u76f8\u8fd1\uff0c\u4e0d\u8fc7\u4e24\u8005\u4e00\u822c\u7528\u4e8e\u4e0d\u540c\u7684\u9886\u57df: garbage collection\u4e00\u822c\u7528\u4e8eprogramming language\uff0cmemory reclamation\u4e00\u822c\u7528\u4e8elock-free data structure\u3002 \u53e6\u5916\uff0cmemory reclamation\u4e5f\u53ef\u4ee5\u5f52\u5165automatic memory management\u9886\u57df\u3002 \u4e09\u3001\u5728\u4f7f\u7528\u7c7b\u4f3c\u4e8ecopy-on-write\u3001RCU\u65b9\u6cd5\u6765\u5b9e\u73b0lock-free data structure\u65f6\uff0c\u5e76\u4e14host programming language\u662f\u6ca1\u6709garbage-collection\u7684\uff0c\u5219\u9700\u8981\u7531programmer\u81ea\u5df1\u6765\u5b9e\u73b0\u5bf9old copy\u7684reclamation\uff0c\u5373\u9700\u8981\u7531programmer\u81ea\u5df1\u6765\u5b9e\u73b0\u4e00\u4e2agarbage collection\u3002 Safe memory reclamation strategy \u672c\u8282\u603b\u7ed3\u4e00\u4e9bmemory reclamation strategy\u3002 Quiescent-state-based-reclamation-QSBR \u53c2\u89c1 Quiescent-state-based-reclamation-QSBR \u7ae0\u8282 Epoch-Based-Reclamation-EBR \u53c2\u89c1 Epoch-Based-Reclamation-EBR \u7ae0\u8282 Reference counting \u4e00\u3001\u8fd9\u662f\u4e00\u79cd\u975e\u5e38\u5e38\u89c1\u7684\u65b9\u6cd5\u3002 \u4e8c\u3001\u53c2\u89c1: 1\u3001\u5de5\u7a0bprogramming-language\u7684 Reference-counting \u7ae0\u8282 2\u3001 http://www.cse.chalmers.se/~tsigas/papers/MemoryReclamation-ReferenceCounting-ISPAN05.pdf stackoverflow Quiescent State Based Reclamation vs Epoch Based Reclamation I'm studying the various types of memory reclamation strategies for lock-free data structures in a non-garbage collected environment (like C or C++). In my experiments, I've implemented a few of these strategies successfully - specifically, both Quiescent State Based Reclamation (QSBR) and Epoch Based Reclamation (EBR). My question concerns one of the key differences between these two strategies. Firstly, I'm aware of how both QSBR and EBR work, and have successfully implemented both of these strategies. QSBR and EBR are in fact very similar. Both of them are deferred reclamation strategies - meaning, they avoid race conditions when deallocating memory by simply deferring the actual deallocation until it can be proven that it is safe to deallocate the memory. With both QSBR and EBR this is achieved using a global \"epoch counter\", and then various thread-local epoch counters for each participating threads. NOTE: 1\u3001 \" thread-local epoch counters \"\u5982\u4f55\u7406\u89e3\uff1f \u5b57\u9762\u610f\u601d: \u7ebf\u7a0b\u5c40\u90e8\u7684epoch\u8ba1\u65f6\u5668 The main difference between QSBR and EBR is that with QSBR, you basically indicate when a thread does not have any references to any shared data. With EBR, you indicate when a thread does have a reference to shared data. So, in practice, code that uses EBR ends up looking more like a traditional mutex lock/unlock critical section, like: enter_critical_section (); /* do some cool lock-free stuff */ exit_critical_section (); ...whereas, with QSBR, it's more like: /* do some cool lock-free stuff */ quiescent_state (); // this thread is done using shared data NOTE: 1\u3001\u4e0a\u9762\u7684\u603b\u7ed3\u975e\u5e38\u597d 2\u3001\u7b80\u800c\u8a00\u4e4b: QSBR\u9700\u8981\u7531application\u4e3b\u52a8\u5730report quiescent state to QSBR library EBR\u9700\u8981\u7531application\u4e3b\u52a8\u5730report \"thread does have a reference to shared data\" So they're very similar. However, one key thing I don't really understand is how all the literature indicates that in practice, QSBR has one major disadvantage: it requires application level support, meaning that it isn't really suitable for use in a generic library. This is mentioned in countless journal articles or library documentation, such as for example in http://www.cs.toronto.edu/~tomhart/papers/tomhart_thesis.pdf , it says: The fact that QSBR is application-dependent is the fundamental difference between QSBR and EBR. EBR, by definition, detects grace periods at the library level. QSBR, by contrast, requires that the application report quiescent(\u6c89\u5bc2\u7684) states to the QSBR library. As we show in Section 5.2, this gives QSBR a significant performance advantage over NOTE: 1\u3001\u7b80\u800c\u8a00\u4e4b: QSBR\u9700\u8981\u7531application\u4e3b\u52a8\u5730report quiescent state to QSBR library\u3002 The docs for the User-space RCU project , which uses a variation of QSBR, also says something similar: However, each thread must periodically invoke rcu_quiescent_state() , just as in the kernel, where schedule() must be invoked periodically. Each thread that is to execute RCU read-side critical sections must also invoke rcu_register_thread() after thread creation and rcu_unregister_thread() before thread exit. These requirements clearly put a stringent(\u4e25\u683c\u7684) constraint on the overall application design that, for example, prohibit the use of QSBR RCU in most library code, but in return, QSBR provides unmatched performance. I'm having difficulty understanding why this is such a problem. What I gather(\u6536\u96c6) here is that with QSBR, the application needs to indicate when it enters a quiescent state . But I fail to understand why this is so hard to do at the library level? Couldn't a lock-free library that provides data structures such as stacks and queues simply indicate that it is entering a quiescent state after each operation completes? Why are there all these caveats about QSBR that indicate it is somehow not easy to use in library code as opposed to application code? Comments You might want to read csng.cs.toronto.edu/publication_files/0000/0159/jpdc07.pdf and you will see \"In the trivial case, a thread could declare a quiescent state after every lockless operation, as shown in Listing 1; however, it is often advantageous to declare quiescent states less frequently\" \u2013 JJ15k Jul 20 '16 at 22:19 A In QSBR quiescent_state() can be called at an arbitrary place where the calling thread holds no references to shared objects. On the other hand, in EBR a thread must access shared objects within critical section annotated by enter_critical_section() and exit_critical_section . What this difference impiles is: 1\u3001QSBR can outperform EBR because it can be used with less frequent synchronization . Yes, as you said, QSBR can be used in the similar way with EBR, but that does not provide the efficiency QSBR claims. 2\u3001In a complex application, identifying quiescent state can be hard. This is why quiescent-based techniques such as RCU usage is mainly restricted to specific environment where there is a natural quiecent state (e.g. context switch in Linux kernel). \u53c2\u8003\u8d44\u6599 1\u3001 http://www.cs.toronto.edu/~tomhart/papers/tomhart_thesis.pdf Implementation rmind / libqsbr en4bz / lockfree","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-reclamation/#memory#reclamation","text":"","title":"Memory reclamation"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-reclamation/#memory#reclamation_1","text":"\u4e00\u3001\u5728 libcds \u7684readme\u4e2d\uff0c\u63d0\u53ca\u4e86memory reclamation: safe memory reclamation (SMR) algorithms like Hazard Pointer and user-space RCU that is used as an epoch-based SMR. \u4e8c\u3001\"reclamation\"\u7684\u610f\u601d\u662f\"\u56de\u6536\"\uff0c\"memory reclamation\"\u5373\"\u5185\u5b58\u56de\u6536\"\uff0c\u5b83\u548c\u6211\u4eec\u5e73\u65f6\u7ecf\u5e38\u6240\u8bf4\u7684\"garbage collection\"\u672c\u8d28\u662f\u6bd4\u8f83\u76f8\u8fd1\uff0c\u4e0d\u8fc7\u4e24\u8005\u4e00\u822c\u7528\u4e8e\u4e0d\u540c\u7684\u9886\u57df: garbage collection\u4e00\u822c\u7528\u4e8eprogramming language\uff0cmemory reclamation\u4e00\u822c\u7528\u4e8elock-free data structure\u3002 \u53e6\u5916\uff0cmemory reclamation\u4e5f\u53ef\u4ee5\u5f52\u5165automatic memory management\u9886\u57df\u3002 \u4e09\u3001\u5728\u4f7f\u7528\u7c7b\u4f3c\u4e8ecopy-on-write\u3001RCU\u65b9\u6cd5\u6765\u5b9e\u73b0lock-free data structure\u65f6\uff0c\u5e76\u4e14host programming language\u662f\u6ca1\u6709garbage-collection\u7684\uff0c\u5219\u9700\u8981\u7531programmer\u81ea\u5df1\u6765\u5b9e\u73b0\u5bf9old copy\u7684reclamation\uff0c\u5373\u9700\u8981\u7531programmer\u81ea\u5df1\u6765\u5b9e\u73b0\u4e00\u4e2agarbage collection\u3002","title":"\u4e3a\u4ec0\u4e48\u9700\u8981memory reclamation?"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-reclamation/#safe#memory#reclamation#strategy","text":"\u672c\u8282\u603b\u7ed3\u4e00\u4e9bmemory reclamation strategy\u3002","title":"Safe memory reclamation strategy"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-reclamation/#quiescent-state-based-reclamation-qsbr","text":"\u53c2\u89c1 Quiescent-state-based-reclamation-QSBR \u7ae0\u8282","title":"Quiescent-state-based-reclamation-QSBR"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-reclamation/#epoch-based-reclamation-ebr","text":"\u53c2\u89c1 Epoch-Based-Reclamation-EBR \u7ae0\u8282","title":"Epoch-Based-Reclamation-EBR"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-reclamation/#reference#counting","text":"\u4e00\u3001\u8fd9\u662f\u4e00\u79cd\u975e\u5e38\u5e38\u89c1\u7684\u65b9\u6cd5\u3002 \u4e8c\u3001\u53c2\u89c1: 1\u3001\u5de5\u7a0bprogramming-language\u7684 Reference-counting \u7ae0\u8282 2\u3001 http://www.cse.chalmers.se/~tsigas/papers/MemoryReclamation-ReferenceCounting-ISPAN05.pdf","title":"Reference counting"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-reclamation/#stackoverflow#quiescent#state#based#reclamation#vs#epoch#based#reclamation","text":"I'm studying the various types of memory reclamation strategies for lock-free data structures in a non-garbage collected environment (like C or C++). In my experiments, I've implemented a few of these strategies successfully - specifically, both Quiescent State Based Reclamation (QSBR) and Epoch Based Reclamation (EBR). My question concerns one of the key differences between these two strategies. Firstly, I'm aware of how both QSBR and EBR work, and have successfully implemented both of these strategies. QSBR and EBR are in fact very similar. Both of them are deferred reclamation strategies - meaning, they avoid race conditions when deallocating memory by simply deferring the actual deallocation until it can be proven that it is safe to deallocate the memory. With both QSBR and EBR this is achieved using a global \"epoch counter\", and then various thread-local epoch counters for each participating threads. NOTE: 1\u3001 \" thread-local epoch counters \"\u5982\u4f55\u7406\u89e3\uff1f \u5b57\u9762\u610f\u601d: \u7ebf\u7a0b\u5c40\u90e8\u7684epoch\u8ba1\u65f6\u5668 The main difference between QSBR and EBR is that with QSBR, you basically indicate when a thread does not have any references to any shared data. With EBR, you indicate when a thread does have a reference to shared data. So, in practice, code that uses EBR ends up looking more like a traditional mutex lock/unlock critical section, like: enter_critical_section (); /* do some cool lock-free stuff */ exit_critical_section (); ...whereas, with QSBR, it's more like: /* do some cool lock-free stuff */ quiescent_state (); // this thread is done using shared data NOTE: 1\u3001\u4e0a\u9762\u7684\u603b\u7ed3\u975e\u5e38\u597d 2\u3001\u7b80\u800c\u8a00\u4e4b: QSBR\u9700\u8981\u7531application\u4e3b\u52a8\u5730report quiescent state to QSBR library EBR\u9700\u8981\u7531application\u4e3b\u52a8\u5730report \"thread does have a reference to shared data\" So they're very similar. However, one key thing I don't really understand is how all the literature indicates that in practice, QSBR has one major disadvantage: it requires application level support, meaning that it isn't really suitable for use in a generic library. This is mentioned in countless journal articles or library documentation, such as for example in http://www.cs.toronto.edu/~tomhart/papers/tomhart_thesis.pdf , it says: The fact that QSBR is application-dependent is the fundamental difference between QSBR and EBR. EBR, by definition, detects grace periods at the library level. QSBR, by contrast, requires that the application report quiescent(\u6c89\u5bc2\u7684) states to the QSBR library. As we show in Section 5.2, this gives QSBR a significant performance advantage over NOTE: 1\u3001\u7b80\u800c\u8a00\u4e4b: QSBR\u9700\u8981\u7531application\u4e3b\u52a8\u5730report quiescent state to QSBR library\u3002 The docs for the User-space RCU project , which uses a variation of QSBR, also says something similar: However, each thread must periodically invoke rcu_quiescent_state() , just as in the kernel, where schedule() must be invoked periodically. Each thread that is to execute RCU read-side critical sections must also invoke rcu_register_thread() after thread creation and rcu_unregister_thread() before thread exit. These requirements clearly put a stringent(\u4e25\u683c\u7684) constraint on the overall application design that, for example, prohibit the use of QSBR RCU in most library code, but in return, QSBR provides unmatched performance. I'm having difficulty understanding why this is such a problem. What I gather(\u6536\u96c6) here is that with QSBR, the application needs to indicate when it enters a quiescent state . But I fail to understand why this is so hard to do at the library level? Couldn't a lock-free library that provides data structures such as stacks and queues simply indicate that it is entering a quiescent state after each operation completes? Why are there all these caveats about QSBR that indicate it is somehow not easy to use in library code as opposed to application code?","title":"stackoverflow Quiescent State Based Reclamation vs Epoch Based Reclamation"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-reclamation/#comments","text":"You might want to read csng.cs.toronto.edu/publication_files/0000/0159/jpdc07.pdf and you will see \"In the trivial case, a thread could declare a quiescent state after every lockless operation, as shown in Listing 1; however, it is often advantageous to declare quiescent states less frequently\" \u2013 JJ15k Jul 20 '16 at 22:19","title":"Comments"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-reclamation/#a","text":"In QSBR quiescent_state() can be called at an arbitrary place where the calling thread holds no references to shared objects. On the other hand, in EBR a thread must access shared objects within critical section annotated by enter_critical_section() and exit_critical_section . What this difference impiles is: 1\u3001QSBR can outperform EBR because it can be used with less frequent synchronization . Yes, as you said, QSBR can be used in the similar way with EBR, but that does not provide the efficiency QSBR claims. 2\u3001In a complex application, identifying quiescent state can be hard. This is why quiescent-based techniques such as RCU usage is mainly restricted to specific environment where there is a natural quiecent state (e.g. context switch in Linux kernel).","title":"A"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-reclamation/#_1","text":"1\u3001 http://www.cs.toronto.edu/~tomhart/papers/tomhart_thesis.pdf","title":"\u53c2\u8003\u8d44\u6599"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-reclamation/#implementation","text":"","title":"Implementation"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-reclamation/#rmindlibqsbr","text":"","title":"rmind/libqsbr"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-reclamation/#en4bzlockfree","text":"","title":"en4bz/lockfree"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-reclamation/Epoch-Based-Reclamation-EBR/","text":"","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-reclamation/Hazard-pointer-based%20reclamation-HPBR/","text":"Hazard-pointer-based reclamation (HPBR) \u4e00\u3001\u662f\u5728\u9605\u8bfb \"cs.toronto-Performance-of-memory-reclamation-for-lockless-synchronization\" \u65f6\uff0c\u5176\u4e2d\u63d0\u53ca\u4e86 \"hazard-pointer-based reclamation\" \u4e8c\u3001\u4e0b\u9762\u662f\u4e00\u4e9b\u6bd4\u8f83\u597d\u7684\u6587\u7ae0: ticki.github Fearless concurrency with hazard pointers simongui.github Improving performance of lockless data structures \u4e09\u3001 https://github.com/topics/hazard-pointer Implementation https://github.com/oliver-giersch/hazptr","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-reclamation/Hazard-pointer-based%20reclamation-HPBR/#hazard-pointer-based#reclamation#hpbr","text":"\u4e00\u3001\u662f\u5728\u9605\u8bfb \"cs.toronto-Performance-of-memory-reclamation-for-lockless-synchronization\" \u65f6\uff0c\u5176\u4e2d\u63d0\u53ca\u4e86 \"hazard-pointer-based reclamation\" \u4e8c\u3001\u4e0b\u9762\u662f\u4e00\u4e9b\u6bd4\u8f83\u597d\u7684\u6587\u7ae0: ticki.github Fearless concurrency with hazard pointers simongui.github Improving performance of lockless data structures \u4e09\u3001 https://github.com/topics/hazard-pointer","title":"Hazard-pointer-based reclamation (HPBR)"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-reclamation/Hazard-pointer-based%20reclamation-HPBR/#implementation","text":"https://github.com/oliver-giersch/hazptr","title":"Implementation"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-reclamation/Hazard-pointer-based%20reclamation-HPBR/TODO-Hazard-pointer/","text":"Hazard pointer \u4e0e hazard pointer\u76f8\u5173\u7684\u6587\u7ae0: 1\u3001infoq Lock-free Programming in C++ with Herb Sutter 2\u3001Herb Sutter Lock-Free Programming or, How to Juggle Razor Blades wikipedia Hazard pointer ticki.github Fearless concurrency with hazard pointers","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-reclamation/Hazard-pointer-based%20reclamation-HPBR/TODO-Hazard-pointer/#hazard#pointer","text":"\u4e0e hazard pointer\u76f8\u5173\u7684\u6587\u7ae0: 1\u3001infoq Lock-free Programming in C++ with Herb Sutter 2\u3001Herb Sutter Lock-Free Programming or, How to Juggle Razor Blades","title":"Hazard pointer"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-reclamation/Hazard-pointer-based%20reclamation-HPBR/TODO-Hazard-pointer/#wikipedia#hazard#pointer","text":"","title":"wikipedia Hazard pointer"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-reclamation/Hazard-pointer-based%20reclamation-HPBR/TODO-Hazard-pointer/#tickigithub#fearless#concurrency#with#hazard#pointers","text":"","title":"ticki.github Fearless concurrency with hazard pointers"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-reclamation/Quiescent-state-based-reclamation-QSBR/","text":"Quiescent state based reclamation/QSBR preshing New Concurrent Hash Maps for C++ # Safe Memory Reclamation All Junction maps rely on a form of safe memory reclamation known as QSBR, or quiescent state-based memory reclamation . QSBR could be described as a primitive garbage collector. preshing Using Quiescent States to Reclaim Memory NOTE: \u8fd9\u7bc7\u6587\u7ae0\u5bf9\u6b64\u8fdb\u884c\u4e86\u8f83\u597d\u7684\u5206\u6790","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-reclamation/Quiescent-state-based-reclamation-QSBR/#quiescent#state#based#reclamationqsbr","text":"","title":"Quiescent state based reclamation/QSBR"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-reclamation/Quiescent-state-based-reclamation-QSBR/#preshing#new#concurrent#hash#maps#for#c#safe#memory#reclamation","text":"All Junction maps rely on a form of safe memory reclamation known as QSBR, or quiescent state-based memory reclamation . QSBR could be described as a primitive garbage collector.","title":"preshing New Concurrent Hash Maps for C++ # Safe Memory Reclamation"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-reclamation/Quiescent-state-based-reclamation-QSBR/#preshing#using#quiescent#states#to#reclaim#memory","text":"NOTE: \u8fd9\u7bc7\u6587\u7ae0\u5bf9\u6b64\u8fdb\u884c\u4e86\u8f83\u597d\u7684\u5206\u6790","title":"preshing Using Quiescent States to Reclaim Memory"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-reclamation/cs.toronto-Comparative-Performance-of-Memory-Reclamation-Strategies/","text":"Comparative Performance of Memory Reclamation Strategies for Lock-free and Concurrently-readable Data Structures Abstract Despite their advantages, lock-free algorithms are often not adopted in practice, partly due to the perception that they perform poorly relative to lock-based alternatives in common situations when there is little contention for objects or the CPUs. We show that memory reclamation can be a dominant performance cost for lock-free algorithms; therefore, choosing the most efficient memory reclamation method is essential to having lock-free algorithms perform well. We compare the costs of three memory reclamation strategies: 1\u3001quiescent-state-based reclamation, 2\u3001epoch-based reclamation, and 3\u3001safe memory reclamation. Our experiments show that changing the workload or execution environment can change which of these schemes is the most efficient. We therefore demonstrate that there is, to date, no panacea(\u7075\u4e39\u5999\u836f\uff1b\u4e07\u80fd\u836f) for memory reclamation for lock-free algorithms. Using a common reclamation scheme, we fairly compare lock-free and concurrently readable hash tables. Our evaluation shows that programmers can choose memory reclamation schemes mostly independently of the target algorithm. 1.2 Memory Reclamation Memory reclamation is required for all dynamic lock-free and concurrently-readable data structures, such as linked lists and queues. We distinguish logical deletion of a node, N (removing it from a shared data structure so that no new references to N may be created) from physical deletion of that node (allowing the memory used for N to be reclaimed for arbitrary reuse). If a thread T1 logically deletes a node N from a lock-free data structure, it cannot physically delete N until no other thread T2 holds a reference to N , since physically deleting N may cause T2 to crash or execute incorrectly. Never physically deleting logically deleted nodes is also unacceptable, since this will eventually lead to out-of-memory errors which will stop threads from making progress. Choosing an inefficient memory reclamation scheme can ruin the performance of a lock-free or concurrently-readable algorithm. Reference counting [61, 12], for example, has high overhead in the base case, and scales poorly in structures for which long chains of nodes must be traversed [47]. Little work has been done on comparing different memory reclamation strategies; we address this de ficiency, showing that the performance of memory reclamation schemes depends both on their base costs, and on the target workload and execution environment. We expect our results can provide some guidance to implementers of lock-free and concurrently-readable algorithms in choosing an appropriate scheme for their specif c applications.","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-reclamation/cs.toronto-Comparative-Performance-of-Memory-Reclamation-Strategies/#comparative#performance#of#memory#reclamation#strategies#for#lock-free#and#concurrently-readable#data#structures","text":"","title":"Comparative Performance of Memory Reclamation Strategies for Lock-free and Concurrently-readable Data Structures"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-reclamation/cs.toronto-Comparative-Performance-of-Memory-Reclamation-Strategies/#abstract","text":"Despite their advantages, lock-free algorithms are often not adopted in practice, partly due to the perception that they perform poorly relative to lock-based alternatives in common situations when there is little contention for objects or the CPUs. We show that memory reclamation can be a dominant performance cost for lock-free algorithms; therefore, choosing the most efficient memory reclamation method is essential to having lock-free algorithms perform well. We compare the costs of three memory reclamation strategies: 1\u3001quiescent-state-based reclamation, 2\u3001epoch-based reclamation, and 3\u3001safe memory reclamation. Our experiments show that changing the workload or execution environment can change which of these schemes is the most efficient. We therefore demonstrate that there is, to date, no panacea(\u7075\u4e39\u5999\u836f\uff1b\u4e07\u80fd\u836f) for memory reclamation for lock-free algorithms. Using a common reclamation scheme, we fairly compare lock-free and concurrently readable hash tables. Our evaluation shows that programmers can choose memory reclamation schemes mostly independently of the target algorithm.","title":"Abstract"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-reclamation/cs.toronto-Comparative-Performance-of-Memory-Reclamation-Strategies/#12#memory#reclamation","text":"Memory reclamation is required for all dynamic lock-free and concurrently-readable data structures, such as linked lists and queues. We distinguish logical deletion of a node, N (removing it from a shared data structure so that no new references to N may be created) from physical deletion of that node (allowing the memory used for N to be reclaimed for arbitrary reuse). If a thread T1 logically deletes a node N from a lock-free data structure, it cannot physically delete N until no other thread T2 holds a reference to N , since physically deleting N may cause T2 to crash or execute incorrectly. Never physically deleting logically deleted nodes is also unacceptable, since this will eventually lead to out-of-memory errors which will stop threads from making progress. Choosing an inefficient memory reclamation scheme can ruin the performance of a lock-free or concurrently-readable algorithm. Reference counting [61, 12], for example, has high overhead in the base case, and scales poorly in structures for which long chains of nodes must be traversed [47]. Little work has been done on comparing different memory reclamation strategies; we address this de ficiency, showing that the performance of memory reclamation schemes depends both on their base costs, and on the target workload and execution environment. We expect our results can provide some guidance to implementers of lock-free and concurrently-readable algorithms in choosing an appropriate scheme for their specif c applications.","title":"1.2 Memory Reclamation"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-reclamation/preshing-Using-Quiescent-States-to-Reclaim-Memory/","text":"preshing Using Quiescent States to Reclaim Memory NOTE: 1\u3001\"Quiescent\"\u7684\u542b\u4e49\u662f: \u9759\u6b62\u7684\uff1b\u4e0d\u6d3b\u52a8\u7684\uff1b\u6c89\u5bc2\u7684 If you want to support multiple readers for a data structure, while protecting against concurrent writes, a read-write lock might seem like the only way \u2013 but it isn\u2019t! You can achieve the same thing without a read-write lock if you allow several copies of the data structure to exist in memory. You just need a way to delete old copies when they\u2019re no longer in use. NOTE: 1\u3001\u663e\u7136\u4e5f\u662f\u57fa\u4e8ecopy on write\u7684 Let\u2019s look at one way to achieve that in C++. We\u2019ll start with an example based on a read-write lock. Using a Read-Write Lock Suppose you have a network server with dozens of threads. Each thread broadcasts messages to dozens of connected clients. Once in a while, a new client connects or an existing client disconnects, so the list of connected clients must change. We can store the list of connected clients in a std::vector and protect it using a read-write lock such as std::shared_mutex . class Server { private : std :: shared_mutex m_rwLock ; // Read-write lock std :: vector < int > m_clients ; // List of connected clients public : void broadcast ( const void * msg , size_t len ) { std :: shared_lock < std :: shared_mutex > shared ( m_rwLock ); // Shared lock for ( int fd : m_clients ) send ( fd , msg , len , 0 ); } void addClient ( int fd ) { std :: unique_lock < std :: shared_mutex > exclusive ( m_rwLock ); // Exclusive lock m_clients . push_back ( fd ); } ... The broadcast function reads from the list of connected clients, but doesn\u2019t modify it, so it takes a read lock (also known as a shared lock). addClient , on the other hand, needs to modify the list, so it takes a write lock (also known as an exclusive lock). That\u2019s all fine and dandy. Now let\u2019s eliminate the read-write lock by allowing multiple copies of the list to exist at the same time. Eliminating the Read-Write Lock NOTE: \u6d88\u9664read-write lock","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-reclamation/preshing-Using-Quiescent-States-to-Reclaim-Memory/#preshing#using#quiescent#states#to#reclaim#memory","text":"NOTE: 1\u3001\"Quiescent\"\u7684\u542b\u4e49\u662f: \u9759\u6b62\u7684\uff1b\u4e0d\u6d3b\u52a8\u7684\uff1b\u6c89\u5bc2\u7684 If you want to support multiple readers for a data structure, while protecting against concurrent writes, a read-write lock might seem like the only way \u2013 but it isn\u2019t! You can achieve the same thing without a read-write lock if you allow several copies of the data structure to exist in memory. You just need a way to delete old copies when they\u2019re no longer in use. NOTE: 1\u3001\u663e\u7136\u4e5f\u662f\u57fa\u4e8ecopy on write\u7684 Let\u2019s look at one way to achieve that in C++. We\u2019ll start with an example based on a read-write lock.","title":"preshing Using Quiescent States to Reclaim Memory"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-reclamation/preshing-Using-Quiescent-States-to-Reclaim-Memory/#using#a#read-write#lock","text":"Suppose you have a network server with dozens of threads. Each thread broadcasts messages to dozens of connected clients. Once in a while, a new client connects or an existing client disconnects, so the list of connected clients must change. We can store the list of connected clients in a std::vector and protect it using a read-write lock such as std::shared_mutex . class Server { private : std :: shared_mutex m_rwLock ; // Read-write lock std :: vector < int > m_clients ; // List of connected clients public : void broadcast ( const void * msg , size_t len ) { std :: shared_lock < std :: shared_mutex > shared ( m_rwLock ); // Shared lock for ( int fd : m_clients ) send ( fd , msg , len , 0 ); } void addClient ( int fd ) { std :: unique_lock < std :: shared_mutex > exclusive ( m_rwLock ); // Exclusive lock m_clients . push_back ( fd ); } ... The broadcast function reads from the list of connected clients, but doesn\u2019t modify it, so it takes a read lock (also known as a shared lock). addClient , on the other hand, needs to modify the list, so it takes a write lock (also known as an exclusive lock). That\u2019s all fine and dandy. Now let\u2019s eliminate the read-write lock by allowing multiple copies of the list to exist at the same time.","title":"Using a Read-Write Lock"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Memory-reclamation/preshing-Using-Quiescent-States-to-Reclaim-Memory/#eliminating#the#read-write#lock","text":"NOTE: \u6d88\u9664read-write lock","title":"Eliminating the Read-Write Lock"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Primer/microsoft-Lockless-Programming-Considerations-for-Xbox-360-and-Microsoft-Windows/","text":"microsoft Lockless Programming Considerations for Xbox 360 and Microsoft Windows Lockless programming is a way to safely share changing data between multiple threads without the cost of acquiring and releasing locks. This sounds like a panacea(\u7075\u4e39\u5999\u836f), but lockless programming is complex and subtle, and sometimes doesn't give the benefits that it promises. Lockless programming is particularly complex on Xbox 360. Using lockless programming correctly and safely requires significant knowledge of both your hardware and your compiler. This article gives an overview of some of the issues to consider when trying to use lockless programming techniques. Programming with Locks NOTE: \u539f\u6587\u8fd9\u4e00\u6bb5\u7684\u5185\u5bb9\u6bd4\u8f83\u666e\u901a Lockless Programming Lockless programming, as the name suggests, is a family of techniques for safely manipulating shared data without using locks. There are lockless algorithms available for passing messages, sharing lists and queues of data, and other tasks. NOTE: \u601d\u8003: \u5728lockless programming\u4e2d\uff0c\u5982\u4f55\u5b9e\u73b0notify\uff1f When doing lockless programming, there are two challenges that you must deal with: non-atomic operations and reordering. Non-Atomic Operations An atomic operation is one that is indivisible\u2014one where other threads are guaranteed to never see the operation when it is half done. Atomic operations are important for lockless programming, because without them, other threads might see half-written values, or otherwise inconsistent state. NOTE: \u8fd9\u91cc\u5173\u4e8eatomic\u7684\u89e3\u91ca\u662f\u975e\u5e38\u597d\u7684: \u548crace condition\u3001inconsistent state\u8fdb\u884c\u4e86\u7ed3\u5408 atomic\u610f\u5473\u7740\u6ca1\u6709: 1\u3001half-completed state 2\u3001half done On all modern processors, you can assume that reads and writes of naturally aligned native types are atomic . As long as the memory bus is at least as wide as the type being read or written, the CPU reads and writes these types in a single bus transaction , making it impossible for other threads to see them in a half-completed state . On x86 and x64 there is no guarantee that reads and writes larger than eight bytes are atomic. This means that 16-byte reads and writes of streaming SIMD extension (SSE) registers, and string operations, might not be atomic. NOTE: \u8fd9\u4e00\u6bb5\u5173\u4e8eatomic\u7684\u8bf4\u660e\u5176\u5b9e\u662f\u5b58\u5728\u4e00\u5b9a\u9519\u8bef\u7684\uff1b bus transaction\u662f\u4e00\u4e2a\u975e\u5e38\u597d\u7684\u63cf\u8ff0\uff1b Reads and writes of types that are not naturally aligned\u2014for instance, writing DWORDs that cross four-byte boundaries\u2014are not guaranteed to be atomic. The CPU may have to do these reads and writes as multiple bus transactions, which could allow another thread to modify or see the data in the middle of the read or write. Composite operations , such as the read-modify-write sequence that occurs when you increment a shared variable, are not atomic. On Xbox 360, these operations are implemented as multiple instructions (lwz, addi, and stw), and the thread could be swapped out partway through the sequence. On x86 and x64, there is a single instruction ( inc ) that can be used to increment a variable in memory. If you use this instruction, incrementing a variable is atomic on single-processor systems, but it is still not atomic on multi-processor systems. Making inc atomic on x86- and x64-based multi-processor systems requires using the lock prefix , which prevents another processor from doing its own read-modify-write sequence between the read and the write of the inc instruction. NOTE: \u4e0a\u8ff0composite operation\u5176\u5b9e\u5c31\u662f\"assemble as atomic primitive\" \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u5c06thread safe\u548catomic\u4e00\u8d77\u6765\u8fdb\u884c\u63cf\u8ff0\u4e86\uff0c\u5b83\u4ece\u6700\u6700\u5e95\u5c42\u3001\u6700\u6700\u6839\u672c\u63cf\u8ff0\u4e86race condition The following code shows some examples: // This write is not atomic because it is not natively aligned. DWORD * pData = ( DWORD * )( pChar + 1 ); * pData = 0 ; // This is not atomic because it is three separate operations. ++ g_globalCounter ; // This write is atomic. g_alignedGlobal = 0 ; // This read is atomic. DWORD local = g_alignedGlobal ; Guaranteeing Atomicity You can be sure you are using atomic operations by a combination of the following: 1\u3001Naturally atomic operations 2\u3001Locks to wrap composite operations 3\u3001Operating system functions that implement atomic versions of popular composite operations Reordering A more subtle problem is reordering . Reads and writes do not always happen in the order that you have written them in your code, and this can lead to very confusing problems. In many multi-threaded algorithms, a thread writes some data and then writes to a flag that tells other threads that the data is ready. This is known as a write-release . If the writes are reordered, other threads may see that the flag is set before they can see the written data. Similarly, in many cases, a thread reads from a flag and then reads some shared data if the flag says that the thread has acquired access to the shared data . This is known as a read-acquire . If reads are reordered, then the data may be read from shared storage before the flag, and the values seen might not be up to date. Reordering of reads and writes can be done both by the compiler and by the processor. Compilers and processors have done this reordering for years, but on single-processor machines it was less of an issue. This is because CPU rearrangement of reads and writes is invisible on single-processor machines (for non-device driver code that is not part of a device driver), and compiler rearrangement of reads and writes is less likely to cause problems on single-processor machines. If the compiler or the CPU rearranges the writes shown in the following code, another thread may see that the alive flag is set while still seeing the old values for x or y . Similar rearrangement can happen when reading.","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Primer/microsoft-Lockless-Programming-Considerations-for-Xbox-360-and-Microsoft-Windows/#microsoft#lockless#programming#considerations#for#xbox#360#and#microsoft#windows","text":"Lockless programming is a way to safely share changing data between multiple threads without the cost of acquiring and releasing locks. This sounds like a panacea(\u7075\u4e39\u5999\u836f), but lockless programming is complex and subtle, and sometimes doesn't give the benefits that it promises. Lockless programming is particularly complex on Xbox 360. Using lockless programming correctly and safely requires significant knowledge of both your hardware and your compiler. This article gives an overview of some of the issues to consider when trying to use lockless programming techniques.","title":"microsoft Lockless Programming Considerations for Xbox 360 and Microsoft Windows"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Primer/microsoft-Lockless-Programming-Considerations-for-Xbox-360-and-Microsoft-Windows/#programming#with#locks","text":"NOTE: \u539f\u6587\u8fd9\u4e00\u6bb5\u7684\u5185\u5bb9\u6bd4\u8f83\u666e\u901a","title":"Programming with Locks"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Primer/microsoft-Lockless-Programming-Considerations-for-Xbox-360-and-Microsoft-Windows/#lockless#programming","text":"Lockless programming, as the name suggests, is a family of techniques for safely manipulating shared data without using locks. There are lockless algorithms available for passing messages, sharing lists and queues of data, and other tasks. NOTE: \u601d\u8003: \u5728lockless programming\u4e2d\uff0c\u5982\u4f55\u5b9e\u73b0notify\uff1f When doing lockless programming, there are two challenges that you must deal with: non-atomic operations and reordering.","title":"Lockless Programming"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Primer/microsoft-Lockless-Programming-Considerations-for-Xbox-360-and-Microsoft-Windows/#non-atomic#operations","text":"An atomic operation is one that is indivisible\u2014one where other threads are guaranteed to never see the operation when it is half done. Atomic operations are important for lockless programming, because without them, other threads might see half-written values, or otherwise inconsistent state. NOTE: \u8fd9\u91cc\u5173\u4e8eatomic\u7684\u89e3\u91ca\u662f\u975e\u5e38\u597d\u7684: \u548crace condition\u3001inconsistent state\u8fdb\u884c\u4e86\u7ed3\u5408 atomic\u610f\u5473\u7740\u6ca1\u6709: 1\u3001half-completed state 2\u3001half done On all modern processors, you can assume that reads and writes of naturally aligned native types are atomic . As long as the memory bus is at least as wide as the type being read or written, the CPU reads and writes these types in a single bus transaction , making it impossible for other threads to see them in a half-completed state . On x86 and x64 there is no guarantee that reads and writes larger than eight bytes are atomic. This means that 16-byte reads and writes of streaming SIMD extension (SSE) registers, and string operations, might not be atomic. NOTE: \u8fd9\u4e00\u6bb5\u5173\u4e8eatomic\u7684\u8bf4\u660e\u5176\u5b9e\u662f\u5b58\u5728\u4e00\u5b9a\u9519\u8bef\u7684\uff1b bus transaction\u662f\u4e00\u4e2a\u975e\u5e38\u597d\u7684\u63cf\u8ff0\uff1b Reads and writes of types that are not naturally aligned\u2014for instance, writing DWORDs that cross four-byte boundaries\u2014are not guaranteed to be atomic. The CPU may have to do these reads and writes as multiple bus transactions, which could allow another thread to modify or see the data in the middle of the read or write. Composite operations , such as the read-modify-write sequence that occurs when you increment a shared variable, are not atomic. On Xbox 360, these operations are implemented as multiple instructions (lwz, addi, and stw), and the thread could be swapped out partway through the sequence. On x86 and x64, there is a single instruction ( inc ) that can be used to increment a variable in memory. If you use this instruction, incrementing a variable is atomic on single-processor systems, but it is still not atomic on multi-processor systems. Making inc atomic on x86- and x64-based multi-processor systems requires using the lock prefix , which prevents another processor from doing its own read-modify-write sequence between the read and the write of the inc instruction. NOTE: \u4e0a\u8ff0composite operation\u5176\u5b9e\u5c31\u662f\"assemble as atomic primitive\" \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u5c06thread safe\u548catomic\u4e00\u8d77\u6765\u8fdb\u884c\u63cf\u8ff0\u4e86\uff0c\u5b83\u4ece\u6700\u6700\u5e95\u5c42\u3001\u6700\u6700\u6839\u672c\u63cf\u8ff0\u4e86race condition The following code shows some examples: // This write is not atomic because it is not natively aligned. DWORD * pData = ( DWORD * )( pChar + 1 ); * pData = 0 ; // This is not atomic because it is three separate operations. ++ g_globalCounter ; // This write is atomic. g_alignedGlobal = 0 ; // This read is atomic. DWORD local = g_alignedGlobal ;","title":"Non-Atomic Operations"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Primer/microsoft-Lockless-Programming-Considerations-for-Xbox-360-and-Microsoft-Windows/#guaranteeing#atomicity","text":"You can be sure you are using atomic operations by a combination of the following: 1\u3001Naturally atomic operations 2\u3001Locks to wrap composite operations 3\u3001Operating system functions that implement atomic versions of popular composite operations","title":"Guaranteeing Atomicity"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Primer/microsoft-Lockless-Programming-Considerations-for-Xbox-360-and-Microsoft-Windows/#reordering","text":"A more subtle problem is reordering . Reads and writes do not always happen in the order that you have written them in your code, and this can lead to very confusing problems. In many multi-threaded algorithms, a thread writes some data and then writes to a flag that tells other threads that the data is ready. This is known as a write-release . If the writes are reordered, other threads may see that the flag is set before they can see the written data. Similarly, in many cases, a thread reads from a flag and then reads some shared data if the flag says that the thread has acquired access to the shared data . This is known as a read-acquire . If reads are reordered, then the data may be read from shared storage before the flag, and the values seen might not be up to date. Reordering of reads and writes can be done both by the compiler and by the processor. Compilers and processors have done this reordering for years, but on single-processor machines it was less of an issue. This is because CPU rearrangement of reads and writes is invisible on single-processor machines (for non-device driver code that is not part of a device driver), and compiler rearrangement of reads and writes is less likely to cause problems on single-processor machines. If the compiler or the CPU rearranges the writes shown in the following code, another thread may see that the alive flag is set while still seeing the old values for x or y . Similar rearrangement can happen when reading.","title":"Reordering"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Read-copy-update/","text":"Read-copy-update \u603b\u7ed3 RCU: \u4e00\u3001multiple reader thread\u3001single writer thread NOTE: \u5173\u4e8e\u8fd9\u4e00\u70b9\uff0c\u53c2\u89c1: 1\u3001lwn.net What is RCU, Fundamentally? 2\u3001geeksforgeeks Read-Copy Update (RCU) \u4e8c\u3001\u7531\u4e8e\u5b83\u53ea\u6709single writer\uff0c\u56e0\u6b64\u5b83\u76f8\u5bf9\u4e8eMCVV\u5c31\u7b80\u5355\u5730\u5f88\u591a: 1\u3001\u4e0d\u9700\u8981\u8003\u8651MVCC\u4e2d\uff0cmultiple version copy\u7684\u95ee\u9898 2\u3001\u53ef\u4ee5\u8ba4\u4e3a\uff0c\u53ea\u6709\u4e00\u4efdcopy \u4e09\u3001memory reclamation RCU\u4e2d\uff0c\u9700\u8981\u8003\u8651memory reclamation\u7684\u95ee\u9898 wikipedia Read-copy-update In computer science , read-copy-update ( RCU ) is a synchronization mechanism that avoids the use of lock primitives while multiple threads concurrently read and update elements that are linked through pointers and that belong to shared data structures (e.g., linked lists , trees , hash tables ).[ 1] NOTE: \u53ef\u4ee5\u4f7f\u7528multiple-model\u6765\u8fdb\u884c\u7406\u89e3: multiple: threads shared data: linked structure read: read update: write \u4e0eMVCC\u7c7b\u4f3c \u611f\u89c9\u5b83\u548cMVCC\u6709\u4e9b\u7c7b\u4f3c: 1\u3001\u90fd\u662f space\u2013time tradeoff 2\u3001\u53c2\u89c1 \"Advantages and disadvantages\"\uff0c\u5176\u4e2d\u70b9\u5230\u4e86\u4e86 version 3\u3001\u5728\u539f\u6587\u7684 \"See also\" \u7ae0\u8282\u4e2d\uff0c\u7ed9\u51fa\u4e86 Multiversion concurrency control \u94fe\u63a5 Whenever a thread is inserting or deleting elements of data structures in shared memory , all readers are guaranteed to see and traverse either the older or the new structure, therefore avoiding inconsistencies (e.g., dereferencing null pointers). [ 1] It is used when performance of reads is crucial and is an example of space\u2013time tradeoff , enabling fast operations at the cost of more space. This makes all readers proceed as if there were no synchronization involved, hence they will be fast, but also making updates more difficult. NOTE: space\u2013time tradeoff : \u4ee5\u7a7a\u95f4\u6362\u65f6\u95f4 Name and overview The name comes from the way that RCU is used to update a linked structure in place . A thread wishing to do this uses the following steps: 1\u3001create a new structure, NOTE: \u6b64\u5904\u7684\"new structure\"\uff0c\u5982\u679c\u4ee5linked list\u4e3a\u4f8b\u7684\u8bdd\uff0c\u5219\u5b83\u8868\u793a\u7684\u662flinked list\u7684node 2\u3001copy the data from the old structure into the new one, and save a pointer to the old structure, NOTE: \u5728new node\u4e2d\uff0c\u6709\u4e00\u4e2apointer to the old structure 3\u3001modify the new, copied, structure NOTE: update\u662f\u53d1\u751f\u4e8enew node 4\u3001update the global pointer to refer to the new structure, and then NOTE: \u8fd9\u662f\u6307\u66f4\u65b0linked list 5\u3001sleep until the operating system kernel determines that there are no readers left using the old structure, for example, in the Linux kernel, by using synchronize_rcu() . NOTE: \u53ef\u4ee5\u4f7f\u7528reference counting When the thread that made the copy is awakened by the kernel, it can safely deallocate the old structure. So the structure is read concurrently with a thread copying in order to do an update , hence the name \"read-copy update\". The abbreviation \"RCU\" was one of many contributions by the Linux community. Other names for similar techniques include passive serialization and MP defer by VM/XA programmers and generations by K42 and Tornado programmers. NOTE: RCU\u5728\u6267\u884cupdate\u7684\u65f6\u5019\uff0c\u5e76\u4e0d\u963b\u585eread Uses By early 2008, there were almost 2,000 uses of the RCU API within the Linux kernel[ 5] including the networking protocol stacks[ 6] and the memory-management system.[ 7] As of March 2014, there were more than 9,000 uses.[ 8] Since 2006, researchers have applied RCU and similar techniques to a number of problems, including management of metadata used in dynamic analysis,[ 9] managing the lifetime of clustered objects,[ 10] managing object lifetime in the K42 research operating system,[ 11] [ 12] and optimizing software transactional memory implementations.[ 13] [ 14] Dragonfly BSD uses a technique similar to RCU that most closely resembles Linux's Sleepable RCU (SRCU) implementation. NOTE: \u5728\u4e0b\u9762\u7684TODO\u7ae0\u8282\u4e2d\uff0c\u7ed9\u51fa\u7684\u94fe\u63a5\uff0c\u90fd\u662fLinux OS kernel\u7684\u3002 Advantages and disadvantages NOTE: \u8fd9\u6bb5\u603b\u7ed3\u975e\u5e38\u597d The ability to wait until all readers are done allows RCU readers to use much lighter-weight synchronization \u2014in some cases, absolutely no synchronization at all. In contrast, in more conventional lock-based schemes, readers must use heavy-weight synchronization in order to prevent an updater from deleting the data structure out from under them. The reason is that lock-based updaters typically update data in place, and must therefore exclude readers. In contrast, RCU-based updaters typically take advantage of the fact that writes to single aligned pointers are atomic on modern CPUs, allowing atomic insertion, removal, and replacement of data in a linked structure without disrupting readers . Concurrent RCU readers can then continue accessing the old versions, and can dispense with(\u8c41\u514d\uff0c\u514d\u9664) the atomic read-modify-write instructions , memory barriers, and cache misses that are so expensive on modern SMP computer systems, even in absence of lock contention.[ 15] [ 16] The lightweight nature of RCU's read-side primitives provides additional advantages beyond excellent performance, scalability, and real-time response. For example, they provide immunity(\u514d\u75ab\u3001\u8c41\u514d) to most deadlock and livelock conditions.[ note 3] NOTE: \u4e0a\u8ff0 writes to single aligned pointers are atomic on modern CPUs, allowing atomic insertion, removal, and replacement of data in a linked structure without disrupting readers \u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5176\u4e2d pointer\u524d\u9762\u662f\u6709\u4e00\u4e2a\u5b9a\u8bed: \"aligned\"\uff0c\u5173\u4e8e\u6b64\u7684\u89e3\u91ca\u53c2\u89c1\u5de5\u7a0bhardware\u7684 CPU\\Memory-access\\Memory-alignment \u7ae0\u8282\u3002 \u5173\u4e8e atomic read-modify-write instructions \uff0c\u53c2\u89c1 Concurrent-computing\\Concurrency-control\\Non-blocking\\Atomic-instruction\\Read\u2013modify\u2013write \u7ae0\u8282\u3002 Of course, RCU also has disadvantages. For example, RCU is a specialized technique that works best in situations with mostly reads and few updates, but is often less applicable to update-only workloads. For another example, although the fact that RCU readers and updaters may execute concurrently is what enables the lightweight nature of RCU's read-side primitives, some algorithms may not be amenable to read/update concurrency. NOTE: \u9700\u8981\u4eceread-write\u7684\u89d2\u5ea6\u5bf9concurrency control technique\u7684\u9009\u62e9\u7ee7\u7eed\u603b\u7ed3\u3002 Despite well over a decade of experience with RCU, the exact extent of its applicability is still a research topic. Example \u5728\u4e0b\u5217\u6587\u7ae0\u4e2d\uff0c\u63d0\u53ca\u4e86read-copy-update: 1\u3001wikipedia Atomicity (database systems) Typically, systems implement Atomicity by providing some mechanism to indicate which transactions have started and which finished; or by keeping a copy of the data before any changes occurred ( read-copy-update ). lwn.net What is RCU, Fundamentally? Read-copy update (RCU) is a synchronization mechanism that was added to the Linux kernel in October of 2002. RCU achieves scalability improvements by allowing reads to occur concurrently with updates. In contrast with conventional locking primitives that ensure mutual exclusion among concurrent threads regardless of whether they be readers or updaters, or with reader-writer locks that allow concurrent reads but not in the presence of updates, RCU supports concurrency between a single updater and multiple readers . RCU ensures that reads are coherent(\u8fde\u8d2f) by maintaining multiple versions of objects and ensuring that they are not freed up until all pre-existing read-side critical sections complete. NOTE: \u6b64\u5904\u7684\"coherent\"\u7684\u7528\u6cd5\uff0c\u8ba9\u6211\u8fde\u63a5\u4e86\u8fd9\u4e2a\u6b64\u7684\u542b\u4e49 RCU defines and uses efficient and scalable mechanisms for publishing and reading new versions of an object, and also for deferring the collection of old versions. NOTE: memory reclamation\uff0c\u4e0a\u8ff0\"deferring\"\uff0c\u8ba9\u6211\u60f3\u5230\u4e86\u5728\"cs.toronto-Comparative-Performance-of-Memory-Reclamation-Strategies\"\u4e2d\uff0c\u4e5f\u4f7f\u7528\u4e86\u8fd9\u4e2a\u8bcd\u8bed\u3002 These mechanisms distribute the work among read and update paths in such a way as to make read paths extremely fast. In some cases (non-preemptable kernels), RCU's read-side primitives have zero overhead. kernel RCU concepts Using RCU to Protect Read-Mostly Arrays Although RCU is more commonly used to protect linked lists, it can also be used to protect arrays. Three situations are as follows: Hash Tables Static Arrays Resizable Arrays Each of these three situations involves an RCU-protected pointer to an array that is separately indexed. TODO 3\u3001mit Read-Copy Update in a Garbage Collected Environment 4\u3001geeksforgeeks Read-Copy Update (RCU) NOTE: \u5176\u4e2d\u7ed9\u51fa\u4e86\u5b9e\u73b0\u6837\u4f8b\u4ee3\u7801","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Read-copy-update/#read-copy-update","text":"","title":"Read-copy-update"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Read-copy-update/#_1","text":"RCU: \u4e00\u3001multiple reader thread\u3001single writer thread NOTE: \u5173\u4e8e\u8fd9\u4e00\u70b9\uff0c\u53c2\u89c1: 1\u3001lwn.net What is RCU, Fundamentally? 2\u3001geeksforgeeks Read-Copy Update (RCU) \u4e8c\u3001\u7531\u4e8e\u5b83\u53ea\u6709single writer\uff0c\u56e0\u6b64\u5b83\u76f8\u5bf9\u4e8eMCVV\u5c31\u7b80\u5355\u5730\u5f88\u591a: 1\u3001\u4e0d\u9700\u8981\u8003\u8651MVCC\u4e2d\uff0cmultiple version copy\u7684\u95ee\u9898 2\u3001\u53ef\u4ee5\u8ba4\u4e3a\uff0c\u53ea\u6709\u4e00\u4efdcopy \u4e09\u3001memory reclamation RCU\u4e2d\uff0c\u9700\u8981\u8003\u8651memory reclamation\u7684\u95ee\u9898","title":"\u603b\u7ed3"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Read-copy-update/#wikipedia#read-copy-update","text":"In computer science , read-copy-update ( RCU ) is a synchronization mechanism that avoids the use of lock primitives while multiple threads concurrently read and update elements that are linked through pointers and that belong to shared data structures (e.g., linked lists , trees , hash tables ).[ 1] NOTE: \u53ef\u4ee5\u4f7f\u7528multiple-model\u6765\u8fdb\u884c\u7406\u89e3: multiple: threads shared data: linked structure read: read update: write","title":"wikipedia Read-copy-update"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Read-copy-update/#mvcc","text":"\u611f\u89c9\u5b83\u548cMVCC\u6709\u4e9b\u7c7b\u4f3c: 1\u3001\u90fd\u662f space\u2013time tradeoff 2\u3001\u53c2\u89c1 \"Advantages and disadvantages\"\uff0c\u5176\u4e2d\u70b9\u5230\u4e86\u4e86 version 3\u3001\u5728\u539f\u6587\u7684 \"See also\" \u7ae0\u8282\u4e2d\uff0c\u7ed9\u51fa\u4e86 Multiversion concurrency control \u94fe\u63a5 Whenever a thread is inserting or deleting elements of data structures in shared memory , all readers are guaranteed to see and traverse either the older or the new structure, therefore avoiding inconsistencies (e.g., dereferencing null pointers). [ 1] It is used when performance of reads is crucial and is an example of space\u2013time tradeoff , enabling fast operations at the cost of more space. This makes all readers proceed as if there were no synchronization involved, hence they will be fast, but also making updates more difficult. NOTE: space\u2013time tradeoff : \u4ee5\u7a7a\u95f4\u6362\u65f6\u95f4","title":"\u4e0eMVCC\u7c7b\u4f3c"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Read-copy-update/#name#and#overview","text":"The name comes from the way that RCU is used to update a linked structure in place . A thread wishing to do this uses the following steps: 1\u3001create a new structure, NOTE: \u6b64\u5904\u7684\"new structure\"\uff0c\u5982\u679c\u4ee5linked list\u4e3a\u4f8b\u7684\u8bdd\uff0c\u5219\u5b83\u8868\u793a\u7684\u662flinked list\u7684node 2\u3001copy the data from the old structure into the new one, and save a pointer to the old structure, NOTE: \u5728new node\u4e2d\uff0c\u6709\u4e00\u4e2apointer to the old structure 3\u3001modify the new, copied, structure NOTE: update\u662f\u53d1\u751f\u4e8enew node 4\u3001update the global pointer to refer to the new structure, and then NOTE: \u8fd9\u662f\u6307\u66f4\u65b0linked list 5\u3001sleep until the operating system kernel determines that there are no readers left using the old structure, for example, in the Linux kernel, by using synchronize_rcu() . NOTE: \u53ef\u4ee5\u4f7f\u7528reference counting When the thread that made the copy is awakened by the kernel, it can safely deallocate the old structure. So the structure is read concurrently with a thread copying in order to do an update , hence the name \"read-copy update\". The abbreviation \"RCU\" was one of many contributions by the Linux community. Other names for similar techniques include passive serialization and MP defer by VM/XA programmers and generations by K42 and Tornado programmers. NOTE: RCU\u5728\u6267\u884cupdate\u7684\u65f6\u5019\uff0c\u5e76\u4e0d\u963b\u585eread","title":"Name and overview"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Read-copy-update/#uses","text":"By early 2008, there were almost 2,000 uses of the RCU API within the Linux kernel[ 5] including the networking protocol stacks[ 6] and the memory-management system.[ 7] As of March 2014, there were more than 9,000 uses.[ 8] Since 2006, researchers have applied RCU and similar techniques to a number of problems, including management of metadata used in dynamic analysis,[ 9] managing the lifetime of clustered objects,[ 10] managing object lifetime in the K42 research operating system,[ 11] [ 12] and optimizing software transactional memory implementations.[ 13] [ 14] Dragonfly BSD uses a technique similar to RCU that most closely resembles Linux's Sleepable RCU (SRCU) implementation. NOTE: \u5728\u4e0b\u9762\u7684TODO\u7ae0\u8282\u4e2d\uff0c\u7ed9\u51fa\u7684\u94fe\u63a5\uff0c\u90fd\u662fLinux OS kernel\u7684\u3002","title":"Uses"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Read-copy-update/#advantages#and#disadvantages","text":"NOTE: \u8fd9\u6bb5\u603b\u7ed3\u975e\u5e38\u597d The ability to wait until all readers are done allows RCU readers to use much lighter-weight synchronization \u2014in some cases, absolutely no synchronization at all. In contrast, in more conventional lock-based schemes, readers must use heavy-weight synchronization in order to prevent an updater from deleting the data structure out from under them. The reason is that lock-based updaters typically update data in place, and must therefore exclude readers. In contrast, RCU-based updaters typically take advantage of the fact that writes to single aligned pointers are atomic on modern CPUs, allowing atomic insertion, removal, and replacement of data in a linked structure without disrupting readers . Concurrent RCU readers can then continue accessing the old versions, and can dispense with(\u8c41\u514d\uff0c\u514d\u9664) the atomic read-modify-write instructions , memory barriers, and cache misses that are so expensive on modern SMP computer systems, even in absence of lock contention.[ 15] [ 16] The lightweight nature of RCU's read-side primitives provides additional advantages beyond excellent performance, scalability, and real-time response. For example, they provide immunity(\u514d\u75ab\u3001\u8c41\u514d) to most deadlock and livelock conditions.[ note 3] NOTE: \u4e0a\u8ff0 writes to single aligned pointers are atomic on modern CPUs, allowing atomic insertion, removal, and replacement of data in a linked structure without disrupting readers \u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5176\u4e2d pointer\u524d\u9762\u662f\u6709\u4e00\u4e2a\u5b9a\u8bed: \"aligned\"\uff0c\u5173\u4e8e\u6b64\u7684\u89e3\u91ca\u53c2\u89c1\u5de5\u7a0bhardware\u7684 CPU\\Memory-access\\Memory-alignment \u7ae0\u8282\u3002 \u5173\u4e8e atomic read-modify-write instructions \uff0c\u53c2\u89c1 Concurrent-computing\\Concurrency-control\\Non-blocking\\Atomic-instruction\\Read\u2013modify\u2013write \u7ae0\u8282\u3002 Of course, RCU also has disadvantages. For example, RCU is a specialized technique that works best in situations with mostly reads and few updates, but is often less applicable to update-only workloads. For another example, although the fact that RCU readers and updaters may execute concurrently is what enables the lightweight nature of RCU's read-side primitives, some algorithms may not be amenable to read/update concurrency. NOTE: \u9700\u8981\u4eceread-write\u7684\u89d2\u5ea6\u5bf9concurrency control technique\u7684\u9009\u62e9\u7ee7\u7eed\u603b\u7ed3\u3002 Despite well over a decade of experience with RCU, the exact extent of its applicability is still a research topic.","title":"Advantages and disadvantages"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Read-copy-update/#example","text":"\u5728\u4e0b\u5217\u6587\u7ae0\u4e2d\uff0c\u63d0\u53ca\u4e86read-copy-update: 1\u3001wikipedia Atomicity (database systems) Typically, systems implement Atomicity by providing some mechanism to indicate which transactions have started and which finished; or by keeping a copy of the data before any changes occurred ( read-copy-update ).","title":"Example"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Read-copy-update/#lwnnet#what#is#rcu#fundamentally","text":"Read-copy update (RCU) is a synchronization mechanism that was added to the Linux kernel in October of 2002. RCU achieves scalability improvements by allowing reads to occur concurrently with updates. In contrast with conventional locking primitives that ensure mutual exclusion among concurrent threads regardless of whether they be readers or updaters, or with reader-writer locks that allow concurrent reads but not in the presence of updates, RCU supports concurrency between a single updater and multiple readers . RCU ensures that reads are coherent(\u8fde\u8d2f) by maintaining multiple versions of objects and ensuring that they are not freed up until all pre-existing read-side critical sections complete. NOTE: \u6b64\u5904\u7684\"coherent\"\u7684\u7528\u6cd5\uff0c\u8ba9\u6211\u8fde\u63a5\u4e86\u8fd9\u4e2a\u6b64\u7684\u542b\u4e49 RCU defines and uses efficient and scalable mechanisms for publishing and reading new versions of an object, and also for deferring the collection of old versions. NOTE: memory reclamation\uff0c\u4e0a\u8ff0\"deferring\"\uff0c\u8ba9\u6211\u60f3\u5230\u4e86\u5728\"cs.toronto-Comparative-Performance-of-Memory-Reclamation-Strategies\"\u4e2d\uff0c\u4e5f\u4f7f\u7528\u4e86\u8fd9\u4e2a\u8bcd\u8bed\u3002 These mechanisms distribute the work among read and update paths in such a way as to make read paths extremely fast. In some cases (non-preemptable kernels), RCU's read-side primitives have zero overhead.","title":"lwn.net What is RCU, Fundamentally?"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Read-copy-update/#kernel#rcu#concepts","text":"","title":"kernel RCU concepts"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Read-copy-update/#using#rcu#to#protect#read-mostly#arrays","text":"Although RCU is more commonly used to protect linked lists, it can also be used to protect arrays. Three situations are as follows: Hash Tables Static Arrays Resizable Arrays Each of these three situations involves an RCU-protected pointer to an array that is separately indexed.","title":"Using RCU to Protect Read-Mostly Arrays"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Read-copy-update/#todo","text":"3\u3001mit Read-Copy Update in a Garbage Collected Environment 4\u3001geeksforgeeks Read-Copy Update (RCU) NOTE: \u5176\u4e2d\u7ed9\u51fa\u4e86\u5b9e\u73b0\u6837\u4f8b\u4ee3\u7801","title":"TODO"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Read-copy-update/lwn-User-space-RCU/","text":"lwn User-space RCU The user-space read-copy update (URCU) library was established in February of 2009, and is now used by several projects and available from several distributions. URCU is similar to its Linux-kernel counterpart, providing a replacement for reader-writer locking, among other uses. Using URCU When is URCU useful? RCU is a specialized synchronization mechanism, and its performance, scalability, and real-time response advantages are due to this specialization. It is always important to use the right tool for the job, and RCU is no exception to this rule. The following diagram gives some rules of thumb for where RCU is best used: NOTE: 1\u3001\u603b\u7ed3\u5f97\u975e\u5e38\u597d","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Read-copy-update/lwn-User-space-RCU/#lwn#user-space#rcu","text":"The user-space read-copy update (URCU) library was established in February of 2009, and is now used by several projects and available from several distributions. URCU is similar to its Linux-kernel counterpart, providing a replacement for reader-writer locking, among other uses.","title":"lwn User-space RCU"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Read-copy-update/lwn-User-space-RCU/#using#urcu","text":"","title":"Using URCU"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Read-copy-update/lwn-User-space-RCU/#when#is#urcu#useful","text":"RCU is a specialized synchronization mechanism, and its performance, scalability, and real-time response advantages are due to this specialization. It is always important to use the right tool for the job, and RCU is no exception to this rule. The following diagram gives some rules of thumb for where RCU is best used: NOTE: 1\u3001\u603b\u7ed3\u5f97\u975e\u5e38\u597d","title":"When is URCU useful?"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/TODO-Level-of-free/","text":"Wait-free\u3001lock-free and obstruction-free \u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\u8ba8\u8bba\u4e86\u8fd9\u4e2atopic: 1\u3001stackoverflow What is the difference between lock-free and obstruction-free? 2\u3001 Expert-1024cores\\Lockfree-Algorithms \u975e\u5e38\u6743\u5a01\uff0c\u91cd\u8981\u53c2\u8003\u8fd9\u7bc7\u6587\u7ae0\u3002 3\u3001 Chapter 21. Boost.Lockfree 4\u3001Herb Sutter Lock Free Programming or, How to Juggle Razor Blades \u5176\u4e2d\u6709\u7740\u975e\u5e38\u597d\u7684\u603b\u7ed3\u3002 5\u3001developer.51cto \u9ad8\u6027\u80fd\u5f00\u53d1\u7684\u201c\u5341\u5927\u6b66\u5668\u201d\uff0c\u7231\u4e86\u7231\u4e86\uff01 \u8fd9\u7bc7\u6587\u7ae0\u4e2d\u4e5f\u6709\u63d0\u53ca","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/TODO-Level-of-free/#wait-freelock-free#and#obstruction-free","text":"\u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\u8ba8\u8bba\u4e86\u8fd9\u4e2atopic: 1\u3001stackoverflow What is the difference between lock-free and obstruction-free? 2\u3001 Expert-1024cores\\Lockfree-Algorithms \u975e\u5e38\u6743\u5a01\uff0c\u91cd\u8981\u53c2\u8003\u8fd9\u7bc7\u6587\u7ae0\u3002 3\u3001 Chapter 21. Boost.Lockfree 4\u3001Herb Sutter Lock Free Programming or, How to Juggle Razor Blades \u5176\u4e2d\u6709\u7740\u975e\u5e38\u597d\u7684\u603b\u7ed3\u3002 5\u3001developer.51cto \u9ad8\u6027\u80fd\u5f00\u53d1\u7684\u201c\u5341\u5927\u6b66\u5668\u201d\uff0c\u7231\u4e86\u7231\u4e86\uff01 \u8fd9\u7bc7\u6587\u7ae0\u4e2d\u4e5f\u6709\u63d0\u53ca","title":"Wait-free\u3001lock-free and obstruction-free"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/TODO-Level-of-free/Liveness/","text":"Liveness \u5728\u9605\u8bfb wikipedia Non-blocking algorithm \u65f6\uff0c\u53d1\u73b0\u7684\u5b83\uff1b \u6211\u89c9\u5f97\u5b83\u548c\u4ee5\u4e0b\u5185\u5bb9\u6709\u5173: 1\u3001wait-free 2\u3001lock-free 3\u3001obstruction-free \u5b83\u4eec\u90fd\u4e0e\"progress\"\u6709\u5173\u3002 wikipedia Liveness In concurrent computing , liveness refers to a set of properties of concurrent systems , that require a system to make progress despite the fact that its concurrently executing components (\"processes\") may have to \"take turns\" in critical sections , parts of the program that cannot be simultaneously run by multiple processes.[ 1] Liveness guarantees are important properties in operating systems and distributed systems .[ 2]","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/TODO-Level-of-free/Liveness/#liveness","text":"\u5728\u9605\u8bfb wikipedia Non-blocking algorithm \u65f6\uff0c\u53d1\u73b0\u7684\u5b83\uff1b \u6211\u89c9\u5f97\u5b83\u548c\u4ee5\u4e0b\u5185\u5bb9\u6709\u5173: 1\u3001wait-free 2\u3001lock-free 3\u3001obstruction-free \u5b83\u4eec\u90fd\u4e0e\"progress\"\u6709\u5173\u3002","title":"Liveness"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/TODO-Level-of-free/Liveness/#wikipedia#liveness","text":"In concurrent computing , liveness refers to a set of properties of concurrent systems , that require a system to make progress despite the fact that its concurrently executing components (\"processes\") may have to \"take turns\" in critical sections , parts of the program that cannot be simultaneously run by multiple processes.[ 1] Liveness guarantees are important properties in operating systems and distributed systems .[ 2]","title":"wikipedia Liveness"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/TODO-Level-of-free/Lock-free/","text":"Lock free What is lock free? preshing An Introduction to Lock-Free Programming \u8fd9\u7bc7\u6587\u7ae0\u4e2d\uff0c\u4e5f\u5bf9lock-free\u8fdb\u884c\u4e86\u4ecb\u7ecd\uff0c\u53c2\u89c1\u76f8\u5173\u7ae0\u8282\uff1b 1024cores Home \u200e > \u200e Lockfree Algorithms \u200e > \u200e Introduction \u8fd9\u7bc7\u6587\u7ae0\u4e2d\uff0c\u4e5f\u5bf9lock-free\u8fdb\u884c\u4e86\u4ecb\u7ecd\uff0c\u53c2\u89c1\u76f8\u5173\u7ae0\u8282\uff1b stackoverflow What is lock-free multithreaded programming? I have seen people/articles/SO posts who say they have designed their own \"lock-free\" container for multithreaded usage. Assuming they haven't used a performance-hitting modulus trick (i.e. each thread can only insert based upon some modulo) how can data structures be multi-threaded but also lock-free??? This question is intended towards C and C++. A NOTE: \u7ed3\u5408\u5177\u4f53\u7684\u4f8b\u5b50\uff0c\u662f\u6bd4\u8f83\u5bb9\u6613\u7406\u89e3\u7684 The key in lock-free programming is to use hardware-intrinsic atomic operations. As a matter of fact, even locks themselves must use those atomic operations! But the difference between locked and lock-free programming is that a lock-free program can never be stalled(\u505c\u6ede) entirely by any single thread. By contrast, if in a locking program one thread acquires a lock and then gets suspended indefinitely(\u65e0\u671f\u9650\u7684), the entire program is blocked and cannot make progress. By contrast, a lock-free program can make progress even if individual threads are suspended indefinitely. Here's a simple example: A concurrent counter increment. We present two versions which are both \"thread-safe\", i.e. which can be called multiple times concurrently. First the locked version: int counter = 0 ; std :: mutex counter_mutex ; void increment_with_lock () { std :: lock_guard < std :: mutex > _ ( counter_mutex ); ++ counter ; } Now the lock-free version: std :: atomic < int > counter ( 0 ); void increment_lockfree () { ++ counter ; } Now imagine hundreds of thread all call the increment_* function concurrently. In the locked version, no thread can make progress until the lock-holding thread unlocks the mutex. By contrast, in the lock-free version, all threads can make progress . If a thread is held up, it just won't do its share of the work, but everyone else gets to get on with their work. Lock-free VS locked It is worth noting that in general lock-free programming trades(\u4ea4\u6362) throughput and mean latency throughput for predictable latency . That is, a lock-free program will usually get less done than a corresponding locking program if there is not too much contention (since atomic operations are slow and affect a lot of the rest of the system), but it guarantees to never produce unpredictably large latencies. TODO rigtorp / awesome-lockfree https://github.com/search?o=desc&q=lockfree&s=stars&type=Repositories","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/TODO-Level-of-free/Lock-free/#lock#free","text":"","title":"Lock free"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/TODO-Level-of-free/Lock-free/#what#is#lock#free","text":"","title":"What is lock free?"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/TODO-Level-of-free/Lock-free/#preshing#an#introduction#to#lock-free#programming","text":"\u8fd9\u7bc7\u6587\u7ae0\u4e2d\uff0c\u4e5f\u5bf9lock-free\u8fdb\u884c\u4e86\u4ecb\u7ecd\uff0c\u53c2\u89c1\u76f8\u5173\u7ae0\u8282\uff1b","title":"preshing An Introduction to Lock-Free Programming"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/TODO-Level-of-free/Lock-free/#1024cores#home#lockfree#algorithms#introduction","text":"\u8fd9\u7bc7\u6587\u7ae0\u4e2d\uff0c\u4e5f\u5bf9lock-free\u8fdb\u884c\u4e86\u4ecb\u7ecd\uff0c\u53c2\u89c1\u76f8\u5173\u7ae0\u8282\uff1b","title":"1024cores Home\u200e &gt; \u200eLockfree Algorithms\u200e &gt; \u200eIntroduction"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/TODO-Level-of-free/Lock-free/#stackoverflow#what#is#lock-free#multithreaded#programming","text":"I have seen people/articles/SO posts who say they have designed their own \"lock-free\" container for multithreaded usage. Assuming they haven't used a performance-hitting modulus trick (i.e. each thread can only insert based upon some modulo) how can data structures be multi-threaded but also lock-free??? This question is intended towards C and C++. A NOTE: \u7ed3\u5408\u5177\u4f53\u7684\u4f8b\u5b50\uff0c\u662f\u6bd4\u8f83\u5bb9\u6613\u7406\u89e3\u7684 The key in lock-free programming is to use hardware-intrinsic atomic operations. As a matter of fact, even locks themselves must use those atomic operations! But the difference between locked and lock-free programming is that a lock-free program can never be stalled(\u505c\u6ede) entirely by any single thread. By contrast, if in a locking program one thread acquires a lock and then gets suspended indefinitely(\u65e0\u671f\u9650\u7684), the entire program is blocked and cannot make progress. By contrast, a lock-free program can make progress even if individual threads are suspended indefinitely. Here's a simple example: A concurrent counter increment. We present two versions which are both \"thread-safe\", i.e. which can be called multiple times concurrently. First the locked version: int counter = 0 ; std :: mutex counter_mutex ; void increment_with_lock () { std :: lock_guard < std :: mutex > _ ( counter_mutex ); ++ counter ; } Now the lock-free version: std :: atomic < int > counter ( 0 ); void increment_lockfree () { ++ counter ; } Now imagine hundreds of thread all call the increment_* function concurrently. In the locked version, no thread can make progress until the lock-holding thread unlocks the mutex. By contrast, in the lock-free version, all threads can make progress . If a thread is held up, it just won't do its share of the work, but everyone else gets to get on with their work. Lock-free VS locked It is worth noting that in general lock-free programming trades(\u4ea4\u6362) throughput and mean latency throughput for predictable latency . That is, a lock-free program will usually get less done than a corresponding locking program if there is not too much contention (since atomic operations are slow and affect a lot of the rest of the system), but it guarantees to never produce unpredictably large latencies.","title":"stackoverflow What is lock-free multithreaded programming?"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/TODO-Level-of-free/Lock-free/#todo","text":"rigtorp / awesome-lockfree https://github.com/search?o=desc&q=lockfree&s=stars&type=Repositories","title":"TODO"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Version-based/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u6807\u9898\"version-based concurrency control\"\u7684\u542b\u4e49\u662f\"\u57fa\u4e8e\u65f6\u95f4\u6765\u8fdb\u884c\u5e76\u53d1\u63a7\u5236\"\u3002\u5c06multiple version concurrency control\u6536\u5f55\u8fdb\u6765\u7684\u539f\u56e0\u662f: version\u4e5f\u662f\u8868\u793a\u7684\u65f6\u95f4\uff0c\u5b83\u4ee5\u7b80\u5355\u7684\u6570\u5b57\u6765\u8868\u793a\u65f6\u95f4\u542b\u4e49\u3002 Version represents time \u672c\u8282\u6807\u9898\u7684\u542b\u4e49\u662fversion\u8868\u793a\u4e86time\u3002 Application version\u5728data system\u4e2d\u6709\u7740\u5e7f\u6cdb\u7684\u5e94\u7528\u3002 MVCC multiple version concurrency control\uff0c\u53c2\u89c1 Concurrent-computing\\Concurrency-control\\Concurrency-control\\Time-based-concurrency-control \u7ae0\u8282\u3002 Schema evaluation protobuf\u3001thrift\u90fd\u4f7f\u7528version\u6765\u5b9e\u73b0schema evaluation\uff0c\u5173\u4e8e\u6b64\uff0c\u53ef\u4ee5\u53c2\u89c1: 1) Designing Data-IntensiveApplications#CHAPTER 4 Encoding and Evolution Zookeeper Nodes and ephemeral nodes ZooKeeper Programmer's Guide#ZNodes Why time can be used in concurrency control? 1\u3001\"time\"\u662fstrict ordering\u7684 2\u3001\u53ef\u4ee5\u4f7f\u7528timestamp\u6765\u4f5c\u4e3aversion ID Version-based concurrency control is optimistic \u6839\u636e Optimistic-and-pessimistic \u7ae0\u8282\u7684\u5185\u5bb9\u53ef\u77e5\uff0cversion-based concurrency control is optimistic\u3002","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Version-based/#_1","text":"\u672c\u7ae0\u6807\u9898\"version-based concurrency control\"\u7684\u542b\u4e49\u662f\"\u57fa\u4e8e\u65f6\u95f4\u6765\u8fdb\u884c\u5e76\u53d1\u63a7\u5236\"\u3002\u5c06multiple version concurrency control\u6536\u5f55\u8fdb\u6765\u7684\u539f\u56e0\u662f: version\u4e5f\u662f\u8868\u793a\u7684\u65f6\u95f4\uff0c\u5b83\u4ee5\u7b80\u5355\u7684\u6570\u5b57\u6765\u8868\u793a\u65f6\u95f4\u542b\u4e49\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Version-based/#version#represents#time","text":"\u672c\u8282\u6807\u9898\u7684\u542b\u4e49\u662fversion\u8868\u793a\u4e86time\u3002","title":"Version represents time"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Version-based/#application","text":"version\u5728data system\u4e2d\u6709\u7740\u5e7f\u6cdb\u7684\u5e94\u7528\u3002","title":"Application"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Version-based/#mvcc","text":"multiple version concurrency control\uff0c\u53c2\u89c1 Concurrent-computing\\Concurrency-control\\Concurrency-control\\Time-based-concurrency-control \u7ae0\u8282\u3002","title":"MVCC"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Version-based/#schema#evaluation","text":"protobuf\u3001thrift\u90fd\u4f7f\u7528version\u6765\u5b9e\u73b0schema evaluation\uff0c\u5173\u4e8e\u6b64\uff0c\u53ef\u4ee5\u53c2\u89c1: 1) Designing Data-IntensiveApplications#CHAPTER 4 Encoding and Evolution","title":"Schema evaluation"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Version-based/#zookeeper","text":"Nodes and ephemeral nodes ZooKeeper Programmer's Guide#ZNodes","title":"Zookeeper"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Version-based/#why#time#can#be#used#in#concurrency#control","text":"1\u3001\"time\"\u662fstrict ordering\u7684 2\u3001\u53ef\u4ee5\u4f7f\u7528timestamp\u6765\u4f5c\u4e3aversion ID","title":"Why time can be used in concurrency control?"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Version-based/#version-based#concurrency#control#is#optimistic","text":"\u6839\u636e Optimistic-and-pessimistic \u7ae0\u8282\u7684\u5185\u5bb9\u53ef\u77e5\uff0cversion-based concurrency control is optimistic\u3002","title":"Version-based concurrency control is optimistic"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Version-based/Timestamp-based-concurrency-control/","text":"Timestamp-based concurrency control wikipedia Timestamp-based concurrency control In computer science , a timestamp-based concurrency control algorithm is a non-lock concurrency control method. It is used in some databases to safely handle transactions, using timestamps . Operation Formal Each transaction ({\\displaystyle T_{i}} ) is an ordered list of actions ({\\displaystyle A_{ix}} ). Before the transaction performs its first action ({\\displaystyle A_{i1}} ), it is marked with the current timestamp , or any other strictly totally ordered sequence: {\\displaystyle TS(T_{i})=NOW()} . Every transaction is also given an initially empty set of transactions upon which it depends, {\\displaystyle DEP(T_{i})=[]} , and an initially empty set of old objects which it updated, {\\displaystyle OLD(T_{i})=[]} . Each object {\\displaystyle (O_{j})} in the database is given two timestamp fields which are not used other than for concurrency control: {\\displaystyle RTS(O_{j})} is the time at which the value of object was last used by a transaction, {\\displaystyle WTS(O_{j})} is the time at which the value of the object was last updated by a transaction. For all {\\displaystyle T_{i}} : To abort :","title":"Timestamp-based-concurrency-control"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Version-based/Timestamp-based-concurrency-control/#timestamp-based#concurrency#control","text":"","title":"Timestamp-based concurrency control"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Version-based/Timestamp-based-concurrency-control/#wikipedia#timestamp-based#concurrency#control","text":"In computer science , a timestamp-based concurrency control algorithm is a non-lock concurrency control method. It is used in some databases to safely handle transactions, using timestamps .","title":"wikipedia Timestamp-based concurrency control"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Version-based/Timestamp-based-concurrency-control/#operation","text":"","title":"Operation"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Version-based/Timestamp-based-concurrency-control/#formal","text":"Each transaction ({\\displaystyle T_{i}} ) is an ordered list of actions ({\\displaystyle A_{ix}} ). Before the transaction performs its first action ({\\displaystyle A_{i1}} ), it is marked with the current timestamp , or any other strictly totally ordered sequence: {\\displaystyle TS(T_{i})=NOW()} . Every transaction is also given an initially empty set of transactions upon which it depends, {\\displaystyle DEP(T_{i})=[]} , and an initially empty set of old objects which it updated, {\\displaystyle OLD(T_{i})=[]} . Each object {\\displaystyle (O_{j})} in the database is given two timestamp fields which are not used other than for concurrency control: {\\displaystyle RTS(O_{j})} is the time at which the value of object was last used by a transaction, {\\displaystyle WTS(O_{j})} is the time at which the value of the object was last updated by a transaction. For all {\\displaystyle T_{i}} : To abort :","title":"Formal"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Version-based/MVCC/","text":"Multiversion concurrency control \u603b\u7ed3 MVCC: 1\u3001\u5141\u8bb8multiple reader\u3001multiple writer 2\u3001\u5982\u679cmultiple writer\u540c\u65f6write\uff0c\u90a3\u4e48\u5c31\u5bfc\u81f4\u540c\u65f6\u5b58\u5728multiple version copy\uff0c\u90a3MVCC\u4e2d\uff0c\u5982\u4f55\u5b9e\u73b0\u5c06multiple version copy\u8fdb\u884c\u5408\u5e76/commit\u7684\u5462\uff1f\u8fd9\u6bd4\u8f83\u590d\u6742\uff0c\u5728 wikipedia Multiversion concurrency control # Implementation \u7ae0\u8282\u4e2d\u8fdb\u884c\u4e86\u7b80\u5355\u8bf4\u660e 3\u3001MVCC\u53ef\u4ee5\u4f5c\u4e3a\u5b9e\u73b0Concurrently-readable Data Structure\u7684\u53c2\u8003 4\u3001MVCC\u4e2d\uff0c\u662f\u5426\u5b58\u5728dirty read\uff1f\u5373\u6ca1\u6709\u8bfb\u5230\u6700\u65b0\u7684\u6570\u636e\uff1f\u6bcf\u4e2areader\u90fd\u8bfb\u5230\u5f53\u524d\u6700\u65b0\u65f6\u523b\u7684\u6570\u636e\uff0c\u5982\u679c\u540c\u65f6\u6709writer\uff0cwriter\u7684\u4fee\u6539\u53d1\u751f\u5728copy\u4e2d\uff0c\u4e0d\u8ba4\u4e3a\u662f\u6700\u65b0\u7684\u6570\u636e\uff0c\u56e0\u6b64\u53ef\u4ee5\u8ba4\u4e3a\u5b83\u4e0d\u5b58\u5728dirty read 5\u3001\u4e0d\u662fin-place write\uff0c\u800c\u662fwrite to copy 6\u3001MVCC\u4e5f\u6709\u7c7b\u4f3c\u4e8e lock-free data structure memory reclamation \u7684\u95ee\u9898 7\u3001 space\u2013time tradeoff wikipedia Multiversion concurrency control Multiversion concurrency control ( MCC or MVCC ), is a concurrency control method commonly used by database management systems to provide concurrent access to the database and in programming languages to implement transactional memory .[ 1] Description NOTE: \u672c\u8d28\u539f\u56e0\u8fd8\u662fmultiple model\u4e2d\u5bf9shared data\u7684read and write\u95ee\u9898 Without concurrency control, if someone is reading from a database at the same time as someone else is writing to it, it is possible that the reader will see a half-written or inconsistent piece of data. NOTE: dirty read For instance, when making a wire transfer between two bank accounts if a reader reads the balance at the bank when the money has been withdrawn from the original account and before it was deposited in the destination account, it would seem that money has disappeared from the bank. Isolation is the property that provides guarantees in the concurrent accesses to data. Isolation is implemented by means of a concurrency control protocol. The simplest way is to make all readers wait until the writer is done, which is known as a read-write lock . Locks are known to create contention especially between long read transactions and update transactions. NOTE: lock contention\uff0c\u53c2\u89c1Lock\u7ae0\u8282 MVCC aims at solving the problem by keeping multiple copies of each data item. In this way, each user connected to the database sees a snapshot of the database at a particular instant in time. Any changes made by a writer will not be seen by other users of the database until the changes have been completed (or, in database terms: until the transaction has been committed.) NOTE: \u601d\u8003: \u5982\u4f55\u6709multiple writer\uff0c\u90a3\u5982\u4f55\u51b3\u5b9a\u5462\uff1f MVCC introduces the challenge of how to remove versions that become obsolete and will never be read. NOTE: \u975e\u5e38\u7c7b\u4f3c\u4e8e lock-less data structure memory reclamation \u7684\u95ee\u9898 Implementation MVCC uses timestamps ( TS ), and incrementing transaction IDs , to achieve transactional consistency . MVCC ensures a transaction ( T ) never has to wait to Read a database object ( P ) by maintaining several versions of the object. Each version of object P has both a Read Timestamp ( RTS ) and a Write Timestamp ( WTS ) which lets a particular transaction Ti read the most recent version of the object which precedes the transaction's Read Timestamp RTS ( Ti ). NOTE: \u663e\u7136MVCC\u662ftime-based\u7684 TODO shiroyasha Multi Version Concurrency Control","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Version-based/MVCC/#multiversion#concurrency#control","text":"","title":"Multiversion concurrency control"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Version-based/MVCC/#_1","text":"MVCC: 1\u3001\u5141\u8bb8multiple reader\u3001multiple writer 2\u3001\u5982\u679cmultiple writer\u540c\u65f6write\uff0c\u90a3\u4e48\u5c31\u5bfc\u81f4\u540c\u65f6\u5b58\u5728multiple version copy\uff0c\u90a3MVCC\u4e2d\uff0c\u5982\u4f55\u5b9e\u73b0\u5c06multiple version copy\u8fdb\u884c\u5408\u5e76/commit\u7684\u5462\uff1f\u8fd9\u6bd4\u8f83\u590d\u6742\uff0c\u5728 wikipedia Multiversion concurrency control # Implementation \u7ae0\u8282\u4e2d\u8fdb\u884c\u4e86\u7b80\u5355\u8bf4\u660e 3\u3001MVCC\u53ef\u4ee5\u4f5c\u4e3a\u5b9e\u73b0Concurrently-readable Data Structure\u7684\u53c2\u8003 4\u3001MVCC\u4e2d\uff0c\u662f\u5426\u5b58\u5728dirty read\uff1f\u5373\u6ca1\u6709\u8bfb\u5230\u6700\u65b0\u7684\u6570\u636e\uff1f\u6bcf\u4e2areader\u90fd\u8bfb\u5230\u5f53\u524d\u6700\u65b0\u65f6\u523b\u7684\u6570\u636e\uff0c\u5982\u679c\u540c\u65f6\u6709writer\uff0cwriter\u7684\u4fee\u6539\u53d1\u751f\u5728copy\u4e2d\uff0c\u4e0d\u8ba4\u4e3a\u662f\u6700\u65b0\u7684\u6570\u636e\uff0c\u56e0\u6b64\u53ef\u4ee5\u8ba4\u4e3a\u5b83\u4e0d\u5b58\u5728dirty read 5\u3001\u4e0d\u662fin-place write\uff0c\u800c\u662fwrite to copy 6\u3001MVCC\u4e5f\u6709\u7c7b\u4f3c\u4e8e lock-free data structure memory reclamation \u7684\u95ee\u9898 7\u3001 space\u2013time tradeoff","title":"\u603b\u7ed3"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Version-based/MVCC/#wikipedia#multiversion#concurrency#control","text":"Multiversion concurrency control ( MCC or MVCC ), is a concurrency control method commonly used by database management systems to provide concurrent access to the database and in programming languages to implement transactional memory .[ 1]","title":"wikipedia Multiversion concurrency control"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Version-based/MVCC/#description","text":"NOTE: \u672c\u8d28\u539f\u56e0\u8fd8\u662fmultiple model\u4e2d\u5bf9shared data\u7684read and write\u95ee\u9898 Without concurrency control, if someone is reading from a database at the same time as someone else is writing to it, it is possible that the reader will see a half-written or inconsistent piece of data. NOTE: dirty read For instance, when making a wire transfer between two bank accounts if a reader reads the balance at the bank when the money has been withdrawn from the original account and before it was deposited in the destination account, it would seem that money has disappeared from the bank. Isolation is the property that provides guarantees in the concurrent accesses to data. Isolation is implemented by means of a concurrency control protocol. The simplest way is to make all readers wait until the writer is done, which is known as a read-write lock . Locks are known to create contention especially between long read transactions and update transactions. NOTE: lock contention\uff0c\u53c2\u89c1Lock\u7ae0\u8282 MVCC aims at solving the problem by keeping multiple copies of each data item. In this way, each user connected to the database sees a snapshot of the database at a particular instant in time. Any changes made by a writer will not be seen by other users of the database until the changes have been completed (or, in database terms: until the transaction has been committed.) NOTE: \u601d\u8003: \u5982\u4f55\u6709multiple writer\uff0c\u90a3\u5982\u4f55\u51b3\u5b9a\u5462\uff1f MVCC introduces the challenge of how to remove versions that become obsolete and will never be read. NOTE: \u975e\u5e38\u7c7b\u4f3c\u4e8e lock-less data structure memory reclamation \u7684\u95ee\u9898","title":"Description"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Version-based/MVCC/#implementation","text":"MVCC uses timestamps ( TS ), and incrementing transaction IDs , to achieve transactional consistency . MVCC ensures a transaction ( T ) never has to wait to Read a database object ( P ) by maintaining several versions of the object. Each version of object P has both a Read Timestamp ( RTS ) and a Write Timestamp ( WTS ) which lets a particular transaction Ti read the most recent version of the object which precedes the transaction's Read Timestamp RTS ( Ti ). NOTE: \u663e\u7136MVCC\u662ftime-based\u7684","title":"Implementation"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking/Version-based/MVCC/#todo","text":"shiroyasha Multi Version Concurrency Control","title":"TODO"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking-VS-blocking/","text":"Non-blocking VS blocking \u53c2\u89c1: 1\u3001 Lock-free-VS-lock-based","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking-VS-blocking/#non-blocking#vs#blocking","text":"\u53c2\u89c1: 1\u3001 Lock-free-VS-lock-based","title":"Non-blocking VS blocking"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking-VS-blocking/Lock-free-VS-lock-based/","text":"Lock-free VS lock-based Performance \u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\uff0c\u5bf9\u6b64\u8fdb\u884c\u4e86\u8ba8\u8bba\u3002 1\u3001stackoverflow What is lock-free multithreaded programming? # A It is worth noting that in general lock-free programming trades(\u4ea4\u6362) throughput and mean latency throughput for predictable latency . That is, a lock-free program will usually get less done than a corresponding locking program if there is not too much contention (since atomic operations are slow and affect a lot of the rest of the system), but it guarantees to never produce unpredictably large latencies. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u8868\u660e\u610f\u601d\u662f: \"\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u4e00\u822c\u6765\u8bf4\uff0c\u65e0\u9501\u7f16\u7a0b\u7528\u541e\u5410\u91cf\u548c\u5e73\u5747\u5ef6\u8fdf\u541e\u5410\u91cf\u4ea4\u6362\u53ef\u9884\u6d4b\u5ef6\u8fdf\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c\u5982\u679c\u6ca1\u6709\u592a\u591a\u7684\u4e89\u7528(\u56e0\u4e3a\u539f\u5b50\u64cd\u4f5c\u5f88\u6162\uff0c\u5e76\u4e14\u4f1a\u5f71\u54cd\u7cfb\u7edf\u7684\u5176\u4ed6\u90e8\u5206)\uff0c\u90a3\u4e48\u65e0\u9501\u7a0b\u5e8f\u901a\u5e38\u6bd4\u76f8\u5e94\u7684\u9501\u5b9a\u7a0b\u5e8f\u5b8c\u6210\u7684\u5de5\u4f5c\u8981\u5c11\uff0c\u4f46\u662f\u5b83\u4fdd\u8bc1\u4e0d\u4f1a\u4ea7\u751f\u4e0d\u53ef\u9884\u6d4b\u7684\u5927\u5ef6\u8fdf\u3002\" 1\u3001lock-free programming \u4ee5 throughout \u548c mean latency throughout \u6765\u6362\u53d6 predictable latency\u3002 2\u3001lock-free \u4e0d\u4e00\u5b9a\u6bd4 lock-base \u8981\u6162: \u5f53\u6ca1\u6709\u592a\u591a\u7684thread contention\u7684\u65f6\u5019\uff0clock-based\u7684\u6027\u80fd\u53ef\u80fd\u66f4\u4f73\u3002 2\u3001arangodb Comparison: Lockless programming with atomics in C++ 11 vs. mutex and RW-locks Methods tested Here is a list of the implemented algorithms with explanations: Read : reads the current value and increases control counter Write : reads the old value, increases it and writes it back Set : writes a loop counter into the protected variable 0: Unlocked 1: Mutexes 2: Writelock / Writelock 3: Writelock / Readlock 4: Atomic Read & Write 5: Atomic Read \u201cconsume\u201d, Atomic Set \u201crelease\u201d for setting 6: Atomic Read \u201cacquire\u201d, Atomic Set \u201crelease\u201d for setting 7: Atomic Read \u201cconsume\u201d, Atomic Set \u201ccst \u2013 consistent\u201d for setting 8: Atomic Read \u201cacquire\u201d, Atomic Set \u201ccst \u2013 consistent\u201d for setting 9: Atomic Read \u201cconsume\u201d, Atomic Set \u201crelaxed\u201d for setting 10: Atomic Read \u201cacquire\u201d, Atomic Set \u201crelaxed\u201d for setting 11: Atomic Read \u201cAcquire\u201d, Atomic exchange weak for writing 12: Atomic Read \u201cconsume\u201d, Atomic exchange weak for writing Test Results Heres a table of the time in s the tests took to execute: TESTCASE / OS WINDOWS 8.1 GRML LINUX X64 MAC OS X LINUX ARM LINUX X64 #0 0.033 0.0300119 0.02503 0.18299 0.02895 #1 479.878 5.7003600 118.47100 21.29970 4.47721 #2 1.45997 4.5296900 142.61700 23.29240 4.72051 #3 4.70791 6.3525200 7.65026 23.82040 7.87677 #4 0.056999 0.0454769 0.03771 0.94990 0.035302 #5 0.033999 0.0263720 0.02774 0.58076 0.017803 #6 0.032999 0.0286388 0.02785 0.58125 0.017604 #7 0.060998 0.0528450 0.03783 0.57926 0.033422 #8 0.060999 0.0536420 0.03807 0.57851 0.033546 #9 0.032999 0.0299869 0.02606 0.55443 0.017258 #10 0.033999 0.0235679 0.02593 0.54985 0.017839 #11 0.058999 0.0534279 0.06688 0.56929 0.030724 #12 0.059998 0.0676858 0.07563 0.57466 0.036788 3\u3001lock-based add VS non-lock atomic add \u4f7f\u7528atomic add\u548c\u7528lock\u6765\u4fdd\u62a4\u4e24\u8005\u90fd\u80fd\u591f\u5b9e\u73b0\u6b63\u786e\u7684\u64cd\u4f5c\uff08\u987a\u5e8f\u3001\u539f\u5b50\u3001\u4e92\u65a5\uff09\uff0c\u4e3a\u4ec0\u4e48\u524d\u8005\u6bd4\u540e\u8005\u6027\u80fd\u66f4\u597d\uff1f \u56e0\u4e3a: lock-based\u5b58\u5728lock contention\uff0c\u56e0\u6b64\u5b58\u5728system call(system call\u7684\u6210\u672c\u662f\u6bd4\u8f83\u9ad8\u7684)\u3001\u5b58\u5728blocking\u3001\u7b49\u7b49\uff0c\u800catomic add\u5219\u6ca1\u6709\u524d\u9762\u63cf\u8ff0\u7684\u5185\u5bb9\u3002 4\u3001infoq Lock-free Programming in C++ with Herb Sutter \u5176\u4e2d\u8fdb\u884c\u4e86\u8f83\u597d\u7684\u603b\u7ed3\u3002 5\u3001stackoverflow Using Boost.Lockfree queue is slower than using mutexes # A 6\u3001preshing Locks Aren't Slow; Lock Contention Is Cooperative VS contention lock\u663e\u7136\u662fcontention lock-free\u662fcooperative \u4e0b\u9762\u662f\u5173\u4e8e\u8fd9\u4e2atopic\u7684\u7d20\u6750: 1\u3001preshing Acquire and Release Semantics Generally speaking, in lock-free programming , there are two ways in which threads can manipulate shared memory: They can compete with each other for a resource, or they can pass information co-operatively from one thread to another. Acquire and release semantics are crucial for the latter: reliable passing of information between threads. 2\u3001\u5de5\u7a0bprogramming-language\u7684 Memory-model-and-atomic-library\\Design \u7ae0\u8282 draft: lock-based VS non-lock \u4e3a\u4ec0\u4e48\u4f7f\u7528atomic add\u6bd4\u7528lock\u6765\u4fdd\u62a4\u4e24\u8005\u90fd\u80fd\u591f\u5b9e\u73b0\u6b63\u786e\u7684\u64cd\u4f5c\uff08\u987a\u5e8f\u3001\u539f\u5b50\u3001\u4e92\u65a5\uff09\uff1f lock-based\u5b58\u5728lock contention\uff0c\u5b58\u5728system call\uff1a\u4eceuser space\u5230kernel space","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking-VS-blocking/Lock-free-VS-lock-based/#lock-free#vs#lock-based","text":"","title":"Lock-free VS lock-based"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking-VS-blocking/Lock-free-VS-lock-based/#performance","text":"\u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\uff0c\u5bf9\u6b64\u8fdb\u884c\u4e86\u8ba8\u8bba\u3002","title":"Performance"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking-VS-blocking/Lock-free-VS-lock-based/#1stackoverflow#what#is#lock-free#multithreaded#programming#a","text":"It is worth noting that in general lock-free programming trades(\u4ea4\u6362) throughput and mean latency throughput for predictable latency . That is, a lock-free program will usually get less done than a corresponding locking program if there is not too much contention (since atomic operations are slow and affect a lot of the rest of the system), but it guarantees to never produce unpredictably large latencies. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u8868\u660e\u610f\u601d\u662f: \"\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u4e00\u822c\u6765\u8bf4\uff0c\u65e0\u9501\u7f16\u7a0b\u7528\u541e\u5410\u91cf\u548c\u5e73\u5747\u5ef6\u8fdf\u541e\u5410\u91cf\u4ea4\u6362\u53ef\u9884\u6d4b\u5ef6\u8fdf\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c\u5982\u679c\u6ca1\u6709\u592a\u591a\u7684\u4e89\u7528(\u56e0\u4e3a\u539f\u5b50\u64cd\u4f5c\u5f88\u6162\uff0c\u5e76\u4e14\u4f1a\u5f71\u54cd\u7cfb\u7edf\u7684\u5176\u4ed6\u90e8\u5206)\uff0c\u90a3\u4e48\u65e0\u9501\u7a0b\u5e8f\u901a\u5e38\u6bd4\u76f8\u5e94\u7684\u9501\u5b9a\u7a0b\u5e8f\u5b8c\u6210\u7684\u5de5\u4f5c\u8981\u5c11\uff0c\u4f46\u662f\u5b83\u4fdd\u8bc1\u4e0d\u4f1a\u4ea7\u751f\u4e0d\u53ef\u9884\u6d4b\u7684\u5927\u5ef6\u8fdf\u3002\" 1\u3001lock-free programming \u4ee5 throughout \u548c mean latency throughout \u6765\u6362\u53d6 predictable latency\u3002 2\u3001lock-free \u4e0d\u4e00\u5b9a\u6bd4 lock-base \u8981\u6162: \u5f53\u6ca1\u6709\u592a\u591a\u7684thread contention\u7684\u65f6\u5019\uff0clock-based\u7684\u6027\u80fd\u53ef\u80fd\u66f4\u4f73\u3002","title":"1\u3001stackoverflow What is lock-free multithreaded programming? # A"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking-VS-blocking/Lock-free-VS-lock-based/#2arangodb#comparison#lockless#programming#with#atomics#in#c#11#vs#mutex#and#rw-locks","text":"","title":"2\u3001arangodb Comparison: Lockless programming with atomics in C++ 11 vs. mutex and RW-locks"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking-VS-blocking/Lock-free-VS-lock-based/#methods#tested","text":"Here is a list of the implemented algorithms with explanations: Read : reads the current value and increases control counter Write : reads the old value, increases it and writes it back Set : writes a loop counter into the protected variable 0: Unlocked 1: Mutexes 2: Writelock / Writelock 3: Writelock / Readlock 4: Atomic Read & Write 5: Atomic Read \u201cconsume\u201d, Atomic Set \u201crelease\u201d for setting 6: Atomic Read \u201cacquire\u201d, Atomic Set \u201crelease\u201d for setting 7: Atomic Read \u201cconsume\u201d, Atomic Set \u201ccst \u2013 consistent\u201d for setting 8: Atomic Read \u201cacquire\u201d, Atomic Set \u201ccst \u2013 consistent\u201d for setting 9: Atomic Read \u201cconsume\u201d, Atomic Set \u201crelaxed\u201d for setting 10: Atomic Read \u201cacquire\u201d, Atomic Set \u201crelaxed\u201d for setting 11: Atomic Read \u201cAcquire\u201d, Atomic exchange weak for writing 12: Atomic Read \u201cconsume\u201d, Atomic exchange weak for writing","title":"Methods tested"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking-VS-blocking/Lock-free-VS-lock-based/#test#results","text":"Heres a table of the time in s the tests took to execute: TESTCASE / OS WINDOWS 8.1 GRML LINUX X64 MAC OS X LINUX ARM LINUX X64 #0 0.033 0.0300119 0.02503 0.18299 0.02895 #1 479.878 5.7003600 118.47100 21.29970 4.47721 #2 1.45997 4.5296900 142.61700 23.29240 4.72051 #3 4.70791 6.3525200 7.65026 23.82040 7.87677 #4 0.056999 0.0454769 0.03771 0.94990 0.035302 #5 0.033999 0.0263720 0.02774 0.58076 0.017803 #6 0.032999 0.0286388 0.02785 0.58125 0.017604 #7 0.060998 0.0528450 0.03783 0.57926 0.033422 #8 0.060999 0.0536420 0.03807 0.57851 0.033546 #9 0.032999 0.0299869 0.02606 0.55443 0.017258 #10 0.033999 0.0235679 0.02593 0.54985 0.017839 #11 0.058999 0.0534279 0.06688 0.56929 0.030724 #12 0.059998 0.0676858 0.07563 0.57466 0.036788","title":"Test Results"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking-VS-blocking/Lock-free-VS-lock-based/#3lock-based#add#vs#non-lock#atomic#add","text":"\u4f7f\u7528atomic add\u548c\u7528lock\u6765\u4fdd\u62a4\u4e24\u8005\u90fd\u80fd\u591f\u5b9e\u73b0\u6b63\u786e\u7684\u64cd\u4f5c\uff08\u987a\u5e8f\u3001\u539f\u5b50\u3001\u4e92\u65a5\uff09\uff0c\u4e3a\u4ec0\u4e48\u524d\u8005\u6bd4\u540e\u8005\u6027\u80fd\u66f4\u597d\uff1f \u56e0\u4e3a: lock-based\u5b58\u5728lock contention\uff0c\u56e0\u6b64\u5b58\u5728system call(system call\u7684\u6210\u672c\u662f\u6bd4\u8f83\u9ad8\u7684)\u3001\u5b58\u5728blocking\u3001\u7b49\u7b49\uff0c\u800catomic add\u5219\u6ca1\u6709\u524d\u9762\u63cf\u8ff0\u7684\u5185\u5bb9\u3002","title":"3\u3001lock-based add VS non-lock atomic add"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking-VS-blocking/Lock-free-VS-lock-based/#4infoq#lock-free#programming#in#c#with#herb#sutter","text":"\u5176\u4e2d\u8fdb\u884c\u4e86\u8f83\u597d\u7684\u603b\u7ed3\u3002","title":"4\u3001infoq Lock-free Programming in C++ with Herb Sutter"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking-VS-blocking/Lock-free-VS-lock-based/#5stackoverflow#using#boostlockfree#queue#is#slower#than#using#mutexes#a","text":"","title":"5\u3001stackoverflow Using Boost.Lockfree queue is slower than using mutexes # A"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking-VS-blocking/Lock-free-VS-lock-based/#6preshing#locks#arent#slow#lock#contention#is","text":"","title":"6\u3001preshing Locks Aren't Slow; Lock Contention Is"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking-VS-blocking/Lock-free-VS-lock-based/#cooperative#vs#contention","text":"lock\u663e\u7136\u662fcontention lock-free\u662fcooperative \u4e0b\u9762\u662f\u5173\u4e8e\u8fd9\u4e2atopic\u7684\u7d20\u6750: 1\u3001preshing Acquire and Release Semantics Generally speaking, in lock-free programming , there are two ways in which threads can manipulate shared memory: They can compete with each other for a resource, or they can pass information co-operatively from one thread to another. Acquire and release semantics are crucial for the latter: reliable passing of information between threads. 2\u3001\u5de5\u7a0bprogramming-language\u7684 Memory-model-and-atomic-library\\Design \u7ae0\u8282","title":"Cooperative VS contention"},{"location":"Concurrent-computing/Concurrency-control/Non-blocking-VS-blocking/Lock-free-VS-lock-based/#draft#lock-based#vs#non-lock","text":"\u4e3a\u4ec0\u4e48\u4f7f\u7528atomic add\u6bd4\u7528lock\u6765\u4fdd\u62a4\u4e24\u8005\u90fd\u80fd\u591f\u5b9e\u73b0\u6b63\u786e\u7684\u64cd\u4f5c\uff08\u987a\u5e8f\u3001\u539f\u5b50\u3001\u4e92\u65a5\uff09\uff1f lock-based\u5b58\u5728lock contention\uff0c\u5b58\u5728system call\uff1a\u4eceuser space\u5230kernel space","title":"draft: lock-based VS non-lock"},{"location":"Concurrent-computing/Concurrency-control/Optimistic-and-pessimistic/","text":"Optimistic and pessimistic \u4e00\u3001tradeoff 1\u3001\u9700\u8981\u6839\u636edata contention\u7684\u9891\u7387\u3001\u91cf\u6765\u9009\u62e9\u5230\u5e95\u662f\u4f7f\u7528optimistic\u8fd8\u662fpessimistic \u4e8c\u3001\u4e4b\u524d\u4ecb\u7ecd\u7684\u5404\u79cdconcurrency control technique\uff0c\u90fd\u662f\u53ef\u4ee5\u6839\u636eoptimistic and pessimistic\u8fdb\u884c\u5206\u7c7b\u7684 \u5165\u95e8\u6587\u7ae0 zhihu \u3010BAT\u9762\u8bd5\u9898\u7cfb\u5217\u3011\u9762\u8bd5\u5b98\uff1a\u4f60\u4e86\u89e3\u4e50\u89c2\u9501\u548c\u60b2\u89c2\u9501\u5417\uff1f \u53c2\u89c1 zhihu-\u4f60\u4e86\u89e3\u4e50\u89c2\u9501\u548c\u60b2\u89c2\u9501\u5417 \u7ae0\u8282\u3002 stackoverflow Optimistic vs. Pessimistic locking NOTE: \u5404\u4e2a\u56de\u7b54\u662f\u4eceDB\u7684\u89d2\u5ea6\u6765\u8c08\u7684 wikipedia Optimistic concurrency control Optimistic concurrency control ( OCC ) is a concurrency control method applied to transactional systems such as relational database management systems and software transactional memory . OCC assumes that multiple transactions can frequently complete without interfering(\u5e72\u6d89) with each other. While running, transactions use data resources without acquiring locks on those resources. Before committing, each transaction verifies that no other transaction has modified the data it has read. If the check reveals conflicting modifications, the committing transaction rolls back and can be restarted.[ 1] Optimistic concurrency control was first proposed by H. T. Kung and John T. Robinson.[ 2] NOTE: 1\u3001\u901a\u8fc7\u4e0a\u9762\u63cf\u8ff0\u7684\u7b56\u7565\u53ef\u4ee5\u770b\u51fa\uff0c\u663e\u7136\u76f8\u6bd4\u4e8elock\uff0coptimistic concurrency control\u5141\u8bb8\u66f4\u5927\u7684concurrency\u3002 2\u3001\u4ee5transaction\u7684\u65b9\u5f0f\u6765\u7406\u89e3optimistic concurrency control\uff0c\u8fd9\u80fd\u591f\u5e2e\u52a9\u6211\u4eec\u7684\u7406\u89e3 OCC is generally used in environments with low data contention (\u6570\u636e\u7ade\u4e89). When conflicts are rare, transactions can complete without the expense of managing locks and without having transactions wait for other transactions' locks to clear, leading to higher throughput than other concurrency control methods. However, if contention for data resources is frequent, the cost of repeatedly restarting transactions hurts performance significantly; it is commonly thought[ who? ] that other concurrency control methods have better performance under these conditions.[ citation needed ] However, locking-based (\"pessimistic\") methods also can deliver poor performance because locking can drastically limit effective concurrency even when deadlocks are avoided. NOTE: lock-based method\u662fpessimistic\u7684\u3002 Examples NOTE: \u4e0b\u6765\u5217\u4e3e\u4e86\u4e00\u4e9b\u6211\u77e5\u9053\u7684 1\u3001 Redis provides OCC through WATCH command.[ 16] 2\u3001 MySQL implements OCC in Group Replication configuration.[ citation needed ] wikipedia Transactional memory#Motivation In concurrent programming, synchronization is required when parallel threads attempt to access a shared resource. Low level thread synchronization constructs such as locks are pessimistic and prohibit threads that are outside a critical section from making any changes. The process of applying and releasing locks often functions as additional overhead in workloads with little conflict among threads. Transactional memory provides optimistic concurrency control by allowing threads to run in parallel with minimal interference.[ 2] The goal of transactional memory systems is to transparently support regions of code marked as transactions by enforcing atomicity , consistency and isolation . Optimistic concurrency control\u7684\u5b9e\u73b0\u65b9\u5f0f \u4e3b\u8981\u53c2\u8003: 1\u3001zhuanlan \u3010BAT\u9762\u8bd5\u9898\u7cfb\u5217\u3011\u9762\u8bd5\u5b98\uff1a\u4f60\u4e86\u89e3\u4e50\u89c2\u9501\u548c\u60b2\u89c2\u9501\u5417\uff1f \u4e0b\u9762\u662f\u6211\u7684\u603b\u7ed3: \u4e24\u79cd\u5b9e\u73b0\u65b9\u5f0f\u90fd\u53ef\u4ee5\u4ecetransaction\u7684\u89d2\u5ea6\u6765\u8fdb\u884c\u601d\u8003: CAS \u901a\u8fc7 value comparison \u6765\u5224\u65ad \"\u72b6\u6001 \u662f\u5426\u53d1\u751f\u4e86\u6539\u53d8\"\u3001\u662f\u5426\u6709\u5176\u4ed6transaction\u53d1\u751f\uff0c\u5982\u679c\u6ca1\u6709\u6539\u53d8\u3001\u6ca1\u6709\u5176\u4ed6transaction\uff0c\u5219commit\uff1b\u5426\u5219rollback\u3002 MVCC \u901a\u8fc7 version ID \u6765\u5224\u65ad \"\u72b6\u6001 \u662f\u5426\u53d1\u751f\u4e86\u6539\u53d8\"\u3001\u662f\u5426\u6709\u5176\u4ed6transaction\uff0c\u5982\u679c\u6ca1\u6709\u6539\u53d8\u3001\u6ca1\u6709\u5176\u4ed6transaction\uff0c\u5219commit\uff1b\u5426\u5219rollback\u3002 Optimistic and pessimistic synchronization in distributed computing https://link.springer.com/chapter/10.1007/BFb0024159 See also redis Transactions wikipedia Optimistic replication TODO https://en.wikipedia.org/wiki/Server_Message_Block#Opportunistic_locking https://docs.jboss.org/jbossas/docs/Server_Configuration_Guide/4/html/TransactionJTA_Overview-Pessimistic_and_optimistic_locking.html dennyzhang Explain: Pessimistic And Optimistic Locking","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Optimistic-and-pessimistic/#optimistic#and#pessimistic","text":"\u4e00\u3001tradeoff 1\u3001\u9700\u8981\u6839\u636edata contention\u7684\u9891\u7387\u3001\u91cf\u6765\u9009\u62e9\u5230\u5e95\u662f\u4f7f\u7528optimistic\u8fd8\u662fpessimistic \u4e8c\u3001\u4e4b\u524d\u4ecb\u7ecd\u7684\u5404\u79cdconcurrency control technique\uff0c\u90fd\u662f\u53ef\u4ee5\u6839\u636eoptimistic and pessimistic\u8fdb\u884c\u5206\u7c7b\u7684","title":"Optimistic and pessimistic"},{"location":"Concurrent-computing/Concurrency-control/Optimistic-and-pessimistic/#_1","text":"","title":"\u5165\u95e8\u6587\u7ae0"},{"location":"Concurrent-computing/Concurrency-control/Optimistic-and-pessimistic/#zhihu#bat","text":"\u53c2\u89c1 zhihu-\u4f60\u4e86\u89e3\u4e50\u89c2\u9501\u548c\u60b2\u89c2\u9501\u5417 \u7ae0\u8282\u3002","title":"zhihu \u3010BAT\u9762\u8bd5\u9898\u7cfb\u5217\u3011\u9762\u8bd5\u5b98\uff1a\u4f60\u4e86\u89e3\u4e50\u89c2\u9501\u548c\u60b2\u89c2\u9501\u5417\uff1f"},{"location":"Concurrent-computing/Concurrency-control/Optimistic-and-pessimistic/#stackoverflow#optimistic#vs#pessimistic#locking","text":"NOTE: \u5404\u4e2a\u56de\u7b54\u662f\u4eceDB\u7684\u89d2\u5ea6\u6765\u8c08\u7684","title":"stackoverflow Optimistic vs. Pessimistic locking"},{"location":"Concurrent-computing/Concurrency-control/Optimistic-and-pessimistic/#wikipedia#optimistic#concurrency#control","text":"Optimistic concurrency control ( OCC ) is a concurrency control method applied to transactional systems such as relational database management systems and software transactional memory . OCC assumes that multiple transactions can frequently complete without interfering(\u5e72\u6d89) with each other. While running, transactions use data resources without acquiring locks on those resources. Before committing, each transaction verifies that no other transaction has modified the data it has read. If the check reveals conflicting modifications, the committing transaction rolls back and can be restarted.[ 1] Optimistic concurrency control was first proposed by H. T. Kung and John T. Robinson.[ 2] NOTE: 1\u3001\u901a\u8fc7\u4e0a\u9762\u63cf\u8ff0\u7684\u7b56\u7565\u53ef\u4ee5\u770b\u51fa\uff0c\u663e\u7136\u76f8\u6bd4\u4e8elock\uff0coptimistic concurrency control\u5141\u8bb8\u66f4\u5927\u7684concurrency\u3002 2\u3001\u4ee5transaction\u7684\u65b9\u5f0f\u6765\u7406\u89e3optimistic concurrency control\uff0c\u8fd9\u80fd\u591f\u5e2e\u52a9\u6211\u4eec\u7684\u7406\u89e3 OCC is generally used in environments with low data contention (\u6570\u636e\u7ade\u4e89). When conflicts are rare, transactions can complete without the expense of managing locks and without having transactions wait for other transactions' locks to clear, leading to higher throughput than other concurrency control methods. However, if contention for data resources is frequent, the cost of repeatedly restarting transactions hurts performance significantly; it is commonly thought[ who? ] that other concurrency control methods have better performance under these conditions.[ citation needed ] However, locking-based (\"pessimistic\") methods also can deliver poor performance because locking can drastically limit effective concurrency even when deadlocks are avoided. NOTE: lock-based method\u662fpessimistic\u7684\u3002","title":"wikipedia Optimistic concurrency control"},{"location":"Concurrent-computing/Concurrency-control/Optimistic-and-pessimistic/#examples","text":"NOTE: \u4e0b\u6765\u5217\u4e3e\u4e86\u4e00\u4e9b\u6211\u77e5\u9053\u7684 1\u3001 Redis provides OCC through WATCH command.[ 16] 2\u3001 MySQL implements OCC in Group Replication configuration.[ citation needed ]","title":"Examples"},{"location":"Concurrent-computing/Concurrency-control/Optimistic-and-pessimistic/#wikipedia#transactional#memorymotivation","text":"In concurrent programming, synchronization is required when parallel threads attempt to access a shared resource. Low level thread synchronization constructs such as locks are pessimistic and prohibit threads that are outside a critical section from making any changes. The process of applying and releasing locks often functions as additional overhead in workloads with little conflict among threads. Transactional memory provides optimistic concurrency control by allowing threads to run in parallel with minimal interference.[ 2] The goal of transactional memory systems is to transparently support regions of code marked as transactions by enforcing atomicity , consistency and isolation .","title":"wikipedia Transactional memory#Motivation"},{"location":"Concurrent-computing/Concurrency-control/Optimistic-and-pessimistic/#optimistic#concurrency#control","text":"\u4e3b\u8981\u53c2\u8003: 1\u3001zhuanlan \u3010BAT\u9762\u8bd5\u9898\u7cfb\u5217\u3011\u9762\u8bd5\u5b98\uff1a\u4f60\u4e86\u89e3\u4e50\u89c2\u9501\u548c\u60b2\u89c2\u9501\u5417\uff1f \u4e0b\u9762\u662f\u6211\u7684\u603b\u7ed3: \u4e24\u79cd\u5b9e\u73b0\u65b9\u5f0f\u90fd\u53ef\u4ee5\u4ecetransaction\u7684\u89d2\u5ea6\u6765\u8fdb\u884c\u601d\u8003: CAS \u901a\u8fc7 value comparison \u6765\u5224\u65ad \"\u72b6\u6001 \u662f\u5426\u53d1\u751f\u4e86\u6539\u53d8\"\u3001\u662f\u5426\u6709\u5176\u4ed6transaction\u53d1\u751f\uff0c\u5982\u679c\u6ca1\u6709\u6539\u53d8\u3001\u6ca1\u6709\u5176\u4ed6transaction\uff0c\u5219commit\uff1b\u5426\u5219rollback\u3002 MVCC \u901a\u8fc7 version ID \u6765\u5224\u65ad \"\u72b6\u6001 \u662f\u5426\u53d1\u751f\u4e86\u6539\u53d8\"\u3001\u662f\u5426\u6709\u5176\u4ed6transaction\uff0c\u5982\u679c\u6ca1\u6709\u6539\u53d8\u3001\u6ca1\u6709\u5176\u4ed6transaction\uff0c\u5219commit\uff1b\u5426\u5219rollback\u3002","title":"Optimistic concurrency control\u7684\u5b9e\u73b0\u65b9\u5f0f"},{"location":"Concurrent-computing/Concurrency-control/Optimistic-and-pessimistic/#optimistic#and#pessimistic#synchronization#in#distributed#computing","text":"https://link.springer.com/chapter/10.1007/BFb0024159","title":"Optimistic and pessimistic synchronization in distributed computing"},{"location":"Concurrent-computing/Concurrency-control/Optimistic-and-pessimistic/#see#also","text":"","title":"See also"},{"location":"Concurrent-computing/Concurrency-control/Optimistic-and-pessimistic/#redis#transactions","text":"","title":"redis Transactions"},{"location":"Concurrent-computing/Concurrency-control/Optimistic-and-pessimistic/#wikipedia#optimistic#replication","text":"","title":"wikipedia Optimistic replication"},{"location":"Concurrent-computing/Concurrency-control/Optimistic-and-pessimistic/#todo","text":"https://en.wikipedia.org/wiki/Server_Message_Block#Opportunistic_locking https://docs.jboss.org/jbossas/docs/Server_Configuration_Guide/4/html/TransactionJTA_Overview-Pessimistic_and_optimistic_locking.html","title":"TODO"},{"location":"Concurrent-computing/Concurrency-control/Optimistic-and-pessimistic/#dennyzhang#explain#pessimistic#and#optimistic#locking","text":"","title":"dennyzhang Explain: Pessimistic And Optimistic Locking"},{"location":"Concurrent-computing/Concurrency-control/Optimistic-and-pessimistic/zhihu-%E4%BD%A0%E4%BA%86%E8%A7%A3%E4%B9%90%E8%A7%82%E9%94%81%E5%92%8C%E6%82%B2%E8%A7%82%E9%94%81%E5%90%97/","text":"zhihu \u3010BAT\u9762\u8bd5\u9898\u7cfb\u5217\u3011\u9762\u8bd5\u5b98\uff1a\u4f60\u4e86\u89e3\u4e50\u89c2\u9501\u548c\u60b2\u89c2\u9501\u5417\uff1f NOTE: 1\u3001\u8fd9\u7bc7\u6587\u7ae0\u8bb2\u5f97\u4e0d\u9519\uff0c\u5b83\u4e0d\u4ec5\u8bf4\u6e05\u4e86\u4e50\u89c2\u9501\u3001\u60b2\u89c2\u9501\u7684\u6982\u5ff5\uff0c\u800c\u4e14\u8fd8\u8bf4\u660e\u4e86\u5982\u4f55\u6765\u5b9e\u73b0\u3002 2\u3001\u4f7f\u7528\u5728 Optimistic-and-pessimistic \u7ae0\u8282\u4e2d\u603b\u7ed3\u7684: \u4f7f\u7528 transaction \u6765\u770b\u5230optimistic lock \u4e00\u3001\u57fa\u672c\u6982\u5ff5 **\u4e50\u89c2\u9501**\u548c**\u60b2\u89c2\u9501**\u662f\u4e24\u79cd\u601d\u60f3\uff0c\u7528\u4e8e\u89e3\u51b3\u5e76\u53d1\u573a\u666f\u4e0b\u7684\u6570\u636e\u7ade\u4e89\u95ee\u9898\u3002 1\u3001\u4e50\u89c2\u9501\uff1a \u4e50\u89c2\u9501**\u5728\u64cd\u4f5c\u6570\u636e\u65f6\u975e\u5e38**\u4e50\u89c2 \uff0c\u8ba4\u4e3a\u522b\u4eba\u4e0d\u4f1a\u540c\u65f6\u4fee\u6539\u6570\u636e\u3002\u56e0\u6b64**\u4e50\u89c2\u9501**\u4e0d\u4f1a\u4e0a\u9501\uff0c\u53ea\u662f\u5728\u6267\u884c**\u66f4\u65b0**\u7684\u65f6\u5019\u5224\u65ad\u4e00\u4e0b\u5728\u6b64\u671f\u95f4\u522b\u4eba\u662f\u5426\u4fee\u6539\u4e86\u6570\u636e\uff1a\u5982\u679c\u522b\u4eba\u4fee\u6539\u4e86\u6570\u636e\u5219\u653e\u5f03\u64cd\u4f5c\uff0c\u5426\u5219\u6267\u884c\u64cd\u4f5c\u3002 NOTE: 1\u3001\u5178\u578b\u7684transaction\u65b9\u5f0f 2\u3001\u60b2\u89c2\u9501\uff1a \u60b2\u89c2\u9501**\u5728\u64cd\u4f5c\u6570\u636e\u65f6\u6bd4\u8f83**\u60b2\u89c2 \uff0c\u8ba4\u4e3a\u522b\u4eba\u4f1a\u540c\u65f6\u4fee\u6539\u6570\u636e\u3002\u56e0\u6b64\u64cd\u4f5c\u6570\u636e\u65f6\u76f4\u63a5\u628a\u6570\u636e\u9501\u4f4f\uff0c\u76f4\u5230\u64cd\u4f5c\u5b8c\u6210\u540e\u624d\u4f1a\u91ca\u653e\u9501\uff1b\u4e0a\u9501\u671f\u95f4\u5176\u4ed6\u4eba\u4e0d\u80fd\u4fee\u6539\u6570\u636e\u3002 \u4e8c\u3001\u5b9e\u73b0\u65b9\u5f0f(\u542b\u5b9e\u4f8b) \u5728\u8bf4\u660e\u5b9e\u73b0\u65b9\u5f0f\u4e4b\u524d\uff0c\u9700\u8981\u660e\u786e\uff1a \u4e50\u89c2\u9501\u548c\u60b2\u89c2\u9501\u662f\u4e24\u79cd\u601d\u60f3\uff0c\u5b83\u4eec\u7684\u4f7f\u7528\u662f\u975e\u5e38\u5e7f\u6cdb\u7684\uff0c\u4e0d\u5c40\u9650\u4e8e\u67d0\u79cd\u7f16\u7a0b\u8bed\u8a00\u6216\u6570\u636e\u5e93\u3002 **\u60b2\u89c2\u9501**\u7684\u5b9e\u73b0\u65b9\u5f0f\u662f\u52a0\u9501\uff0c\u52a0\u9501\u65e2\u53ef\u4ee5\u662f\u5bf9\u4ee3\u7801\u5757\u52a0\u9501\uff08\u5982Java\u7684 synchronized \u5173\u952e\u5b57\uff09\uff0c\u4e5f\u53ef\u4ee5\u662f\u5bf9\u6570\u636e\u52a0\u9501\uff08\u5982MySQL\u4e2d\u7684\u6392\u5b83\u9501\uff09\u3002 \u4e50\u89c2\u9501\u7684\u5b9e\u73b0\u65b9\u5f0f\u4e3b\u8981\u6709\u4e24\u79cd\uff1aCAS\u673a\u5236\u548c\u7248\u672c\u53f7\u673a\u5236\uff0c\u4e0b\u9762\u8be6\u7ec6\u4ecb\u7ecd\u3002 NOTE: 1\u3001\u8fd9\u4e24\u79cd\u5b9e\u73b0\u662f\u5728\u5176\u4ed6\u7684\u7ae0\u8282\u4e2d\u90fd\u6709\u8bf4\u660e 1\u3001CAS\uff08Compare And Swap\uff09 \u4e0b\u9762\u4ee5Java\u4e2d\u7684\u81ea\u589e\u64cd\u4f5c(i++)\u4e3a\u4f8b\uff0c\u770b\u4e00\u4e0b\u60b2\u89c2\u9501\u548cCAS\u5206\u522b\u662f\u5982\u4f55\u4fdd\u8bc1\u7ebf\u7a0b\u5b89\u5168\u7684\u3002\u6211\u4eec\u77e5\u9053\uff0c\u5728Java\u4e2d\u81ea\u589e\u64cd\u4f5c\u4e0d\u662f\u539f\u5b50\u64cd\u4f5c\uff0c\u5b83\u5b9e\u9645\u4e0a\u5305\u542b\u4e09\u4e2a\u72ec\u7acb\u7684\u64cd\u4f5c\uff1a \uff081\uff09\u8bfb\u53d6i\u503c\uff1b \uff082\uff09\u52a01\uff1b \uff083\uff09\u5c06\u65b0\u503c\u5199\u56dei \u56e0\u6b64\uff0c\u5982\u679c\u5e76\u53d1\u6267\u884c\u81ea\u589e\u64cd\u4f5c\uff0c\u53ef\u80fd\u5bfc\u81f4\u8ba1\u7b97\u7ed3\u679c\u7684\u4e0d\u51c6\u786e\u3002\u5728\u4e0b\u9762\u7684\u4ee3\u7801\u793a\u4f8b\u4e2d\uff1avalue1\u6ca1\u6709\u8fdb\u884c\u4efb\u4f55\u7ebf\u7a0b\u5b89\u5168\u65b9\u9762\u7684\u4fdd\u62a4\uff0cvalue2\u4f7f\u7528\u4e86\u4e50\u89c2\u9501(CAS)\uff0cvalue3\u4f7f\u7528\u4e86\u60b2\u89c2\u9501(synchronized)\u3002\u8fd0\u884c\u7a0b\u5e8f\uff0c\u4f7f\u75281000\u4e2a\u7ebf\u7a0b\u540c\u65f6\u5bf9value1\u3001value2\u548cvalue3\u8fdb\u884c\u81ea\u589e\u64cd\u4f5c\uff0c\u53ef\u4ee5\u53d1\u73b0\uff1avalue2\u548cvalue3\u7684\u503c\u603b\u662f\u7b49\u4e8e1000\uff0c\u800cvalue1\u7684\u503c\u5e38\u5e38\u5c0f\u4e8e1000\u3002 public class Test { //value1\uff1a\u7ebf\u7a0b\u4e0d\u5b89\u5168 private static int value1 = 0 ; //value2\uff1a\u4f7f\u7528\u4e50\u89c2\u9501 private static AtomicInteger value2 = new AtomicInteger ( 0 ); //value3\uff1a\u4f7f\u7528\u60b2\u89c2\u9501 private static int value3 = 0 ; private static synchronized void increaseValue3 (){ value3 ++ ; } public static void main ( String [] args ) throws Exception { //\u5f00\u542f1000\u4e2a\u7ebf\u7a0b\uff0c\u5e76\u6267\u884c\u81ea\u589e\u64cd\u4f5c for ( int i = 0 ; i < 1000 ; ++ i ){ new Thread ( new Runnable () { @Override public void run () { try { Thread . sleep ( 100 ); } catch ( InterruptedException e ) { e . printStackTrace (); } value1 ++ ; value2 . getAndIncrement (); increaseValue3 (); } }). start (); } //\u6253\u5370\u7ed3\u679c Thread . sleep ( 1000 ); System . out . println ( \"\u7ebf\u7a0b\u4e0d\u5b89\u5168\uff1a\" + value1 ); System . out . println ( \"\u4e50\u89c2\u9501(AtomicInteger)\uff1a\" + value2 ); System . out . println ( \"\u60b2\u89c2\u9501(synchronized)\uff1a\" + value3 ); } } NOTE: 1\u3001\u539f\u6587\u540e\u9762\u662f\u5bf9 getAndIncrement() \u5b9e\u73b0\u7684\u5206\u6790\uff0c\u5728 Compare-and-swap \u7ae0\u8282\u4e2d\u7684 \"Example application: atomic adder\" \u4e2d\u4f8b\u5b50\u7c7b\u4f3c\uff0c\u539f\u7406\u76f8\u540c\u3002 2\u3001\u7248\u672c\u53f7\u673a\u5236 NOTE: 1\u3001MVCC\uff0c\u901a\u8fc7\u7248\u672c\u53f7\u7684\u53d8\u66f4\u6765\u5224\u65ad\u662f\u5426\u53d1\u751f\u4e86\u66f4\u6539\u3002 \u9664\u4e86CAS\uff0c\u7248\u672c\u53f7\u673a\u5236\u4e5f\u53ef\u4ee5\u7528\u6765\u5b9e\u73b0\u4e50\u89c2\u9501\u3002\u7248\u672c\u53f7\u673a\u5236\u7684\u57fa\u672c\u601d\u8def\u662f\u5728\u6570\u636e\u4e2d\u589e\u52a0\u4e00\u4e2a\u5b57\u6bb5version\uff0c\u8868\u793a\u8be5\u6570\u636e\u7684\u7248\u672c\u53f7\uff0c\u6bcf\u5f53\u6570\u636e\u88ab\u4fee\u6539\uff0c\u7248\u672c\u53f7\u52a01\u3002\u5f53\u67d0\u4e2a\u7ebf\u7a0b\u67e5\u8be2\u6570\u636e\u65f6\uff0c\u5c06\u8be5\u6570\u636e\u7684\u7248\u672c\u53f7\u4e00\u8d77\u67e5\u51fa\u6765\uff1b\u5f53\u8be5\u7ebf\u7a0b\u66f4\u65b0\u6570\u636e\u65f6\uff0c\u5224\u65ad\u5f53\u524d\u7248\u672c\u53f7\u4e0e\u4e4b\u524d\u8bfb\u53d6\u7684\u7248\u672c\u53f7\u662f\u5426\u4e00\u81f4\uff0c\u5982\u679c\u4e00\u81f4\u624d\u8fdb\u884c\u64cd\u4f5c\u3002 \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86\u7248\u672c\u53f7\u4f5c\u4e3a\u5224\u65ad\u6570\u636e\u53d8\u5316\u7684\u6807\u8bb0\uff0c\u5b9e\u9645\u4e0a\u53ef\u4ee5\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u9009\u7528\u5176\u4ed6\u80fd\u591f\u6807\u8bb0\u6570\u636e\u7248\u672c\u7684\u5b57\u6bb5\uff0c\u5982\u65f6\u95f4\u6233\u7b49\u3002 \u4e0b\u9762\u4ee5\u201c\u66f4\u65b0\u73a9\u5bb6\u91d1\u5e01\u6570\u201d\u4e3a\u4f8b\uff08\u6570\u636e\u5e93\u4e3aMySQL\uff0c\u5176\u4ed6\u6570\u636e\u5e93\u540c\u7406\uff09\uff0c\u770b\u770b\u60b2\u89c2\u9501\u548c\u7248\u672c\u53f7\u673a\u5236\u662f\u5982\u4f55\u5e94\u5bf9\u5e76\u53d1\u95ee\u9898\u7684\u3002 \u975e\u7ebf\u7a0b\u5b89\u5168 \u4e0b\u9762\u7684\u5b9e\u73b0\u65b9\u5f0f\uff0c\u6ca1\u6709\u8fdb\u884c\u4efb\u4f55\u7ebf\u7a0b\u5b89\u5168\u65b9\u9762\u7684\u4fdd\u62a4\u3002\u5982\u679c\u6709\u5176\u4ed6\u7ebf\u7a0b\u5728query\u548cupdate\u4e4b\u95f4\u66f4\u65b0\u4e86\u73a9\u5bb6\u7684\u4fe1\u606f\uff0c\u4f1a\u5bfc\u81f4\u73a9\u5bb6\u91d1\u5e01\u6570\u7684\u4e0d\u51c6\u786e\u3002 @Transactional public void updateCoins ( Integer playerId ){ //\u6839\u636eplayer_id\u67e5\u8be2\u73a9\u5bb6\u4fe1\u606f Player player = query ( \"select coins, level from player where player_id = {0}\" , playerId ); //\u6839\u636e\u73a9\u5bb6\u5f53\u524d\u4fe1\u606f\u53ca\u5176\u4ed6\u4fe1\u606f\uff0c\u8ba1\u7b97\u65b0\u7684\u91d1\u5e01\u6570 Long newCoins = \u2026\u2026 ; //\u66f4\u65b0\u91d1\u5e01\u6570 update ( \"update player set coins = {0} where player_id = {1}\" , newCoins , playerId ); } select \u2026\u2026 for update \u6392\u5b83\u9501 \u4e3a\u4e86\u907f\u514d\u8fd9\u4e2a\u95ee\u9898\uff0c\u60b2\u89c2\u9501\u901a\u8fc7\u52a0\u9501\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4ee3\u7801\u5982\u4e0b\u6240\u793a\u3002\u5728\u67e5\u8be2\u73a9\u5bb6\u4fe1\u606f\u65f6\uff0c\u4f7f\u7528select \u2026\u2026 for update\u8fdb\u884c\u67e5\u8be2\uff1b\u8be5\u67e5\u8be2\u8bed\u53e5\u4f1a\u4e3a\u8be5\u73a9\u5bb6\u6570\u636e\u52a0\u4e0a\u6392\u5b83\u9501\uff0c\u76f4\u5230\u4e8b\u52a1\u63d0\u4ea4\u6216\u56de\u6eda\u65f6\u624d\u4f1a\u91ca\u653e\u6392\u5b83\u9501\uff1b\u5728\u6b64\u671f\u95f4\uff0c\u5982\u679c\u5176\u4ed6\u7ebf\u7a0b\u8bd5\u56fe\u66f4\u65b0\u8be5\u73a9\u5bb6\u4fe1\u606f\u6216\u8005\u6267\u884cselect for update\uff0c\u4f1a\u88ab\u963b\u585e\u3002 @Transactional public void updateCoins ( Integer playerId ){ //\u6839\u636eplayer_id\u67e5\u8be2\u73a9\u5bb6\u4fe1\u606f\uff08\u52a0\u6392\u5b83\u9501\uff09 Player player = queryForUpdate ( \"select coins, level from player where player_id = {0} for update\" , playerId ); //\u6839\u636e\u73a9\u5bb6\u5f53\u524d\u4fe1\u606f\u53ca\u5176\u4ed6\u4fe1\u606f\uff0c\u8ba1\u7b97\u65b0\u7684\u91d1\u5e01\u6570 Long newCoins = \u2026\u2026 ; //\u66f4\u65b0\u91d1\u5e01\u6570 update ( \"update player set coins = {0} where player_id = {1}\" , newCoins , playerId ); } \u7248\u672c\u53f7\u673a\u5236 \u7248\u672c\u53f7\u673a\u5236\u5219\u662f\u53e6\u4e00\u79cd\u601d\u8def\uff0c\u5b83\u4e3a\u73a9\u5bb6\u4fe1\u606f\u589e\u52a0\u4e00\u4e2a\u5b57\u6bb5\uff1aversion\u3002\u5728\u521d\u6b21\u67e5\u8be2\u73a9\u5bb6\u4fe1\u606f\u65f6\uff0c\u540c\u65f6\u67e5\u8be2\u51faversion\u4fe1\u606f\uff1b\u5728\u6267\u884cupdate\u64cd\u4f5c\u65f6\uff0c\u6821\u9a8cversion\u662f\u5426\u53d1\u751f\u4e86\u53d8\u5316\uff0c\u5982\u679cversion\u53d8\u5316\uff0c\u5219\u4e0d\u8fdb\u884c\u66f4\u65b0\u3002 @Transactional public void updateCoins ( Integer playerId ){ //\u6839\u636eplayer_id\u67e5\u8be2\u73a9\u5bb6\u4fe1\u606f\uff0c\u5305\u542bversion\u4fe1\u606f Player player = query ( \"select coins, level, version from player where player_id = {0}\" , playerId ); //\u6839\u636e\u73a9\u5bb6\u5f53\u524d\u4fe1\u606f\u53ca\u5176\u4ed6\u4fe1\u606f\uff0c\u8ba1\u7b97\u65b0\u7684\u91d1\u5e01\u6570 Long newCoins = \u2026\u2026 ; //\u66f4\u65b0\u91d1\u5e01\u6570\uff0c\u6761\u4ef6\u4e2d\u589e\u52a0\u5bf9version\u7684\u6821\u9a8c update ( \"update player set coins = {0}, version = version + 1 where player_id = {1} and version = {2}\" , newCoins , playerId , player . version ); } \u4e09\u3001\u4f18\u7f3a\u70b9\u548c\u9002\u7528\u573a\u666f \u4e50\u89c2\u9501\u548c\u60b2\u89c2\u9501\u5e76\u6ca1\u6709\u4f18\u52a3\u4e4b\u5206\uff0c\u5b83\u4eec\u6709\u5404\u81ea\u9002\u5408\u7684\u573a\u666f\uff1b\u4e0b\u9762\u4ece\u4e24\u4e2a\u65b9\u9762\u8fdb\u884c\u8bf4\u660e\u3002 1\u3001\u529f\u80fd\u9650\u5236 \u4e0e\u60b2\u89c2\u9501\u76f8\u6bd4\uff0c\u4e50\u89c2\u9501\u9002\u7528\u7684\u573a\u666f\u53d7\u5230\u4e86\u66f4\u591a\u7684\u9650\u5236\uff0c\u65e0\u8bba\u662fCAS\u8fd8\u662f\u7248\u672c\u53f7\u673a\u5236\u3002 \u4f8b\u5982\uff0cCAS\u53ea\u80fd\u4fdd\u8bc1\u5355\u4e2a\u53d8\u91cf\u64cd\u4f5c\u7684\u539f\u5b50\u6027\uff0c\u5f53\u6d89\u53ca\u5230\u591a\u4e2a\u53d8\u91cf\u65f6\uff0cCAS\u662f\u65e0\u80fd\u4e3a\u529b\u7684\uff0c\u800c synchronized \u5219\u53ef\u4ee5\u901a\u8fc7\u5bf9\u6574\u4e2a\u4ee3\u7801\u5757\u52a0\u9501\u6765\u5904\u7406\u3002\u518d\u6bd4\u5982\u7248\u672c\u53f7\u673a\u5236\uff0c\u5982\u679cquery\u7684\u65f6\u5019\u662f\u9488\u5bf9\u88681\uff0c\u800cupdate\u7684\u65f6\u5019\u662f\u9488\u5bf9\u88682\uff0c\u4e5f\u5f88\u96be\u901a\u8fc7\u7b80\u5355\u7684\u7248\u672c\u53f7\u6765\u5b9e\u73b0\u4e50\u89c2\u9501\u3002 2\u3001\u7ade\u4e89\u6fc0\u70c8\u7a0b\u5ea6 \u5982\u679c\u60b2\u89c2\u9501\u548c\u4e50\u89c2\u9501\u90fd\u53ef\u4ee5\u4f7f\u7528\uff0c\u90a3\u4e48\u9009\u62e9\u5c31\u8981\u8003\u8651\u7ade\u4e89\u7684\u6fc0\u70c8\u7a0b\u5ea6\uff1a a\u3001\u5f53\u7ade\u4e89\u4e0d\u6fc0\u70c8 (\u51fa\u73b0\u5e76\u53d1\u51b2\u7a81\u7684\u6982\u7387\u5c0f)\u65f6\uff0c\u4e50\u89c2\u9501\u66f4\u6709\u4f18\u52bf\uff0c\u56e0\u4e3a\u60b2\u89c2\u9501\u4f1a\u9501\u4f4f\u4ee3\u7801\u5757\u6216\u6570\u636e\uff0c\u5176\u4ed6\u7ebf\u7a0b\u65e0\u6cd5\u540c\u65f6\u8bbf\u95ee\uff0c\u5f71\u54cd\u5e76\u53d1\uff0c\u800c\u4e14\u52a0\u9501\u548c\u91ca\u653e\u9501\u90fd\u9700\u8981\u6d88\u8017\u989d\u5916\u7684\u8d44\u6e90\u3002 b\u3001\u5f53\u7ade\u4e89\u6fc0\u70c8(\u51fa\u73b0\u5e76\u53d1\u51b2\u7a81\u7684\u6982\u7387\u5927)\u65f6\uff0c\u60b2\u89c2\u9501\u66f4\u6709\u4f18\u52bf\uff0c\u56e0\u4e3a\u4e50\u89c2\u9501\u5728\u6267\u884c\u66f4\u65b0\u65f6\u9891\u7e41\u5931\u8d25\uff0c\u9700\u8981\u4e0d\u65ad\u91cd\u8bd5\uff0c\u6d6a\u8d39CPU\u8d44\u6e90\u3002 \u56db\u3001\u9762\u8bd5\u5b98\u8ffd\u95ee\uff1a\u4e50\u89c2\u9501\u52a0\u9501\u5417\uff1f \u4e0b\u9762\u662f\u6211\u5bf9\u8fd9\u4e2a\u95ee\u9898\u7684\u7406\u89e3\uff1a \uff081\uff09\u4e50\u89c2\u9501\u672c\u8eab\u662f\u4e0d\u52a0\u9501\u7684\uff0c\u53ea\u662f\u5728\u66f4\u65b0\u65f6\u5224\u65ad\u4e00\u4e0b\u6570\u636e\u662f\u5426\u88ab\u5176\u4ed6\u7ebf\u7a0b\u66f4\u65b0\u4e86\uff1bAtomicInteger\u4fbf\u662f\u4e00\u4e2a\u4f8b\u5b50\u3002 \uff082\uff09\u6709\u65f6\u4e50\u89c2\u9501\u53ef\u80fd\u4e0e\u52a0\u9501\u64cd\u4f5c\u5408\u4f5c\uff0c\u4f8b\u5982\uff0c\u5728\u524d\u8ff0updateCoins()\u7684\u4f8b\u5b50\u4e2d\uff0cMySQL\u5728\u6267\u884cupdate\u65f6\u4f1a\u52a0\u6392\u5b83\u9501\u3002\u4f46\u8fd9\u53ea\u662f\u4e50\u89c2\u9501\u4e0e\u52a0\u9501\u64cd\u4f5c\u5408\u4f5c\u7684\u4f8b\u5b50\uff0c\u4e0d\u80fd\u6539\u53d8\u201c\u4e50\u89c2\u9501\u672c\u8eab\u4e0d\u52a0\u9501\u201d\u8fd9\u4e00\u4e8b\u5b9e\u3002 \u4e94\u3001\u9762\u8bd5\u5b98\u8ffd\u95ee\uff1aCAS\u6709\u54ea\u4e9b\u7f3a\u70b9\uff1f \u4e0b\u9762\u662fCAS\u4e00\u4e9b\u4e0d\u90a3\u4e48\u5b8c\u7f8e\u7684\u5730\u65b9\uff1a 1\u3001ABA\u95ee\u9898 \u5728 AtomicInteger \u7684\u4f8b\u5b50\u4e2d\uff0cABA\u4f3c\u4e4e\u6ca1\u6709\u4ec0\u4e48\u5371\u5bb3\u3002\u4f46\u662f\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\uff0cABA\u5374\u4f1a\u5e26\u6765\u9690\u60a3\uff0c\u4f8b\u5982\u6808\u9876\u95ee\u9898\uff1a\u4e00\u4e2a\u6808\u7684\u6808\u9876\u7ecf\u8fc7\u4e24\u6b21(\u6216\u591a\u6b21)\u53d8\u5316\u53c8\u6062\u590d\u4e86\u539f\u503c\uff0c\u4f46\u662f\u6808\u53ef\u80fd\u5df2\u53d1\u751f\u4e86\u53d8\u5316\u3002 \u5bf9\u4e8eABA\u95ee\u9898\uff0c\u6bd4\u8f83\u6709\u6548\u7684\u65b9\u6848\u662f\u5f15\u5165\u7248\u672c\u53f7\uff0c\u5185\u5b58\u4e2d\u7684\u503c\u6bcf\u53d1\u751f\u4e00\u6b21\u53d8\u5316\uff0c\u7248\u672c\u53f7\u90fd+1\uff1b\u5728\u8fdb\u884cCAS\u64cd\u4f5c\u65f6\uff0c\u4e0d\u4ec5\u6bd4\u8f83\u5185\u5b58\u4e2d\u7684\u503c\uff0c\u4e5f\u4f1a\u6bd4\u8f83\u7248\u672c\u53f7\uff0c\u53ea\u6709\u5f53\u4e8c\u8005\u90fd\u6ca1\u6709\u53d8\u5316\u65f6\uff0cCAS\u624d\u80fd\u6267\u884c\u6210\u529f\u3002Java\u4e2d\u7684 AtomicStampedReference \u7c7b\u4fbf\u662f\u4f7f\u7528\u7248\u672c\u53f7\u6765\u89e3\u51b3ABA\u95ee\u9898\u7684\u3002 2\u3001\u9ad8\u7ade\u4e89\u4e0b\u7684\u5f00\u9500\u95ee\u9898 \u5728\u5e76\u53d1\u51b2\u7a81\u6982\u7387\u5927\u7684\u9ad8\u7ade\u4e89\u73af\u5883\u4e0b\uff0c\u5982\u679cCAS\u4e00\u76f4\u5931\u8d25\uff0c\u4f1a\u4e00\u76f4\u91cd\u8bd5\uff0cCPU\u5f00\u9500\u8f83\u5927\u3002\u9488\u5bf9\u8fd9\u4e2a\u95ee\u9898\u7684\u4e00\u4e2a\u601d\u8def\u662f\u5f15\u5165\u9000\u51fa\u673a\u5236\uff0c\u5982\u91cd\u8bd5\u6b21\u6570\u8d85\u8fc7\u4e00\u5b9a\u9608\u503c\u540e\u5931\u8d25\u9000\u51fa\u3002\u5f53\u7136\uff0c\u66f4\u91cd\u8981\u7684\u662f\u907f\u514d\u5728\u9ad8\u7ade\u4e89\u73af\u5883\u4e0b\u4f7f\u7528\u4e50\u89c2\u9501\u3002 3\u3001\u529f\u80fd\u9650\u5236 CAS\u7684\u529f\u80fd\u662f\u6bd4\u8f83\u53d7\u9650\u7684\uff0c\u4f8b\u5982CAS\u53ea\u80fd\u4fdd\u8bc1\u5355\u4e2a\u53d8\u91cf\uff08\u6216\u8005\u8bf4\u5355\u4e2a\u5185\u5b58\u503c\uff09\u64cd\u4f5c\u7684\u539f\u5b50\u6027\uff0c\u8fd9\u610f\u5473\u7740\uff1a(1)\u539f\u5b50\u6027\u4e0d\u4e00\u5b9a\u80fd\u4fdd\u8bc1\u7ebf\u7a0b\u5b89\u5168\uff0c\u4f8b\u5982\u5728Java\u4e2d\u9700\u8981\u4e0evolatile\u914d\u5408\u6765\u4fdd\u8bc1\u7ebf\u7a0b\u5b89\u5168\uff1b(2)\u5f53\u6d89\u53ca\u5230\u591a\u4e2a\u53d8\u91cf(\u5185\u5b58\u503c)\u65f6\uff0cCAS\u4e5f\u65e0\u80fd\u4e3a\u529b\u3002 NOTE: 1\u3001\u591a\u4e2aCAS optimistic lock\u65e0\u6cd5\u5de5\u4f5c \u9664\u6b64\u4e4b\u5916\uff0cCAS\u7684\u5b9e\u73b0\u9700\u8981\u786c\u4ef6\u5c42\u9762\u5904\u7406\u5668\u7684\u652f\u6301\uff0c\u5728Java\u4e2d\u666e\u901a\u7528\u6237\u65e0\u6cd5\u76f4\u63a5\u4f7f\u7528\uff0c\u53ea\u80fd\u501f\u52a9atomic\u5305\u4e0b\u7684\u539f\u5b50\u7c7b\u4f7f\u7528\uff0c\u7075\u6d3b\u6027\u53d7\u5230\u9650\u5236\u3002","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Optimistic-and-pessimistic/zhihu-%E4%BD%A0%E4%BA%86%E8%A7%A3%E4%B9%90%E8%A7%82%E9%94%81%E5%92%8C%E6%82%B2%E8%A7%82%E9%94%81%E5%90%97/#zhihu#bat","text":"NOTE: 1\u3001\u8fd9\u7bc7\u6587\u7ae0\u8bb2\u5f97\u4e0d\u9519\uff0c\u5b83\u4e0d\u4ec5\u8bf4\u6e05\u4e86\u4e50\u89c2\u9501\u3001\u60b2\u89c2\u9501\u7684\u6982\u5ff5\uff0c\u800c\u4e14\u8fd8\u8bf4\u660e\u4e86\u5982\u4f55\u6765\u5b9e\u73b0\u3002 2\u3001\u4f7f\u7528\u5728 Optimistic-and-pessimistic \u7ae0\u8282\u4e2d\u603b\u7ed3\u7684: \u4f7f\u7528 transaction \u6765\u770b\u5230optimistic lock","title":"zhihu \u3010BAT\u9762\u8bd5\u9898\u7cfb\u5217\u3011\u9762\u8bd5\u5b98\uff1a\u4f60\u4e86\u89e3\u4e50\u89c2\u9501\u548c\u60b2\u89c2\u9501\u5417\uff1f"},{"location":"Concurrent-computing/Concurrency-control/Optimistic-and-pessimistic/zhihu-%E4%BD%A0%E4%BA%86%E8%A7%A3%E4%B9%90%E8%A7%82%E9%94%81%E5%92%8C%E6%82%B2%E8%A7%82%E9%94%81%E5%90%97/#_1","text":"**\u4e50\u89c2\u9501**\u548c**\u60b2\u89c2\u9501**\u662f\u4e24\u79cd\u601d\u60f3\uff0c\u7528\u4e8e\u89e3\u51b3\u5e76\u53d1\u573a\u666f\u4e0b\u7684\u6570\u636e\u7ade\u4e89\u95ee\u9898\u3002 1\u3001\u4e50\u89c2\u9501\uff1a \u4e50\u89c2\u9501**\u5728\u64cd\u4f5c\u6570\u636e\u65f6\u975e\u5e38**\u4e50\u89c2 \uff0c\u8ba4\u4e3a\u522b\u4eba\u4e0d\u4f1a\u540c\u65f6\u4fee\u6539\u6570\u636e\u3002\u56e0\u6b64**\u4e50\u89c2\u9501**\u4e0d\u4f1a\u4e0a\u9501\uff0c\u53ea\u662f\u5728\u6267\u884c**\u66f4\u65b0**\u7684\u65f6\u5019\u5224\u65ad\u4e00\u4e0b\u5728\u6b64\u671f\u95f4\u522b\u4eba\u662f\u5426\u4fee\u6539\u4e86\u6570\u636e\uff1a\u5982\u679c\u522b\u4eba\u4fee\u6539\u4e86\u6570\u636e\u5219\u653e\u5f03\u64cd\u4f5c\uff0c\u5426\u5219\u6267\u884c\u64cd\u4f5c\u3002 NOTE: 1\u3001\u5178\u578b\u7684transaction\u65b9\u5f0f 2\u3001\u60b2\u89c2\u9501\uff1a \u60b2\u89c2\u9501**\u5728\u64cd\u4f5c\u6570\u636e\u65f6\u6bd4\u8f83**\u60b2\u89c2 \uff0c\u8ba4\u4e3a\u522b\u4eba\u4f1a\u540c\u65f6\u4fee\u6539\u6570\u636e\u3002\u56e0\u6b64\u64cd\u4f5c\u6570\u636e\u65f6\u76f4\u63a5\u628a\u6570\u636e\u9501\u4f4f\uff0c\u76f4\u5230\u64cd\u4f5c\u5b8c\u6210\u540e\u624d\u4f1a\u91ca\u653e\u9501\uff1b\u4e0a\u9501\u671f\u95f4\u5176\u4ed6\u4eba\u4e0d\u80fd\u4fee\u6539\u6570\u636e\u3002","title":"\u4e00\u3001\u57fa\u672c\u6982\u5ff5"},{"location":"Concurrent-computing/Concurrency-control/Optimistic-and-pessimistic/zhihu-%E4%BD%A0%E4%BA%86%E8%A7%A3%E4%B9%90%E8%A7%82%E9%94%81%E5%92%8C%E6%82%B2%E8%A7%82%E9%94%81%E5%90%97/#_2","text":"\u5728\u8bf4\u660e\u5b9e\u73b0\u65b9\u5f0f\u4e4b\u524d\uff0c\u9700\u8981\u660e\u786e\uff1a \u4e50\u89c2\u9501\u548c\u60b2\u89c2\u9501\u662f\u4e24\u79cd\u601d\u60f3\uff0c\u5b83\u4eec\u7684\u4f7f\u7528\u662f\u975e\u5e38\u5e7f\u6cdb\u7684\uff0c\u4e0d\u5c40\u9650\u4e8e\u67d0\u79cd\u7f16\u7a0b\u8bed\u8a00\u6216\u6570\u636e\u5e93\u3002 **\u60b2\u89c2\u9501**\u7684\u5b9e\u73b0\u65b9\u5f0f\u662f\u52a0\u9501\uff0c\u52a0\u9501\u65e2\u53ef\u4ee5\u662f\u5bf9\u4ee3\u7801\u5757\u52a0\u9501\uff08\u5982Java\u7684 synchronized \u5173\u952e\u5b57\uff09\uff0c\u4e5f\u53ef\u4ee5\u662f\u5bf9\u6570\u636e\u52a0\u9501\uff08\u5982MySQL\u4e2d\u7684\u6392\u5b83\u9501\uff09\u3002 \u4e50\u89c2\u9501\u7684\u5b9e\u73b0\u65b9\u5f0f\u4e3b\u8981\u6709\u4e24\u79cd\uff1aCAS\u673a\u5236\u548c\u7248\u672c\u53f7\u673a\u5236\uff0c\u4e0b\u9762\u8be6\u7ec6\u4ecb\u7ecd\u3002 NOTE: 1\u3001\u8fd9\u4e24\u79cd\u5b9e\u73b0\u662f\u5728\u5176\u4ed6\u7684\u7ae0\u8282\u4e2d\u90fd\u6709\u8bf4\u660e","title":"\u4e8c\u3001\u5b9e\u73b0\u65b9\u5f0f(\u542b\u5b9e\u4f8b)"},{"location":"Concurrent-computing/Concurrency-control/Optimistic-and-pessimistic/zhihu-%E4%BD%A0%E4%BA%86%E8%A7%A3%E4%B9%90%E8%A7%82%E9%94%81%E5%92%8C%E6%82%B2%E8%A7%82%E9%94%81%E5%90%97/#1cascompare#and#swap","text":"\u4e0b\u9762\u4ee5Java\u4e2d\u7684\u81ea\u589e\u64cd\u4f5c(i++)\u4e3a\u4f8b\uff0c\u770b\u4e00\u4e0b\u60b2\u89c2\u9501\u548cCAS\u5206\u522b\u662f\u5982\u4f55\u4fdd\u8bc1\u7ebf\u7a0b\u5b89\u5168\u7684\u3002\u6211\u4eec\u77e5\u9053\uff0c\u5728Java\u4e2d\u81ea\u589e\u64cd\u4f5c\u4e0d\u662f\u539f\u5b50\u64cd\u4f5c\uff0c\u5b83\u5b9e\u9645\u4e0a\u5305\u542b\u4e09\u4e2a\u72ec\u7acb\u7684\u64cd\u4f5c\uff1a \uff081\uff09\u8bfb\u53d6i\u503c\uff1b \uff082\uff09\u52a01\uff1b \uff083\uff09\u5c06\u65b0\u503c\u5199\u56dei \u56e0\u6b64\uff0c\u5982\u679c\u5e76\u53d1\u6267\u884c\u81ea\u589e\u64cd\u4f5c\uff0c\u53ef\u80fd\u5bfc\u81f4\u8ba1\u7b97\u7ed3\u679c\u7684\u4e0d\u51c6\u786e\u3002\u5728\u4e0b\u9762\u7684\u4ee3\u7801\u793a\u4f8b\u4e2d\uff1avalue1\u6ca1\u6709\u8fdb\u884c\u4efb\u4f55\u7ebf\u7a0b\u5b89\u5168\u65b9\u9762\u7684\u4fdd\u62a4\uff0cvalue2\u4f7f\u7528\u4e86\u4e50\u89c2\u9501(CAS)\uff0cvalue3\u4f7f\u7528\u4e86\u60b2\u89c2\u9501(synchronized)\u3002\u8fd0\u884c\u7a0b\u5e8f\uff0c\u4f7f\u75281000\u4e2a\u7ebf\u7a0b\u540c\u65f6\u5bf9value1\u3001value2\u548cvalue3\u8fdb\u884c\u81ea\u589e\u64cd\u4f5c\uff0c\u53ef\u4ee5\u53d1\u73b0\uff1avalue2\u548cvalue3\u7684\u503c\u603b\u662f\u7b49\u4e8e1000\uff0c\u800cvalue1\u7684\u503c\u5e38\u5e38\u5c0f\u4e8e1000\u3002 public class Test { //value1\uff1a\u7ebf\u7a0b\u4e0d\u5b89\u5168 private static int value1 = 0 ; //value2\uff1a\u4f7f\u7528\u4e50\u89c2\u9501 private static AtomicInteger value2 = new AtomicInteger ( 0 ); //value3\uff1a\u4f7f\u7528\u60b2\u89c2\u9501 private static int value3 = 0 ; private static synchronized void increaseValue3 (){ value3 ++ ; } public static void main ( String [] args ) throws Exception { //\u5f00\u542f1000\u4e2a\u7ebf\u7a0b\uff0c\u5e76\u6267\u884c\u81ea\u589e\u64cd\u4f5c for ( int i = 0 ; i < 1000 ; ++ i ){ new Thread ( new Runnable () { @Override public void run () { try { Thread . sleep ( 100 ); } catch ( InterruptedException e ) { e . printStackTrace (); } value1 ++ ; value2 . getAndIncrement (); increaseValue3 (); } }). start (); } //\u6253\u5370\u7ed3\u679c Thread . sleep ( 1000 ); System . out . println ( \"\u7ebf\u7a0b\u4e0d\u5b89\u5168\uff1a\" + value1 ); System . out . println ( \"\u4e50\u89c2\u9501(AtomicInteger)\uff1a\" + value2 ); System . out . println ( \"\u60b2\u89c2\u9501(synchronized)\uff1a\" + value3 ); } } NOTE: 1\u3001\u539f\u6587\u540e\u9762\u662f\u5bf9 getAndIncrement() \u5b9e\u73b0\u7684\u5206\u6790\uff0c\u5728 Compare-and-swap \u7ae0\u8282\u4e2d\u7684 \"Example application: atomic adder\" \u4e2d\u4f8b\u5b50\u7c7b\u4f3c\uff0c\u539f\u7406\u76f8\u540c\u3002","title":"1\u3001CAS\uff08Compare And Swap\uff09"},{"location":"Concurrent-computing/Concurrency-control/Optimistic-and-pessimistic/zhihu-%E4%BD%A0%E4%BA%86%E8%A7%A3%E4%B9%90%E8%A7%82%E9%94%81%E5%92%8C%E6%82%B2%E8%A7%82%E9%94%81%E5%90%97/#2","text":"NOTE: 1\u3001MVCC\uff0c\u901a\u8fc7\u7248\u672c\u53f7\u7684\u53d8\u66f4\u6765\u5224\u65ad\u662f\u5426\u53d1\u751f\u4e86\u66f4\u6539\u3002 \u9664\u4e86CAS\uff0c\u7248\u672c\u53f7\u673a\u5236\u4e5f\u53ef\u4ee5\u7528\u6765\u5b9e\u73b0\u4e50\u89c2\u9501\u3002\u7248\u672c\u53f7\u673a\u5236\u7684\u57fa\u672c\u601d\u8def\u662f\u5728\u6570\u636e\u4e2d\u589e\u52a0\u4e00\u4e2a\u5b57\u6bb5version\uff0c\u8868\u793a\u8be5\u6570\u636e\u7684\u7248\u672c\u53f7\uff0c\u6bcf\u5f53\u6570\u636e\u88ab\u4fee\u6539\uff0c\u7248\u672c\u53f7\u52a01\u3002\u5f53\u67d0\u4e2a\u7ebf\u7a0b\u67e5\u8be2\u6570\u636e\u65f6\uff0c\u5c06\u8be5\u6570\u636e\u7684\u7248\u672c\u53f7\u4e00\u8d77\u67e5\u51fa\u6765\uff1b\u5f53\u8be5\u7ebf\u7a0b\u66f4\u65b0\u6570\u636e\u65f6\uff0c\u5224\u65ad\u5f53\u524d\u7248\u672c\u53f7\u4e0e\u4e4b\u524d\u8bfb\u53d6\u7684\u7248\u672c\u53f7\u662f\u5426\u4e00\u81f4\uff0c\u5982\u679c\u4e00\u81f4\u624d\u8fdb\u884c\u64cd\u4f5c\u3002 \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86\u7248\u672c\u53f7\u4f5c\u4e3a\u5224\u65ad\u6570\u636e\u53d8\u5316\u7684\u6807\u8bb0\uff0c\u5b9e\u9645\u4e0a\u53ef\u4ee5\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u9009\u7528\u5176\u4ed6\u80fd\u591f\u6807\u8bb0\u6570\u636e\u7248\u672c\u7684\u5b57\u6bb5\uff0c\u5982\u65f6\u95f4\u6233\u7b49\u3002 \u4e0b\u9762\u4ee5\u201c\u66f4\u65b0\u73a9\u5bb6\u91d1\u5e01\u6570\u201d\u4e3a\u4f8b\uff08\u6570\u636e\u5e93\u4e3aMySQL\uff0c\u5176\u4ed6\u6570\u636e\u5e93\u540c\u7406\uff09\uff0c\u770b\u770b\u60b2\u89c2\u9501\u548c\u7248\u672c\u53f7\u673a\u5236\u662f\u5982\u4f55\u5e94\u5bf9\u5e76\u53d1\u95ee\u9898\u7684\u3002","title":"2\u3001\u7248\u672c\u53f7\u673a\u5236"},{"location":"Concurrent-computing/Concurrency-control/Optimistic-and-pessimistic/zhihu-%E4%BD%A0%E4%BA%86%E8%A7%A3%E4%B9%90%E8%A7%82%E9%94%81%E5%92%8C%E6%82%B2%E8%A7%82%E9%94%81%E5%90%97/#_3","text":"\u4e0b\u9762\u7684\u5b9e\u73b0\u65b9\u5f0f\uff0c\u6ca1\u6709\u8fdb\u884c\u4efb\u4f55\u7ebf\u7a0b\u5b89\u5168\u65b9\u9762\u7684\u4fdd\u62a4\u3002\u5982\u679c\u6709\u5176\u4ed6\u7ebf\u7a0b\u5728query\u548cupdate\u4e4b\u95f4\u66f4\u65b0\u4e86\u73a9\u5bb6\u7684\u4fe1\u606f\uff0c\u4f1a\u5bfc\u81f4\u73a9\u5bb6\u91d1\u5e01\u6570\u7684\u4e0d\u51c6\u786e\u3002 @Transactional public void updateCoins ( Integer playerId ){ //\u6839\u636eplayer_id\u67e5\u8be2\u73a9\u5bb6\u4fe1\u606f Player player = query ( \"select coins, level from player where player_id = {0}\" , playerId ); //\u6839\u636e\u73a9\u5bb6\u5f53\u524d\u4fe1\u606f\u53ca\u5176\u4ed6\u4fe1\u606f\uff0c\u8ba1\u7b97\u65b0\u7684\u91d1\u5e01\u6570 Long newCoins = \u2026\u2026 ; //\u66f4\u65b0\u91d1\u5e01\u6570 update ( \"update player set coins = {0} where player_id = {1}\" , newCoins , playerId ); }","title":"\u975e\u7ebf\u7a0b\u5b89\u5168"},{"location":"Concurrent-computing/Concurrency-control/Optimistic-and-pessimistic/zhihu-%E4%BD%A0%E4%BA%86%E8%A7%A3%E4%B9%90%E8%A7%82%E9%94%81%E5%92%8C%E6%82%B2%E8%A7%82%E9%94%81%E5%90%97/#select#for#update","text":"\u4e3a\u4e86\u907f\u514d\u8fd9\u4e2a\u95ee\u9898\uff0c\u60b2\u89c2\u9501\u901a\u8fc7\u52a0\u9501\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4ee3\u7801\u5982\u4e0b\u6240\u793a\u3002\u5728\u67e5\u8be2\u73a9\u5bb6\u4fe1\u606f\u65f6\uff0c\u4f7f\u7528select \u2026\u2026 for update\u8fdb\u884c\u67e5\u8be2\uff1b\u8be5\u67e5\u8be2\u8bed\u53e5\u4f1a\u4e3a\u8be5\u73a9\u5bb6\u6570\u636e\u52a0\u4e0a\u6392\u5b83\u9501\uff0c\u76f4\u5230\u4e8b\u52a1\u63d0\u4ea4\u6216\u56de\u6eda\u65f6\u624d\u4f1a\u91ca\u653e\u6392\u5b83\u9501\uff1b\u5728\u6b64\u671f\u95f4\uff0c\u5982\u679c\u5176\u4ed6\u7ebf\u7a0b\u8bd5\u56fe\u66f4\u65b0\u8be5\u73a9\u5bb6\u4fe1\u606f\u6216\u8005\u6267\u884cselect for update\uff0c\u4f1a\u88ab\u963b\u585e\u3002 @Transactional public void updateCoins ( Integer playerId ){ //\u6839\u636eplayer_id\u67e5\u8be2\u73a9\u5bb6\u4fe1\u606f\uff08\u52a0\u6392\u5b83\u9501\uff09 Player player = queryForUpdate ( \"select coins, level from player where player_id = {0} for update\" , playerId ); //\u6839\u636e\u73a9\u5bb6\u5f53\u524d\u4fe1\u606f\u53ca\u5176\u4ed6\u4fe1\u606f\uff0c\u8ba1\u7b97\u65b0\u7684\u91d1\u5e01\u6570 Long newCoins = \u2026\u2026 ; //\u66f4\u65b0\u91d1\u5e01\u6570 update ( \"update player set coins = {0} where player_id = {1}\" , newCoins , playerId ); }","title":"select \u2026\u2026 for update \u6392\u5b83\u9501"},{"location":"Concurrent-computing/Concurrency-control/Optimistic-and-pessimistic/zhihu-%E4%BD%A0%E4%BA%86%E8%A7%A3%E4%B9%90%E8%A7%82%E9%94%81%E5%92%8C%E6%82%B2%E8%A7%82%E9%94%81%E5%90%97/#_4","text":"\u7248\u672c\u53f7\u673a\u5236\u5219\u662f\u53e6\u4e00\u79cd\u601d\u8def\uff0c\u5b83\u4e3a\u73a9\u5bb6\u4fe1\u606f\u589e\u52a0\u4e00\u4e2a\u5b57\u6bb5\uff1aversion\u3002\u5728\u521d\u6b21\u67e5\u8be2\u73a9\u5bb6\u4fe1\u606f\u65f6\uff0c\u540c\u65f6\u67e5\u8be2\u51faversion\u4fe1\u606f\uff1b\u5728\u6267\u884cupdate\u64cd\u4f5c\u65f6\uff0c\u6821\u9a8cversion\u662f\u5426\u53d1\u751f\u4e86\u53d8\u5316\uff0c\u5982\u679cversion\u53d8\u5316\uff0c\u5219\u4e0d\u8fdb\u884c\u66f4\u65b0\u3002 @Transactional public void updateCoins ( Integer playerId ){ //\u6839\u636eplayer_id\u67e5\u8be2\u73a9\u5bb6\u4fe1\u606f\uff0c\u5305\u542bversion\u4fe1\u606f Player player = query ( \"select coins, level, version from player where player_id = {0}\" , playerId ); //\u6839\u636e\u73a9\u5bb6\u5f53\u524d\u4fe1\u606f\u53ca\u5176\u4ed6\u4fe1\u606f\uff0c\u8ba1\u7b97\u65b0\u7684\u91d1\u5e01\u6570 Long newCoins = \u2026\u2026 ; //\u66f4\u65b0\u91d1\u5e01\u6570\uff0c\u6761\u4ef6\u4e2d\u589e\u52a0\u5bf9version\u7684\u6821\u9a8c update ( \"update player set coins = {0}, version = version + 1 where player_id = {1} and version = {2}\" , newCoins , playerId , player . version ); }","title":"\u7248\u672c\u53f7\u673a\u5236"},{"location":"Concurrent-computing/Concurrency-control/Optimistic-and-pessimistic/zhihu-%E4%BD%A0%E4%BA%86%E8%A7%A3%E4%B9%90%E8%A7%82%E9%94%81%E5%92%8C%E6%82%B2%E8%A7%82%E9%94%81%E5%90%97/#_5","text":"\u4e50\u89c2\u9501\u548c\u60b2\u89c2\u9501\u5e76\u6ca1\u6709\u4f18\u52a3\u4e4b\u5206\uff0c\u5b83\u4eec\u6709\u5404\u81ea\u9002\u5408\u7684\u573a\u666f\uff1b\u4e0b\u9762\u4ece\u4e24\u4e2a\u65b9\u9762\u8fdb\u884c\u8bf4\u660e\u3002","title":"\u4e09\u3001\u4f18\u7f3a\u70b9\u548c\u9002\u7528\u573a\u666f"},{"location":"Concurrent-computing/Concurrency-control/Optimistic-and-pessimistic/zhihu-%E4%BD%A0%E4%BA%86%E8%A7%A3%E4%B9%90%E8%A7%82%E9%94%81%E5%92%8C%E6%82%B2%E8%A7%82%E9%94%81%E5%90%97/#1","text":"\u4e0e\u60b2\u89c2\u9501\u76f8\u6bd4\uff0c\u4e50\u89c2\u9501\u9002\u7528\u7684\u573a\u666f\u53d7\u5230\u4e86\u66f4\u591a\u7684\u9650\u5236\uff0c\u65e0\u8bba\u662fCAS\u8fd8\u662f\u7248\u672c\u53f7\u673a\u5236\u3002 \u4f8b\u5982\uff0cCAS\u53ea\u80fd\u4fdd\u8bc1\u5355\u4e2a\u53d8\u91cf\u64cd\u4f5c\u7684\u539f\u5b50\u6027\uff0c\u5f53\u6d89\u53ca\u5230\u591a\u4e2a\u53d8\u91cf\u65f6\uff0cCAS\u662f\u65e0\u80fd\u4e3a\u529b\u7684\uff0c\u800c synchronized \u5219\u53ef\u4ee5\u901a\u8fc7\u5bf9\u6574\u4e2a\u4ee3\u7801\u5757\u52a0\u9501\u6765\u5904\u7406\u3002\u518d\u6bd4\u5982\u7248\u672c\u53f7\u673a\u5236\uff0c\u5982\u679cquery\u7684\u65f6\u5019\u662f\u9488\u5bf9\u88681\uff0c\u800cupdate\u7684\u65f6\u5019\u662f\u9488\u5bf9\u88682\uff0c\u4e5f\u5f88\u96be\u901a\u8fc7\u7b80\u5355\u7684\u7248\u672c\u53f7\u6765\u5b9e\u73b0\u4e50\u89c2\u9501\u3002","title":"1\u3001\u529f\u80fd\u9650\u5236"},{"location":"Concurrent-computing/Concurrency-control/Optimistic-and-pessimistic/zhihu-%E4%BD%A0%E4%BA%86%E8%A7%A3%E4%B9%90%E8%A7%82%E9%94%81%E5%92%8C%E6%82%B2%E8%A7%82%E9%94%81%E5%90%97/#2_1","text":"\u5982\u679c\u60b2\u89c2\u9501\u548c\u4e50\u89c2\u9501\u90fd\u53ef\u4ee5\u4f7f\u7528\uff0c\u90a3\u4e48\u9009\u62e9\u5c31\u8981\u8003\u8651\u7ade\u4e89\u7684\u6fc0\u70c8\u7a0b\u5ea6\uff1a a\u3001\u5f53\u7ade\u4e89\u4e0d\u6fc0\u70c8 (\u51fa\u73b0\u5e76\u53d1\u51b2\u7a81\u7684\u6982\u7387\u5c0f)\u65f6\uff0c\u4e50\u89c2\u9501\u66f4\u6709\u4f18\u52bf\uff0c\u56e0\u4e3a\u60b2\u89c2\u9501\u4f1a\u9501\u4f4f\u4ee3\u7801\u5757\u6216\u6570\u636e\uff0c\u5176\u4ed6\u7ebf\u7a0b\u65e0\u6cd5\u540c\u65f6\u8bbf\u95ee\uff0c\u5f71\u54cd\u5e76\u53d1\uff0c\u800c\u4e14\u52a0\u9501\u548c\u91ca\u653e\u9501\u90fd\u9700\u8981\u6d88\u8017\u989d\u5916\u7684\u8d44\u6e90\u3002 b\u3001\u5f53\u7ade\u4e89\u6fc0\u70c8(\u51fa\u73b0\u5e76\u53d1\u51b2\u7a81\u7684\u6982\u7387\u5927)\u65f6\uff0c\u60b2\u89c2\u9501\u66f4\u6709\u4f18\u52bf\uff0c\u56e0\u4e3a\u4e50\u89c2\u9501\u5728\u6267\u884c\u66f4\u65b0\u65f6\u9891\u7e41\u5931\u8d25\uff0c\u9700\u8981\u4e0d\u65ad\u91cd\u8bd5\uff0c\u6d6a\u8d39CPU\u8d44\u6e90\u3002","title":"2\u3001\u7ade\u4e89\u6fc0\u70c8\u7a0b\u5ea6"},{"location":"Concurrent-computing/Concurrency-control/Optimistic-and-pessimistic/zhihu-%E4%BD%A0%E4%BA%86%E8%A7%A3%E4%B9%90%E8%A7%82%E9%94%81%E5%92%8C%E6%82%B2%E8%A7%82%E9%94%81%E5%90%97/#_6","text":"\u4e0b\u9762\u662f\u6211\u5bf9\u8fd9\u4e2a\u95ee\u9898\u7684\u7406\u89e3\uff1a \uff081\uff09\u4e50\u89c2\u9501\u672c\u8eab\u662f\u4e0d\u52a0\u9501\u7684\uff0c\u53ea\u662f\u5728\u66f4\u65b0\u65f6\u5224\u65ad\u4e00\u4e0b\u6570\u636e\u662f\u5426\u88ab\u5176\u4ed6\u7ebf\u7a0b\u66f4\u65b0\u4e86\uff1bAtomicInteger\u4fbf\u662f\u4e00\u4e2a\u4f8b\u5b50\u3002 \uff082\uff09\u6709\u65f6\u4e50\u89c2\u9501\u53ef\u80fd\u4e0e\u52a0\u9501\u64cd\u4f5c\u5408\u4f5c\uff0c\u4f8b\u5982\uff0c\u5728\u524d\u8ff0updateCoins()\u7684\u4f8b\u5b50\u4e2d\uff0cMySQL\u5728\u6267\u884cupdate\u65f6\u4f1a\u52a0\u6392\u5b83\u9501\u3002\u4f46\u8fd9\u53ea\u662f\u4e50\u89c2\u9501\u4e0e\u52a0\u9501\u64cd\u4f5c\u5408\u4f5c\u7684\u4f8b\u5b50\uff0c\u4e0d\u80fd\u6539\u53d8\u201c\u4e50\u89c2\u9501\u672c\u8eab\u4e0d\u52a0\u9501\u201d\u8fd9\u4e00\u4e8b\u5b9e\u3002","title":"\u56db\u3001\u9762\u8bd5\u5b98\u8ffd\u95ee\uff1a\u4e50\u89c2\u9501\u52a0\u9501\u5417\uff1f"},{"location":"Concurrent-computing/Concurrency-control/Optimistic-and-pessimistic/zhihu-%E4%BD%A0%E4%BA%86%E8%A7%A3%E4%B9%90%E8%A7%82%E9%94%81%E5%92%8C%E6%82%B2%E8%A7%82%E9%94%81%E5%90%97/#cas","text":"\u4e0b\u9762\u662fCAS\u4e00\u4e9b\u4e0d\u90a3\u4e48\u5b8c\u7f8e\u7684\u5730\u65b9\uff1a","title":"\u4e94\u3001\u9762\u8bd5\u5b98\u8ffd\u95ee\uff1aCAS\u6709\u54ea\u4e9b\u7f3a\u70b9\uff1f"},{"location":"Concurrent-computing/Concurrency-control/Optimistic-and-pessimistic/zhihu-%E4%BD%A0%E4%BA%86%E8%A7%A3%E4%B9%90%E8%A7%82%E9%94%81%E5%92%8C%E6%82%B2%E8%A7%82%E9%94%81%E5%90%97/#1aba","text":"\u5728 AtomicInteger \u7684\u4f8b\u5b50\u4e2d\uff0cABA\u4f3c\u4e4e\u6ca1\u6709\u4ec0\u4e48\u5371\u5bb3\u3002\u4f46\u662f\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\uff0cABA\u5374\u4f1a\u5e26\u6765\u9690\u60a3\uff0c\u4f8b\u5982\u6808\u9876\u95ee\u9898\uff1a\u4e00\u4e2a\u6808\u7684\u6808\u9876\u7ecf\u8fc7\u4e24\u6b21(\u6216\u591a\u6b21)\u53d8\u5316\u53c8\u6062\u590d\u4e86\u539f\u503c\uff0c\u4f46\u662f\u6808\u53ef\u80fd\u5df2\u53d1\u751f\u4e86\u53d8\u5316\u3002 \u5bf9\u4e8eABA\u95ee\u9898\uff0c\u6bd4\u8f83\u6709\u6548\u7684\u65b9\u6848\u662f\u5f15\u5165\u7248\u672c\u53f7\uff0c\u5185\u5b58\u4e2d\u7684\u503c\u6bcf\u53d1\u751f\u4e00\u6b21\u53d8\u5316\uff0c\u7248\u672c\u53f7\u90fd+1\uff1b\u5728\u8fdb\u884cCAS\u64cd\u4f5c\u65f6\uff0c\u4e0d\u4ec5\u6bd4\u8f83\u5185\u5b58\u4e2d\u7684\u503c\uff0c\u4e5f\u4f1a\u6bd4\u8f83\u7248\u672c\u53f7\uff0c\u53ea\u6709\u5f53\u4e8c\u8005\u90fd\u6ca1\u6709\u53d8\u5316\u65f6\uff0cCAS\u624d\u80fd\u6267\u884c\u6210\u529f\u3002Java\u4e2d\u7684 AtomicStampedReference \u7c7b\u4fbf\u662f\u4f7f\u7528\u7248\u672c\u53f7\u6765\u89e3\u51b3ABA\u95ee\u9898\u7684\u3002","title":"1\u3001ABA\u95ee\u9898"},{"location":"Concurrent-computing/Concurrency-control/Optimistic-and-pessimistic/zhihu-%E4%BD%A0%E4%BA%86%E8%A7%A3%E4%B9%90%E8%A7%82%E9%94%81%E5%92%8C%E6%82%B2%E8%A7%82%E9%94%81%E5%90%97/#2_2","text":"\u5728\u5e76\u53d1\u51b2\u7a81\u6982\u7387\u5927\u7684\u9ad8\u7ade\u4e89\u73af\u5883\u4e0b\uff0c\u5982\u679cCAS\u4e00\u76f4\u5931\u8d25\uff0c\u4f1a\u4e00\u76f4\u91cd\u8bd5\uff0cCPU\u5f00\u9500\u8f83\u5927\u3002\u9488\u5bf9\u8fd9\u4e2a\u95ee\u9898\u7684\u4e00\u4e2a\u601d\u8def\u662f\u5f15\u5165\u9000\u51fa\u673a\u5236\uff0c\u5982\u91cd\u8bd5\u6b21\u6570\u8d85\u8fc7\u4e00\u5b9a\u9608\u503c\u540e\u5931\u8d25\u9000\u51fa\u3002\u5f53\u7136\uff0c\u66f4\u91cd\u8981\u7684\u662f\u907f\u514d\u5728\u9ad8\u7ade\u4e89\u73af\u5883\u4e0b\u4f7f\u7528\u4e50\u89c2\u9501\u3002","title":"2\u3001\u9ad8\u7ade\u4e89\u4e0b\u7684\u5f00\u9500\u95ee\u9898"},{"location":"Concurrent-computing/Concurrency-control/Optimistic-and-pessimistic/zhihu-%E4%BD%A0%E4%BA%86%E8%A7%A3%E4%B9%90%E8%A7%82%E9%94%81%E5%92%8C%E6%82%B2%E8%A7%82%E9%94%81%E5%90%97/#3","text":"CAS\u7684\u529f\u80fd\u662f\u6bd4\u8f83\u53d7\u9650\u7684\uff0c\u4f8b\u5982CAS\u53ea\u80fd\u4fdd\u8bc1\u5355\u4e2a\u53d8\u91cf\uff08\u6216\u8005\u8bf4\u5355\u4e2a\u5185\u5b58\u503c\uff09\u64cd\u4f5c\u7684\u539f\u5b50\u6027\uff0c\u8fd9\u610f\u5473\u7740\uff1a(1)\u539f\u5b50\u6027\u4e0d\u4e00\u5b9a\u80fd\u4fdd\u8bc1\u7ebf\u7a0b\u5b89\u5168\uff0c\u4f8b\u5982\u5728Java\u4e2d\u9700\u8981\u4e0evolatile\u914d\u5408\u6765\u4fdd\u8bc1\u7ebf\u7a0b\u5b89\u5168\uff1b(2)\u5f53\u6d89\u53ca\u5230\u591a\u4e2a\u53d8\u91cf(\u5185\u5b58\u503c)\u65f6\uff0cCAS\u4e5f\u65e0\u80fd\u4e3a\u529b\u3002 NOTE: 1\u3001\u591a\u4e2aCAS optimistic lock\u65e0\u6cd5\u5de5\u4f5c \u9664\u6b64\u4e4b\u5916\uff0cCAS\u7684\u5b9e\u73b0\u9700\u8981\u786c\u4ef6\u5c42\u9762\u5904\u7406\u5668\u7684\u652f\u6301\uff0c\u5728Java\u4e2d\u666e\u901a\u7528\u6237\u65e0\u6cd5\u76f4\u63a5\u4f7f\u7528\uff0c\u53ea\u80fd\u501f\u52a9atomic\u5305\u4e0b\u7684\u539f\u5b50\u7c7b\u4f7f\u7528\uff0c\u7075\u6d3b\u6027\u53d7\u5230\u9650\u5236\u3002","title":"3\u3001\u529f\u80fd\u9650\u5236"},{"location":"Concurrent-computing/Concurrency-control/Synchronization/","text":"Synchronization \"synchronization\"\u5373\u201c\u540c\u6b65\u201d\uff0c\u5f53\u6d89\u53ca\u5230\u591a\u4e2aentity\u65f6\uff0c\u5b83\u4eec\u4e4b\u95f4\u7684synchronization\u975e\u5e38\u91cd\u8981\u3002 wikipedia Synchronization (computer science) In computer science , synchronization refers to one of two distinct but related concepts: synchronization of processes , and synchronization of data . NOTE: \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u539f\u6587\u4e2d\u4f7f\u7528\u7684\u662fprocess\uff0c\u6211\u4eec\u4e0d\u80fd\u591f\u4ec5\u4ec5\u5c40\u9650\u4e8eprocess\uff0c\u800c\u662f\u4efb\u4f55\u80fd\u591fsynchronization\u7684entity\uff0c\u6bd4\u5982thread\u3002 Process synchronization refers to the idea that multiple processes are to join up \uff08\u6c47\u5408\uff09or handshake at a certain point, in order to reach an agreement or commit to a certain sequence of action. Data synchronization refers to the idea of keeping multiple copies of a dataset in coherence with one another, or to maintain data integrity . Process synchronization primitives are commonly used to implement data synchronization. NOTE: \u8fd9\u6bb5\u8bdd\u63cf\u8ff0\u4e86\u4e24\u79cdsynchronization\u4e4b\u95f4\u7684\u5173\u7cfb\u3002","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Synchronization/#synchronization","text":"\"synchronization\"\u5373\u201c\u540c\u6b65\u201d\uff0c\u5f53\u6d89\u53ca\u5230\u591a\u4e2aentity\u65f6\uff0c\u5b83\u4eec\u4e4b\u95f4\u7684synchronization\u975e\u5e38\u91cd\u8981\u3002","title":"Synchronization"},{"location":"Concurrent-computing/Concurrency-control/Synchronization/#wikipedia#synchronization#computer#science","text":"In computer science , synchronization refers to one of two distinct but related concepts: synchronization of processes , and synchronization of data . NOTE: \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u539f\u6587\u4e2d\u4f7f\u7528\u7684\u662fprocess\uff0c\u6211\u4eec\u4e0d\u80fd\u591f\u4ec5\u4ec5\u5c40\u9650\u4e8eprocess\uff0c\u800c\u662f\u4efb\u4f55\u80fd\u591fsynchronization\u7684entity\uff0c\u6bd4\u5982thread\u3002 Process synchronization refers to the idea that multiple processes are to join up \uff08\u6c47\u5408\uff09or handshake at a certain point, in order to reach an agreement or commit to a certain sequence of action. Data synchronization refers to the idea of keeping multiple copies of a dataset in coherence with one another, or to maintain data integrity . Process synchronization primitives are commonly used to implement data synchronization. NOTE: \u8fd9\u6bb5\u8bdd\u63cf\u8ff0\u4e86\u4e24\u79cdsynchronization\u4e4b\u95f4\u7684\u5173\u7cfb\u3002","title":"wikipedia Synchronization (computer science)"},{"location":"Concurrent-computing/Concurrency-control/Transactional-memory/","text":"Transactional memory \u5728\u9605\u8bfb C++ reference \u65f6\uff0c\u65e0\u610f\u4e2d\u53d1\u73b0\u4e86 \" Transactional Memory (TM TS)\"\uff0c\u770b\u4e86\u4e00\u4e0b\uff0c\u53d1\u73b0\u5b83\u975e\u5e38\u7c7b\u4f3c\u4e8eJava\u7684\u5199\u6cd5\uff0c\u9042\u51b3\u5b9a\u5c06transactional memory\u7684\u5185\u5bb9\u8fdb\u884c\u6574\u7406\u3002 Transactional memory VS low-level thread synchronization \u76f8\u6bd4\u4e8elow-level thread synchronization\uff0ctransactional memory \u7684\u4f18\u52bf\u5305\u62ec: High-level abstraction \u76f8\u6bd4\u4e8elow-level thread synchronization\uff0ctransaction\u662f\u4e00\u4e2a high-level abstraction\uff0c\u663e\u7136\u5b83\u80fd\u591f\u9690\u85cf\u975e\u5e38\u591a\u7684\u5e95\u5c42\u7ec6\u8282\uff0c\u66f4\u5bb9\u6613\u7406\u89e3\uff0c\u8ba9programmer\u80fd\u591f\u66f4\u597d\u5730\u7f16\u5199\u7ebf\u7a0b\u5b89\u5168\u7684\u7a0b\u5e8f\uff0c\u5728 wikipedia Software transactional memory # Conceptual advantages and disadvantages\u7ae0\u8282\u4e2d\uff0c\u4e5f\u6709\u5173\u4e8e\u6b64\u7684\u8ba8\u8bba\u3002 \u5173\u4e8etransaction\u662f\u4e00\u4e2a\u975e\u5e38\u597d\u7684\u62bd\u8c61\uff0c\u53c2\u89c1 Distributed-computing\\Theory\\Abstraction \u7ae0\u8282\u3002 Higher concurrency \u76f8\u6bd4\u4e8elow-level thread synchronization\uff0ctransactional memory\u80fd\u591f\u63d0\u4f9bhigher concurrency\uff0c\u8fd9\u5728: 1) wikipedia Transactional memory # Motivation 2) wikipedia Software transactional memory # Performance \u4e2d\u90fd\u6709\u63cf\u8ff0\u3002 Pessimistic VS optimistic Transactional memory\u662foptimistic concurrency control\uff0c\u800clock\u662fpessimistic\u3002 wikipedia Transactional memory In computer science and engineering , transactional memory attempts to simplify concurrent programming by allowing a group of load and store instructions to execute in an atomic way. It is a concurrency control mechanism analogous to database transactions for controlling access to shared memory in concurrent computing . Transactional memory systems provide high-level abstraction as an alternative to low-level thread synchronization. This abstraction allows for coordination between concurrent reads and writes of shared data in parallel systems.[ 1] NOTE: \u5728\u8bed\u8a00\u5c42\u652f\u6301transactional memory\u7684programming language\uff0c\u5f80\u5f80\u4f1a\u63d0\u4f9b\u8bed\u6cd5\u7cd6\u6765\u8ba9programmer\u8f7b\u677e\u5730\u4f7f\u7528transactional memory\uff1b Motivation Low level thread synchronization constructs such as locks are pessimistic . Transactional memory provides optimistic concurrency control by allowing threads to run in parallel with minimal interference. The goal of transactional memory systems is to transparently support regions of code marked as transactions by enforcing atomicity , consistency and isolation . NOTE: \u539f\u6587\u7b2c\u4e00\u6bb5\u4e3b\u8981\u4eceoptimistic\u548cpessimistic\u7684\u89d2\u5ea6\u6765\u8fdb\u884c\u8bf4\u660e\u3002 Transactional memory\u610f\u5473\u7740ACI(\u6ca1\u6709D\uff0c\u5728DBMS\u4e2dD\u662f\u6307 durability )\u3002 NOTE: \u539f\u6587\u7b2c\u4e8c\u6bb5\u4e3b\u8981\u4ecb\u7ecd\u7684transaction\u7684\u539f\u7406\u3002 With these constructs in place, transactional memory provides a high level programming abstraction by allowing programmers to enclose their methods within transactional blocks . Correct implementations ensure that data cannot be shared between threads without going through a transaction and produce a serializable outcome. For example, code can be written as: def transfer_money ( from_account , to_account , amount ): \"\"\"Transfer money from one account to another.\"\"\" with transaction (): from_account -= amount to_account += amount In the code, the block defined by \"transaction\" is guaranteed atomicity, consistency and isolation by the underlying transactional memory implementation and is transparent to the programmer. Although transactional memory programs cannot produce a deadlock, programs may still suffer from a livelock or resource starvation . For example, longer transactions may repeatedly revert(\u91cd\u590d) in response to multiple smaller transactions, wasting both time and energy.[ 2] Hardware vs. software The abstraction of atomicity in transactional memory requires a hardware mechanism to detect conflicts and undo any changes made to shared data.[ 3] Software transactional memory provides transactional memory semantics in a software runtime library or the programming language,[ 9] and requires minimal hardware support (typically an atomic compare and swap operation, or equivalent). As the downside, software implementations usually come with a performance penalty, when compared to hardware solutions. History As of GCC 4.7, an experimental library for transactional memory is available which utilizes a hybrid implementation. The PyPy variant of Python also introduces transactional memory to the language. NOTE: \u5173\u4e8eC++\u5bf9transactional memory\u7684\u652f\u6301\uff0c\u53c2\u89c1 C++\\Language-reference\\Basic-concept\\Abstract-machine\\Memory-model\\Transactional-memory \u7ae0\u8282\u3002 wikipedia Software transactional memory In computer science , software transactional memory ( STM ) is a concurrency control mechanism analogous to database transactions for controlling access to shared memory in concurrent computing . It is an alternative to lock-based synchronization . STM is a strategy implemented in software, rather than as a hardware component. Performance Unlike the locking techniques used in most modern multithreaded applications, STM is often very optimistic : Conceptual advantages and disadvantages In addition to their performance benefits[ citation needed ], STM greatly simplifies conceptual understanding of multithreaded programs and helps make programs more maintainable by working in harmony with existing high-level abstractions such as objects and modules. Lock-based programming has a number of well-known problems that frequently arise in practice Implementations C/C++ G++ 4.7 now supports STM for C/C++ directly in the compiler. The feature is still listed as \"experimental\", but may still provide the necessary functionality for testing.","title":"Introduction"},{"location":"Concurrent-computing/Concurrency-control/Transactional-memory/#transactional#memory","text":"\u5728\u9605\u8bfb C++ reference \u65f6\uff0c\u65e0\u610f\u4e2d\u53d1\u73b0\u4e86 \" Transactional Memory (TM TS)\"\uff0c\u770b\u4e86\u4e00\u4e0b\uff0c\u53d1\u73b0\u5b83\u975e\u5e38\u7c7b\u4f3c\u4e8eJava\u7684\u5199\u6cd5\uff0c\u9042\u51b3\u5b9a\u5c06transactional memory\u7684\u5185\u5bb9\u8fdb\u884c\u6574\u7406\u3002","title":"Transactional memory"},{"location":"Concurrent-computing/Concurrency-control/Transactional-memory/#transactional#memory#vs#low-level#thread#synchronization","text":"\u76f8\u6bd4\u4e8elow-level thread synchronization\uff0ctransactional memory \u7684\u4f18\u52bf\u5305\u62ec:","title":"Transactional memory VS low-level thread synchronization"},{"location":"Concurrent-computing/Concurrency-control/Transactional-memory/#high-level#abstraction","text":"\u76f8\u6bd4\u4e8elow-level thread synchronization\uff0ctransaction\u662f\u4e00\u4e2a high-level abstraction\uff0c\u663e\u7136\u5b83\u80fd\u591f\u9690\u85cf\u975e\u5e38\u591a\u7684\u5e95\u5c42\u7ec6\u8282\uff0c\u66f4\u5bb9\u6613\u7406\u89e3\uff0c\u8ba9programmer\u80fd\u591f\u66f4\u597d\u5730\u7f16\u5199\u7ebf\u7a0b\u5b89\u5168\u7684\u7a0b\u5e8f\uff0c\u5728 wikipedia Software transactional memory # Conceptual advantages and disadvantages\u7ae0\u8282\u4e2d\uff0c\u4e5f\u6709\u5173\u4e8e\u6b64\u7684\u8ba8\u8bba\u3002 \u5173\u4e8etransaction\u662f\u4e00\u4e2a\u975e\u5e38\u597d\u7684\u62bd\u8c61\uff0c\u53c2\u89c1 Distributed-computing\\Theory\\Abstraction \u7ae0\u8282\u3002","title":"High-level abstraction"},{"location":"Concurrent-computing/Concurrency-control/Transactional-memory/#higher#concurrency","text":"\u76f8\u6bd4\u4e8elow-level thread synchronization\uff0ctransactional memory\u80fd\u591f\u63d0\u4f9bhigher concurrency\uff0c\u8fd9\u5728: 1) wikipedia Transactional memory # Motivation 2) wikipedia Software transactional memory # Performance \u4e2d\u90fd\u6709\u63cf\u8ff0\u3002","title":"Higher concurrency"},{"location":"Concurrent-computing/Concurrency-control/Transactional-memory/#pessimistic#vs#optimistic","text":"Transactional memory\u662foptimistic concurrency control\uff0c\u800clock\u662fpessimistic\u3002","title":"Pessimistic VS optimistic"},{"location":"Concurrent-computing/Concurrency-control/Transactional-memory/#wikipedia#transactional#memory","text":"In computer science and engineering , transactional memory attempts to simplify concurrent programming by allowing a group of load and store instructions to execute in an atomic way. It is a concurrency control mechanism analogous to database transactions for controlling access to shared memory in concurrent computing . Transactional memory systems provide high-level abstraction as an alternative to low-level thread synchronization. This abstraction allows for coordination between concurrent reads and writes of shared data in parallel systems.[ 1] NOTE: \u5728\u8bed\u8a00\u5c42\u652f\u6301transactional memory\u7684programming language\uff0c\u5f80\u5f80\u4f1a\u63d0\u4f9b\u8bed\u6cd5\u7cd6\u6765\u8ba9programmer\u8f7b\u677e\u5730\u4f7f\u7528transactional memory\uff1b","title":"wikipedia Transactional memory"},{"location":"Concurrent-computing/Concurrency-control/Transactional-memory/#motivation","text":"Low level thread synchronization constructs such as locks are pessimistic . Transactional memory provides optimistic concurrency control by allowing threads to run in parallel with minimal interference. The goal of transactional memory systems is to transparently support regions of code marked as transactions by enforcing atomicity , consistency and isolation . NOTE: \u539f\u6587\u7b2c\u4e00\u6bb5\u4e3b\u8981\u4eceoptimistic\u548cpessimistic\u7684\u89d2\u5ea6\u6765\u8fdb\u884c\u8bf4\u660e\u3002 Transactional memory\u610f\u5473\u7740ACI(\u6ca1\u6709D\uff0c\u5728DBMS\u4e2dD\u662f\u6307 durability )\u3002 NOTE: \u539f\u6587\u7b2c\u4e8c\u6bb5\u4e3b\u8981\u4ecb\u7ecd\u7684transaction\u7684\u539f\u7406\u3002 With these constructs in place, transactional memory provides a high level programming abstraction by allowing programmers to enclose their methods within transactional blocks . Correct implementations ensure that data cannot be shared between threads without going through a transaction and produce a serializable outcome. For example, code can be written as: def transfer_money ( from_account , to_account , amount ): \"\"\"Transfer money from one account to another.\"\"\" with transaction (): from_account -= amount to_account += amount In the code, the block defined by \"transaction\" is guaranteed atomicity, consistency and isolation by the underlying transactional memory implementation and is transparent to the programmer. Although transactional memory programs cannot produce a deadlock, programs may still suffer from a livelock or resource starvation . For example, longer transactions may repeatedly revert(\u91cd\u590d) in response to multiple smaller transactions, wasting both time and energy.[ 2]","title":"Motivation"},{"location":"Concurrent-computing/Concurrency-control/Transactional-memory/#hardware#vs#software","text":"The abstraction of atomicity in transactional memory requires a hardware mechanism to detect conflicts and undo any changes made to shared data.[ 3] Software transactional memory provides transactional memory semantics in a software runtime library or the programming language,[ 9] and requires minimal hardware support (typically an atomic compare and swap operation, or equivalent). As the downside, software implementations usually come with a performance penalty, when compared to hardware solutions.","title":"Hardware vs. software"},{"location":"Concurrent-computing/Concurrency-control/Transactional-memory/#history","text":"As of GCC 4.7, an experimental library for transactional memory is available which utilizes a hybrid implementation. The PyPy variant of Python also introduces transactional memory to the language. NOTE: \u5173\u4e8eC++\u5bf9transactional memory\u7684\u652f\u6301\uff0c\u53c2\u89c1 C++\\Language-reference\\Basic-concept\\Abstract-machine\\Memory-model\\Transactional-memory \u7ae0\u8282\u3002","title":"History"},{"location":"Concurrent-computing/Concurrency-control/Transactional-memory/#wikipedia#software#transactional#memory","text":"In computer science , software transactional memory ( STM ) is a concurrency control mechanism analogous to database transactions for controlling access to shared memory in concurrent computing . It is an alternative to lock-based synchronization . STM is a strategy implemented in software, rather than as a hardware component.","title":"wikipedia Software transactional memory"},{"location":"Concurrent-computing/Concurrency-control/Transactional-memory/#performance","text":"Unlike the locking techniques used in most modern multithreaded applications, STM is often very optimistic :","title":"Performance"},{"location":"Concurrent-computing/Concurrency-control/Transactional-memory/#conceptual#advantages#and#disadvantages","text":"In addition to their performance benefits[ citation needed ], STM greatly simplifies conceptual understanding of multithreaded programs and helps make programs more maintainable by working in harmony with existing high-level abstractions such as objects and modules. Lock-based programming has a number of well-known problems that frequently arise in practice","title":"Conceptual advantages and disadvantages"},{"location":"Concurrent-computing/Concurrency-control/Transactional-memory/#implementations","text":"","title":"Implementations"},{"location":"Concurrent-computing/Concurrency-control/Transactional-memory/#cc","text":"G++ 4.7 now supports STM for C/C++ directly in the compiler. The feature is still listed as \"experimental\", but may still provide the necessary functionality for testing.","title":"C/C++"},{"location":"Concurrent-computing/Concurrent-data-structure/","text":"Concurrent data structure 1\u3001\u672c\u7ae0\u603b\u7ed3Concurrent data structure 2\u3001\u4ee5data structure\u4f5c\u4e3a\u5206\u7c7b\u6807\u51c6\uff0c\u800c\u4e0d\u662f\u4ee5lock free\u3001lock-based\u4f5c\u4e3a\u5206\u7c7b\u6807\u51c6 3\u3001\u672c\u7ae0\u6536\u5f55\u4e86\u4e00\u4e9blibrary\u3001implementation Wikipedia Concurrent data structure TODO drdobbs Andrei Alexandrescu Lock-Free Data Structures kukuruku: Lock-free Data Structures stackoverflow Is there a production ready lock-free queue or hash implementation in C++ [closed] http://amino-cbbs.sourceforge.net/ http://atomic-ptr-plus.sourceforge.net/ http://www.johantorp.com/","title":"Introduction"},{"location":"Concurrent-computing/Concurrent-data-structure/#concurrent#data#structure","text":"1\u3001\u672c\u7ae0\u603b\u7ed3Concurrent data structure 2\u3001\u4ee5data structure\u4f5c\u4e3a\u5206\u7c7b\u6807\u51c6\uff0c\u800c\u4e0d\u662f\u4ee5lock free\u3001lock-based\u4f5c\u4e3a\u5206\u7c7b\u6807\u51c6 3\u3001\u672c\u7ae0\u6536\u5f55\u4e86\u4e00\u4e9blibrary\u3001implementation","title":"Concurrent data structure"},{"location":"Concurrent-computing/Concurrent-data-structure/#wikipedia#concurrent#data#structure","text":"","title":"Wikipedia Concurrent data structure"},{"location":"Concurrent-computing/Concurrent-data-structure/#todo","text":"drdobbs Andrei Alexandrescu Lock-Free Data Structures kukuruku: Lock-free Data Structures stackoverflow Is there a production ready lock-free queue or hash implementation in C++ [closed] http://amino-cbbs.sourceforge.net/ http://atomic-ptr-plus.sourceforge.net/ http://www.johantorp.com/","title":"TODO"},{"location":"Concurrent-computing/Concurrent-data-structure/Collection-library/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u6536\u5f55\u4e86\u4e00\u4e9bcollection library\uff0c\u5373\u8fd9\u4e9blibrary\u56ca\u62ec\u4e86\u591a\u79cdconcurrent data structure\u7684\u5b9e\u73b0\u3002 \u5305\u62ec: 1\u3001 khizmax / libcds 2\u3001 Boost.Lockfree 3\u3001 https://github.com/concurrencykit/ck Concurrency primitives, safe memory reclamation mechanisms and non-blocking (including lock-free) data structures designed to aid in the research, design and implementation of high performance concurrent systems developed in C99+.","title":"Introduction"},{"location":"Concurrent-computing/Concurrent-data-structure/Collection-library/#_1","text":"\u672c\u7ae0\u6536\u5f55\u4e86\u4e00\u4e9bcollection library\uff0c\u5373\u8fd9\u4e9blibrary\u56ca\u62ec\u4e86\u591a\u79cdconcurrent data structure\u7684\u5b9e\u73b0\u3002 \u5305\u62ec: 1\u3001 khizmax / libcds 2\u3001 Boost.Lockfree 3\u3001 https://github.com/concurrencykit/ck Concurrency primitives, safe memory reclamation mechanisms and non-blocking (including lock-free) data structures designed to aid in the research, design and implementation of high performance concurrent systems developed in C99+.","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Concurrent-computing/Concurrent-data-structure/Collection-library/Library-libcds/","text":"khizmax / libcds NOTE: 1\u3001\u975e\u5e38\u9f50\u5168\uff0c\u5305\u542b\u591a\u79cddata structure 2\u3001\u6bd4\u8f83\u6210\u719f\uff0c\u7ed9\u51fa\u4e86implementation\u7684reference paper\uff0c\u53c2\u89c1 References \u6bb5 The Concurrent Data Structures (CDS) library is a collection of concurrent containers that don't require external (manual) synchronization for shared access, and safe memory reclamation (SMR) algorithms like Hazard Pointer and user-space RCU that is used as an epoch-based SMR. The library contains the implementations of the following containers: 1\u3001 lock-free stack with optional elimination(\u6d88\u9664) support 2\u3001several algo for lock-free queue, including classic Michael & Scott algorithm and its derivatives, the flat combining queue, the segmented queue. 3\u3001several implementation of unordered set/map - lock-free and fine-grained lock-based 4\u3001 flat-combining technique 5\u3001lock-free skip-list 6\u3001lock-free FeldmanHashMap/Set Multi-Level Array Hash with thread-safe bidirectional iterator support 7\u3001Bronson's et al algorithm for fine-grained lock-based AVL tree NOTE: 1\u3001\u4e0a\u8ff0\u5185\u5bb9\u5df2\u7ecf\u6dfb\u52a0\u4e86tag\uff0c\u4fbf\u4e8e\u88ab\u68c0\u7d22\u5230 Generally, each container has an intrusive and non-intrusive (STL-like) version belonging to cds::intrusive and cds::container namespace respectively. NOTE: 1\u3001\"intrusive and non-intrusive \" \u662f\u4ec0\u4e48\u542b\u4e49","title":"Introduction"},{"location":"Concurrent-computing/Concurrent-data-structure/Collection-library/Library-libcds/#khizmaxlibcds","text":"NOTE: 1\u3001\u975e\u5e38\u9f50\u5168\uff0c\u5305\u542b\u591a\u79cddata structure 2\u3001\u6bd4\u8f83\u6210\u719f\uff0c\u7ed9\u51fa\u4e86implementation\u7684reference paper\uff0c\u53c2\u89c1 References \u6bb5 The Concurrent Data Structures (CDS) library is a collection of concurrent containers that don't require external (manual) synchronization for shared access, and safe memory reclamation (SMR) algorithms like Hazard Pointer and user-space RCU that is used as an epoch-based SMR. The library contains the implementations of the following containers: 1\u3001 lock-free stack with optional elimination(\u6d88\u9664) support 2\u3001several algo for lock-free queue, including classic Michael & Scott algorithm and its derivatives, the flat combining queue, the segmented queue. 3\u3001several implementation of unordered set/map - lock-free and fine-grained lock-based 4\u3001 flat-combining technique 5\u3001lock-free skip-list 6\u3001lock-free FeldmanHashMap/Set Multi-Level Array Hash with thread-safe bidirectional iterator support 7\u3001Bronson's et al algorithm for fine-grained lock-based AVL tree NOTE: 1\u3001\u4e0a\u8ff0\u5185\u5bb9\u5df2\u7ecf\u6dfb\u52a0\u4e86tag\uff0c\u4fbf\u4e8e\u88ab\u68c0\u7d22\u5230 Generally, each container has an intrusive and non-intrusive (STL-like) version belonging to cds::intrusive and cds::container namespace respectively. NOTE: 1\u3001\"intrusive and non-intrusive \" \u662f\u4ec0\u4e48\u542b\u4e49","title":"khizmax/libcds"},{"location":"Concurrent-computing/Concurrent-data-structure/Course-Practical-lock-free-data-structures/","text":"cl.cam.ac Practical lock-free data structures Introduction Through careful design and implementation it's possible to build data structures that are safe for concurrent use without needing to manage locks or block threads. These non-blocking data structures can increase performance by allowing extra concurrency and can improve robustness by avoiding some of the problems caused by priority inversion in local settings, or machine and link failures in distributed systems. The best overall introduction to our non-blocking algorithms is the paper Concurrent programming without locks , currently under submission, which covers our designs for multi-word compare-and-swap, word-based software transactional memory and object-based software transactional memory. The papers Language support for lightweight transactions and Exceptions and side-effects in atomic blocks cover the integration of a software transactional memory with a managed run-time environment. Keir Fraser's dissertation, Practical lock freedom , presents a large number of new designs for concurrent data structures such as skip-lists, red-black trees and binary search trees, including new lock-based designs as well as lock-free versions. Source code Publications","title":"Introduction"},{"location":"Concurrent-computing/Concurrent-data-structure/Course-Practical-lock-free-data-structures/#clcamac#practical#lock-free#data#structures","text":"","title":"cl.cam.ac Practical lock-free data structures"},{"location":"Concurrent-computing/Concurrent-data-structure/Course-Practical-lock-free-data-structures/#introduction","text":"Through careful design and implementation it's possible to build data structures that are safe for concurrent use without needing to manage locks or block threads. These non-blocking data structures can increase performance by allowing extra concurrency and can improve robustness by avoiding some of the problems caused by priority inversion in local settings, or machine and link failures in distributed systems. The best overall introduction to our non-blocking algorithms is the paper Concurrent programming without locks , currently under submission, which covers our designs for multi-word compare-and-swap, word-based software transactional memory and object-based software transactional memory. The papers Language support for lightweight transactions and Exceptions and side-effects in atomic blocks cover the integration of a software transactional memory with a managed run-time environment. Keir Fraser's dissertation, Practical lock freedom , presents a large number of new designs for concurrent data structures such as skip-lists, red-black trees and binary search trees, including new lock-based designs as well as lock-free versions.","title":"Introduction"},{"location":"Concurrent-computing/Concurrent-data-structure/Course-Practical-lock-free-data-structures/#source#code","text":"","title":"Source code"},{"location":"Concurrent-computing/Concurrent-data-structure/Course-Practical-lock-free-data-structures/#publications","text":"","title":"Publications"},{"location":"Concurrent-computing/Concurrent-data-structure/Course-Practical-lock-free-data-structures/paper-Concurrent-Programming-Without-Locks/","text":"cl.cam.ac Concurrent Programming Without Locks NOTE: \u5728 stackoverflow Skip List vs. Binary Search Tree # A \u4e2d\uff0c\u63d0\u53ca\u4e86\u8fd9\u7bc7paper","title":"Introduction"},{"location":"Concurrent-computing/Concurrent-data-structure/Course-Practical-lock-free-data-structures/paper-Concurrent-Programming-Without-Locks/#clcamac#concurrent#programming#without#locks","text":"NOTE: \u5728 stackoverflow Skip List vs. Binary Search Tree # A \u4e2d\uff0c\u63d0\u53ca\u4e86\u8fd9\u7bc7paper","title":"cl.cam.ac Concurrent Programming Without Locks"},{"location":"Concurrent-computing/Concurrent-data-structure/Course-Practical-lock-free-data-structures/paper-Non-blocking-hashtables-with-open-addressing/","text":"cl.cam.ac Non-blocking hashtables with open addressing","title":"Introduction"},{"location":"Concurrent-computing/Concurrent-data-structure/Course-Practical-lock-free-data-structures/paper-Non-blocking-hashtables-with-open-addressing/#clcamac#non-blocking#hashtables#with#open#addressing","text":"","title":"cl.cam.ac Non-blocking hashtables with open addressing"},{"location":"Concurrent-computing/Concurrent-data-structure/Hash-table/","text":"Concurrent hash table \u53c2\u89c1: 1\u3001preshing New Concurrent Hash Maps for C++ preshing\u4e2d\u6709\u591a\u7bc7\u5173\u4e8eConcurrent hash table\u7684\u6587\u7ae0\uff0c\u503c\u5f97\u9605\u8bfb\u3002","title":"Introduction"},{"location":"Concurrent-computing/Concurrent-data-structure/Hash-table/#concurrent#hash#table","text":"\u53c2\u89c1: 1\u3001preshing New Concurrent Hash Maps for C++ preshing\u4e2d\u6709\u591a\u7bc7\u5173\u4e8eConcurrent hash table\u7684\u6587\u7ae0\uff0c\u503c\u5f97\u9605\u8bfb\u3002","title":"Concurrent hash table"},{"location":"Concurrent-computing/Concurrent-data-structure/Linked-list/","text":"Concurrent linked list \u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\uff0c\u8ba8\u8bba\u4e86Concurrent linked list\u7684implementation: 1\u3001\u5de5\u7a0b programming-language \u7684 Lock-Free-Programming-or-How-to-Juggle-Razor-Blades \u6bb5 Implementation github ConcLinkedList","title":"Introduction"},{"location":"Concurrent-computing/Concurrent-data-structure/Linked-list/#concurrent#linked#list","text":"\u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\uff0c\u8ba8\u8bba\u4e86Concurrent linked list\u7684implementation: 1\u3001\u5de5\u7a0b programming-language \u7684 Lock-Free-Programming-or-How-to-Juggle-Razor-Blades \u6bb5","title":"Concurrent linked list"},{"location":"Concurrent-computing/Concurrent-data-structure/Linked-list/#implementation","text":"","title":"Implementation"},{"location":"Concurrent-computing/Concurrent-data-structure/Linked-list/#github#conclinkedlist","text":"","title":"github ConcLinkedList"},{"location":"Concurrent-computing/Concurrent-data-structure/Queue/","text":"\u5173\u4e8e\u672c\u7ae0 1\u3001queue\u662f\u6700\u6700\u5e38\u89c1\u7684\u4e00\u79cd\u9700\u6c42\u3002 2\u3001\u5173\u4e8econcurrent queue\uff0c\u6211\u89c9\u5f97\u603b\u7ed3\u7684\u975e\u5e38\u597d\u7684\u6587\u7ae0\u662f: 1024cores Producer-Consumer Queues \u8fd9\u7bc7\u6587\u7ae0\u975e\u5e38\u7cfb\u7edf\u3002 Implementation \u4e0b\u9762\u6309\u7167\u4f18\u5148\u7ea7\u8fdb\u884c\u6392\u5e8f\u3002 cameron314 \u975e\u5e38\u6210\u719f\u7684\u5b9e\u73b0\u3002 1\u3001 readerwriterqueue A fast single-producer, single-consumer lock-free queue for C++ 2\u3001 concurrentqueue A fast multi-producer, multi-consumer lock-free concurrent queue for C++11 rigtorp \u8f83\u6210\u719f\u7684\u5b9e\u73b0\u3002 1\u3001 SPSCQueue A bounded single-producer single-consumer wait-free and lock-free queue written in C++11 2\u3001 MPMCQueue mstump queues A public domain lock free queues implemented in C++11 Public domain implementation of four different lock free queues: SPSC lock free dynamic queue which requires a memory allocation with each insert. MPSC lock free dynamic queue which requires a memory allocation with each insert. SPSC wait free bound queue/ring buffer which which uses a fixed size pre-allocated buffer. MPMC lock free bound queue/ring buffer which which uses a fixed size pre-allocated buffer. huangfcn lockfree","title":"Introduction"},{"location":"Concurrent-computing/Concurrent-data-structure/Queue/#_1","text":"1\u3001queue\u662f\u6700\u6700\u5e38\u89c1\u7684\u4e00\u79cd\u9700\u6c42\u3002 2\u3001\u5173\u4e8econcurrent queue\uff0c\u6211\u89c9\u5f97\u603b\u7ed3\u7684\u975e\u5e38\u597d\u7684\u6587\u7ae0\u662f: 1024cores Producer-Consumer Queues \u8fd9\u7bc7\u6587\u7ae0\u975e\u5e38\u7cfb\u7edf\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Concurrent-computing/Concurrent-data-structure/Queue/#implementation","text":"\u4e0b\u9762\u6309\u7167\u4f18\u5148\u7ea7\u8fdb\u884c\u6392\u5e8f\u3002","title":"Implementation"},{"location":"Concurrent-computing/Concurrent-data-structure/Queue/#cameron314","text":"\u975e\u5e38\u6210\u719f\u7684\u5b9e\u73b0\u3002 1\u3001 readerwriterqueue A fast single-producer, single-consumer lock-free queue for C++ 2\u3001 concurrentqueue A fast multi-producer, multi-consumer lock-free concurrent queue for C++11","title":"cameron314"},{"location":"Concurrent-computing/Concurrent-data-structure/Queue/#rigtorp","text":"\u8f83\u6210\u719f\u7684\u5b9e\u73b0\u3002 1\u3001 SPSCQueue A bounded single-producer single-consumer wait-free and lock-free queue written in C++11 2\u3001 MPMCQueue","title":"rigtorp"},{"location":"Concurrent-computing/Concurrent-data-structure/Queue/#mstump#queues","text":"A public domain lock free queues implemented in C++11 Public domain implementation of four different lock free queues: SPSC lock free dynamic queue which requires a memory allocation with each insert. MPSC lock free dynamic queue which requires a memory allocation with each insert. SPSC wait free bound queue/ring buffer which which uses a fixed size pre-allocated buffer. MPMC lock free bound queue/ring buffer which which uses a fixed size pre-allocated buffer.","title":"mstump queues"},{"location":"Concurrent-computing/Concurrent-data-structure/Queue/#huangfcn#lockfree","text":"","title":"huangfcn lockfree"},{"location":"Concurrent-computing/Concurrent-data-structure/Queue/Circular-queue/","text":"Lock-free circular array \u672c\u7ae0\u4ecb\u7ecd\u5982\u4f55\u5b9e\u73b0Lock-free circular array\u3002 Implementation 1\u3001spdlog 2\u3001boost Wait-free ring buffer 3\u3001cameron314 readerwriterqueue 4\u3001rigtorp SPSCQueue TODO cnblogs \u7406\u89e3 Memory barrier\uff08\u5185\u5b58\u5c4f\u969c\uff09\u65e0\u9501\u73af\u5f62\u961f\u5217","title":"Introduction"},{"location":"Concurrent-computing/Concurrent-data-structure/Queue/Circular-queue/#lock-free#circular#array","text":"\u672c\u7ae0\u4ecb\u7ecd\u5982\u4f55\u5b9e\u73b0Lock-free circular array\u3002","title":"Lock-free circular array"},{"location":"Concurrent-computing/Concurrent-data-structure/Queue/Circular-queue/#implementation","text":"1\u3001spdlog 2\u3001boost Wait-free ring buffer 3\u3001cameron314 readerwriterqueue 4\u3001rigtorp SPSCQueue","title":"Implementation"},{"location":"Concurrent-computing/Concurrent-data-structure/Queue/Circular-queue/#todo","text":"cnblogs \u7406\u89e3 Memory barrier\uff08\u5185\u5b58\u5c4f\u969c\uff09\u65e0\u9501\u73af\u5f62\u961f\u5217","title":"TODO"},{"location":"Concurrent-computing/Concurrent-data-structure/Queue/TODO-paper-obstruction-free%20synchronization-double-ended-queues/","text":"Obstruction-Free Synchronization: Double-Ended Queues as an Example NOTE: \u5728\u9605\u8bfb Home \u200e > \u200e Lockfree Algorithms \u200e > \u200e Introduction \u65f6\uff0c\u5176**Obstruction-freedom**\u6bb5\u4e2d\u7ed9\u51fa\u4e86\u94fe\u63a5\u5230\u672c\u7bc7\u8bba\u6587\u7684\u94fe\u63a5: I am unable to come up with a single example, so I refer you to the original paper Obstruction-Free Synchronization: Double-Ended Queues as an Example .","title":"Introduction"},{"location":"Concurrent-computing/Concurrent-data-structure/Queue/TODO-paper-obstruction-free%20synchronization-double-ended-queues/#obstruction-free#synchronization#double-ended#queues#as#an#example","text":"NOTE: \u5728\u9605\u8bfb Home \u200e > \u200e Lockfree Algorithms \u200e > \u200e Introduction \u65f6\uff0c\u5176**Obstruction-freedom**\u6bb5\u4e2d\u7ed9\u51fa\u4e86\u94fe\u63a5\u5230\u672c\u7bc7\u8bba\u6587\u7684\u94fe\u63a5: I am unable to come up with a single example, so I refer you to the original paper Obstruction-Free Synchronization: Double-Ended Queues as an Example .","title":"Obstruction-Free Synchronization: Double-Ended Queues as an Example"},{"location":"Concurrent-computing/Concurrent-data-structure/Queue/TODO-paper-wait-free-queue-with-multiple-enqueuers-and-dequeuers/","text":"Wait-Free Queues With Multiple Enqueuers and Dequeuers \u2217 NOTE: \u5728 wikipedia Non-blocking algorithm # Wait-freedom \u6bb5\u4e2d\u6709\u8fd9\u6837\u7684\u63cf\u8ff0: Wait-free algorithms were rare until 2011, both in research and in practice. However, in 2011 Kogan and Petrank [ 17] presented a wait-free queue building on the CAS primitive, generally available on common hardware. Their construction expanded the lock-free queue of Michael and Scott,[ 18] which is an efficient queue often used in practice. \u5176\u4e2d\u7684 Petrank [ 17] \u5c31\u662f\u94fe\u63a5\u7684\u8fd9\u7bc7\u8bba\u6587","title":"Introduction"},{"location":"Concurrent-computing/Concurrent-data-structure/Queue/TODO-paper-wait-free-queue-with-multiple-enqueuers-and-dequeuers/#wait-free#queues#with#multiple#enqueuers#and#dequeuers","text":"NOTE: \u5728 wikipedia Non-blocking algorithm # Wait-freedom \u6bb5\u4e2d\u6709\u8fd9\u6837\u7684\u63cf\u8ff0: Wait-free algorithms were rare until 2011, both in research and in practice. However, in 2011 Kogan and Petrank [ 17] presented a wait-free queue building on the CAS primitive, generally available on common hardware. Their construction expanded the lock-free queue of Michael and Scott,[ 18] which is an efficient queue often used in practice. \u5176\u4e2d\u7684 Petrank [ 17] \u5c31\u662f\u94fe\u63a5\u7684\u8fd9\u7bc7\u8bba\u6587","title":"Wait-Free Queues With Multiple Enqueuers and Dequeuers \u2217"},{"location":"Concurrent-computing/Concurrent-data-structure/TODO-simongui-Improving-performance-of-lockless-data-structures/","text":"simongui.github Improving performance of lockless data structures","title":"Introduction"},{"location":"Concurrent-computing/Concurrent-data-structure/TODO-simongui-Improving-performance-of-lockless-data-structures/#simonguigithub#improving#performance#of#lockless#data#structures","text":"","title":"simongui.github Improving performance of lockless data structures"},{"location":"Concurrent-computing/Course-tau-Multiprocessor-Programming-CS-0368-4061-01/","text":"tau Multiprocessor Programming Chapter 7 Spin Locks and Contention Management NOTE: 1\u3001\u4e0b\u8f7d\u4e86\uff0c\u8fd9\u4e2a\u5185\u5bb9\u8fd8\u4e0d\u9519 7.1 Introduction public interface Lock { void acquire (); void release (); } public class FAIRegister { private int value ; private Lock lock ; public FAIRegister () {} int read () { return value ; } void write ( int x ) { value = x ; } int fetchAndInc () { int oldValue ; lock . acquire (); oldValue = value ++ ; lock . release (); return oldValue ; } } public class TASRegister implements Register { private int value ; public synchronized int TAS () { int oldValue = value ; this . value = 1 ; return oldValue ; } public int read () { return value ; } public void write ( int x ) { value = x ; } } public class TASLock implements Lock { private TASRegister value = new TASRegister ( 0 ); public void acquire () { while ( value . TAS () == 1 ) {} } public void release () { value . write ( 0 ); } } public class TTASLock implements Lock { private TASRegister value = new TASRegister ( 0 ); public void acquire () { while ( true ) { while ( value . read () == 1 ) {} if ( value . TAS () == 0 ) return ; } } public void release () { value . write ( 0 ); } } 7.2 Multiprocessor Architectures Simplifying slightly, there are four kinds of basic architectures. 7.3 Cache Memory & Consistency Cache coherence protocols have two ways of dealing with modified data. A write-through coherence protocol immediately broadcasts the new value, so that both the memory and other processors\u2019 caches become aware of the new value. By contrast, Write-through protocols have the advantage that all cached copies of the data agree, but they have the disadvantage that every modification to a location requires bus traffic. Since the vast majority of all updates are to memory locations that are not shared among processors, write-through protocols are generally not used. A write-back coherence protocol sends out an invalidation message message when the value is first modified, instructing the other processors to discard that value from their caches. Once the processor has invalidated the other cached values, it can make subsequent modifications without further bus traffic. A value that has been modified in the cache but not written back is called dirty. If the processor needs to use the cache for another value, however, it must remember to write back any dirty values. Real cache coherence protocols can be very complex. For example, some protocols mix write-through and write-back strategies, some distinguish between exclusive and shared access, and almost all modern multiprocessors have multilevel caches, where each processor has an on-chip (L1) cache, and clusters of processors share an off-chip (L2) cache. Cache architecture is a fascinating subject in its own right, but we already know most of what we need to understand the relative performance of the two TAS-based lock implementations. 7.4 TAS-Based Spin Locks In the simple TASLock implementation, each TAS operation goes over the bus(\u4f1a\u7ecf\u8fc7bus). Since all of the waiting threads are continually using the bus, all threads, even those not waiting for the lock, end up having to wait to use the bus for their memory accesses. Even worse, the TAS operation invalidates all cached copies of the lock, so every spinning thread encounters a cache miss every time, and has to use the bus. When the thread holding the lock tries to release it, it may be delayed waiting to use the bus that is currently monopolized(\u5784\u65ad) by the spinners. No wonder the TASLock performs so poorly. NOTE: \u8fd9\u6bb5\u8bdd\u603b\u7ed3\u5730\u975e\u5e38\u597d: 1\u3001\u6bcf\u4e2a \" TAS operation\" \u90fd\u4f1a\u7ecf\u8fc7bus\uff0c\u56e0\u4e3a\u6240\u6709\u7684waiting thread(\u90fd\u5728\u4e0d\u505c\u5730\u6267\u884c \" TAS operation\" )\u90fd\u5728\u4e0d\u65ad\u4f7f\u7528bus\uff0c\u663e\u7136\u8fd9\u4f1a\u9020\u6210\u975e\u5e38\u4e25\u91cd\u7684**bus traffic**\u3001flood\uff0c\u53ca\u65f6\u6ca1\u6709\"waiting for the lock\"\u7684thread\uff0c\u4e5f\u4f1a\u53d7\u5f71\u54cd\u3002 2\u3001\u7531\u4e8e \" TAS operation\"\uff0c\u4f1a \"invalidates all cached copies of the lock\"\uff0c\u56e0\u6b64\uff0c\u6bcf\u4e2aspinning thread\u90fd\u4f1a \"encounters a cache miss every time\" 3\u3001\u6700\u540e\u4e00\u6bb5\u8bdd\u7684\u610f\u601d\u662f: \u5f53\u6301\u6709lock\u7684thread\u53bb\u91ca\u653e\u5b83\u7684\u65f6\u5019\uff0c\u5b83\u53ef\u80fd\u88ab\"delayed\"\uff0c\u56e0\u4e3a\u6b64\u65f6bus\u53ef\u80fd\u88ab\u5176\u4ed6\u7684spinner \"monopoliz\" Now consider the behavior of the TTASLock implementation while the lock is held by some thread. The first time a thread reads the lock it takes a cache miss and loads the value (which is 1) into its cache. As long as the lock is held, the thread repeatedly rereads the value, and each time it hits in its cache. While it is waiting, it produces no bus traffic , allowing other threads to get to memory unhindered by bus traffic, without hindering other processors, and allowing lock holder to release it without having to contest control of the bus from spinners. NOTE: 1\u3001\u4e0a\u9762\u603b\u7ed3\u4e86 TTASLock \u7684\u4f18\u52bf Things get worse, however, when the lock is released. The lock holder releases the lock by writing 0 to the lock variable, which immediately invalidates the spinners\u2019 cached copies. They each take a cache miss , load the new value, and all (more-or-less simultaneously) call TAS to grab the lock. The first to succeed invalidates the others, who then sequentially reread the value, causing a storm of bus traffic until the processors settle down once again to local spinning. NOTE: 1\u3001\u4e0a\u9762\u603b\u7ed3\u4e86 TTASLock \u7684\u52a3\u52bf This notion of local spinning , where threads repeatedly reread cached values instead of repeatedly using the bus, is an important principle critical to the design of efficient spin locks. 7.5 Introducing Delays 7.6 Delay After Release 7.6.1 Static Delays 7.6.2 Dynamic Delays 7.7 Delay After Every Lock Reference 7.7.1 Exponential Backoff 7.8 Queue Locks 7.8.1 An Array Based Queue Lock In Figure 7.14 we present the code of the A-lock . The idea is simple, keep an array and a tail counter which indexes into the array. The tail counter is accessed by a fetch-and-increment operation by every thread wishing to acquire the lock. Initially all entries in the queue except location 0 are set to WAIT . The tail counter is set to 0 and location 0 is set to ENTER . Each thread performs a fetch-and-increment on the tail , receiving the index of the slot it must wait on and incrementing the counter to point to the slot in which the next thread to arrive will spin on. It then spins on that slot in the array until a thread leaving the critical section sets it to ENTER . The lock improves on exponential backoff since it reduces invalidations to a minimum, provides FIFO ordering, and schedules access to the critical section tightly, minimizing the time from when it is freed by one thread to when it is reacquired by another. The fact that fetch-and-increment is not available on modern hardware is not a limitation since an efficient lock-free implementation of fetch-and-increment from compareand-swap is easy to implement. The A-lock\u2019s main limitation is that it is not clear how to cache the array slots in memory. In any case we need to allocated the full array per lock even if only a subset of threads will ever access it. NOTE: 1\u3001\u4e0a\u9762\u603b\u7ed3\u4e86A-lock\u7684\u4f18\u52bf\u3001\u52a3\u52bf public class alock implements lock { private RMWRegister tail = new RMWRegister ( 0 ); public void acquire () { myslot [ i ] = tail . fetchInc (); while ( flags [ myslot [ i ]] % n ) == WAIT ) {}; flags [ myslot [ i ] % n ] = WAIT ; } public void release () { flags [ myslot [ i ] + 1 % n ] = ENTER ; } } NOTE: 1\u3001 RMWRegister \u4e2d\u7684 RMW \u662f\u4ec0\u4e48\u542b\u4e49\uff1f\u5e94\u8be5\u662f \"Read\u2013modify\u2013write\" \u7684\u610f\u601d 7.8.2 The CLH queue lock The CLH lock improves on the A-lock by allocating the slots on which threads spin dynamically. Figure 7.15 shows a simplified version of the CLH Queue Lock. The lock is a virtual linked list of Qnode objects, each waiting to enter the critical section. We use the term \u201cvirtual\u201d since unlike conventional linked lists, it cannot be passively traversed because the pointers in the list are the threads\u2019 private pred variables. We will not deal here with recycling(\u5faa\u73af\u5229\u7528) of the Qnode objects though this can be done rather efficiently through simple modifications of the above code. NOTE: 1\u3001 pred \u662f \"predecessor(\u524d\u8f88\u3001\u524d\u9a71)\" \u7684\u7f29\u5199\uff0c\u56e0\u4e3a\u5728CLH queue \u4e2d\uff0c\u6bcf\u4e2anode\u6240\u5173\u6ce8\u7684\u662f\u5b83\u7684\"\u524d\u9a71\u8282\u70b9\" To acquire the lock, a thread creates a Qnode structure, sets its locked variable to indicate that it has not yet released the critical section, and atomically places its own Qnode at the tail of the list while finding the Qnode object of its predecessor. It then spins on the locked variable of its predecessor (if there is one), waiting until the predecessor(\u524d\u8f88\u3001\u524d\u9a71) sets the locked field of its Qnode to false. To release the lock, a thread sets the locked field of its Qnode to false . The key point here is that each thread spins on a distinct location, so when one thread releases its lock, it does not invalidate every waiting thread\u2019s cache, only the cache of its immediate successor. public class CLHLock { class Qnode { boolean locked = true ; } /** Queue points to an unowned Qnode with locked == false */ private RMWRegister queue = new RMWRegister ( new Qnode ()); /** Creates a new instance of TASLock */ public CLHLock () {} public void acquire ( Qnode mynode ) { /** mynode.locked == true */ Qnode pred = ( Qnode ) queue . swap ( mynode ); /** find predecessor */ while ( pred . locked ) {} } public void release ( Qnode mynode ) { mynode . locked = false ; /** can use pred as Qnode for next acquire */ } } 7.8.3 The MCS queue lock The CLH algorithm relies on the fact that a thread spins on a locally cached copy of its prececessor\u2019s Qnode , since the Qnode itself was created by another thread, and on a distributed memory machine will be local to that thread. What do we do however if the machine is distributed, that is, is a NUMA machine, and is non-coherent, so the cost of spinning on a remote location is high? The answer is the MCS queue lock algorithm. NOTE: \u4e00\u3001\u4ece\u4e0a\u9762\u8fd9\u6bb5\u8bdd\uff0c\u5176\u5b9e\u53ef\u4ee5\u603b\u7ed3\u51faMCS queue lock VS CLH queue lock\u7684\u672c\u8d28\u5dee\u5f02: 1\u3001CLH queue lock: \u6bcf\u4e2a thread spin\u7684\u662f\u5b83\u7684predecessor QNode \u7684 locked \u663e\u7136\uff0c\u7531\u4e8espin\u7684\u9891\u7387\u662f\u975e\u5e38\u9ad8\u7684\uff0c\u5982\u679c\u5b83\u7684predecessor\u548c\u5b83\u4e4b\u95f4\u7684\u901a\u4fe1\u6210\u672c\u662f\u9ad8\u7684\uff0c\u663e\u7136\u8fd9\u79cd\u65b9\u5f0f\u5c31\u4e0d\u9002\u5408\u4e86 2\u3001MCS queue lock: \u6bcf\u4e2a thread spin\u7684\u662f\u5b83\u81ea\u5df1\u7684 QNode \u7684 locked \u663e\u7136\uff0c\u53ca\u65f6\u5b83\u7684predecessor\u548c\u5b83\u4e4b\u95f4\u7684\u901a\u4fe1\u6210\u672c\u662f\u9ad8\u7684\uff0c\u8fd9\u79cd\u65b9\u5f0f\u4e5f\u80fd\u591fhold\u4f4f The MCS Queue Lock (shown in Figure 7.16) implements a queue of processors in a linked list waiting to enter the critical section. Like CLH the lock is linked-list of Qnode objects, where each Qnode represents either a lock holder or a thread waiting to acquire the lock. This is not a virtual list however. To acquire the lock, a thread creates a Qnode structure, and places its own Qnode atomically at the head of the list. If it has a predecessor it directs a pointer from its predecessor to its Qnode so that its predecessor can locate the Qnode . It then spins on a local locked field in its own Qnode waiting until its predecessor sets this field to false . To release the lock, a thread locates its predecessor and sets the locked field of its predecessor\u2019s Qnode to false . The key point here is that each thread spins on a distinct local object, so the cost of spinning is low. NOTE: \u4e00\u3001\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7684\"predecessor\"\u7684\u542b\u4e49\u548c\u6211\u4eec\u5e73\u65f6\u6240\u8bf4\u7684linked list\u7684predecessor\u7684\u542b\u4e49\u662f\u4e0d\u540c\u7684 1\u3001linked list\u4e2d\u7684predecessor\u6307\u7684\u662f\u524d\u9a71\u8282\u70b9\uff0c\u524d\u9a71\u8282\u70b9\u7684next\u6307\u9488\u6307\u5411current node 2\u3001\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7684\"predecessor\"\u6307\u7684\u662f\u66f4\u65e9\u8fdb\u5165MCS queue\u7684 \u4e8c\u3001\"To acquire the lock, a thread creates a Qnode structure, and places its own Qnode atomically at the head of the list.\" \u8fd9\u6bb5\u8bdd\u4e2d\u7684\"at the head of the list\"\u662f\u6709\u8bef\u7684\uff0c\u5176\u5b9e\u662ftail of the queue The relative performance of the queue lock on a cacheless NUMA machine are shown in Figure 7.17. This is an architecture from the 1980s. As can be seen, MCS outperforms all other techniques. However, the gap between exponential backoff and the MCS lock is small. The relative performance of the queue lock on a state-of-art cache coherent NUMA machine are shown in Figure 7.17. This is an architecture from the As can be seen, exponential backoff on a modern NUMA machine is not a reasonable option since it does not scale, and MCS starts to outperform it from 20 processors and on. public class MCSLock { class Qnode { boolean locked = false ; Qnode next = null ; } private RMWRegister queue = new RMWRegister ( new Qnode ()); /** Creates a new instance of TASLock */ public MCSLock () {} public void acquire ( Qnode mynode ) { Qnode pred = ( Qnode ) queue . swap ( mynode ); if ( pred != null ) { mynode . locked = true ; pred . next = mynode ; while ( mynode . locked ) {} } } public void release ( Qnode mynode ) { if ( mynode . next == null ) { if ( queue . CAS ( mynode , null )) return ; while ( mynode . next == null ) {} } mynode . next . locked = false ; } } 7.9 Locking in Java 7.10 Chapter Notes","title":"Introduction"},{"location":"Concurrent-computing/Course-tau-Multiprocessor-Programming-CS-0368-4061-01/#tau#multiprocessor#programming","text":"","title":"tau Multiprocessor Programming"},{"location":"Concurrent-computing/Course-tau-Multiprocessor-Programming-CS-0368-4061-01/#chapter#7#spin#locks#and#contention#management","text":"NOTE: 1\u3001\u4e0b\u8f7d\u4e86\uff0c\u8fd9\u4e2a\u5185\u5bb9\u8fd8\u4e0d\u9519","title":"Chapter 7 Spin Locks and Contention Management"},{"location":"Concurrent-computing/Course-tau-Multiprocessor-Programming-CS-0368-4061-01/#71#introduction","text":"public interface Lock { void acquire (); void release (); } public class FAIRegister { private int value ; private Lock lock ; public FAIRegister () {} int read () { return value ; } void write ( int x ) { value = x ; } int fetchAndInc () { int oldValue ; lock . acquire (); oldValue = value ++ ; lock . release (); return oldValue ; } } public class TASRegister implements Register { private int value ; public synchronized int TAS () { int oldValue = value ; this . value = 1 ; return oldValue ; } public int read () { return value ; } public void write ( int x ) { value = x ; } } public class TASLock implements Lock { private TASRegister value = new TASRegister ( 0 ); public void acquire () { while ( value . TAS () == 1 ) {} } public void release () { value . write ( 0 ); } } public class TTASLock implements Lock { private TASRegister value = new TASRegister ( 0 ); public void acquire () { while ( true ) { while ( value . read () == 1 ) {} if ( value . TAS () == 0 ) return ; } } public void release () { value . write ( 0 ); } }","title":"7.1 Introduction"},{"location":"Concurrent-computing/Course-tau-Multiprocessor-Programming-CS-0368-4061-01/#72#multiprocessor#architectures","text":"Simplifying slightly, there are four kinds of basic architectures.","title":"7.2 Multiprocessor Architectures"},{"location":"Concurrent-computing/Course-tau-Multiprocessor-Programming-CS-0368-4061-01/#73#cache#memory#consistency","text":"Cache coherence protocols have two ways of dealing with modified data. A write-through coherence protocol immediately broadcasts the new value, so that both the memory and other processors\u2019 caches become aware of the new value. By contrast, Write-through protocols have the advantage that all cached copies of the data agree, but they have the disadvantage that every modification to a location requires bus traffic. Since the vast majority of all updates are to memory locations that are not shared among processors, write-through protocols are generally not used. A write-back coherence protocol sends out an invalidation message message when the value is first modified, instructing the other processors to discard that value from their caches. Once the processor has invalidated the other cached values, it can make subsequent modifications without further bus traffic. A value that has been modified in the cache but not written back is called dirty. If the processor needs to use the cache for another value, however, it must remember to write back any dirty values. Real cache coherence protocols can be very complex. For example, some protocols mix write-through and write-back strategies, some distinguish between exclusive and shared access, and almost all modern multiprocessors have multilevel caches, where each processor has an on-chip (L1) cache, and clusters of processors share an off-chip (L2) cache. Cache architecture is a fascinating subject in its own right, but we already know most of what we need to understand the relative performance of the two TAS-based lock implementations.","title":"7.3 Cache Memory &amp; Consistency"},{"location":"Concurrent-computing/Course-tau-Multiprocessor-Programming-CS-0368-4061-01/#74#tas-based#spin#locks","text":"In the simple TASLock implementation, each TAS operation goes over the bus(\u4f1a\u7ecf\u8fc7bus). Since all of the waiting threads are continually using the bus, all threads, even those not waiting for the lock, end up having to wait to use the bus for their memory accesses. Even worse, the TAS operation invalidates all cached copies of the lock, so every spinning thread encounters a cache miss every time, and has to use the bus. When the thread holding the lock tries to release it, it may be delayed waiting to use the bus that is currently monopolized(\u5784\u65ad) by the spinners. No wonder the TASLock performs so poorly. NOTE: \u8fd9\u6bb5\u8bdd\u603b\u7ed3\u5730\u975e\u5e38\u597d: 1\u3001\u6bcf\u4e2a \" TAS operation\" \u90fd\u4f1a\u7ecf\u8fc7bus\uff0c\u56e0\u4e3a\u6240\u6709\u7684waiting thread(\u90fd\u5728\u4e0d\u505c\u5730\u6267\u884c \" TAS operation\" )\u90fd\u5728\u4e0d\u65ad\u4f7f\u7528bus\uff0c\u663e\u7136\u8fd9\u4f1a\u9020\u6210\u975e\u5e38\u4e25\u91cd\u7684**bus traffic**\u3001flood\uff0c\u53ca\u65f6\u6ca1\u6709\"waiting for the lock\"\u7684thread\uff0c\u4e5f\u4f1a\u53d7\u5f71\u54cd\u3002 2\u3001\u7531\u4e8e \" TAS operation\"\uff0c\u4f1a \"invalidates all cached copies of the lock\"\uff0c\u56e0\u6b64\uff0c\u6bcf\u4e2aspinning thread\u90fd\u4f1a \"encounters a cache miss every time\" 3\u3001\u6700\u540e\u4e00\u6bb5\u8bdd\u7684\u610f\u601d\u662f: \u5f53\u6301\u6709lock\u7684thread\u53bb\u91ca\u653e\u5b83\u7684\u65f6\u5019\uff0c\u5b83\u53ef\u80fd\u88ab\"delayed\"\uff0c\u56e0\u4e3a\u6b64\u65f6bus\u53ef\u80fd\u88ab\u5176\u4ed6\u7684spinner \"monopoliz\" Now consider the behavior of the TTASLock implementation while the lock is held by some thread. The first time a thread reads the lock it takes a cache miss and loads the value (which is 1) into its cache. As long as the lock is held, the thread repeatedly rereads the value, and each time it hits in its cache. While it is waiting, it produces no bus traffic , allowing other threads to get to memory unhindered by bus traffic, without hindering other processors, and allowing lock holder to release it without having to contest control of the bus from spinners. NOTE: 1\u3001\u4e0a\u9762\u603b\u7ed3\u4e86 TTASLock \u7684\u4f18\u52bf Things get worse, however, when the lock is released. The lock holder releases the lock by writing 0 to the lock variable, which immediately invalidates the spinners\u2019 cached copies. They each take a cache miss , load the new value, and all (more-or-less simultaneously) call TAS to grab the lock. The first to succeed invalidates the others, who then sequentially reread the value, causing a storm of bus traffic until the processors settle down once again to local spinning. NOTE: 1\u3001\u4e0a\u9762\u603b\u7ed3\u4e86 TTASLock \u7684\u52a3\u52bf This notion of local spinning , where threads repeatedly reread cached values instead of repeatedly using the bus, is an important principle critical to the design of efficient spin locks.","title":"7.4 TAS-Based Spin Locks"},{"location":"Concurrent-computing/Course-tau-Multiprocessor-Programming-CS-0368-4061-01/#75#introducing#delays","text":"","title":"7.5 Introducing Delays"},{"location":"Concurrent-computing/Course-tau-Multiprocessor-Programming-CS-0368-4061-01/#76#delay#after#release","text":"","title":"7.6 Delay After Release"},{"location":"Concurrent-computing/Course-tau-Multiprocessor-Programming-CS-0368-4061-01/#761#static#delays","text":"","title":"7.6.1 Static Delays"},{"location":"Concurrent-computing/Course-tau-Multiprocessor-Programming-CS-0368-4061-01/#762#dynamic#delays","text":"","title":"7.6.2 Dynamic Delays"},{"location":"Concurrent-computing/Course-tau-Multiprocessor-Programming-CS-0368-4061-01/#77#delay#after#every#lock#reference","text":"","title":"7.7 Delay After Every Lock Reference"},{"location":"Concurrent-computing/Course-tau-Multiprocessor-Programming-CS-0368-4061-01/#771#exponential#backoff","text":"","title":"7.7.1 Exponential Backoff"},{"location":"Concurrent-computing/Course-tau-Multiprocessor-Programming-CS-0368-4061-01/#78#queue#locks","text":"","title":"7.8 Queue Locks"},{"location":"Concurrent-computing/Course-tau-Multiprocessor-Programming-CS-0368-4061-01/#781#an#array#based#queue#lock","text":"In Figure 7.14 we present the code of the A-lock . The idea is simple, keep an array and a tail counter which indexes into the array. The tail counter is accessed by a fetch-and-increment operation by every thread wishing to acquire the lock. Initially all entries in the queue except location 0 are set to WAIT . The tail counter is set to 0 and location 0 is set to ENTER . Each thread performs a fetch-and-increment on the tail , receiving the index of the slot it must wait on and incrementing the counter to point to the slot in which the next thread to arrive will spin on. It then spins on that slot in the array until a thread leaving the critical section sets it to ENTER . The lock improves on exponential backoff since it reduces invalidations to a minimum, provides FIFO ordering, and schedules access to the critical section tightly, minimizing the time from when it is freed by one thread to when it is reacquired by another. The fact that fetch-and-increment is not available on modern hardware is not a limitation since an efficient lock-free implementation of fetch-and-increment from compareand-swap is easy to implement. The A-lock\u2019s main limitation is that it is not clear how to cache the array slots in memory. In any case we need to allocated the full array per lock even if only a subset of threads will ever access it. NOTE: 1\u3001\u4e0a\u9762\u603b\u7ed3\u4e86A-lock\u7684\u4f18\u52bf\u3001\u52a3\u52bf public class alock implements lock { private RMWRegister tail = new RMWRegister ( 0 ); public void acquire () { myslot [ i ] = tail . fetchInc (); while ( flags [ myslot [ i ]] % n ) == WAIT ) {}; flags [ myslot [ i ] % n ] = WAIT ; } public void release () { flags [ myslot [ i ] + 1 % n ] = ENTER ; } } NOTE: 1\u3001 RMWRegister \u4e2d\u7684 RMW \u662f\u4ec0\u4e48\u542b\u4e49\uff1f\u5e94\u8be5\u662f \"Read\u2013modify\u2013write\" \u7684\u610f\u601d","title":"7.8.1 An Array Based Queue Lock"},{"location":"Concurrent-computing/Course-tau-Multiprocessor-Programming-CS-0368-4061-01/#782#the#clh#queue#lock","text":"The CLH lock improves on the A-lock by allocating the slots on which threads spin dynamically. Figure 7.15 shows a simplified version of the CLH Queue Lock. The lock is a virtual linked list of Qnode objects, each waiting to enter the critical section. We use the term \u201cvirtual\u201d since unlike conventional linked lists, it cannot be passively traversed because the pointers in the list are the threads\u2019 private pred variables. We will not deal here with recycling(\u5faa\u73af\u5229\u7528) of the Qnode objects though this can be done rather efficiently through simple modifications of the above code. NOTE: 1\u3001 pred \u662f \"predecessor(\u524d\u8f88\u3001\u524d\u9a71)\" \u7684\u7f29\u5199\uff0c\u56e0\u4e3a\u5728CLH queue \u4e2d\uff0c\u6bcf\u4e2anode\u6240\u5173\u6ce8\u7684\u662f\u5b83\u7684\"\u524d\u9a71\u8282\u70b9\" To acquire the lock, a thread creates a Qnode structure, sets its locked variable to indicate that it has not yet released the critical section, and atomically places its own Qnode at the tail of the list while finding the Qnode object of its predecessor. It then spins on the locked variable of its predecessor (if there is one), waiting until the predecessor(\u524d\u8f88\u3001\u524d\u9a71) sets the locked field of its Qnode to false. To release the lock, a thread sets the locked field of its Qnode to false . The key point here is that each thread spins on a distinct location, so when one thread releases its lock, it does not invalidate every waiting thread\u2019s cache, only the cache of its immediate successor. public class CLHLock { class Qnode { boolean locked = true ; } /** Queue points to an unowned Qnode with locked == false */ private RMWRegister queue = new RMWRegister ( new Qnode ()); /** Creates a new instance of TASLock */ public CLHLock () {} public void acquire ( Qnode mynode ) { /** mynode.locked == true */ Qnode pred = ( Qnode ) queue . swap ( mynode ); /** find predecessor */ while ( pred . locked ) {} } public void release ( Qnode mynode ) { mynode . locked = false ; /** can use pred as Qnode for next acquire */ } }","title":"7.8.2 The CLH queue lock"},{"location":"Concurrent-computing/Course-tau-Multiprocessor-Programming-CS-0368-4061-01/#783#the#mcs#queue#lock","text":"The CLH algorithm relies on the fact that a thread spins on a locally cached copy of its prececessor\u2019s Qnode , since the Qnode itself was created by another thread, and on a distributed memory machine will be local to that thread. What do we do however if the machine is distributed, that is, is a NUMA machine, and is non-coherent, so the cost of spinning on a remote location is high? The answer is the MCS queue lock algorithm. NOTE: \u4e00\u3001\u4ece\u4e0a\u9762\u8fd9\u6bb5\u8bdd\uff0c\u5176\u5b9e\u53ef\u4ee5\u603b\u7ed3\u51faMCS queue lock VS CLH queue lock\u7684\u672c\u8d28\u5dee\u5f02: 1\u3001CLH queue lock: \u6bcf\u4e2a thread spin\u7684\u662f\u5b83\u7684predecessor QNode \u7684 locked \u663e\u7136\uff0c\u7531\u4e8espin\u7684\u9891\u7387\u662f\u975e\u5e38\u9ad8\u7684\uff0c\u5982\u679c\u5b83\u7684predecessor\u548c\u5b83\u4e4b\u95f4\u7684\u901a\u4fe1\u6210\u672c\u662f\u9ad8\u7684\uff0c\u663e\u7136\u8fd9\u79cd\u65b9\u5f0f\u5c31\u4e0d\u9002\u5408\u4e86 2\u3001MCS queue lock: \u6bcf\u4e2a thread spin\u7684\u662f\u5b83\u81ea\u5df1\u7684 QNode \u7684 locked \u663e\u7136\uff0c\u53ca\u65f6\u5b83\u7684predecessor\u548c\u5b83\u4e4b\u95f4\u7684\u901a\u4fe1\u6210\u672c\u662f\u9ad8\u7684\uff0c\u8fd9\u79cd\u65b9\u5f0f\u4e5f\u80fd\u591fhold\u4f4f The MCS Queue Lock (shown in Figure 7.16) implements a queue of processors in a linked list waiting to enter the critical section. Like CLH the lock is linked-list of Qnode objects, where each Qnode represents either a lock holder or a thread waiting to acquire the lock. This is not a virtual list however. To acquire the lock, a thread creates a Qnode structure, and places its own Qnode atomically at the head of the list. If it has a predecessor it directs a pointer from its predecessor to its Qnode so that its predecessor can locate the Qnode . It then spins on a local locked field in its own Qnode waiting until its predecessor sets this field to false . To release the lock, a thread locates its predecessor and sets the locked field of its predecessor\u2019s Qnode to false . The key point here is that each thread spins on a distinct local object, so the cost of spinning is low. NOTE: \u4e00\u3001\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7684\"predecessor\"\u7684\u542b\u4e49\u548c\u6211\u4eec\u5e73\u65f6\u6240\u8bf4\u7684linked list\u7684predecessor\u7684\u542b\u4e49\u662f\u4e0d\u540c\u7684 1\u3001linked list\u4e2d\u7684predecessor\u6307\u7684\u662f\u524d\u9a71\u8282\u70b9\uff0c\u524d\u9a71\u8282\u70b9\u7684next\u6307\u9488\u6307\u5411current node 2\u3001\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7684\"predecessor\"\u6307\u7684\u662f\u66f4\u65e9\u8fdb\u5165MCS queue\u7684 \u4e8c\u3001\"To acquire the lock, a thread creates a Qnode structure, and places its own Qnode atomically at the head of the list.\" \u8fd9\u6bb5\u8bdd\u4e2d\u7684\"at the head of the list\"\u662f\u6709\u8bef\u7684\uff0c\u5176\u5b9e\u662ftail of the queue The relative performance of the queue lock on a cacheless NUMA machine are shown in Figure 7.17. This is an architecture from the 1980s. As can be seen, MCS outperforms all other techniques. However, the gap between exponential backoff and the MCS lock is small. The relative performance of the queue lock on a state-of-art cache coherent NUMA machine are shown in Figure 7.17. This is an architecture from the As can be seen, exponential backoff on a modern NUMA machine is not a reasonable option since it does not scale, and MCS starts to outperform it from 20 processors and on. public class MCSLock { class Qnode { boolean locked = false ; Qnode next = null ; } private RMWRegister queue = new RMWRegister ( new Qnode ()); /** Creates a new instance of TASLock */ public MCSLock () {} public void acquire ( Qnode mynode ) { Qnode pred = ( Qnode ) queue . swap ( mynode ); if ( pred != null ) { mynode . locked = true ; pred . next = mynode ; while ( mynode . locked ) {} } } public void release ( Qnode mynode ) { if ( mynode . next == null ) { if ( queue . CAS ( mynode , null )) return ; while ( mynode . next == null ) {} } mynode . next . locked = false ; } }","title":"7.8.3 The MCS queue lock"},{"location":"Concurrent-computing/Course-tau-Multiprocessor-Programming-CS-0368-4061-01/#79#locking#in#java","text":"","title":"7.9 Locking in Java"},{"location":"Concurrent-computing/Course-tau-Multiprocessor-Programming-CS-0368-4061-01/#710#chapter#notes","text":"","title":"7.10 Chapter Notes"},{"location":"Concurrent-computing/Course-uhcl-Concurrency/","text":"sceweb.uhcl.edu Concepts: Concurrency NOTE: 1\u3001\u5728\u641c\u7d22 \"Mailbox golang\" \u65f6\uff0c\u641c\u7d22\u5230\u7684\uff0c\u5176\u4e2d\u5173\u4e8e\"Active Object Model\"\u7684\u4ecb\u7ecd\uff0c\u5f15\u8d77\u4e86\u6211\u7684\u6ce8\u610f The Active Object Model Objects with their own threads of control are called \u201cactive objects\u201d. In order to support asynchronous communication with other active objects, each active object is provided with a message queue or \u201cmailbox.\u201d When an object is created, the environment gives it its own thread of control, which the object encapsulates until it dies. Like a passive object, the active object is idle until the arrival of a message from outside. The object executes whatever code is appropriate to process the message. Any messages which arrive while the object is busy are enqueued in the mailbox. When the object completes the processing of a message, it returns to pick up the next waiting message in the mailbox, or waits for one to arrive. Good candidates for active objects in the elevator system include the elevators themselves, the call stations on each floor, and the dispatcher.","title":"Introduction"},{"location":"Concurrent-computing/Course-uhcl-Concurrency/#scewebuhcledu#concepts#concurrency","text":"NOTE: 1\u3001\u5728\u641c\u7d22 \"Mailbox golang\" \u65f6\uff0c\u641c\u7d22\u5230\u7684\uff0c\u5176\u4e2d\u5173\u4e8e\"Active Object Model\"\u7684\u4ecb\u7ecd\uff0c\u5f15\u8d77\u4e86\u6211\u7684\u6ce8\u610f","title":"sceweb.uhcl.edu Concepts:  Concurrency"},{"location":"Concurrent-computing/Course-uhcl-Concurrency/#the#active#object#model","text":"Objects with their own threads of control are called \u201cactive objects\u201d. In order to support asynchronous communication with other active objects, each active object is provided with a message queue or \u201cmailbox.\u201d When an object is created, the environment gives it its own thread of control, which the object encapsulates until it dies. Like a passive object, the active object is idle until the arrival of a message from outside. The object executes whatever code is appropriate to process the message. Any messages which arrive while the object is busy are enqueued in the mailbox. When the object completes the processing of a message, it returns to pick up the next waiting message in the mailbox, or waits for one to arrive. Good candidates for active objects in the elevator system include the elevators themselves, the call stations on each floor, and the dispatcher.","title":"The Active Object Model"},{"location":"Concurrent-computing/Design-and-optimize/","text":"Design and optimize 1\u3001Optimize\u7684\u76ee\u7684\u662f\u63d0\u9ad8**\u5e76\u53d1**\uff0c\u6700\u7ec8\u76ee\u7684\u662f\u63d0\u4f9bperformance\u3001\u9075\u5faaoptimization principle\u3002 2\u3001\u4ee5Herb Sutter\u7684\"Effective concurrency\"\u7cfb\u5217\u6587\u7ae0\u4e3a\u84dd\u672c\u6765\u8fdb\u884c\u603b\u7ed3 3\u3001\u9700\u8981\u57fa\u4e8e hardware CPU \u7684\u8fd0\u884c\u673a\u5236\u6765\u8fdb\u884c\u4f18\u5316 \u6848\u4f8b jemalloc NOTE: 1\u3001\u53c2\u89c1\u5de5\u7a0b programming-language \u7684 library-jemalloc \u7ae0\u8282 NOTE: 1\u3001\u5982\u4f55\u964d\u4f4elock contention \uff0c\u8fd9\u975e\u5e38\u91cd\u8981 2\u3001\u4ece\u4e0b\u9762\u7684\u63cf\u8ff0\u6765\u770b\uff0cLarson and Krishnan (1998) \u7684\u601d\u60f3\u662f: \"divide to reduce lock granularity\"\uff0c\u5373\"They tried pushing locks down in their allocator, so that rather than using a single allocator lock, each free list had its own lock\"\uff0c\u663e\u7136\uff0c\u8fd9\u964d\u4f4e\u4e86lock granularity\u3002 3\u3001jemalloc \u501f\u9274\u4e86Larson and Krishnan (1998) \u7684\"multiple arenas\"\u7b56\u7565\uff0c\u4f46\u662f\"uses a more reliable mechanism than hashing for assignment of threads to arenas\"\uff0c\u901a\u8fc7\u540e\u6587\u53ef\u77e5\uff0c\u662f\"round-robin\" \u5728\u4e0b\u9762\u7684\u56fe\u4e2d\uff0c\u89e3\u91ca\u4e86\"hashing of the thread identifiers\"\u7684\u52a3\u52bf\uff0c\u7b80\u5355\u800c\u8a00: \u4f2a\u968f\u673a\u8fc7\u7a0b\uff0c\u5e76\u4e0d\u80fd\u591f\u4fdd\u8bc1\u5747\u7b49\uff0c\u56e0\u6b64\u65e0\u6cd5\u4fdd\u8bc1load balance\uff08\u6b64\u5904\u4f7f\u7528load balance\u662f\u4e0d\u51c6\u786e\u7684\uff09 One of the main goals for this allocator was to reduce lock contention for multi-threaded applications running on multi-processor systems. Larson and Krishnan (1998) did an excellent job of presenting and testing strategies. They tried pushing locks down in their allocator, so that rather than using a single allocator lock, each free list had its own lock. This helped some, but did not scale adequately, despite minimal lock contention. They attributed this to \u201ccache sloshing\u201d \u2013 the quick migration of cached data among processors during the manipulation of allocator data structures. Their solution was to use multiple arenas for allocation, and assign threads to arenas via hashing of the thread identifiers (Figure 2). This works quite well, and has since been used by other implementations (Berger et al., 2000; Bonwick and Adams, 2001). jemalloc uses multiple arenas, but uses a more reliable mechanism than hashing for assignment of threads to arenas. Multiple processors : Use four times as many arenas as there are processors. By assigning threads to a set of arenas, the probability of a single arena being used concurrently decreases. NOTE: 1\u3001\u901a\u8fc7\u5206\u89e3\u6765\u964d\u4f4e\u9501\u7c92\u5ea6\uff0c\u51cf\u5c11\u7ade\u4e89\uff0c\u63d0\u9ad8\u5e76\u53d1\u6027 TCMalloc : Thread-Caching Malloc 1\u3001Fast, uncontended allocation and deallocation for most objects. Objects are cached, depending on mode, either per-thread, or per-logical-CPU. Most allocations do not need to take locks, so there is low contention and good scaling for multi-threaded applications. DB \u884c\u7ea7\u9501\u3001\u8868\u7ea7\u9501 Scalable concurrency\u3001 re-enable free lunch\u3001O(n) scalability Herb Sutter 2005\u5e74\u53d1\u8868\u7684 The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software \u6587\u7ae0\uff0c\u5176\u4e2d\u603b\u7ed3\u4e86CPU\u7684\u53d1\u5c55\u65b9\u5411\uff0c\u5bf9software\u7684\u5f71\u54cd\uff0cprogrammer\u8981\u5982\u4f55\u505a\u3002 \u5728 How-Much-Scalability-Do-You-Have-or-Need \u4e2d\uff0c\u63d0\u51fa\u4e86\u5982\u4e0b\u6838\u5fc3\u89c2\u70b9: 1\u3001scale well to multiple core CPU 2\u3001\u80fd\u591f\u5145\u5206\u53d1\u6325hardware\u7684computation power\uff0c\u7406\u60f3\u60c5\u51b5\u662f\u7ebf\u6027\u589e\u52a0: software\u7684\u5904\u7406\u80fd\u529b\u548c\u6838\u6570\u5448\u7ebf\u6027\u5173\u7cfb 3\u3001\u4f5c\u4e3asoftware engineer\uff0c\u9700\u8981\u5f00\u53d1\u51fa\u5177\u6709scalability\u7684software\uff0c\u8fd9\u6837\u624d\u80fd\u591fre-enable the free lunch \u67e5\u8be2\u6838\u6570\u6765\u5b9e\u73b0O(n) scalability \u4e3a\u4e86\u5b9e\u73b0O(n) scalability\uff0c\u53ef\u80fd\u9700\u8981\u67e5\u8be2\u6838\u6570\uff0c\u4e0b\u9762\u662f\u4e00\u4e9b\u6848\u4f8b: 1\u3001jemalloc Pipeline \u53c2\u89c1 How-Much-Scalability-Do-You-Have-or-Need \u7ae0\u8282\u3002 Read and write tradeoff/\u6839\u636eread and write\u6765\u9009\u62e9 \u5728multiple model\u4e2d\uff0c\u5bf9\u4e8eshared data\u7684read\u3001write\u9700\u8981\u8fdb\u884cConcurrency control\uff0c\u5728\u672c\u7ae0\u603b\u7ed3\u4e86\u591a\u79cdconcurrency control\u6280\u672f\uff0c\u5728\u9009\u62e9\u8fd9\u4e9b\u6280\u672f\u7684\u65f6\u5019\uff0c\u975e\u5e38\u91cd\u8981\u7684\u4e00\u70b9\u662f: read and write tradeoff\u3002 \u6709\u4e9b\u6280\u672f\u5728read\u8fdc\u591a\u4e8ewrite\u7684\u65f6\u5019\uff0c\u662f\u975e\u5e38\u6709\u6548\u7684\u3002 \u6839\u636eread\u3001write\u7684\u591a\u5c11\u6765\u9009\u62e9concurrency control technique\u3002 \u4e0b\u9762\u662f\u7d20\u6750: 1\u3001wikipedia Read-copy-update 2\u3001wikipedia Readers\u2013writer lock 3\u3001wikipedia Seqlock Choose-Concurrency-Friendly-Data-Structures \u8fd9\u662f\u5728 12-Choose-Concurrency-Friendly-Data-Structures \u4e2d\u63d0\u51fa\u7684\u3002 Divide shared data to \u964d\u4f4elock\u7684\u7c92\u5ea6\u3001\u51cf\u5c11\u7ebf\u7a0b\u7ade\u4e89 \u8fd9\u662f\u63d0\u9ad8\u5e76\u53d1\u7684\u76f4\u63a5\u505a\u6cd5\u3002 1\u3001jemalloc 2\u3001DB \u884c\u7ea7\u9501\u3001\u8868\u7ea7\u9501 draft: cache locality and scalability and contention \u5982\u679c\u591a\u4e2athread\u90fd\u4f7f\u7528(read\u3001write)\u540c\u4e00\u4e2alock\u3001variable\uff0c\u5219\u5c31\u4f1a\u51fa\u73b0high contention \u5982\u679c\u80fd\u591f\u8ba9\u6bcf\u4e2athread\u4f7f\u7528\u5b83\u81ea\u5df1\u7684local variable\uff0c\u90a3\u4e48\u5c31\u80fd\u591f\u589e\u52a0cache local\uff0c\u51cf\u5c11contention\uff0c\u589e\u52a0scalability\u3002 Per-thread\u3001avoid shared data\u6d88\u9664\u7ade\u4e89 1\u3001 Spinning-lock-optimization TAS spin lock\u4e2d\uff0c\u6240\u6709\u7684thread\u90fd\u4f7f\u7528\u540c\u4e00\u4e2ashared data\u2192CLH Lock\u3001MCS Lock\uff0c\u6bcf\u4e2athread\u90fd\u6709\u8981\u7ed9\u81ea\u5df1\u7684node\uff0c\u5b83\u4eec\u53ea\u9700\u8981poll\u81ea\u5df1\u7684node\u3002 2\u3001\u4ee5redis\u7ebf\u7a0b\u6a21\u578b\u4e3a\u4f8b\u6765\u8fdb\u884c\u8bf4\u660e\uff0c\u6bcf\u4e2athread\u4e00\u4e2a\u79c1\u6709\u7684queue\uff0c\u8fd9\u6837\u6709\u6548\u5730\u907f\u514d\u7ade\u4e89\u3002 3\u3001tcmalloc \u4f7f\u7528non-blocking\u3001lock-less\u6280\u672f \u5b83\u4eec\u90fd\u80fd\u591f\u907f\u514d\u8fdb\u5165system call\u3002 Thoughts \u4e3a\u4e86\u6027\u80fd\uff0c\u5982\u679c\u52a0\u9501\u7684\u65f6\u95f4\u4e0d\u957f\uff0c\u53ef\u4ee5\u4f7f\u7528spinning lock\uff0c\u4e0d\u4f7f\u7528system lock\uff0c\u8fd9\u6837\u53ef\u4ee5\u907f\u514d\u7cfb\u7edf\u8c03\u7528\u95ee\u9898\u3002 Parallel divide and conquer \u5173\u4e8e\u6b64\uff0c\u53c2\u89c1: 1\u3001 02-How-Much-Scalability-Do-You-Have-or-Need 2\u3001\u5de5\u7a0b discrete \u7684 Divide-and-Conquer \u7ae0\u8282 Cache optimization \u907f\u514dcache sloshing-\u6643\u52a8 1\u3001jemalloc \u5c31\u662f\u4e00\u4e2a\u975e\u5e38\u597d\u7684\u6848\u4f8b 2\u3001 02-How-Much-Scalability-Do-You-Have-or-Need 3\u3001 12-Choose-Concurrency-Friendly-Data-Structures \u589e\u52a0cache performance 1\u3001align-to-cache line-optimization 2\u3001padding-to-cache line-optimization-avoid false sharing","title":"Introduction"},{"location":"Concurrent-computing/Design-and-optimize/#design#and#optimize","text":"1\u3001Optimize\u7684\u76ee\u7684\u662f\u63d0\u9ad8**\u5e76\u53d1**\uff0c\u6700\u7ec8\u76ee\u7684\u662f\u63d0\u4f9bperformance\u3001\u9075\u5faaoptimization principle\u3002 2\u3001\u4ee5Herb Sutter\u7684\"Effective concurrency\"\u7cfb\u5217\u6587\u7ae0\u4e3a\u84dd\u672c\u6765\u8fdb\u884c\u603b\u7ed3 3\u3001\u9700\u8981\u57fa\u4e8e hardware CPU \u7684\u8fd0\u884c\u673a\u5236\u6765\u8fdb\u884c\u4f18\u5316","title":"Design and optimize"},{"location":"Concurrent-computing/Design-and-optimize/#_1","text":"","title":"\u6848\u4f8b"},{"location":"Concurrent-computing/Design-and-optimize/#jemalloc","text":"NOTE: 1\u3001\u53c2\u89c1\u5de5\u7a0b programming-language \u7684 library-jemalloc \u7ae0\u8282 NOTE: 1\u3001\u5982\u4f55\u964d\u4f4elock contention \uff0c\u8fd9\u975e\u5e38\u91cd\u8981 2\u3001\u4ece\u4e0b\u9762\u7684\u63cf\u8ff0\u6765\u770b\uff0cLarson and Krishnan (1998) \u7684\u601d\u60f3\u662f: \"divide to reduce lock granularity\"\uff0c\u5373\"They tried pushing locks down in their allocator, so that rather than using a single allocator lock, each free list had its own lock\"\uff0c\u663e\u7136\uff0c\u8fd9\u964d\u4f4e\u4e86lock granularity\u3002 3\u3001jemalloc \u501f\u9274\u4e86Larson and Krishnan (1998) \u7684\"multiple arenas\"\u7b56\u7565\uff0c\u4f46\u662f\"uses a more reliable mechanism than hashing for assignment of threads to arenas\"\uff0c\u901a\u8fc7\u540e\u6587\u53ef\u77e5\uff0c\u662f\"round-robin\" \u5728\u4e0b\u9762\u7684\u56fe\u4e2d\uff0c\u89e3\u91ca\u4e86\"hashing of the thread identifiers\"\u7684\u52a3\u52bf\uff0c\u7b80\u5355\u800c\u8a00: \u4f2a\u968f\u673a\u8fc7\u7a0b\uff0c\u5e76\u4e0d\u80fd\u591f\u4fdd\u8bc1\u5747\u7b49\uff0c\u56e0\u6b64\u65e0\u6cd5\u4fdd\u8bc1load balance\uff08\u6b64\u5904\u4f7f\u7528load balance\u662f\u4e0d\u51c6\u786e\u7684\uff09 One of the main goals for this allocator was to reduce lock contention for multi-threaded applications running on multi-processor systems. Larson and Krishnan (1998) did an excellent job of presenting and testing strategies. They tried pushing locks down in their allocator, so that rather than using a single allocator lock, each free list had its own lock. This helped some, but did not scale adequately, despite minimal lock contention. They attributed this to \u201ccache sloshing\u201d \u2013 the quick migration of cached data among processors during the manipulation of allocator data structures. Their solution was to use multiple arenas for allocation, and assign threads to arenas via hashing of the thread identifiers (Figure 2). This works quite well, and has since been used by other implementations (Berger et al., 2000; Bonwick and Adams, 2001). jemalloc uses multiple arenas, but uses a more reliable mechanism than hashing for assignment of threads to arenas. Multiple processors : Use four times as many arenas as there are processors. By assigning threads to a set of arenas, the probability of a single arena being used concurrently decreases. NOTE: 1\u3001\u901a\u8fc7\u5206\u89e3\u6765\u964d\u4f4e\u9501\u7c92\u5ea6\uff0c\u51cf\u5c11\u7ade\u4e89\uff0c\u63d0\u9ad8\u5e76\u53d1\u6027","title":"jemalloc"},{"location":"Concurrent-computing/Design-and-optimize/#tcmalloc#thread-caching#malloc","text":"1\u3001Fast, uncontended allocation and deallocation for most objects. Objects are cached, depending on mode, either per-thread, or per-logical-CPU. Most allocations do not need to take locks, so there is low contention and good scaling for multi-threaded applications.","title":"TCMalloc : Thread-Caching Malloc"},{"location":"Concurrent-computing/Design-and-optimize/#db","text":"\u884c\u7ea7\u9501\u3001\u8868\u7ea7\u9501","title":"DB"},{"location":"Concurrent-computing/Design-and-optimize/#scalable#concurrency#re-enable#free#lunchon#scalability","text":"Herb Sutter 2005\u5e74\u53d1\u8868\u7684 The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software \u6587\u7ae0\uff0c\u5176\u4e2d\u603b\u7ed3\u4e86CPU\u7684\u53d1\u5c55\u65b9\u5411\uff0c\u5bf9software\u7684\u5f71\u54cd\uff0cprogrammer\u8981\u5982\u4f55\u505a\u3002 \u5728 How-Much-Scalability-Do-You-Have-or-Need \u4e2d\uff0c\u63d0\u51fa\u4e86\u5982\u4e0b\u6838\u5fc3\u89c2\u70b9: 1\u3001scale well to multiple core CPU 2\u3001\u80fd\u591f\u5145\u5206\u53d1\u6325hardware\u7684computation power\uff0c\u7406\u60f3\u60c5\u51b5\u662f\u7ebf\u6027\u589e\u52a0: software\u7684\u5904\u7406\u80fd\u529b\u548c\u6838\u6570\u5448\u7ebf\u6027\u5173\u7cfb 3\u3001\u4f5c\u4e3asoftware engineer\uff0c\u9700\u8981\u5f00\u53d1\u51fa\u5177\u6709scalability\u7684software\uff0c\u8fd9\u6837\u624d\u80fd\u591fre-enable the free lunch","title":"Scalable concurrency\u3001 re-enable free lunch\u3001O(n) scalability"},{"location":"Concurrent-computing/Design-and-optimize/#on#scalability","text":"\u4e3a\u4e86\u5b9e\u73b0O(n) scalability\uff0c\u53ef\u80fd\u9700\u8981\u67e5\u8be2\u6838\u6570\uff0c\u4e0b\u9762\u662f\u4e00\u4e9b\u6848\u4f8b: 1\u3001jemalloc","title":"\u67e5\u8be2\u6838\u6570\u6765\u5b9e\u73b0O(n) scalability"},{"location":"Concurrent-computing/Design-and-optimize/#pipeline","text":"\u53c2\u89c1 How-Much-Scalability-Do-You-Have-or-Need \u7ae0\u8282\u3002","title":"Pipeline"},{"location":"Concurrent-computing/Design-and-optimize/#read#and#write#tradeoffread#and#write","text":"\u5728multiple model\u4e2d\uff0c\u5bf9\u4e8eshared data\u7684read\u3001write\u9700\u8981\u8fdb\u884cConcurrency control\uff0c\u5728\u672c\u7ae0\u603b\u7ed3\u4e86\u591a\u79cdconcurrency control\u6280\u672f\uff0c\u5728\u9009\u62e9\u8fd9\u4e9b\u6280\u672f\u7684\u65f6\u5019\uff0c\u975e\u5e38\u91cd\u8981\u7684\u4e00\u70b9\u662f: read and write tradeoff\u3002 \u6709\u4e9b\u6280\u672f\u5728read\u8fdc\u591a\u4e8ewrite\u7684\u65f6\u5019\uff0c\u662f\u975e\u5e38\u6709\u6548\u7684\u3002 \u6839\u636eread\u3001write\u7684\u591a\u5c11\u6765\u9009\u62e9concurrency control technique\u3002 \u4e0b\u9762\u662f\u7d20\u6750: 1\u3001wikipedia Read-copy-update 2\u3001wikipedia Readers\u2013writer lock 3\u3001wikipedia Seqlock","title":"Read and write tradeoff/\u6839\u636eread and write\u6765\u9009\u62e9"},{"location":"Concurrent-computing/Design-and-optimize/#choose-concurrency-friendly-data-structures","text":"\u8fd9\u662f\u5728 12-Choose-Concurrency-Friendly-Data-Structures \u4e2d\u63d0\u51fa\u7684\u3002","title":"Choose-Concurrency-Friendly-Data-Structures"},{"location":"Concurrent-computing/Design-and-optimize/#divide#shared#data#to#lock","text":"\u8fd9\u662f\u63d0\u9ad8\u5e76\u53d1\u7684\u76f4\u63a5\u505a\u6cd5\u3002 1\u3001jemalloc 2\u3001DB \u884c\u7ea7\u9501\u3001\u8868\u7ea7\u9501","title":"Divide shared data to \u964d\u4f4elock\u7684\u7c92\u5ea6\u3001\u51cf\u5c11\u7ebf\u7a0b\u7ade\u4e89"},{"location":"Concurrent-computing/Design-and-optimize/#draft#cache#locality#and#scalability#and#contention","text":"\u5982\u679c\u591a\u4e2athread\u90fd\u4f7f\u7528(read\u3001write)\u540c\u4e00\u4e2alock\u3001variable\uff0c\u5219\u5c31\u4f1a\u51fa\u73b0high contention \u5982\u679c\u80fd\u591f\u8ba9\u6bcf\u4e2athread\u4f7f\u7528\u5b83\u81ea\u5df1\u7684local variable\uff0c\u90a3\u4e48\u5c31\u80fd\u591f\u589e\u52a0cache local\uff0c\u51cf\u5c11contention\uff0c\u589e\u52a0scalability\u3002","title":"draft: cache locality and scalability and contention"},{"location":"Concurrent-computing/Design-and-optimize/#per-threadavoid#shared#data","text":"1\u3001 Spinning-lock-optimization TAS spin lock\u4e2d\uff0c\u6240\u6709\u7684thread\u90fd\u4f7f\u7528\u540c\u4e00\u4e2ashared data\u2192CLH Lock\u3001MCS Lock\uff0c\u6bcf\u4e2athread\u90fd\u6709\u8981\u7ed9\u81ea\u5df1\u7684node\uff0c\u5b83\u4eec\u53ea\u9700\u8981poll\u81ea\u5df1\u7684node\u3002 2\u3001\u4ee5redis\u7ebf\u7a0b\u6a21\u578b\u4e3a\u4f8b\u6765\u8fdb\u884c\u8bf4\u660e\uff0c\u6bcf\u4e2athread\u4e00\u4e2a\u79c1\u6709\u7684queue\uff0c\u8fd9\u6837\u6709\u6548\u5730\u907f\u514d\u7ade\u4e89\u3002 3\u3001tcmalloc","title":"Per-thread\u3001avoid shared data\u6d88\u9664\u7ade\u4e89"},{"location":"Concurrent-computing/Design-and-optimize/#non-blockinglock-less","text":"\u5b83\u4eec\u90fd\u80fd\u591f\u907f\u514d\u8fdb\u5165system call\u3002","title":"\u4f7f\u7528non-blocking\u3001lock-less\u6280\u672f"},{"location":"Concurrent-computing/Design-and-optimize/#thoughts","text":"\u4e3a\u4e86\u6027\u80fd\uff0c\u5982\u679c\u52a0\u9501\u7684\u65f6\u95f4\u4e0d\u957f\uff0c\u53ef\u4ee5\u4f7f\u7528spinning lock\uff0c\u4e0d\u4f7f\u7528system lock\uff0c\u8fd9\u6837\u53ef\u4ee5\u907f\u514d\u7cfb\u7edf\u8c03\u7528\u95ee\u9898\u3002","title":"Thoughts"},{"location":"Concurrent-computing/Design-and-optimize/#parallel#divide#and#conquer","text":"\u5173\u4e8e\u6b64\uff0c\u53c2\u89c1: 1\u3001 02-How-Much-Scalability-Do-You-Have-or-Need 2\u3001\u5de5\u7a0b discrete \u7684 Divide-and-Conquer \u7ae0\u8282","title":"Parallel divide and conquer"},{"location":"Concurrent-computing/Design-and-optimize/#cache#optimization","text":"","title":"Cache optimization"},{"location":"Concurrent-computing/Design-and-optimize/#cache#sloshing-","text":"1\u3001jemalloc \u5c31\u662f\u4e00\u4e2a\u975e\u5e38\u597d\u7684\u6848\u4f8b 2\u3001 02-How-Much-Scalability-Do-You-Have-or-Need 3\u3001 12-Choose-Concurrency-Friendly-Data-Structures","title":"\u907f\u514dcache sloshing-\u6643\u52a8"},{"location":"Concurrent-computing/Design-and-optimize/#cache#performance","text":"1\u3001align-to-cache line-optimization 2\u3001padding-to-cache line-optimization-avoid false sharing","title":"\u589e\u52a0cache performance"},{"location":"Concurrent-computing/Expert-1024cores/","text":"1024cores \u5728parallel computing\u9886\u57df\u7684\u4e13\u5bb6\u3002 1024cores","title":"Introduction"},{"location":"Concurrent-computing/Expert-1024cores/#1024cores","text":"\u5728parallel computing\u9886\u57df\u7684\u4e13\u5bb6\u3002","title":"1024cores"},{"location":"Concurrent-computing/Expert-1024cores/#1024cores_1","text":"","title":"1024cores"},{"location":"Concurrent-computing/Expert-1024cores/Lockfree-Algorithms/","text":"1024cores Lockfree Algorithms","title":"Lockfree-Algorithms"},{"location":"Concurrent-computing/Expert-1024cores/Lockfree-Algorithms/#1024cores#lockfree#algorithms","text":"","title":"1024cores Lockfree Algorithms"},{"location":"Concurrent-computing/Expert-1024cores/Lockfree-Algorithms/TODO-Introduction/","text":"Home \u200e > \u200e Lockfree Algorithms \u200e > \u200e Introduction NOTE: 1\u3001wait-free > Lock-free > obstruction-free I bet you had heard terms like \"lockfree\" and \"waitfree\". So what it's all about? Let's start with some definitions. Wait-freedom Wait-freedom means that each thread moves forward regardless of external factors like contention from other threads, other thread blocking. Each operations is executed in a bounded number of steps. It's the strongest guarantee for synchronization algorithms . Wait-free algorithms usually use such primitives as atomic_exchange , atomic_fetch_add ( InterlockedExchange , InterlockedIncrement , InterlockedExchangeAdd , __sync_fetch_and_add ), and they do not contain cycles that can be affected by other threads. atomic_compare_exchange primitive ( InterlockedCompareExchange , __sync_val_compare_and_swap ) is usually not used, because it is usually tied with a \"repeat until succeed\" cycle. NOTE: \u4e0a\u8ff0cycle\u662f\u4ec0\u4e48\u542b\u4e49\uff1f\u53c2\u89c1\u4e0b\u9762\u7684**Lock-freedom**\u7ae0\u8282\u4e2d\u7ed9\u51fa\u7684\u4f8b\u5b50\uff1b \u4e0a\u9762\u8fd9\u6bb5\u8bdd + \u4e0b\u9762\u7684\u4e24\u4e2a\u4f8b\u5b50\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u51fa\u5982\u4e0b\u5bf9\u5e94\u5173\u7cfb: Below is an example of a wait-free algorithm : void increment_reference_counter ( rc_base * obj ) { atomic_increment ( obj -> rc ); } void decrement_reference_counter ( rc_base * obj ) { if ( 0 == atomic_decrement ( obj -> rc )) delete obj ; } NOTE: \u53c2\u89c1: 1\u3001atomic_increment: std::atomic_fetch_add Each thread is able to execute the function in a bounded number of steps regardless of any external factors. Lock-freedom Lock-freedom means that a system as a whole moves forward regardless of anything. Forward progress for each individual thread is not guaranteed (that is, individual threads can starve). It's a weaker guarantee than wait-freedom. Lockfree algorithms usually use atomic_compare_exchange primitive ( InterlockedCompareExchange , __sync_val_compare_and_swap ). An example of a lockfree algorithm is: void stack_push ( stack * s , node * n ) { node * head ; do { head = s -> head ; n -> next = head ; } while ( ! atomic_compare_exchange ( s -> head , head , n )); } NOTE: \u53c2\u89c1: 1\u3001cppreference std::atomic_compare_exchange_strong \u4e0a\u8ff0\u8fc7\u7a0b\u8ba9\u6211\u60f3\u5230\u4e86busy-waiting\u3001CAS As can be seen, a thread can \"whirl\"(\u56de\u65cb) in the cycle theoretically infinitely. But every repeat of the cycle means that some other thread had made forward progress (that is, successfully pushed a node to the stack). A blocked/interrupted/terminated thread can not prevent(\u963b\u6b62) forward progress of other threads. Consequently, the system as a whole undoubtedly makes forward progress. Obstruction-freedom NOTE: \"Obstruction\"\u7684\u610f\u601d\u662f: \u963b\u788d Obstruction-freedom guarantee means that a thread makes forward progress only if it does not encounter contention from other threads. That is, two threads can prevent each other's progress and lead to a livelock . It's even weaker guarantee than lo\u0441k-freedom. This guarantee may look a bit strange at first. However, note that (1) blocked/interrupted/terminated threads can not prevent progress of other threads, and (2) obstruction-free algorithms can be faster than lockfree algorithms. I am unable to come up with a single example, so I refer you to the original paper Obstruction-Free Synchronization: Double-Ended Queues as an Example . Termination-safety Waitfree, lockfree and obstruction-free algorithms provide a guarantee of termination-safety. That is, a terminated thread does not prevent system-wide forward progress. NOTE: \u5b83\u4eec\u80fd\u591f\u4fdd\u8bc1\u5f53thread terminate\u7684\u65f6\u5019\uff0csystem-wide forward progress\u4e0d\u4f1a\u88ab\u4ed6\u963b\u6b62","title":"Introduction"},{"location":"Concurrent-computing/Expert-1024cores/Lockfree-Algorithms/TODO-Introduction/#home#lockfree#algorithms#introduction","text":"NOTE: 1\u3001wait-free > Lock-free > obstruction-free I bet you had heard terms like \"lockfree\" and \"waitfree\". So what it's all about? Let's start with some definitions.","title":"Home\u200e &gt; \u200eLockfree Algorithms\u200e &gt; \u200eIntroduction"},{"location":"Concurrent-computing/Expert-1024cores/Lockfree-Algorithms/TODO-Introduction/#wait-freedom","text":"Wait-freedom means that each thread moves forward regardless of external factors like contention from other threads, other thread blocking. Each operations is executed in a bounded number of steps. It's the strongest guarantee for synchronization algorithms . Wait-free algorithms usually use such primitives as atomic_exchange , atomic_fetch_add ( InterlockedExchange , InterlockedIncrement , InterlockedExchangeAdd , __sync_fetch_and_add ), and they do not contain cycles that can be affected by other threads. atomic_compare_exchange primitive ( InterlockedCompareExchange , __sync_val_compare_and_swap ) is usually not used, because it is usually tied with a \"repeat until succeed\" cycle. NOTE: \u4e0a\u8ff0cycle\u662f\u4ec0\u4e48\u542b\u4e49\uff1f\u53c2\u89c1\u4e0b\u9762\u7684**Lock-freedom**\u7ae0\u8282\u4e2d\u7ed9\u51fa\u7684\u4f8b\u5b50\uff1b \u4e0a\u9762\u8fd9\u6bb5\u8bdd + \u4e0b\u9762\u7684\u4e24\u4e2a\u4f8b\u5b50\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u51fa\u5982\u4e0b\u5bf9\u5e94\u5173\u7cfb: Below is an example of a wait-free algorithm : void increment_reference_counter ( rc_base * obj ) { atomic_increment ( obj -> rc ); } void decrement_reference_counter ( rc_base * obj ) { if ( 0 == atomic_decrement ( obj -> rc )) delete obj ; } NOTE: \u53c2\u89c1: 1\u3001atomic_increment: std::atomic_fetch_add Each thread is able to execute the function in a bounded number of steps regardless of any external factors.","title":"Wait-freedom"},{"location":"Concurrent-computing/Expert-1024cores/Lockfree-Algorithms/TODO-Introduction/#lock-freedom","text":"Lock-freedom means that a system as a whole moves forward regardless of anything. Forward progress for each individual thread is not guaranteed (that is, individual threads can starve). It's a weaker guarantee than wait-freedom. Lockfree algorithms usually use atomic_compare_exchange primitive ( InterlockedCompareExchange , __sync_val_compare_and_swap ). An example of a lockfree algorithm is: void stack_push ( stack * s , node * n ) { node * head ; do { head = s -> head ; n -> next = head ; } while ( ! atomic_compare_exchange ( s -> head , head , n )); } NOTE: \u53c2\u89c1: 1\u3001cppreference std::atomic_compare_exchange_strong \u4e0a\u8ff0\u8fc7\u7a0b\u8ba9\u6211\u60f3\u5230\u4e86busy-waiting\u3001CAS As can be seen, a thread can \"whirl\"(\u56de\u65cb) in the cycle theoretically infinitely. But every repeat of the cycle means that some other thread had made forward progress (that is, successfully pushed a node to the stack). A blocked/interrupted/terminated thread can not prevent(\u963b\u6b62) forward progress of other threads. Consequently, the system as a whole undoubtedly makes forward progress.","title":"Lock-freedom"},{"location":"Concurrent-computing/Expert-1024cores/Lockfree-Algorithms/TODO-Introduction/#obstruction-freedom","text":"NOTE: \"Obstruction\"\u7684\u610f\u601d\u662f: \u963b\u788d Obstruction-freedom guarantee means that a thread makes forward progress only if it does not encounter contention from other threads. That is, two threads can prevent each other's progress and lead to a livelock . It's even weaker guarantee than lo\u0441k-freedom. This guarantee may look a bit strange at first. However, note that (1) blocked/interrupted/terminated threads can not prevent progress of other threads, and (2) obstruction-free algorithms can be faster than lockfree algorithms. I am unable to come up with a single example, so I refer you to the original paper Obstruction-Free Synchronization: Double-Ended Queues as an Example .","title":"Obstruction-freedom"},{"location":"Concurrent-computing/Expert-1024cores/Lockfree-Algorithms/TODO-Introduction/#termination-safety","text":"Waitfree, lockfree and obstruction-free algorithms provide a guarantee of termination-safety. That is, a terminated thread does not prevent system-wide forward progress. NOTE: \u5b83\u4eec\u80fd\u591f\u4fdd\u8bc1\u5f53thread terminate\u7684\u65f6\u5019\uff0csystem-wide forward progress\u4e0d\u4f1a\u88ab\u4ed6\u963b\u6b62","title":"Termination-safety"},{"location":"Concurrent-computing/Expert-1024cores/Lockfree-Algorithms/TODO-Lazy-Concurrent-Initialization/","text":"1024cores Lazy Concurrent Initialization","title":"Introduction"},{"location":"Concurrent-computing/Expert-1024cores/Lockfree-Algorithms/TODO-Lazy-Concurrent-Initialization/#1024cores#lazy#concurrent#initialization","text":"","title":"1024cores Lazy Concurrent Initialization"},{"location":"Concurrent-computing/Expert-1024cores/Lockfree-Algorithms/TODO-Producer-Consumer-Queues/","text":"1024cores Producer-Consumer Queues NOTE: 1\u3001\u8fd9\u662f\u6211\u89c1\u5230\u7684\uff0c\u603b\u7ed3\u5f97\u6700\u6700\u8be6\u7ec6\u7684 Producer-consumer queues are one of the most fundamental components in concurrent systems, they represent means to transfer data/messages/tasks/transactions between threads/stages/agents. \"one-size-fits-all\" NOTE: 1\u3001\u8fd9\u662f\u975e\u5e38\u91cd\u8981\u7684\u4e00\u4e2a\u4e8b\u5b9e 2\u3001\u540e\u9762\u4f5c\u8005\u4ece\u591a\u4e2a\u89d2\u5ea6\u8fdb\u884c\u4e86\u533a\u5206\uff0cprogrammer\u5e94\u8be5\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u8fdb\u884ctradeoff If you hope that there is a single magical \"one-size-fits-all\" concurrent queue (MS PPL and Intel TTB fallacy), sorry, there is no. So what flavours of queues are there? NOTE: 1\u3001\u540e\u9762\u4ece\u5404\u4e2a\u89d2\u5ea6\u3001\u6807\u51c6\u6765\u8fdb\u884c\u533a\u5206 Number of producer and consumer threads Depending on allowed number of producer and consumer threads : - Multi-producer/multi-consumer queues (MPMC) - Single-producer/multi-consumer queues (SPMC) - Multi-producer/single-consumer queues (MPSC) - Single-producer/single-consumer queues (SPSC) I hope this aspect is clear - for example, if you have only 1 producer and 1 consumer thread, you can use SPSC queue instead of more general MPMC queue, and as you may guess it will be significantly faster. Underlying data structure Depending on underlying data structure : - Array-based - Linked-list-based - Hybrid Array-based queues are generally faster, however they are usually not strictly lockfree . The drawback is that they need to preallocate memory for the worst case. Linked-list queues grow dynamically, thus no need to preallocate any memory up-front. And hybrid queues (linked-list of small fixed-size arrays) try to combine advantages of both.","title":"Introduction"},{"location":"Concurrent-computing/Expert-1024cores/Lockfree-Algorithms/TODO-Producer-Consumer-Queues/#1024cores#producer-consumer#queues","text":"NOTE: 1\u3001\u8fd9\u662f\u6211\u89c1\u5230\u7684\uff0c\u603b\u7ed3\u5f97\u6700\u6700\u8be6\u7ec6\u7684 Producer-consumer queues are one of the most fundamental components in concurrent systems, they represent means to transfer data/messages/tasks/transactions between threads/stages/agents.","title":"1024cores Producer-Consumer Queues"},{"location":"Concurrent-computing/Expert-1024cores/Lockfree-Algorithms/TODO-Producer-Consumer-Queues/#one-size-fits-all","text":"NOTE: 1\u3001\u8fd9\u662f\u975e\u5e38\u91cd\u8981\u7684\u4e00\u4e2a\u4e8b\u5b9e 2\u3001\u540e\u9762\u4f5c\u8005\u4ece\u591a\u4e2a\u89d2\u5ea6\u8fdb\u884c\u4e86\u533a\u5206\uff0cprogrammer\u5e94\u8be5\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u8fdb\u884ctradeoff If you hope that there is a single magical \"one-size-fits-all\" concurrent queue (MS PPL and Intel TTB fallacy), sorry, there is no. So what flavours of queues are there? NOTE: 1\u3001\u540e\u9762\u4ece\u5404\u4e2a\u89d2\u5ea6\u3001\u6807\u51c6\u6765\u8fdb\u884c\u533a\u5206","title":"\"one-size-fits-all\""},{"location":"Concurrent-computing/Expert-1024cores/Lockfree-Algorithms/TODO-Producer-Consumer-Queues/#number#of#producer#and#consumer#threads","text":"Depending on allowed number of producer and consumer threads : - Multi-producer/multi-consumer queues (MPMC) - Single-producer/multi-consumer queues (SPMC) - Multi-producer/single-consumer queues (MPSC) - Single-producer/single-consumer queues (SPSC) I hope this aspect is clear - for example, if you have only 1 producer and 1 consumer thread, you can use SPSC queue instead of more general MPMC queue, and as you may guess it will be significantly faster.","title":"Number of producer and consumer threads"},{"location":"Concurrent-computing/Expert-1024cores/Lockfree-Algorithms/TODO-Producer-Consumer-Queues/#underlying#data#structure","text":"Depending on underlying data structure : - Array-based - Linked-list-based - Hybrid Array-based queues are generally faster, however they are usually not strictly lockfree . The drawback is that they need to preallocate memory for the worst case. Linked-list queues grow dynamically, thus no need to preallocate any memory up-front. And hybrid queues (linked-list of small fixed-size arrays) try to combine advantages of both.","title":"Underlying data structure"},{"location":"Concurrent-computing/Expert-Cameron/","text":"Cameron \u5728\u67e5\u8be2lock free queue\u7684\u65f6\u5019\uff0c\u53d1\u73b0\u7684\u3002 \u4e2a\u4eba\u7f51\u7ad9 moodycamel github cameron314 moodycamel A Fast General Purpose Lock-Free Queue for C++","title":"Introduction"},{"location":"Concurrent-computing/Expert-Cameron/#cameron","text":"\u5728\u67e5\u8be2lock free queue\u7684\u65f6\u5019\uff0c\u53d1\u73b0\u7684\u3002","title":"Cameron"},{"location":"Concurrent-computing/Expert-Cameron/#moodycamel","text":"","title":"\u4e2a\u4eba\u7f51\u7ad9 moodycamel"},{"location":"Concurrent-computing/Expert-Cameron/#github#cameron314","text":"","title":"github cameron314"},{"location":"Concurrent-computing/Expert-Cameron/#moodycamel#a#fast#general#purpose#lock-free#queue#for#c","text":"","title":"moodycamel A Fast General Purpose Lock-Free Queue for C++"},{"location":"Concurrent-computing/Expert-Cameron/Library-concurrentqueue/","text":"cameron314 / concurrentqueue An industrial-strength lock-free queue for C++. Note: If all you need is a single-producer, single-consumer queue, I have one of those too .","title":"Introduction"},{"location":"Concurrent-computing/Expert-Cameron/Library-concurrentqueue/#cameron314concurrentqueue","text":"An industrial-strength lock-free queue for C++. Note: If all you need is a single-producer, single-consumer queue, I have one of those too .","title":"cameron314/concurrentqueue"},{"location":"Concurrent-computing/Expert-Cameron/Library-readerwriterqueue/","text":"cameron314 / readerwriterqueue A single-producer, single-consumer lock-free queue for C++ This mini-repository has my very own implementation of a lock-free queue (that I designed from scratch) for C++.","title":"Introduction"},{"location":"Concurrent-computing/Expert-Cameron/Library-readerwriterqueue/#cameron314readerwriterqueue","text":"","title":"cameron314/readerwriterqueue"},{"location":"Concurrent-computing/Expert-Cameron/Library-readerwriterqueue/#a#single-producer#single-consumer#lock-free#queue#for#c","text":"This mini-repository has my very own implementation of a lock-free queue (that I designed from scratch) for C++.","title":"A single-producer, single-consumer lock-free queue for C++"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/","text":"Jeff Preshing C++\u3001concurrent programming\u9886\u57df\u4e13\u5bb6\u3002 Preshing on Programming","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/#jeff#preshing","text":"C++\u3001concurrent programming\u9886\u57df\u4e13\u5bb6\u3002","title":"Jeff Preshing"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/#preshing#on#programming","text":"","title":"Preshing on Programming"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Hash-map/","text":"","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Hash-map/Hash-Collision-Probabilities/","text":"preshing Hash Collision Probabilities NOTE: \u8fd9\u7bc7\u6587\u7ae0\u91cd\u8981\u63cf\u8ff0\u8ba1\u7b97hash collision\u7684\u53ef\u80fd\u6027/\u6982\u7387","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Hash-map/Hash-Collision-Probabilities/#preshing#hash#collision#probabilities","text":"NOTE: \u8fd9\u7bc7\u6587\u7ae0\u91cd\u8981\u63cf\u8ff0\u8ba1\u7b97hash collision\u7684\u53ef\u80fd\u6027/\u6982\u7387","title":"preshing Hash Collision Probabilities"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Hash-map/Hash-Table-Performance-Tests/","text":"preshing Hash Table Performance Tests NOTE: \u5bf9\u5404\u79cdhash table\u8fdb\u884cbenchmark","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Hash-map/Hash-Table-Performance-Tests/#preshing#hash#table#performance#tests","text":"NOTE: \u5bf9\u5404\u79cdhash table\u8fdb\u884cbenchmark","title":"preshing Hash Table Performance Tests"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Library-cpp11-on-multicore/","text":"preshing / cpp11-on-multicore","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Library-cpp11-on-multicore/#preshingcpp11-on-multicore","text":"","title":"preshing/cpp11-on-multicore"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Library-cpp11-on-multicore/Read-code/","text":"","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Library-cpp11-on-multicore/Read-code/inmemorylogger/","text":"preshing / cpp11-on-multicore inmemorylogger Source code cpp11-on-multicore / common / inmemorylogger.h cpp11-on-multicore / common / inmemorylogger.cpp Shared variable/data class InMemoryLogger { struct Page { std :: unique_ptr < Page > next ; std :: atomic < int > index ; // This can exceed EVENTS_PER_PAGE, but it's harmless. Just means page is full. Event events [ EVENTS_PER_PAGE ]; Page () : index ( 0 ) { } }; std :: atomic < Page *> m_tail ; }; m_tail \u3001 Page.index \u5c31\u662fshared variable/data\uff0c\u56e0\u6b64\u5c06\u5b83\u4eec\u90fd\u58f0\u660e\u4e3a std::atomic \u7c7b\u578b\uff1b Double checked locking \u5728\u4f7f\u7528double checked locking\u6765\u5b9e\u73b0singleton\u65f6\uff0c\u53ea\u6709\u5f53 if (pInstance == 0)// 1st test \u6ee1\u8db3\u7684\u65f6\u5019\uff0c\u624d\u4f1a\u8fdb\u884clock\uff0c\u7531\u4e8eTOCTTOU\uff0c\u56e0\u6b64\uff0c\u5728lock\u540e\uff0c\u8fd8\u9700\u8981\u8fdb\u884c\u7b2c\u4e8c\u6b21check: if (pInstance == 0)// 2nd test \u3002 \u5728 inmemorylogger \u4e2d\uff0c\u4e5f\u4f7f\u7528 double checked locking\uff0c\u5b83\u53ea\u6709\u5f53\u5f53\u524dpage\u6ee1\u7684\u65f6\u5019(1 st test)\uff0c\u5b83\u624d\u9700\u8981\u8fdb\u884callocate\uff0c\u624d\u4f1a\u8fdb\u884clock\uff0c\u7531\u4e8eTOCTTOU\uff0c\u56e0\u6b64\uff0c\u5728lock\u540e\uff0c\u8fd8\u9700\u8981\u8fdb\u884c\u7b2c\u4e8c\u6b21check: if (oldTail->index.load(std::memory_order_relaxed) < EVENTS_PER_PAGE) \u3002 log void log ( const char * msg , size_t param = 0 ) { std :: atomic_signal_fence ( std :: memory_order_seq_cst ); // Compiler barrier // On weak CPUs and current C++ compilers, memory_order_consume costs the same as acquire. :( // (If you don't like that, you can probably demote this load to relaxed and get away with it. // Technically, you'd be violating the spec, but in practice it will likely work. Just // inspect the assembly and make sure there is a data dependency between m_tail.load and // both subsequent uses of page, and you're golden. The only way I can imagine the dependency // chain being broken is if the compiler knows the addresses that will be allocated // in allocateEventFromNewPage at runtime, which is a huuuuuuuuuge leap of the imagination.) // http://preshing.com/20140709/the-purpose-of-memory_order_consume-in-cpp11 Page * page = m_tail . load ( std :: memory_order_consume ); Event * evt ; int index = page -> index . fetch_add ( 1 , std :: memory_order_relaxed ); if ( index < EVENTS_PER_PAGE ) evt = & page -> events [ index ]; else evt = allocateEventFromNewPage (); // Double-checked locking is performed inside here. evt -> tid = std :: this_thread :: get_id (); evt -> msg = msg ; evt -> param = param ; std :: atomic_signal_fence ( std :: memory_order_seq_cst ); // Compiler barrier } \u5982\u4f55\u7406\u89e3 std::atomic_signal_fence(std::memory_order_seq_cst) \uff1f \u53c2\u89c1 preshing The Purpose of memory_order_consume in C++11","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Library-cpp11-on-multicore/Read-code/inmemorylogger/#preshingcpp11-on-multicore#inmemorylogger","text":"","title":"preshing/cpp11-on-multicore inmemorylogger"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Library-cpp11-on-multicore/Read-code/inmemorylogger/#source#code","text":"cpp11-on-multicore / common / inmemorylogger.h cpp11-on-multicore / common / inmemorylogger.cpp","title":"Source code"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Library-cpp11-on-multicore/Read-code/inmemorylogger/#shared#variabledata","text":"class InMemoryLogger { struct Page { std :: unique_ptr < Page > next ; std :: atomic < int > index ; // This can exceed EVENTS_PER_PAGE, but it's harmless. Just means page is full. Event events [ EVENTS_PER_PAGE ]; Page () : index ( 0 ) { } }; std :: atomic < Page *> m_tail ; }; m_tail \u3001 Page.index \u5c31\u662fshared variable/data\uff0c\u56e0\u6b64\u5c06\u5b83\u4eec\u90fd\u58f0\u660e\u4e3a std::atomic \u7c7b\u578b\uff1b","title":"Shared variable/data"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Library-cpp11-on-multicore/Read-code/inmemorylogger/#double#checked#locking","text":"\u5728\u4f7f\u7528double checked locking\u6765\u5b9e\u73b0singleton\u65f6\uff0c\u53ea\u6709\u5f53 if (pInstance == 0)// 1st test \u6ee1\u8db3\u7684\u65f6\u5019\uff0c\u624d\u4f1a\u8fdb\u884clock\uff0c\u7531\u4e8eTOCTTOU\uff0c\u56e0\u6b64\uff0c\u5728lock\u540e\uff0c\u8fd8\u9700\u8981\u8fdb\u884c\u7b2c\u4e8c\u6b21check: if (pInstance == 0)// 2nd test \u3002 \u5728 inmemorylogger \u4e2d\uff0c\u4e5f\u4f7f\u7528 double checked locking\uff0c\u5b83\u53ea\u6709\u5f53\u5f53\u524dpage\u6ee1\u7684\u65f6\u5019(1 st test)\uff0c\u5b83\u624d\u9700\u8981\u8fdb\u884callocate\uff0c\u624d\u4f1a\u8fdb\u884clock\uff0c\u7531\u4e8eTOCTTOU\uff0c\u56e0\u6b64\uff0c\u5728lock\u540e\uff0c\u8fd8\u9700\u8981\u8fdb\u884c\u7b2c\u4e8c\u6b21check: if (oldTail->index.load(std::memory_order_relaxed) < EVENTS_PER_PAGE) \u3002","title":"Double checked locking"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Library-cpp11-on-multicore/Read-code/inmemorylogger/#log","text":"void log ( const char * msg , size_t param = 0 ) { std :: atomic_signal_fence ( std :: memory_order_seq_cst ); // Compiler barrier // On weak CPUs and current C++ compilers, memory_order_consume costs the same as acquire. :( // (If you don't like that, you can probably demote this load to relaxed and get away with it. // Technically, you'd be violating the spec, but in practice it will likely work. Just // inspect the assembly and make sure there is a data dependency between m_tail.load and // both subsequent uses of page, and you're golden. The only way I can imagine the dependency // chain being broken is if the compiler knows the addresses that will be allocated // in allocateEventFromNewPage at runtime, which is a huuuuuuuuuge leap of the imagination.) // http://preshing.com/20140709/the-purpose-of-memory_order_consume-in-cpp11 Page * page = m_tail . load ( std :: memory_order_consume ); Event * evt ; int index = page -> index . fetch_add ( 1 , std :: memory_order_relaxed ); if ( index < EVENTS_PER_PAGE ) evt = & page -> events [ index ]; else evt = allocateEventFromNewPage (); // Double-checked locking is performed inside here. evt -> tid = std :: this_thread :: get_id (); evt -> msg = msg ; evt -> param = param ; std :: atomic_signal_fence ( std :: memory_order_seq_cst ); // Compiler barrier }","title":"log"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Library-cpp11-on-multicore/Read-code/inmemorylogger/#stdatomic_signal_fencestdmemory_order_seq_cst","text":"\u53c2\u89c1 preshing The Purpose of memory_order_consume in C++11","title":"\u5982\u4f55\u7406\u89e3std::atomic_signal_fence(std::memory_order_seq_cst)\uff1f"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u8bb0\u5f55\u9605\u8bfb lock free \u4e3b\u9898\u7684\u6587\u7ae0\u3002 \u6587\u7ae0\u603b\u7ed3 \u4f5c\u8005\u7684\u5199\u4f5c\u601d\u8def: \u5148\u629b\u51fa\u95ee\u9898\uff0c\u7136\u540e\u6982\u8ff0\u6280\u672f\u56fe\u666f\uff0c\u7136\u540e\u81ea\u5e95\u5411\u4e0a\u5730\u63cf\u8ff0\u5404\u79cd\u6280\u672f\uff1b \u5728 preshing Memory Reordering Caught in the Act \u4e2d\uff0c\u629b\u51fa\u95ee\u9898\uff1b \u5728 preshing An Introduction to Lock-Free Programming \u7efc\u8ff0 lock-free programming\u7684\u6280\u672f\uff1b \u5728\u4e0b\u9762\u4e24\u7bc7\u6587\u7ae0\u4e2d\u63cf\u8ff0\u5404\u79cd\u53ef\u80fd\u7684memory reordering\uff0c\u5e95\u5c42\u7684\u3001\u63a7\u5236memory ordering\u7684\u6280\u672f: 1\u3001 compile-time: preshing Memory Ordering at Compile Time 2\u3001runtime: preshing Memory Barriers Are Like Source Control Operations \u5728 preshing Weak vs. Strong Memory Models \u4e2d\uff0c\u8ba8\u8bba\u4e86hardware memory model\uff0c\u5b83\u51b3\u5b9a\u4e86runtime memory ordering\u3002 preshing Memory Reordering Caught in the Act \u7ed3\u5408\u5177\u4f53\u7684\u4f8b\u5b50\u6765\u8bf4\u660e memory reordering\u7684\u5b58\u5728\uff0c\u663e\u7136memory reordering\u662f\u7f16\u5199lock-free concurrent program\u7684\u6311\u6218\u3001 \u9605\u8bfb\u5b8c\u8fd9\u7bc7\u6587\u7ae0\uff0c\u6709\u5982\u4e0b\u7591\u95ee: 1\u3001memory barrier \u548c full memory barrier 2\u3001memory barrier \u548c acquire-release semantic preshing An Introduction to Lock-Free Programming \u4ecb\u7ecd\u4e86lock-free\u7684\u542b\u4e49 \u4ecb\u7ecd\u4e86lock-free programming\u7684\u6280\u672f\u603b\u89c8 preshing Memory Ordering at Compile Time \u4ecb\u7ecd\u4e86compiler barrier\uff0c\u4ee5\u53camemory barrier \u548c compiler barrier\u7684\u5173\u7cfb\u3002 Memory barrier \u548c compiler barrier\u7684\u5173\u7cfb memory barrier \u4f1a\u4ea7\u751f compiler barrier\u7684\u6548\u679c\uff0c\u5b83\u4eec\u662f Implied Compiler Barriers\u3002 \u5173\u4e8e\u8fd9\u4e00\u70b9\uff0c\u5728 preshing Memory Barriers Are Like Source Control Operations \u4e2d\u4e5f\u6709\u603b\u7ed3: any operation which acts as a memory barrier also prevents compiler reordering . preshing Memory Barriers Are Like Source Control Operations \u4ecb\u7ecd\u5404\u79cdmemory barrier\uff0c\u6211\u4eec\u4f7f\u7528memory barrier\u7684\u76ee\u7684\u662f: \"You can enforce correct memory ordering on the processor by issuing any instruction which acts as a memory barrier . \" \u5728\u9ad8\u7ea7programming language\uff0c\u6bd4\u5982C++\u4e2d\uff0c\u5404\u79cdmemory semantic\u5176\u5b9e\u6700\u7ec8\u90fd\u662f\u4f9d\u8d56\u4e8e\u8fd9\u4e9bmemory barrier instruction\u6765\u5b9e\u73b0\u7684\uff0c\u5173\u4e8e\u8fd9\u4e00\u70b9\uff0c\u5728: 1\u3001\u5728 preshing Acquire and Release Semantics \u4e2d\uff0c\u6709\u8fd9\u6837\u7684\u4ecb\u7ecd: it\u2019s not hard to see that acquire and release semantics can be achieved using simple combinations of the memory barrier types I described at length in my previous post . Memory barrier and memory reordering \u8fd9\u7bc7\u6587\u7ae0\u6240\u603b\u7ed3\u7684\u56db\u79cdmemory barrier\u5206\u522b\u5bf9\u5e94\u4e86\u56db\u79cdmemory reordering\u3002 Memory reordering\u7684\u57fa\u672c\u539f\u5219 compiler\u3001CPU\u8fdb\u884cmemory reordering\u7684\u57fa\u672c\u539f\u5219 Memory model of CPU/hardware memory model \u8fd9\u662f\u4f5c\u8005\u7684\u7cfb\u5217\u6587\u7ae0\u90fd\u4f1a\u6d89\u53ca\u5230\u7684\u4e00\u4e2a\u95ee\u9898\u3002\u4e3b\u8981\u662f\u5728\u6587\u7ae0 preshing Weak vs. Strong Memory Models \u4e2d\u8fdb\u884c\u4e86\u8ba8\u8bba\u3002 \u63a7\u5236memory ordering \u63a7\u5236runtime memory ordering \u8fd9\u6d89\u53ca\u4e86CPU\u7684\u8fd0\u884c\u673a\u5236 \u63a7\u5236compile-time memory ordering \u8fd9\u6d89\u53ca\u4e86compiler\u3001programming language specialization\uff0c\u5c24\u5176\u662fexpression evaluation\u7684\u95ee\u9898 \u4e0b\u9762\u662f\u4e00\u4e9bdraft draft function call acts as a compiler barrier \u5728\u9605\u8bfb Memory Reordering Caught in the Act \u7684\u4f8b\u5b50\u7684source code\u7684\u65f6\u5019\uff0c\u53d1\u73b0\u4e86\u4e0b\u9762\u7684\u5185\u5bb9: class MersenneTwister { unsigned int m_buffer [ MT_LEN ]; int m_index ; public : MersenneTwister ( unsigned int seed ); // Declare noinline so that the function call acts as a compiler barrier: unsigned int integer () __attribute__ (( noinline )); }; \u663e\u7136\uff0c\u4e0a\u8ff0compiler barrier\u662fcompile-time\u5bf9memory ordering\u8fdb\u884c\u63a7\u5236\u3002 \u4e0b\u9762\u662fGoogle: function call acts as a compiler barrier stackoverflow c++ atomic: would function call act as memory barrier? preshing Memory Ordering at Compile Time Fence VS atomic variable \u5728 preshing Acquire and Release Semantics # Without Fences in Portable C++11 \u4e2d\uff0c\u5bf9\u8fd9\u4e2a\u8bdd\u9898\u8fdb\u884c\u4e86\u8ba8\u8bba\u3002","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/#_1","text":"\u672c\u7ae0\u8bb0\u5f55\u9605\u8bfb lock free \u4e3b\u9898\u7684\u6587\u7ae0\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/#_2","text":"\u4f5c\u8005\u7684\u5199\u4f5c\u601d\u8def: \u5148\u629b\u51fa\u95ee\u9898\uff0c\u7136\u540e\u6982\u8ff0\u6280\u672f\u56fe\u666f\uff0c\u7136\u540e\u81ea\u5e95\u5411\u4e0a\u5730\u63cf\u8ff0\u5404\u79cd\u6280\u672f\uff1b \u5728 preshing Memory Reordering Caught in the Act \u4e2d\uff0c\u629b\u51fa\u95ee\u9898\uff1b \u5728 preshing An Introduction to Lock-Free Programming \u7efc\u8ff0 lock-free programming\u7684\u6280\u672f\uff1b \u5728\u4e0b\u9762\u4e24\u7bc7\u6587\u7ae0\u4e2d\u63cf\u8ff0\u5404\u79cd\u53ef\u80fd\u7684memory reordering\uff0c\u5e95\u5c42\u7684\u3001\u63a7\u5236memory ordering\u7684\u6280\u672f: 1\u3001 compile-time: preshing Memory Ordering at Compile Time 2\u3001runtime: preshing Memory Barriers Are Like Source Control Operations \u5728 preshing Weak vs. Strong Memory Models \u4e2d\uff0c\u8ba8\u8bba\u4e86hardware memory model\uff0c\u5b83\u51b3\u5b9a\u4e86runtime memory ordering\u3002","title":"\u6587\u7ae0\u603b\u7ed3"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/#preshing#memory#reordering#caught#in#the#act","text":"\u7ed3\u5408\u5177\u4f53\u7684\u4f8b\u5b50\u6765\u8bf4\u660e memory reordering\u7684\u5b58\u5728\uff0c\u663e\u7136memory reordering\u662f\u7f16\u5199lock-free concurrent program\u7684\u6311\u6218\u3001 \u9605\u8bfb\u5b8c\u8fd9\u7bc7\u6587\u7ae0\uff0c\u6709\u5982\u4e0b\u7591\u95ee: 1\u3001memory barrier \u548c full memory barrier 2\u3001memory barrier \u548c acquire-release semantic","title":"preshing Memory Reordering Caught in the Act"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/#preshing#an#introduction#to#lock-free#programming","text":"\u4ecb\u7ecd\u4e86lock-free\u7684\u542b\u4e49 \u4ecb\u7ecd\u4e86lock-free programming\u7684\u6280\u672f\u603b\u89c8","title":"preshing An Introduction to Lock-Free Programming"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/#preshing#memory#ordering#at#compile#time","text":"\u4ecb\u7ecd\u4e86compiler barrier\uff0c\u4ee5\u53camemory barrier \u548c compiler barrier\u7684\u5173\u7cfb\u3002","title":"preshing Memory Ordering at Compile Time"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/#memory#barrier#compiler#barrier","text":"memory barrier \u4f1a\u4ea7\u751f compiler barrier\u7684\u6548\u679c\uff0c\u5b83\u4eec\u662f Implied Compiler Barriers\u3002 \u5173\u4e8e\u8fd9\u4e00\u70b9\uff0c\u5728 preshing Memory Barriers Are Like Source Control Operations \u4e2d\u4e5f\u6709\u603b\u7ed3: any operation which acts as a memory barrier also prevents compiler reordering .","title":"Memory barrier \u548c compiler barrier\u7684\u5173\u7cfb"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/#preshing#memory#barriers#are#like#source#control#operations","text":"\u4ecb\u7ecd\u5404\u79cdmemory barrier\uff0c\u6211\u4eec\u4f7f\u7528memory barrier\u7684\u76ee\u7684\u662f: \"You can enforce correct memory ordering on the processor by issuing any instruction which acts as a memory barrier . \" \u5728\u9ad8\u7ea7programming language\uff0c\u6bd4\u5982C++\u4e2d\uff0c\u5404\u79cdmemory semantic\u5176\u5b9e\u6700\u7ec8\u90fd\u662f\u4f9d\u8d56\u4e8e\u8fd9\u4e9bmemory barrier instruction\u6765\u5b9e\u73b0\u7684\uff0c\u5173\u4e8e\u8fd9\u4e00\u70b9\uff0c\u5728: 1\u3001\u5728 preshing Acquire and Release Semantics \u4e2d\uff0c\u6709\u8fd9\u6837\u7684\u4ecb\u7ecd: it\u2019s not hard to see that acquire and release semantics can be achieved using simple combinations of the memory barrier types I described at length in my previous post .","title":"preshing Memory Barriers Are Like Source Control Operations"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/#memory#barrier#and#memory#reordering","text":"\u8fd9\u7bc7\u6587\u7ae0\u6240\u603b\u7ed3\u7684\u56db\u79cdmemory barrier\u5206\u522b\u5bf9\u5e94\u4e86\u56db\u79cdmemory reordering\u3002","title":"Memory barrier and memory reordering"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/#memory#reordering","text":"compiler\u3001CPU\u8fdb\u884cmemory reordering\u7684\u57fa\u672c\u539f\u5219","title":"Memory reordering\u7684\u57fa\u672c\u539f\u5219"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/#memory#model#of#cpuhardware#memory#model","text":"\u8fd9\u662f\u4f5c\u8005\u7684\u7cfb\u5217\u6587\u7ae0\u90fd\u4f1a\u6d89\u53ca\u5230\u7684\u4e00\u4e2a\u95ee\u9898\u3002\u4e3b\u8981\u662f\u5728\u6587\u7ae0 preshing Weak vs. Strong Memory Models \u4e2d\u8fdb\u884c\u4e86\u8ba8\u8bba\u3002","title":"Memory model of CPU/hardware memory model"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/#memory#ordering","text":"","title":"\u63a7\u5236memory ordering"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/#runtime#memory#ordering","text":"\u8fd9\u6d89\u53ca\u4e86CPU\u7684\u8fd0\u884c\u673a\u5236","title":"\u63a7\u5236runtime memory ordering"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/#compile-time#memory#ordering","text":"\u8fd9\u6d89\u53ca\u4e86compiler\u3001programming language specialization\uff0c\u5c24\u5176\u662fexpression evaluation\u7684\u95ee\u9898 \u4e0b\u9762\u662f\u4e00\u4e9bdraft","title":"\u63a7\u5236compile-time memory ordering"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/#draft#function#call#acts#as#a#compiler#barrier","text":"\u5728\u9605\u8bfb Memory Reordering Caught in the Act \u7684\u4f8b\u5b50\u7684source code\u7684\u65f6\u5019\uff0c\u53d1\u73b0\u4e86\u4e0b\u9762\u7684\u5185\u5bb9: class MersenneTwister { unsigned int m_buffer [ MT_LEN ]; int m_index ; public : MersenneTwister ( unsigned int seed ); // Declare noinline so that the function call acts as a compiler barrier: unsigned int integer () __attribute__ (( noinline )); }; \u663e\u7136\uff0c\u4e0a\u8ff0compiler barrier\u662fcompile-time\u5bf9memory ordering\u8fdb\u884c\u63a7\u5236\u3002 \u4e0b\u9762\u662fGoogle: function call acts as a compiler barrier stackoverflow c++ atomic: would function call act as memory barrier? preshing Memory Ordering at Compile Time","title":"draft function call acts as a compiler barrier"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/#fence#vs#atomic#variable","text":"\u5728 preshing Acquire and Release Semantics # Without Fences in Portable C++11 \u4e2d\uff0c\u5bf9\u8fd9\u4e2a\u8bdd\u9898\u8fdb\u884c\u4e86\u8ba8\u8bba\u3002","title":"Fence VS atomic variable"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/02-Memory-Reordering-Caught-in-the-Act/","text":"preshing Memory Reordering Caught in the Act When writing lock-free code in C or C++, one must often take special care to enforce correct memory ordering. Otherwise, surprising things can happen. NOTE: 1\u3001\"enforce correct memory ordering\" \u5373 control \u7ecf\u5178\u6848\u4f8b Intel lists several such surprises in Volume 3, \u00a78.2.3 of their x86/64 Architecture Specification . Here\u2019s one of the simplest examples. Suppose you have two integers X and Y somewhere in memory, both initially 0. Two processors, running in parallel, execute the following machine code: Don\u2019t be thrown off by the use of assembly language in this example. It\u2019s really the best way to illustrate CPU ordering. Each processor stores 1 into one of the integer variables, then loads the other integer into a register. ( r1 and r2 are just placeholder names for actual x86 registers, such as eax .) NOTE: \u4e0a\u8ff0\u4f8b\u5b50\u5176\u5b9e\u662f\u6700\u6700\u7ecf\u5178\u7684\u3001\u7b80\u5355\u7684\u3001\u7528\u4e8e\u5c55\u793amemory ordering\u7684\u4f8b\u5b50\uff0c\u5b83\u7684source code\u5982\u4e0b: #include <iostream> #include <thread> int x = 0 ; int y = 0 ; void foo () { x = 1 ; std :: cout << y << std :: endl ; } void bar () { y = 1 ; std :: cout << x << std :: endl ; } int main () { std :: thread t1 ( foo ); std :: thread t2 ( bar ); t1 . join (); t2 . join (); } // g++ -std=c++11 -Wall -pedantic -pthread main.cpp && ./a.out Counterintuitive result(\u8fdd\u53cd\u76f4\u89c9\u7684\u7ed3\u679c) Now, no matter which processor writes 1 to memory first, it\u2019s natural to expect the other processor to read that value back, which means we should end up with either r1 = 1 , r2 = 1 , or perhaps both. But according to Intel\u2019s specification, that won\u2019t necessarily be the case. The specification says it\u2019s legal for both r1 and r2 to equal 0 at the end of this example \u2013 a counterintuitive(\u8fdd\u53cd\u76f4\u89c9\u7684) result, to say the least! Why\uff1f One way to understand this is that Intel x86/64 processors, like most processor families, are allowed to reorder the memory interactions of machine instructions according to certain rules, as long it never changes the execution of a single-threaded program. In particular, each processor is allowed to delay the effect of a store past any load from a different location. As a result, it might end up as though the instructions had executed in this order: NOTE: 1\u3001\"are allowed to reorder the memory interactions of machine instructions according to certain rules\" \u4e2d\u7684\"certain rules\"\u662f\u6307\u4ec0\u4e48rule\uff1f 2\u3001\"each processor is allowed to delay the effect of a store past any load from a different location\" \u8fd9\u6bb5\u8bdd\u7684\u610f\u601d\u662f: processor\u662f\u5141\u8bb8\u5c06store\u7684\u6548\u679c\u5ef6\u8fdf\u5230\u4efb\u4f55\u6765\u81ea\u4e0d\u540c\u5730\u65b9\u7684load\u4e4b\u540e\u7684\uff0c\u5373 processor\u662f\u5141\u8bb8\u4e0d\u540c\u7684core\u8bfb\u5230\u65e7\u503c\u7684\uff0c\u5173\u4e8e\u8fd9\u4e00\u70b9\uff0c\u5728 aristeia-C++and-the-Perils-of-Double-Checked-Locking \u4e2d\u6709\u4ecb\u7ecd\u3002 \u4e0a\u9762\u8fd9\u6bb5\u8bdd\uff0c\u5176\u5b9e\u6240\u603b\u7ed3\u7684CPU runtime instruction reordering\u3002 Let\u2019s Make It Happen NOTE: \u5b9e\u9a8c\u6765\u8fdb\u884c\u9a8c\u8bc1: \u4f5c\u8005\u7684\u5b9e\u9a8c\u7a0b\u5e8f\u662f\u6bd4\u8f83\u7b80\u5355\u7684\uff0c\u5b83\u57fa\u672c\u4e0a\u662f\u524d\u9762\u7684\u6d4b\u8bd5\u7a0b\u5e8f\u7684\u6539\u8fdb\u7248\u672c\uff0c\u589e\u52a0\u4e86\u534f\u4f5c \u548c \u9a8c\u8bc1 It\u2019s all well and good to be told this kind of thing might happen, but there\u2019s nothing like seeing it with your own eyes. That\u2019s why I\u2019ve written a small sample program to show this type of reordering actually happening . You can download the source code here . It spawns two worker threads which repeat the above transaction indefinitely, while the main thread synchronizes their work and checks each result. First worker thread Here\u2019s the source code for the first worker thread. X , Y , r1 and r2 are all globals, and POSIX semaphores are used to co-ordinate the beginning and end of each loop. sem_t beginSema1 ; sem_t endSema ; int X , Y ; int r1 , r2 ; void * thread1Func ( void * param ) { MersenneTwister random ( 1 ); // Initialize random number generator for (;;) // Loop indefinitely { sem_wait ( & beginSema1 ); // Wait for signal from main thread while ( random . integer () % 8 != 0 ) {} // Add a short, random delay // ----- THE TRANSACTION! ----- X = 1 ; asm volatile ( \"\" ::: \"memory\" ); // Prevent compiler reordering r1 = Y ; sem_post ( & endSema ); // Notify transaction complete } return NULL ; // Never returns }; Random delay NOTE: \u7b80\u5355\u6765\u8bf4\uff0c\u6dfb\u52a0random delay\u7684\u76ee\u6807\u662f: stagger(\u4ea4\u9519\u5b89\u6392) the timing of the thread A short, random delay is added before each transaction in order to stagger(\u4ea4\u9519\u5b89\u6392) the timing of the thread. Remember, there are two worker threads, and we\u2019re trying to get their instructions to overlap. The random delay is achieved using the same MersenneTwister implementation I\u2019ve used in previous posts, such as when measuring lock contention and when validating that the recursive Benaphore worked . Explicit Compiler Barrier: asm volatile NOTE: 1\u3001 X = 1; \u662fwrite 2\u3001 r1 = Y; \u662fread Don\u2019t be spooked(\u60ca\u5413) by the presence of the asm volatile line in the above code listing. This is just a directive telling the GCC compiler not to rearrange the store and the load when generating machine code, just in case it starts to get any funny ideas during optimization. We can verify this by checking the assembly code listing, as seen below. As expected, the store and the load occur in the desired order. The instruction after that writes the resulting register eax back to the global variable r1 . $ gcc -O2 -c -S -masm=intel ordering.cpp $ cat ordering.s ... mov DWORD PTR _X, 1 mov eax, DWORD PTR _Y mov DWORD PTR _r1, eax ... Main thread The main thread source code is shown below. It performs all the administrative work. After initialization, it loops indefinitely, resetting X and Y back to 0 before kicking off the worker threads on each iteration. Semaphores give us acquire and release semantics on every platform Pay particular attention to the way all writes to shared memory occur before sem_post , and all reads from shared memory occur after sem_wait . The same rules are followed in the worker threads when communicating with the main thread. Semaphores give us acquire and release semantics on every platform. That means we are guaranteed that the initial values of X = 0 and Y = 0 will propagate completely to the worker threads, and that the resulting values of r1 and r2 will propagate fully back here. In other words, the semaphores prevent memory reordering issues in the framework, allowing us to focus entirely on the experiment itself! NOTE: \u8fd9\u6bb5\u5173\u4e8e semaphore \u7684 acquire and release semantic \u7684\u63cf\u8ff0\u662f\u975e\u5e38\u597d\u7684\uff0c\u80fd\u591f\u975e\u5e38\u597d\u7684\u89e3\u91ca\u95ee\u9898\u3002 int main () { // Initialize the semaphores sem_init ( & beginSema1 , 0 , 0 ); sem_init ( & beginSema2 , 0 , 0 ); sem_init ( & endSema , 0 , 0 ); // Spawn the threads pthread_t thread1 , thread2 ; pthread_create ( & thread1 , NULL , thread1Func , NULL ); pthread_create ( & thread2 , NULL , thread2Func , NULL ); // Repeat the experiment ad infinitum int detected = 0 ; for ( int iterations = 1 ; ; iterations ++ ) { // Reset X and Y X = 0 ; Y = 0 ; // Signal both threads sem_post ( & beginSema1 ); sem_post ( & beginSema2 ); // Wait for both threads sem_wait ( & endSema ); sem_wait ( & endSema ); // Check if there was a simultaneous reorder if ( r1 == 0 && r2 == 0 ) { detected ++ ; printf ( \"%d reorders detected after %d iterations \\n \" , detected , iterations ); } } return 0 ; // Never returns } \u8fd0\u884c \u548c \u68c0\u67e5\u7ed3\u679c Finally, the moment of truth. Here\u2019s some sample output while running in Cygwin on an Intel Xeon W3520. And there you have it! During this run, a memory reordering was detected approximately once every 6600 iterations. When I tested in Ubuntu on a Core 2 Duo E6300, the occurrences were even more rare. One begins to appreciate how subtle timing bugs can creep undetected into lock-free code. Preventing It With thread affinity Now, suppose you wanted to eliminate those reorderings. There are at least two ways to do it. One way is to set thread affinities so that both worker threads run exclusively on the same CPU core. There\u2019s no portable way to set affinities with Pthreads, but on Linux it can be accomplished as follows: cpu_set_t cpus ; CPU_ZERO ( & cpus ); CPU_SET ( 0 , & cpus ); pthread_setaffinity_np ( thread1 , sizeof ( cpu_set_t ), & cpus ); pthread_setaffinity_np ( thread2 , sizeof ( cpu_set_t ), & cpus ); After this change, the reordering disappears. That\u2019s because a single processor never sees its own operations out of order, even when threads are pre-empted and rescheduled at arbitrary times. Of course, by locking both threads to a single core, we\u2019ve left the other cores unused. On a related note, I compiled and ran this sample on Playstation 3, and no memory reordering was detected. This suggests (but doesn\u2019t confirm) that the two hardware threads inside the PPU may effectively act as a single processor, with very fine-grained hardware scheduling. Preventing It With a StoreLoad Barrier Another way to prevent memory reordering in this sample is to introduce a CPU barrier between the two instructions. Here, we\u2019d like to prevent the effective reordering of a store followed by a load. In common barrier parlance, we need a StoreLoad barrier. NOTE: \u6b64\u5904\u7684store\u7136\u540eload\u6307\u7684\u662f\u5148store X\u3001Y\u7684\u503c\uff0c\u7136\u540eload\u5b83\u4eec\u7684\u503c\uff1b\u663e\u7136\uff0c StoreLoad barrier\u5b9e\u73b0: store happens-before load On x86/64 processors, there is no specific instruction which acts only as a StoreLoad barrier, but there are several instructions which do that and more. The mfence instruction is a full memory barrier, which prevents memory reordering of any kind. In GCC, it can be implemented as follows: for (;;) // Loop indefinitely { sem_wait ( & beginSema1 ); // Wait for signal from main thread while ( random . integer () % 8 != 0 ) {} // Add a short, random delay // ----- THE TRANSACTION! ----- X = 1 ; asm volatile ( \"mfence\" ::: \"memory\" ); // Prevent memory reordering r1 = Y ; sem_post ( & endSema ); // Notify transaction complete } Again, you can verify its presence by looking at the assembly code listing. ... mov DWORD PTR _X, 1 mfence mov eax, DWORD PTR _Y mov DWORD PTR _r1, eax ... With this modification, the memory reordering disappears, and we\u2019ve still allowed both threads to run on separate CPU cores. Similar Instructions and Different Platforms Interestingly, mfence isn\u2019t the only instruction which acts as a full memory barrier on x86/64. On these processors, any locked instruction, such as xchg , also acts as a full memory barrier \u2013 provided you don\u2019t use SSE instructions or write-combined memory, which this sample doesn\u2019t. In fact, the Microsoft C++ compiler generates xchg when you use the MemoryBarrier intrinsic, at least in Visual Studio 2008. The mfence instruction is specific to x86/64. If you want to make the code more portable, you could wrap this intrinsic in a preprocessor macro. The Linux kernel has wrapped it in a macro named smp_mb , along with related macros such as smp_rmb and smp_wmb , and provided alternate implementations on different architectures . For example, on PowerPC, smp_mb is implemented as sync . All these different CPU families, each having unique instructions to enforce memory ordering, with each compiler exposing them through different instrincs, and each cross-platform project implementing its own portability layer\u2026 none of this helps simplify lock-free programming! This is partially why the C++11 atomic library standard was recently introduced. It\u2019s an attempt to standardize things, and make it easier to write portable lock-free code.","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/02-Memory-Reordering-Caught-in-the-Act/#preshing#memory#reordering#caught#in#the#act","text":"When writing lock-free code in C or C++, one must often take special care to enforce correct memory ordering. Otherwise, surprising things can happen. NOTE: 1\u3001\"enforce correct memory ordering\" \u5373 control","title":"preshing Memory Reordering Caught in the Act"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/02-Memory-Reordering-Caught-in-the-Act/#_1","text":"Intel lists several such surprises in Volume 3, \u00a78.2.3 of their x86/64 Architecture Specification . Here\u2019s one of the simplest examples. Suppose you have two integers X and Y somewhere in memory, both initially 0. Two processors, running in parallel, execute the following machine code: Don\u2019t be thrown off by the use of assembly language in this example. It\u2019s really the best way to illustrate CPU ordering. Each processor stores 1 into one of the integer variables, then loads the other integer into a register. ( r1 and r2 are just placeholder names for actual x86 registers, such as eax .) NOTE: \u4e0a\u8ff0\u4f8b\u5b50\u5176\u5b9e\u662f\u6700\u6700\u7ecf\u5178\u7684\u3001\u7b80\u5355\u7684\u3001\u7528\u4e8e\u5c55\u793amemory ordering\u7684\u4f8b\u5b50\uff0c\u5b83\u7684source code\u5982\u4e0b: #include <iostream> #include <thread> int x = 0 ; int y = 0 ; void foo () { x = 1 ; std :: cout << y << std :: endl ; } void bar () { y = 1 ; std :: cout << x << std :: endl ; } int main () { std :: thread t1 ( foo ); std :: thread t2 ( bar ); t1 . join (); t2 . join (); } // g++ -std=c++11 -Wall -pedantic -pthread main.cpp && ./a.out","title":"\u7ecf\u5178\u6848\u4f8b"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/02-Memory-Reordering-Caught-in-the-Act/#counterintuitive#result","text":"Now, no matter which processor writes 1 to memory first, it\u2019s natural to expect the other processor to read that value back, which means we should end up with either r1 = 1 , r2 = 1 , or perhaps both. But according to Intel\u2019s specification, that won\u2019t necessarily be the case. The specification says it\u2019s legal for both r1 and r2 to equal 0 at the end of this example \u2013 a counterintuitive(\u8fdd\u53cd\u76f4\u89c9\u7684) result, to say the least!","title":"Counterintuitive result(\u8fdd\u53cd\u76f4\u89c9\u7684\u7ed3\u679c)"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/02-Memory-Reordering-Caught-in-the-Act/#why","text":"One way to understand this is that Intel x86/64 processors, like most processor families, are allowed to reorder the memory interactions of machine instructions according to certain rules, as long it never changes the execution of a single-threaded program. In particular, each processor is allowed to delay the effect of a store past any load from a different location. As a result, it might end up as though the instructions had executed in this order: NOTE: 1\u3001\"are allowed to reorder the memory interactions of machine instructions according to certain rules\" \u4e2d\u7684\"certain rules\"\u662f\u6307\u4ec0\u4e48rule\uff1f 2\u3001\"each processor is allowed to delay the effect of a store past any load from a different location\" \u8fd9\u6bb5\u8bdd\u7684\u610f\u601d\u662f: processor\u662f\u5141\u8bb8\u5c06store\u7684\u6548\u679c\u5ef6\u8fdf\u5230\u4efb\u4f55\u6765\u81ea\u4e0d\u540c\u5730\u65b9\u7684load\u4e4b\u540e\u7684\uff0c\u5373 processor\u662f\u5141\u8bb8\u4e0d\u540c\u7684core\u8bfb\u5230\u65e7\u503c\u7684\uff0c\u5173\u4e8e\u8fd9\u4e00\u70b9\uff0c\u5728 aristeia-C++and-the-Perils-of-Double-Checked-Locking \u4e2d\u6709\u4ecb\u7ecd\u3002 \u4e0a\u9762\u8fd9\u6bb5\u8bdd\uff0c\u5176\u5b9e\u6240\u603b\u7ed3\u7684CPU runtime instruction reordering\u3002","title":"Why\uff1f"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/02-Memory-Reordering-Caught-in-the-Act/#lets#make#it#happen","text":"NOTE: \u5b9e\u9a8c\u6765\u8fdb\u884c\u9a8c\u8bc1: \u4f5c\u8005\u7684\u5b9e\u9a8c\u7a0b\u5e8f\u662f\u6bd4\u8f83\u7b80\u5355\u7684\uff0c\u5b83\u57fa\u672c\u4e0a\u662f\u524d\u9762\u7684\u6d4b\u8bd5\u7a0b\u5e8f\u7684\u6539\u8fdb\u7248\u672c\uff0c\u589e\u52a0\u4e86\u534f\u4f5c \u548c \u9a8c\u8bc1 It\u2019s all well and good to be told this kind of thing might happen, but there\u2019s nothing like seeing it with your own eyes. That\u2019s why I\u2019ve written a small sample program to show this type of reordering actually happening . You can download the source code here . It spawns two worker threads which repeat the above transaction indefinitely, while the main thread synchronizes their work and checks each result.","title":"Let\u2019s Make It Happen"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/02-Memory-Reordering-Caught-in-the-Act/#first#worker#thread","text":"Here\u2019s the source code for the first worker thread. X , Y , r1 and r2 are all globals, and POSIX semaphores are used to co-ordinate the beginning and end of each loop. sem_t beginSema1 ; sem_t endSema ; int X , Y ; int r1 , r2 ; void * thread1Func ( void * param ) { MersenneTwister random ( 1 ); // Initialize random number generator for (;;) // Loop indefinitely { sem_wait ( & beginSema1 ); // Wait for signal from main thread while ( random . integer () % 8 != 0 ) {} // Add a short, random delay // ----- THE TRANSACTION! ----- X = 1 ; asm volatile ( \"\" ::: \"memory\" ); // Prevent compiler reordering r1 = Y ; sem_post ( & endSema ); // Notify transaction complete } return NULL ; // Never returns };","title":"First worker thread"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/02-Memory-Reordering-Caught-in-the-Act/#random#delay","text":"NOTE: \u7b80\u5355\u6765\u8bf4\uff0c\u6dfb\u52a0random delay\u7684\u76ee\u6807\u662f: stagger(\u4ea4\u9519\u5b89\u6392) the timing of the thread A short, random delay is added before each transaction in order to stagger(\u4ea4\u9519\u5b89\u6392) the timing of the thread. Remember, there are two worker threads, and we\u2019re trying to get their instructions to overlap. The random delay is achieved using the same MersenneTwister implementation I\u2019ve used in previous posts, such as when measuring lock contention and when validating that the recursive Benaphore worked .","title":"Random delay"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/02-Memory-Reordering-Caught-in-the-Act/#explicit#compiler#barrier#asm#volatile","text":"NOTE: 1\u3001 X = 1; \u662fwrite 2\u3001 r1 = Y; \u662fread Don\u2019t be spooked(\u60ca\u5413) by the presence of the asm volatile line in the above code listing. This is just a directive telling the GCC compiler not to rearrange the store and the load when generating machine code, just in case it starts to get any funny ideas during optimization. We can verify this by checking the assembly code listing, as seen below. As expected, the store and the load occur in the desired order. The instruction after that writes the resulting register eax back to the global variable r1 . $ gcc -O2 -c -S -masm=intel ordering.cpp $ cat ordering.s ... mov DWORD PTR _X, 1 mov eax, DWORD PTR _Y mov DWORD PTR _r1, eax ...","title":"Explicit Compiler Barrier: asm volatile"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/02-Memory-Reordering-Caught-in-the-Act/#main#thread","text":"The main thread source code is shown below. It performs all the administrative work. After initialization, it loops indefinitely, resetting X and Y back to 0 before kicking off the worker threads on each iteration.","title":"Main thread"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/02-Memory-Reordering-Caught-in-the-Act/#semaphores#give#us#acquire#and#release#semantics#on#every#platform","text":"Pay particular attention to the way all writes to shared memory occur before sem_post , and all reads from shared memory occur after sem_wait . The same rules are followed in the worker threads when communicating with the main thread. Semaphores give us acquire and release semantics on every platform. That means we are guaranteed that the initial values of X = 0 and Y = 0 will propagate completely to the worker threads, and that the resulting values of r1 and r2 will propagate fully back here. In other words, the semaphores prevent memory reordering issues in the framework, allowing us to focus entirely on the experiment itself! NOTE: \u8fd9\u6bb5\u5173\u4e8e semaphore \u7684 acquire and release semantic \u7684\u63cf\u8ff0\u662f\u975e\u5e38\u597d\u7684\uff0c\u80fd\u591f\u975e\u5e38\u597d\u7684\u89e3\u91ca\u95ee\u9898\u3002 int main () { // Initialize the semaphores sem_init ( & beginSema1 , 0 , 0 ); sem_init ( & beginSema2 , 0 , 0 ); sem_init ( & endSema , 0 , 0 ); // Spawn the threads pthread_t thread1 , thread2 ; pthread_create ( & thread1 , NULL , thread1Func , NULL ); pthread_create ( & thread2 , NULL , thread2Func , NULL ); // Repeat the experiment ad infinitum int detected = 0 ; for ( int iterations = 1 ; ; iterations ++ ) { // Reset X and Y X = 0 ; Y = 0 ; // Signal both threads sem_post ( & beginSema1 ); sem_post ( & beginSema2 ); // Wait for both threads sem_wait ( & endSema ); sem_wait ( & endSema ); // Check if there was a simultaneous reorder if ( r1 == 0 && r2 == 0 ) { detected ++ ; printf ( \"%d reorders detected after %d iterations \\n \" , detected , iterations ); } } return 0 ; // Never returns }","title":"Semaphores give us acquire and release semantics on every platform"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/02-Memory-Reordering-Caught-in-the-Act/#_2","text":"Finally, the moment of truth. Here\u2019s some sample output while running in Cygwin on an Intel Xeon W3520. And there you have it! During this run, a memory reordering was detected approximately once every 6600 iterations. When I tested in Ubuntu on a Core 2 Duo E6300, the occurrences were even more rare. One begins to appreciate how subtle timing bugs can creep undetected into lock-free code.","title":"\u8fd0\u884c \u548c \u68c0\u67e5\u7ed3\u679c"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/02-Memory-Reordering-Caught-in-the-Act/#preventing#it#with#thread#affinity","text":"Now, suppose you wanted to eliminate those reorderings. There are at least two ways to do it. One way is to set thread affinities so that both worker threads run exclusively on the same CPU core. There\u2019s no portable way to set affinities with Pthreads, but on Linux it can be accomplished as follows: cpu_set_t cpus ; CPU_ZERO ( & cpus ); CPU_SET ( 0 , & cpus ); pthread_setaffinity_np ( thread1 , sizeof ( cpu_set_t ), & cpus ); pthread_setaffinity_np ( thread2 , sizeof ( cpu_set_t ), & cpus ); After this change, the reordering disappears. That\u2019s because a single processor never sees its own operations out of order, even when threads are pre-empted and rescheduled at arbitrary times. Of course, by locking both threads to a single core, we\u2019ve left the other cores unused. On a related note, I compiled and ran this sample on Playstation 3, and no memory reordering was detected. This suggests (but doesn\u2019t confirm) that the two hardware threads inside the PPU may effectively act as a single processor, with very fine-grained hardware scheduling.","title":"Preventing It With thread affinity"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/02-Memory-Reordering-Caught-in-the-Act/#preventing#it#with#a#storeload#barrier","text":"Another way to prevent memory reordering in this sample is to introduce a CPU barrier between the two instructions. Here, we\u2019d like to prevent the effective reordering of a store followed by a load. In common barrier parlance, we need a StoreLoad barrier. NOTE: \u6b64\u5904\u7684store\u7136\u540eload\u6307\u7684\u662f\u5148store X\u3001Y\u7684\u503c\uff0c\u7136\u540eload\u5b83\u4eec\u7684\u503c\uff1b\u663e\u7136\uff0c StoreLoad barrier\u5b9e\u73b0: store happens-before load On x86/64 processors, there is no specific instruction which acts only as a StoreLoad barrier, but there are several instructions which do that and more. The mfence instruction is a full memory barrier, which prevents memory reordering of any kind. In GCC, it can be implemented as follows: for (;;) // Loop indefinitely { sem_wait ( & beginSema1 ); // Wait for signal from main thread while ( random . integer () % 8 != 0 ) {} // Add a short, random delay // ----- THE TRANSACTION! ----- X = 1 ; asm volatile ( \"mfence\" ::: \"memory\" ); // Prevent memory reordering r1 = Y ; sem_post ( & endSema ); // Notify transaction complete } Again, you can verify its presence by looking at the assembly code listing. ... mov DWORD PTR _X, 1 mfence mov eax, DWORD PTR _Y mov DWORD PTR _r1, eax ... With this modification, the memory reordering disappears, and we\u2019ve still allowed both threads to run on separate CPU cores.","title":"Preventing It With a StoreLoad Barrier"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/02-Memory-Reordering-Caught-in-the-Act/#similar#instructions#and#different#platforms","text":"Interestingly, mfence isn\u2019t the only instruction which acts as a full memory barrier on x86/64. On these processors, any locked instruction, such as xchg , also acts as a full memory barrier \u2013 provided you don\u2019t use SSE instructions or write-combined memory, which this sample doesn\u2019t. In fact, the Microsoft C++ compiler generates xchg when you use the MemoryBarrier intrinsic, at least in Visual Studio 2008. The mfence instruction is specific to x86/64. If you want to make the code more portable, you could wrap this intrinsic in a preprocessor macro. The Linux kernel has wrapped it in a macro named smp_mb , along with related macros such as smp_rmb and smp_wmb , and provided alternate implementations on different architectures . For example, on PowerPC, smp_mb is implemented as sync . All these different CPU families, each having unique instructions to enforce memory ordering, with each compiler exposing them through different instrincs, and each cross-platform project implementing its own portability layer\u2026 none of this helps simplify lock-free programming! This is partially why the C++11 atomic library standard was recently introduced. It\u2019s an attempt to standardize things, and make it easier to write portable lock-free code.","title":"Similar Instructions and Different Platforms"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/02-Memory-Reordering-Caught-in-the-Act/gcc-source-code/","text":"gcc source code #include <pthread.h> #include <semaphore.h> #include <stdio.h> // Set either of these to 1 to prevent CPU reordering #define USE_CPU_FENCE 0 #define USE_SINGLE_HW_THREAD 0 // Supported on Linux, but not Cygwin or PS3 #if USE_SINGLE_HW_THREAD #include <sched.h> #endif //------------------------------------- // MersenneTwister // A thread-safe random number generator with good randomness // in a small number of instructions. We'll use it to introduce // random timing delays. //------------------------------------- #define MT_IA 397 #define MT_LEN 624 class MersenneTwister { unsigned int m_buffer [ MT_LEN ]; int m_index ; public : MersenneTwister ( unsigned int seed ); // Declare noinline so that the function call acts as a compiler barrier: unsigned int integer () __attribute__ (( noinline )); }; MersenneTwister :: MersenneTwister ( unsigned int seed ) { // Initialize by filling with the seed, then iterating // the algorithm a bunch of times to shuffle things up. for ( int i = 0 ; i < MT_LEN ; i ++ ) m_buffer [ i ] = seed ; m_index = 0 ; for ( int i = 0 ; i < MT_LEN * 100 ; i ++ ) integer (); } unsigned int MersenneTwister :: integer () { // Indices int i = m_index ; int i2 = m_index + 1 ; if ( i2 >= MT_LEN ) i2 = 0 ; // wrap-around int j = m_index + MT_IA ; if ( j >= MT_LEN ) j -= MT_LEN ; // wrap-around // Twist unsigned int s = ( m_buffer [ i ] & 0x80000000 ) | ( m_buffer [ i2 ] & 0x7fffffff ); unsigned int r = m_buffer [ j ] ^ ( s >> 1 ) ^ (( s & 1 ) * 0x9908B0DF ); m_buffer [ m_index ] = r ; m_index = i2 ; // Swizzle r ^= ( r >> 11 ); r ^= ( r << 7 ) & 0x9d2c5680UL ; r ^= ( r << 15 ) & 0xefc60000UL ; r ^= ( r >> 18 ); return r ; } //------------------------------------- // Main program, as decribed in the post //------------------------------------- sem_t beginSema1 ; sem_t beginSema2 ; sem_t endSema ; int X , Y ; int r1 , r2 ; void * thread1Func ( void * param ) { MersenneTwister random ( 1 ); for (;;) { sem_wait ( & beginSema1 ); // Wait for signal while ( random . integer () % 8 != 0 ) {} // Random delay // ----- THE TRANSACTION! ----- X = 1 ; #if USE_CPU_FENCE asm volatile ( \"mfence\" ::: \"memory\" ); // Prevent CPU reordering #else asm volatile ( \"\" ::: \"memory\" ); // Prevent compiler reordering #endif r1 = Y ; sem_post ( & endSema ); // Notify transaction complete } return NULL ; // Never returns }; void * thread2Func ( void * param ) { MersenneTwister random ( 2 ); for (;;) { sem_wait ( & beginSema2 ); // Wait for signal while ( random . integer () % 8 != 0 ) {} // Random delay // ----- THE TRANSACTION! ----- Y = 1 ; #if USE_CPU_FENCE asm volatile ( \"mfence\" ::: \"memory\" ); // Prevent CPU reordering #else asm volatile ( \"\" ::: \"memory\" ); // Prevent compiler reordering #endif r2 = X ; sem_post ( & endSema ); // Notify transaction complete } return NULL ; // Never returns }; int main () { // Initialize the semaphores sem_init ( & beginSema1 , 0 , 0 ); sem_init ( & beginSema2 , 0 , 0 ); sem_init ( & endSema , 0 , 0 ); // Spawn the threads pthread_t thread1 , thread2 ; pthread_create ( & thread1 , NULL , thread1Func , NULL ); pthread_create ( & thread2 , NULL , thread2Func , NULL ); #if USE_SINGLE_HW_THREAD // Force thread affinities to the same cpu core. cpu_set_t cpus ; CPU_ZERO ( & cpus ); CPU_SET ( 0 , & cpus ); pthread_setaffinity_np ( thread1 , sizeof ( cpu_set_t ), & cpus ); pthread_setaffinity_np ( thread2 , sizeof ( cpu_set_t ), & cpus ); #endif // Repeat the experiment ad infinitum int detected = 0 ; for ( int iterations = 1 ; ; iterations ++ ) { // Reset X and Y X = 0 ; Y = 0 ; // Signal both threads sem_post ( & beginSema1 ); sem_post ( & beginSema2 ); // Wait for both threads sem_wait ( & endSema ); sem_wait ( & endSema ); // Check if there was a simultaneous reorder if ( r1 == 0 && r2 == 0 ) { detected ++ ; printf ( \"%d reorders detected after %d iterations \\n \" , detected , iterations ); } } return 0 ; // Never returns } Makefile ordering : ordering . cpp gcc -o ordering -O2 ordering.cpp -lpthread","title":"gcc-source-code"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/02-Memory-Reordering-Caught-in-the-Act/gcc-source-code/#gcc#source#code","text":"#include <pthread.h> #include <semaphore.h> #include <stdio.h> // Set either of these to 1 to prevent CPU reordering #define USE_CPU_FENCE 0 #define USE_SINGLE_HW_THREAD 0 // Supported on Linux, but not Cygwin or PS3 #if USE_SINGLE_HW_THREAD #include <sched.h> #endif //------------------------------------- // MersenneTwister // A thread-safe random number generator with good randomness // in a small number of instructions. We'll use it to introduce // random timing delays. //------------------------------------- #define MT_IA 397 #define MT_LEN 624 class MersenneTwister { unsigned int m_buffer [ MT_LEN ]; int m_index ; public : MersenneTwister ( unsigned int seed ); // Declare noinline so that the function call acts as a compiler barrier: unsigned int integer () __attribute__ (( noinline )); }; MersenneTwister :: MersenneTwister ( unsigned int seed ) { // Initialize by filling with the seed, then iterating // the algorithm a bunch of times to shuffle things up. for ( int i = 0 ; i < MT_LEN ; i ++ ) m_buffer [ i ] = seed ; m_index = 0 ; for ( int i = 0 ; i < MT_LEN * 100 ; i ++ ) integer (); } unsigned int MersenneTwister :: integer () { // Indices int i = m_index ; int i2 = m_index + 1 ; if ( i2 >= MT_LEN ) i2 = 0 ; // wrap-around int j = m_index + MT_IA ; if ( j >= MT_LEN ) j -= MT_LEN ; // wrap-around // Twist unsigned int s = ( m_buffer [ i ] & 0x80000000 ) | ( m_buffer [ i2 ] & 0x7fffffff ); unsigned int r = m_buffer [ j ] ^ ( s >> 1 ) ^ (( s & 1 ) * 0x9908B0DF ); m_buffer [ m_index ] = r ; m_index = i2 ; // Swizzle r ^= ( r >> 11 ); r ^= ( r << 7 ) & 0x9d2c5680UL ; r ^= ( r << 15 ) & 0xefc60000UL ; r ^= ( r >> 18 ); return r ; } //------------------------------------- // Main program, as decribed in the post //------------------------------------- sem_t beginSema1 ; sem_t beginSema2 ; sem_t endSema ; int X , Y ; int r1 , r2 ; void * thread1Func ( void * param ) { MersenneTwister random ( 1 ); for (;;) { sem_wait ( & beginSema1 ); // Wait for signal while ( random . integer () % 8 != 0 ) {} // Random delay // ----- THE TRANSACTION! ----- X = 1 ; #if USE_CPU_FENCE asm volatile ( \"mfence\" ::: \"memory\" ); // Prevent CPU reordering #else asm volatile ( \"\" ::: \"memory\" ); // Prevent compiler reordering #endif r1 = Y ; sem_post ( & endSema ); // Notify transaction complete } return NULL ; // Never returns }; void * thread2Func ( void * param ) { MersenneTwister random ( 2 ); for (;;) { sem_wait ( & beginSema2 ); // Wait for signal while ( random . integer () % 8 != 0 ) {} // Random delay // ----- THE TRANSACTION! ----- Y = 1 ; #if USE_CPU_FENCE asm volatile ( \"mfence\" ::: \"memory\" ); // Prevent CPU reordering #else asm volatile ( \"\" ::: \"memory\" ); // Prevent compiler reordering #endif r2 = X ; sem_post ( & endSema ); // Notify transaction complete } return NULL ; // Never returns }; int main () { // Initialize the semaphores sem_init ( & beginSema1 , 0 , 0 ); sem_init ( & beginSema2 , 0 , 0 ); sem_init ( & endSema , 0 , 0 ); // Spawn the threads pthread_t thread1 , thread2 ; pthread_create ( & thread1 , NULL , thread1Func , NULL ); pthread_create ( & thread2 , NULL , thread2Func , NULL ); #if USE_SINGLE_HW_THREAD // Force thread affinities to the same cpu core. cpu_set_t cpus ; CPU_ZERO ( & cpus ); CPU_SET ( 0 , & cpus ); pthread_setaffinity_np ( thread1 , sizeof ( cpu_set_t ), & cpus ); pthread_setaffinity_np ( thread2 , sizeof ( cpu_set_t ), & cpus ); #endif // Repeat the experiment ad infinitum int detected = 0 ; for ( int iterations = 1 ; ; iterations ++ ) { // Reset X and Y X = 0 ; Y = 0 ; // Signal both threads sem_post ( & beginSema1 ); sem_post ( & beginSema2 ); // Wait for both threads sem_wait ( & endSema ); sem_wait ( & endSema ); // Check if there was a simultaneous reorder if ( r1 == 0 && r2 == 0 ) { detected ++ ; printf ( \"%d reorders detected after %d iterations \\n \" , detected , iterations ); } } return 0 ; // Never returns }","title":"gcc source code"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/02-Memory-Reordering-Caught-in-the-Act/gcc-source-code/#makefile","text":"ordering : ordering . cpp gcc -o ordering -O2 ordering.cpp -lpthread","title":"Makefile"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/03-An-Introduction-to-Lock-Free-Programming/","text":"preshing An Introduction to Lock-Free Programming I was fortunate in that my first introduction to lock-free (also known as lockless) programming was Bruce Dawson\u2019s excellent and comprehensive white paper, Lockless Programming Considerations . And like many, I\u2019ve had the occasion to put Bruce\u2019s advice into practice developing and debugging lock-free code on platforms such as the Xbox 360. NOTE: Lockless Programming Considerations \u6240\u94fe\u63a5\u7684\u6587\u7ae0\u6536\u5f55\u5728 \"microsoft-Lockless-Programming-Considerations-for-Xbox-360-and-Microsoft-Windows\" \u7ae0\u8282 What Is It? People often describe lock-free programming as programming without mutexes, which are also referred to as locks . That\u2019s true, but it\u2019s only part of the story. The generally accepted definition, based on academic literature, is a bit more broad. At its essence, lock-free is a property used to describe some code, without saying too much about how that code was actually written. NOTE: \"At its essence, lock-free is a property used to describe some code, without saying too much about how that code was actually written.\" \u8fd9\u6bb5\u8bdd\u7684\u610f\u601d\u662f: lock-free \u6240\u63cf\u8ff0\u7684\u662f property\uff0c\u5b83\u7684definition\u5e76\u4e0d\u6d89\u53ca\u5b9e\u73b0\uff0c\u56e0\u6b64\u6211\u4eec\u5728\u7406\u89e3lock free\u7684\u65f6\u5019\uff0c\u5e94\u8be5\u662f\u4ece\u6027\u8d28\u5165\u624b\uff0c\u800c\u4e0d\u662f\u4ece\u5b9e\u73b0\u5165\u624b Basically, if some part of your program satisfies the following conditions, then that part can rightfully be considered lock-free. Conversely, if a given part of your code doesn\u2019t satisfy these conditions, then that part is not lock-free. NOTE: \"can the threads block each other?\"\u7684\u6ce8\u91ca \"is there some way to schedule the threads which would lock up indefinitely?\" \u7684\u5b57\u9762\u610f\u601d\u662f: \u662f\u5426\u6709\u4e00\u4e9b\u65b9\u6cd5\u6765\u8c03\u5ea6\u90a3\u4e9b\u5c06\u88ab\u65e0\u9650\u671f\u9501\u5b9a\u7684\u7ebf\u7a0b\uff0c\u90a3\u5b83\u8981\u5982\u4f55\u7406\u89e3\uff1f lock \u7684\u542b\u4e49 NOTE: lock\u7684\u542b\u4e49\u662f: \"the possibility of \u201clocking up\u201d the entire application in some way, whether it\u2019s deadlock, livelock \u2013 or even due to hypothetical thread scheduling decisions made by your worst enemy\" \u56e0\u6b64\uff0clock-less\u4e2d\u7684lock\uff0c\u9664\u4e86\u5305\u62ec mutexes\uff0c\u8fd8\u5305\u62ec\u4e0b\u9762\u7684\"Example \"\u7ae0\u8282\u4e2d\u7684\u4f8b\u5b50 In this sense, the lock in lock-free does not refer directly to mutexes, but rather to the possibility of \u201clocking up\u201d the entire application in some way, whether it\u2019s deadlock, livelock \u2013 or even due to hypothetical thread scheduling decisions made by your worst enemy(\u4e0b\u9762\u7684\"Example\"\u7ae0\u8282\u4e2d\u7684\u4f8b\u5b50\u5c31\u662f\u5bf9\u6b64\u7684\u8bf4\u660e). That last point sounds funny, but it\u2019s key. Shared mutexes are ruled out(\u6392\u9664) trivially, because as soon as one thread obtains the mutex, your worst enemy could simply never schedule that thread again. Of course, real operating systems don\u2019t work that way \u2013 we\u2019re merely defining terms. Example Here\u2019s a simple example of an operation which contains no mutexes, but is still not lock-free. Initially, X = 0 . As an exercise for the reader, consider how two threads could be scheduled in a way such that neither thread exits the loop. while ( X == 0 ) { X = 1 - X ; } NOTE: \u53ef\u80fd\u5bfc\u81f4\"\u201clocking up\u201d the entire application\"\u7684\u8c03\u5ea6\u7b56\u7565\u662f: 1\u3001\u4e24\u4e2athread\u540c\u65f6\u8fdb\u5165 while 2\u3001\u6b64\u65f6 X \u4e3a0\uff0c\u7b2c\u4e00\u4e2athread\u6267\u884c X = 1 - X; \uff0c\u5219 X \u4e3a1\uff1b 3\u3001\u6b64\u65f6 X \u4e3a1\uff0c\u7b2c\u4e8c\u4e2athread\u6267\u884c X = 1 - X; \uff0c\u5219 X \u4e3a0\uff0c\u663e\u7136\u6b64\u65f6\u56de\u5230\u4e86\u539f\u70b9 4\u3001\u6309\u71671-3\u6267\u884c\uff0c\u5219\u6c38\u8fdc\u9677\u5165\u8fd9\u4e2a\u6b7b\u5faa\u73af Nobody expects a large application to be entirely lock-free. Typically, we identify a specific set of lock-free operations out of the whole codebase. For example, in a lock-free queue, there might be a handful of lock-free operations such as push , pop , perhaps isEmpty , and so on. Definition by Herlihy & Shavit, authors of The Art of Multiprocessor Programming NOTE: \u7531Herlihy & Shavit\u7ed9\u51fa\u7684lock free\u7684\u5b9a\u4e49 Herlihy & Shavit, authors of The Art of Multiprocessor Programming , tend to express such operations as class methods, and offer the following succinct definition of lock-free (see slide 150 ): \u201cIn an infinite execution, infinitely often some method call finishes.\u201d In other words, as long as the program is able to keep calling those lock-free operations, the number of completed calls keeps increasing, no matter what. It is algorithmically impossible for the system to lock up during those operations. NOTE: \u5b57\u9762\u610f\u601d: Herlihy & Shavit\uff0c\u300a\u591a\u5904\u7406\u5668\u7f16\u7a0b\u7684\u827a\u672f\u300b\u7684\u4f5c\u8005\uff0c\u503e\u5411\u4e8e\u5c06\u8fd9\u4e9b\u64cd\u4f5c\u8868\u793a\u4e3a\u7c7b\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u4ee5\u4e0b\u65e0\u9501\u7684\u7b80\u6d01\u5b9a\u4e49(\u89c1\u5e7b\u706f\u7247150):\u201c\u5728\u65e0\u9650\u6267\u884c\u4e2d\uff0c\u4e00\u4e9b\u65b9\u6cd5\u8c03\u7528\u603b\u662f\u65e0\u9650\u5730\u7ed3\u675f\u3002 \u6362\u53e5\u8bdd\u8bf4\uff0c\u53ea\u8981\u7a0b\u5e8f\u80fd\u591f\u7ee7\u7eed*\u8c03\u7528*\u8fd9\u4e9b\u65e0\u9501\u64cd\u4f5c\uff0c*\u5b8c\u6210\u7684*\u8c03\u7528\u7684\u6570\u91cf\u5c31\u4f1a\u4e0d\u65ad\u589e\u52a0\uff0c\u65e0\u8bba\u53d1\u751f\u4ec0\u4e48\u3002 \u5728\u8fd9\u4e9b\u64cd\u4f5c\u671f\u95f4\uff0c\u7cfb\u7edf\u5728\u7b97\u6cd5\u4e0a\u662f\u4e0d\u53ef\u80fd\u9501\u5b9a\u7684\u3002 Consequence of lock-free programming NOTE: lock-free programming \u7684\u6027\u6027\u8d28\u7684\u4e00\u4e9b\u5e94\u7528\u573a\u666f One important consequence of lock-free programming is that if you suspend a single thread, it will never prevent other threads from making progress, as a group, through their own lock-free operations. This hints(\u6697\u793a) at the value of lock-free programming when writing interrupt handlers and real-time systems, where certain tasks must complete within a certain time limit, no matter what state the rest of the program is in. A final precision: Operations that are designed to block do not disqualify the algorithm. For example, a queue\u2019s pop operation may intentionally block when the queue is empty. The remaining codepaths can still be considered lock-free. NOTE: \u8fd9\u4e00\u6bb5\u7684\u5b57\u9762\u610f\u601d: \u6700\u7ec8\u7cbe\u5ea6:\u88ab\u8bbe\u8ba1\u4e3a\u963b\u585e\u7684\u64cd\u4f5c\u4e0d\u4f1a\u53d6\u6d88\u7b97\u6cd5\u7684\u8d44\u683c\u3002\u4f8b\u5982\uff0c\u5f53\u961f\u5217\u4e3a\u7a7a\u65f6\uff0c\u961f\u5217\u7684pop\u64cd\u4f5c\u53ef\u80fd\u4f1a\u6545\u610f\u963b\u585e\u3002\u5176\u4f59\u7684\u4ee3\u7801\u8def\u5f84\u4ecd\u7136\u53ef\u4ee5\u8ba4\u4e3a\u662f\u65e0\u9501\u7684\u3002 \u542b\u4e49\u662f: blocking operation\u5e76\u4e0d\u4f1a\u4f7f\u5f97\u4f7f\u7528\u5b83\u7684algorithm\u6210\u4e3a\u975elock-free\u7684 Lock-Free Programming Techniques So how do these techniques relate to one another? To illustrate, I\u2019ve put together the following flowchart. I\u2019ll elaborate on each one below. Atomic Read-Modify-Write Operations Atomic operations are ones which manipulate memory in a way that appears indivisible: No thread can observe the operation half-complete. On modern processors, lots of operations are already atomic. For example, aligned reads and writes of simple types are usually atomic . Read-modify-write (RMW) operations go a step further, allowing you to perform more complex transactions atomically. They\u2019re especially useful when a lock-free algorithm must support multiple writers, because when multiple threads attempt an RMW on the same address, they\u2019ll effectively line up in a row and execute those operations one-at-a-time. I\u2019ve already touched upon RMW operations in this blog, such as when implementing a lightweight mutex , a recursive mutex and a lightweight logging system . Examples of RMW operations include 1\u3001 _InterlockedIncrement on Win32, 2\u3001 OSAtomicAdd32 on iOS, and 3\u3001 std::atomic::fetch_add in C++11. Be aware that the C++11 atomic standard does not guarantee that the implementation will be lock-free on every platform, so it\u2019s best to know the capabilities of your platform and toolchain. You can call std::atomic<>::is_lock_free to make sure. Different CPU families support RMW in different ways . Processors such as PowerPC and ARM expose load-link/store-conditional instructions, which effectively allow you to implement your own RMW primitive at a low level, though this is not often done. The common RMW operations are usually sufficient. As illustrated by the flowchart, atomic RMWs are a necessary part of lock-free programming even on single-processor systems. Without atomicity, a thread could be interrupted halfway through the transaction, possibly leading to an inconsistent state. Compare-And-Swap Loops Perhaps the most often-discussed RMW operation is compare-and-swap (CAS). On Win32, CAS is provided via a family of intrinsics such as _InterlockedCompareExchange . Often, programmers perform compare-and-swap in a loop to repeatedly attempt a transaction. This pattern typically involves copying a shared variable to a local variable, performing some speculative work, and attempting to publish the changes using CAS: void LockFreeQueue :: push ( Node * newHead ) { for (;;) { // Copy a shared variable (m_Head) to a local. Node * oldHead = m_Head ; // Do some speculative work, not yet visible to other threads. newHead -> next = oldHead ; // Next, attempt to publish our changes to the shared variable. // If the shared variable hasn't changed, the CAS succeeds and we return. // Otherwise, repeat. if ( _InterlockedCompareExchange ( & m_Head , newHead , oldHead ) == oldHead ) return ; } } NOTE: \u4ee3\u7801\u7406\u89e3: \u5982\u679c m_Head (\u5f53\u524d\u7684head)\u548c oldHead (\u4e4b\u524d\u7684head)\u76f8\u7b49\uff0c\u8bf4\u660e\u5728\u8fd9\u4e4b\u95f4\uff0c\u6ca1\u6709thread\u4fee\u6539queue\uff0c\u5219\u5c06 newHead \u5199\u5165\uff1b \u53c2\u89c1 _InterlockedCompareExchange : long _InterlockedCompareExchange ( long volatile * Destination , long Exchange , long Comparand ); _InterlockedCompareExchange does an atomic comparison of the Destination value with the Comparand value. If the Destination value is equal to the Comparand value, the Exchange value is stored in the address specified by Destination . Otherwise, does no operation. Such loops still qualify(\u6709\u8d44\u683c) as lock-free, because if the test fails for one thread, it means it must have succeeded for another \u2013 though some architectures offer a weaker variant of CAS where that\u2019s not necessarily true. Whenever implementing a CAS loop, special care must be taken to avoid the ABA problem . Sequential Consistency Sequential consistency means that all threads agree on the order in which memory operations occurred, and that order is consistent with the order of operations in the program source code. Under sequential consistency, it\u2019s impossible to experience memory reordering shenanigans(\u6076\u4f5c\u5267) like the one I demonstrated in a previous post . A simple (but obviously impractical) way to achieve sequential consistency is to disable compiler optimizations and force all your threads to run on a single processor. A processor never sees its own memory effects out of order, even when threads are pre-empted and scheduled at arbitrary times. Some programming languages offer sequentially consistency even for optimized code running in a multiprocessor environment. In C++11, you can declare all shared variables as C++11 atomic types with default memory ordering constraints. In Java, you can mark all shared variables as volatile . Here\u2019s the example from my previous post , rewritten in C++11 style: std :: atomic < int > X ( 0 ), Y ( 0 ); int r1 , r2 ; void thread1 () { X . store ( 1 ); r1 = Y . load (); } void thread2 () { Y . store ( 1 ); r2 = X . load (); } NOTE: \u8fd9\u4e2a\u4f8b\u5b50\u662fsequential consistency\u7684\u5178\u578b\u4f8b\u5b50 Because the C++11 atomic types guarantee sequential consistency, the outcome r1 = r2 = 0 is impossible. To achieve this, the compiler outputs additional instructions behind the scenes \u2013 typically memory fences and/or RMW operations. Those additional instructions may make the implementation less efficient compared to one where the programmer has dealt with memory ordering directly. Memory Ordering As the flowchart suggests, any time you do lock-free programming for multicore (or any symmetric multiprocessor ), and your environment does not guarantee sequential consistency, you must consider how to prevent memory reordering . On today\u2019s architectures, the tools to enforce correct memory ordering generally fall into three categories, which prevent both compiler reordering and processor reordering : 1\u3001A lightweight sync or fence instruction, which I\u2019ll talk about in future posts ; 2\u3001A full memory fence instruction, which I\u2019ve demonstrated previously ; 3\u3001Memory operations which provide acquire or release semantics . Acquire semantics prevent memory reordering of operations which follow it in program order, and release semantics prevent memory reordering of operations preceding it. These semantics are particularly suitable in cases when there\u2019s a producer/consumer relationship, where one thread publishes some information and the other reads it. I\u2019ll also talk about this more in a future post . Different Processors Have Different Memory Models Different CPU families have different habits when it comes to memory reordering. The rules are documented by each CPU vendor and followed strictly by the hardware. For instance, PowerPC and ARM processors can change the order of memory stores relative to the instructions themselves, but normally, the x86/64 family of processors from Intel and AMD do not. We say the former processors have a more relaxed memory model . There\u2019s a temptation to abstract away such platform-specific details, especially with C++11 offering us a standard way to write portable lock-free code. But currently, I think most lock-free programmers have at least some appreciation of platform differences. If there\u2019s one key difference to remember, it\u2019s that at the x86/64 instruction level, every load from memory comes with acquire semantics, and every store to memory provides release semantics \u2013 at least for non-SSE instructions and non-write-combined memory. As a result, it\u2019s been common in the past to write lock-free code which works on x86/64, but fails on other processors . If you\u2019re interested in the hardware details of how and why processors perform memory reordering, I\u2019d recommend Appendix C of Is Parallel Programming Hard . In any case, keep in mind that memory reordering can also occur due to compiler reordering of instructions. In this post, I haven\u2019t said much about the practical side of lock-free programming, such as: When do we do it? How much do we really need? I also haven\u2019t mentioned the importance of validating your lock-free algorithms. Nonetheless, I hope for some readers, this introduction has provided a basic familiarity with lock-free concepts, so you can proceed into the additional reading without feeling too bewildered. As usual, if you spot any inaccuracies, let me know in the comments. [This article was featured in Issue #29 of Hacker Monthly .] Additional References Anthony Williams\u2019 blog and his book, C++ Concurrency in Action Dmitriy V\u2019jukov\u2019s website and various forum discussions Bartosz Milewski\u2019s blog Charles Bloom\u2019s Low-Level Threading series on his blog Doug Lea\u2019s JSR-133 Cookbook Howells and McKenney\u2019s memory-barriers.txt document Hans Boehm\u2019s collection of links about the C++11 memory model Herb Sutter\u2019s Effective Concurrency series","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/03-An-Introduction-to-Lock-Free-Programming/#preshing#an#introduction#to#lock-free#programming","text":"I was fortunate in that my first introduction to lock-free (also known as lockless) programming was Bruce Dawson\u2019s excellent and comprehensive white paper, Lockless Programming Considerations . And like many, I\u2019ve had the occasion to put Bruce\u2019s advice into practice developing and debugging lock-free code on platforms such as the Xbox 360. NOTE: Lockless Programming Considerations \u6240\u94fe\u63a5\u7684\u6587\u7ae0\u6536\u5f55\u5728 \"microsoft-Lockless-Programming-Considerations-for-Xbox-360-and-Microsoft-Windows\" \u7ae0\u8282","title":"preshing An Introduction to Lock-Free Programming"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/03-An-Introduction-to-Lock-Free-Programming/#what#is#it","text":"People often describe lock-free programming as programming without mutexes, which are also referred to as locks . That\u2019s true, but it\u2019s only part of the story. The generally accepted definition, based on academic literature, is a bit more broad. At its essence, lock-free is a property used to describe some code, without saying too much about how that code was actually written. NOTE: \"At its essence, lock-free is a property used to describe some code, without saying too much about how that code was actually written.\" \u8fd9\u6bb5\u8bdd\u7684\u610f\u601d\u662f: lock-free \u6240\u63cf\u8ff0\u7684\u662f property\uff0c\u5b83\u7684definition\u5e76\u4e0d\u6d89\u53ca\u5b9e\u73b0\uff0c\u56e0\u6b64\u6211\u4eec\u5728\u7406\u89e3lock free\u7684\u65f6\u5019\uff0c\u5e94\u8be5\u662f\u4ece\u6027\u8d28\u5165\u624b\uff0c\u800c\u4e0d\u662f\u4ece\u5b9e\u73b0\u5165\u624b Basically, if some part of your program satisfies the following conditions, then that part can rightfully be considered lock-free. Conversely, if a given part of your code doesn\u2019t satisfy these conditions, then that part is not lock-free. NOTE: \"can the threads block each other?\"\u7684\u6ce8\u91ca \"is there some way to schedule the threads which would lock up indefinitely?\" \u7684\u5b57\u9762\u610f\u601d\u662f: \u662f\u5426\u6709\u4e00\u4e9b\u65b9\u6cd5\u6765\u8c03\u5ea6\u90a3\u4e9b\u5c06\u88ab\u65e0\u9650\u671f\u9501\u5b9a\u7684\u7ebf\u7a0b\uff0c\u90a3\u5b83\u8981\u5982\u4f55\u7406\u89e3\uff1f","title":"What Is It?"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/03-An-Introduction-to-Lock-Free-Programming/#lock","text":"NOTE: lock\u7684\u542b\u4e49\u662f: \"the possibility of \u201clocking up\u201d the entire application in some way, whether it\u2019s deadlock, livelock \u2013 or even due to hypothetical thread scheduling decisions made by your worst enemy\" \u56e0\u6b64\uff0clock-less\u4e2d\u7684lock\uff0c\u9664\u4e86\u5305\u62ec mutexes\uff0c\u8fd8\u5305\u62ec\u4e0b\u9762\u7684\"Example \"\u7ae0\u8282\u4e2d\u7684\u4f8b\u5b50 In this sense, the lock in lock-free does not refer directly to mutexes, but rather to the possibility of \u201clocking up\u201d the entire application in some way, whether it\u2019s deadlock, livelock \u2013 or even due to hypothetical thread scheduling decisions made by your worst enemy(\u4e0b\u9762\u7684\"Example\"\u7ae0\u8282\u4e2d\u7684\u4f8b\u5b50\u5c31\u662f\u5bf9\u6b64\u7684\u8bf4\u660e). That last point sounds funny, but it\u2019s key. Shared mutexes are ruled out(\u6392\u9664) trivially, because as soon as one thread obtains the mutex, your worst enemy could simply never schedule that thread again. Of course, real operating systems don\u2019t work that way \u2013 we\u2019re merely defining terms.","title":"lock \u7684\u542b\u4e49"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/03-An-Introduction-to-Lock-Free-Programming/#example","text":"Here\u2019s a simple example of an operation which contains no mutexes, but is still not lock-free. Initially, X = 0 . As an exercise for the reader, consider how two threads could be scheduled in a way such that neither thread exits the loop. while ( X == 0 ) { X = 1 - X ; } NOTE: \u53ef\u80fd\u5bfc\u81f4\"\u201clocking up\u201d the entire application\"\u7684\u8c03\u5ea6\u7b56\u7565\u662f: 1\u3001\u4e24\u4e2athread\u540c\u65f6\u8fdb\u5165 while 2\u3001\u6b64\u65f6 X \u4e3a0\uff0c\u7b2c\u4e00\u4e2athread\u6267\u884c X = 1 - X; \uff0c\u5219 X \u4e3a1\uff1b 3\u3001\u6b64\u65f6 X \u4e3a1\uff0c\u7b2c\u4e8c\u4e2athread\u6267\u884c X = 1 - X; \uff0c\u5219 X \u4e3a0\uff0c\u663e\u7136\u6b64\u65f6\u56de\u5230\u4e86\u539f\u70b9 4\u3001\u6309\u71671-3\u6267\u884c\uff0c\u5219\u6c38\u8fdc\u9677\u5165\u8fd9\u4e2a\u6b7b\u5faa\u73af Nobody expects a large application to be entirely lock-free. Typically, we identify a specific set of lock-free operations out of the whole codebase. For example, in a lock-free queue, there might be a handful of lock-free operations such as push , pop , perhaps isEmpty , and so on.","title":"Example"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/03-An-Introduction-to-Lock-Free-Programming/#definition#by#herlihy#shavit#authors#of#the#art#of#multiprocessor#programming","text":"NOTE: \u7531Herlihy & Shavit\u7ed9\u51fa\u7684lock free\u7684\u5b9a\u4e49 Herlihy & Shavit, authors of The Art of Multiprocessor Programming , tend to express such operations as class methods, and offer the following succinct definition of lock-free (see slide 150 ): \u201cIn an infinite execution, infinitely often some method call finishes.\u201d In other words, as long as the program is able to keep calling those lock-free operations, the number of completed calls keeps increasing, no matter what. It is algorithmically impossible for the system to lock up during those operations. NOTE: \u5b57\u9762\u610f\u601d: Herlihy & Shavit\uff0c\u300a\u591a\u5904\u7406\u5668\u7f16\u7a0b\u7684\u827a\u672f\u300b\u7684\u4f5c\u8005\uff0c\u503e\u5411\u4e8e\u5c06\u8fd9\u4e9b\u64cd\u4f5c\u8868\u793a\u4e3a\u7c7b\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u4ee5\u4e0b\u65e0\u9501\u7684\u7b80\u6d01\u5b9a\u4e49(\u89c1\u5e7b\u706f\u7247150):\u201c\u5728\u65e0\u9650\u6267\u884c\u4e2d\uff0c\u4e00\u4e9b\u65b9\u6cd5\u8c03\u7528\u603b\u662f\u65e0\u9650\u5730\u7ed3\u675f\u3002 \u6362\u53e5\u8bdd\u8bf4\uff0c\u53ea\u8981\u7a0b\u5e8f\u80fd\u591f\u7ee7\u7eed*\u8c03\u7528*\u8fd9\u4e9b\u65e0\u9501\u64cd\u4f5c\uff0c*\u5b8c\u6210\u7684*\u8c03\u7528\u7684\u6570\u91cf\u5c31\u4f1a\u4e0d\u65ad\u589e\u52a0\uff0c\u65e0\u8bba\u53d1\u751f\u4ec0\u4e48\u3002 \u5728\u8fd9\u4e9b\u64cd\u4f5c\u671f\u95f4\uff0c\u7cfb\u7edf\u5728\u7b97\u6cd5\u4e0a\u662f\u4e0d\u53ef\u80fd\u9501\u5b9a\u7684\u3002","title":"Definition  by Herlihy &amp; Shavit, authors of The Art of Multiprocessor Programming"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/03-An-Introduction-to-Lock-Free-Programming/#consequence#of#lock-free#programming","text":"NOTE: lock-free programming \u7684\u6027\u6027\u8d28\u7684\u4e00\u4e9b\u5e94\u7528\u573a\u666f One important consequence of lock-free programming is that if you suspend a single thread, it will never prevent other threads from making progress, as a group, through their own lock-free operations. This hints(\u6697\u793a) at the value of lock-free programming when writing interrupt handlers and real-time systems, where certain tasks must complete within a certain time limit, no matter what state the rest of the program is in. A final precision: Operations that are designed to block do not disqualify the algorithm. For example, a queue\u2019s pop operation may intentionally block when the queue is empty. The remaining codepaths can still be considered lock-free. NOTE: \u8fd9\u4e00\u6bb5\u7684\u5b57\u9762\u610f\u601d: \u6700\u7ec8\u7cbe\u5ea6:\u88ab\u8bbe\u8ba1\u4e3a\u963b\u585e\u7684\u64cd\u4f5c\u4e0d\u4f1a\u53d6\u6d88\u7b97\u6cd5\u7684\u8d44\u683c\u3002\u4f8b\u5982\uff0c\u5f53\u961f\u5217\u4e3a\u7a7a\u65f6\uff0c\u961f\u5217\u7684pop\u64cd\u4f5c\u53ef\u80fd\u4f1a\u6545\u610f\u963b\u585e\u3002\u5176\u4f59\u7684\u4ee3\u7801\u8def\u5f84\u4ecd\u7136\u53ef\u4ee5\u8ba4\u4e3a\u662f\u65e0\u9501\u7684\u3002 \u542b\u4e49\u662f: blocking operation\u5e76\u4e0d\u4f1a\u4f7f\u5f97\u4f7f\u7528\u5b83\u7684algorithm\u6210\u4e3a\u975elock-free\u7684","title":"Consequence of lock-free programming"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/03-An-Introduction-to-Lock-Free-Programming/#lock-free#programming#techniques","text":"So how do these techniques relate to one another? To illustrate, I\u2019ve put together the following flowchart. I\u2019ll elaborate on each one below.","title":"Lock-Free Programming Techniques"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/03-An-Introduction-to-Lock-Free-Programming/#atomic#read-modify-write#operations","text":"Atomic operations are ones which manipulate memory in a way that appears indivisible: No thread can observe the operation half-complete. On modern processors, lots of operations are already atomic. For example, aligned reads and writes of simple types are usually atomic . Read-modify-write (RMW) operations go a step further, allowing you to perform more complex transactions atomically. They\u2019re especially useful when a lock-free algorithm must support multiple writers, because when multiple threads attempt an RMW on the same address, they\u2019ll effectively line up in a row and execute those operations one-at-a-time. I\u2019ve already touched upon RMW operations in this blog, such as when implementing a lightweight mutex , a recursive mutex and a lightweight logging system . Examples of RMW operations include 1\u3001 _InterlockedIncrement on Win32, 2\u3001 OSAtomicAdd32 on iOS, and 3\u3001 std::atomic::fetch_add in C++11. Be aware that the C++11 atomic standard does not guarantee that the implementation will be lock-free on every platform, so it\u2019s best to know the capabilities of your platform and toolchain. You can call std::atomic<>::is_lock_free to make sure. Different CPU families support RMW in different ways . Processors such as PowerPC and ARM expose load-link/store-conditional instructions, which effectively allow you to implement your own RMW primitive at a low level, though this is not often done. The common RMW operations are usually sufficient. As illustrated by the flowchart, atomic RMWs are a necessary part of lock-free programming even on single-processor systems. Without atomicity, a thread could be interrupted halfway through the transaction, possibly leading to an inconsistent state.","title":"Atomic Read-Modify-Write Operations"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/03-An-Introduction-to-Lock-Free-Programming/#compare-and-swap#loops","text":"Perhaps the most often-discussed RMW operation is compare-and-swap (CAS). On Win32, CAS is provided via a family of intrinsics such as _InterlockedCompareExchange . Often, programmers perform compare-and-swap in a loop to repeatedly attempt a transaction. This pattern typically involves copying a shared variable to a local variable, performing some speculative work, and attempting to publish the changes using CAS: void LockFreeQueue :: push ( Node * newHead ) { for (;;) { // Copy a shared variable (m_Head) to a local. Node * oldHead = m_Head ; // Do some speculative work, not yet visible to other threads. newHead -> next = oldHead ; // Next, attempt to publish our changes to the shared variable. // If the shared variable hasn't changed, the CAS succeeds and we return. // Otherwise, repeat. if ( _InterlockedCompareExchange ( & m_Head , newHead , oldHead ) == oldHead ) return ; } } NOTE: \u4ee3\u7801\u7406\u89e3: \u5982\u679c m_Head (\u5f53\u524d\u7684head)\u548c oldHead (\u4e4b\u524d\u7684head)\u76f8\u7b49\uff0c\u8bf4\u660e\u5728\u8fd9\u4e4b\u95f4\uff0c\u6ca1\u6709thread\u4fee\u6539queue\uff0c\u5219\u5c06 newHead \u5199\u5165\uff1b \u53c2\u89c1 _InterlockedCompareExchange : long _InterlockedCompareExchange ( long volatile * Destination , long Exchange , long Comparand ); _InterlockedCompareExchange does an atomic comparison of the Destination value with the Comparand value. If the Destination value is equal to the Comparand value, the Exchange value is stored in the address specified by Destination . Otherwise, does no operation. Such loops still qualify(\u6709\u8d44\u683c) as lock-free, because if the test fails for one thread, it means it must have succeeded for another \u2013 though some architectures offer a weaker variant of CAS where that\u2019s not necessarily true. Whenever implementing a CAS loop, special care must be taken to avoid the ABA problem .","title":"Compare-And-Swap Loops"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/03-An-Introduction-to-Lock-Free-Programming/#sequential#consistency","text":"Sequential consistency means that all threads agree on the order in which memory operations occurred, and that order is consistent with the order of operations in the program source code. Under sequential consistency, it\u2019s impossible to experience memory reordering shenanigans(\u6076\u4f5c\u5267) like the one I demonstrated in a previous post . A simple (but obviously impractical) way to achieve sequential consistency is to disable compiler optimizations and force all your threads to run on a single processor. A processor never sees its own memory effects out of order, even when threads are pre-empted and scheduled at arbitrary times. Some programming languages offer sequentially consistency even for optimized code running in a multiprocessor environment. In C++11, you can declare all shared variables as C++11 atomic types with default memory ordering constraints. In Java, you can mark all shared variables as volatile . Here\u2019s the example from my previous post , rewritten in C++11 style: std :: atomic < int > X ( 0 ), Y ( 0 ); int r1 , r2 ; void thread1 () { X . store ( 1 ); r1 = Y . load (); } void thread2 () { Y . store ( 1 ); r2 = X . load (); } NOTE: \u8fd9\u4e2a\u4f8b\u5b50\u662fsequential consistency\u7684\u5178\u578b\u4f8b\u5b50 Because the C++11 atomic types guarantee sequential consistency, the outcome r1 = r2 = 0 is impossible. To achieve this, the compiler outputs additional instructions behind the scenes \u2013 typically memory fences and/or RMW operations. Those additional instructions may make the implementation less efficient compared to one where the programmer has dealt with memory ordering directly.","title":"Sequential Consistency"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/03-An-Introduction-to-Lock-Free-Programming/#memory#ordering","text":"As the flowchart suggests, any time you do lock-free programming for multicore (or any symmetric multiprocessor ), and your environment does not guarantee sequential consistency, you must consider how to prevent memory reordering . On today\u2019s architectures, the tools to enforce correct memory ordering generally fall into three categories, which prevent both compiler reordering and processor reordering : 1\u3001A lightweight sync or fence instruction, which I\u2019ll talk about in future posts ; 2\u3001A full memory fence instruction, which I\u2019ve demonstrated previously ; 3\u3001Memory operations which provide acquire or release semantics . Acquire semantics prevent memory reordering of operations which follow it in program order, and release semantics prevent memory reordering of operations preceding it. These semantics are particularly suitable in cases when there\u2019s a producer/consumer relationship, where one thread publishes some information and the other reads it. I\u2019ll also talk about this more in a future post .","title":"Memory Ordering"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/03-An-Introduction-to-Lock-Free-Programming/#different#processors#have#different#memory#models","text":"Different CPU families have different habits when it comes to memory reordering. The rules are documented by each CPU vendor and followed strictly by the hardware. For instance, PowerPC and ARM processors can change the order of memory stores relative to the instructions themselves, but normally, the x86/64 family of processors from Intel and AMD do not. We say the former processors have a more relaxed memory model . There\u2019s a temptation to abstract away such platform-specific details, especially with C++11 offering us a standard way to write portable lock-free code. But currently, I think most lock-free programmers have at least some appreciation of platform differences. If there\u2019s one key difference to remember, it\u2019s that at the x86/64 instruction level, every load from memory comes with acquire semantics, and every store to memory provides release semantics \u2013 at least for non-SSE instructions and non-write-combined memory. As a result, it\u2019s been common in the past to write lock-free code which works on x86/64, but fails on other processors . If you\u2019re interested in the hardware details of how and why processors perform memory reordering, I\u2019d recommend Appendix C of Is Parallel Programming Hard . In any case, keep in mind that memory reordering can also occur due to compiler reordering of instructions. In this post, I haven\u2019t said much about the practical side of lock-free programming, such as: When do we do it? How much do we really need? I also haven\u2019t mentioned the importance of validating your lock-free algorithms. Nonetheless, I hope for some readers, this introduction has provided a basic familiarity with lock-free concepts, so you can proceed into the additional reading without feeling too bewildered. As usual, if you spot any inaccuracies, let me know in the comments. [This article was featured in Issue #29 of Hacker Monthly .]","title":"Different Processors Have Different Memory Models"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/03-An-Introduction-to-Lock-Free-Programming/#additional#references","text":"Anthony Williams\u2019 blog and his book, C++ Concurrency in Action Dmitriy V\u2019jukov\u2019s website and various forum discussions Bartosz Milewski\u2019s blog Charles Bloom\u2019s Low-Level Threading series on his blog Doug Lea\u2019s JSR-133 Cookbook Howells and McKenney\u2019s memory-barriers.txt document Hans Boehm\u2019s collection of links about the C++11 memory model Herb Sutter\u2019s Effective Concurrency series","title":"Additional References"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/04-Memory-Ordering-at-Compile-Time/","text":"preshing Memory Ordering at Compile Time Between the time you type in some C/C++ source code and the time it executes on a CPU, the memory interactions of that code may be reordered according to certain rules. Changes to memory ordering are made both by the compiler (at compile time) and by the processor (at run time), all in the name of making your code run faster. NOTE: optimization principle The cardinal(\u4e3b\u8981\u7684) rule of memory reordering , which is universally followed by compiler developers and CPU vendors, could be phrased as follows: Thou shalt not modify the behavior of a single-threaded program. NOTE: \u4e0d\u80fd\u4fee\u6539\u5355\u7ebf\u7a0b\u7a0b\u5e8f\u7684\u884c\u4e3a\u3002 As a result of this rule, memory reordering goes largely unnoticed by programmers writing single-threaded code. It often goes unnoticed in multithreaded programming, too, since mutexes , semaphores and events are all designed to prevent memory reordering around their call sites. It\u2019s only when lock-free techniques are used \u2013 when memory is shared between threads without any kind of mutual exclusion \u2013 that the cat is finally out of the bag, and the effects of memory reordering can be plainly observed . NOTE: \u53ea\u6709\u5f53\u4f7f\u7528\u4e86\u65e0\u9501\u6280\u672f\u65f6\u2014\u2014\u5f53\u5185\u5b58\u5728\u7ebf\u7a0b\u4e4b\u95f4\u5171\u4eab\u800c\u4e0d\u5b58\u5728\u4efb\u4f55\u4e92\u65a5\u73b0\u8c61\u65f6\u2014\u2014\u95ee\u9898\u624d\u6700\u7ec8\u5f97\u5230\u89e3\u51b3\uff0c\u5e76\u4e14\u5185\u5b58\u91cd\u65b0\u6392\u5e8f\u7684\u6548\u679c\u53ef\u4ee5\u6e05\u695a\u5730\u89c2\u5bdf\u5230\u3002 Mind you, it is possible to write lock-free code for multicore platforms without the hassles(\u9ebb\u70e6\u4e8b) of memory reordering . As I mentioned in my introduction to lock-free programming , one can take advantage of sequentially consistent types , such as volatile variables in Java or C++11 atomics \u2013 possibly at the price of a little performance. I won\u2019t go into detail about those here. In this post, I\u2019ll focus on the impact of the compiler on memory ordering for regular, non-sequentially-consistent types . Compiler Instruction Reordering As you know, the job of a compiler is to convert human-readable source code into machine-readable code for the CPU. During this conversion, the compiler is free to take many liberties. Once such liberty is the reordering of instructions \u2013 again, only in cases where single-threaded program behavior does not change. Such instruction reordering typically happens only when compiler optimizations are enabled. Consider the following function: int A , B ; void foo () { A = B + 1 ; B = 0 ; } If we compile this function using GCC 4.6.1 without compiler optimization, it generates the following machine code, which we can view as an assembly listing using the -S option. The memory store to global variable B occurs right after the store to A , just as it does in the original source code. $ gcc - S - masm = intel foo . c $ cat foo . s ... mov eax , DWORD PTR _B ( redo this at home ...) add eax , 1 mov DWORD PTR _A , eax mov DWORD PTR _B , 0 ... Compare that to the resulting assembly listing when optimizations are enabled using -O2 : $ gcc -O2 -S -masm=intel foo.c $ cat foo.s ... mov eax, DWORD PTR B mov DWORD PTR B, 0 add eax, 1 mov DWORD PTR A, eax ... This time, the compiler has chosen to exercise its liberties, and reordered the store to B before the store to A . And why shouldn\u2019t it? The cardinal rule of memory ordering is not broken. A single-threaded program would never know the difference. On the other hand, such compiler reorderings can cause problems when writing lock-free code. Here\u2019s a commonly-cited example, where a shared flag is used to indicate that some other shared data has been published: int Value ; int IsPublished = 0 ; void sendValue ( int x ) { Value = x ; IsPublished = 1 ; } NOTE: \u8fd9\u662facquire-release semantic\u7684\u7ecf\u5178\u4f8b\u5b50 Imagine what would happen if the compiler reordered the store to IsPublished before the store to Value . Even on a single-processor system, we\u2019d have a problem: a thread could very well be pre-empted by the operating system between the two stores, leaving other threads to believe that Value has been updated when in fact, it hasn\u2019t. NOTE: access outside of object lifetime Of course, the compiler might not reorder those operations, and the resulting machine code would work fine as a lock-free operation on any multicore CPU having a strong memory model , such as an x86/64 \u2013 or in a single-processor environment, any type of CPU at all. If that\u2019s the case, we should consider ourselves lucky. Needless to say, it\u2019s much better practice to recognize the possibility of memory reordering for shared variables, and to ensure that the correct ordering is enforced. Explicit Compiler Barriers NOTE: \u672c\u8282\u4ecb\u7ecd\u7684\u662f\u663e\u5f0f\u7684compiler barrier The minimalist approach to preventing compiler reordering is by using a special directive known as a compiler barrier. I\u2019ve already demonstrated compiler barriers in a previous post . The following is a full compiler barrier in GCC. In Microsoft Visual C++, _ReadWriteBarrier serves the same purpose. int A , B ; void foo () { A = B + 1 ; asm volatile ( \"\" ::: \"memory\" ); B = 0 ; } With this change, we can leave optimizations enabled, and the memory store instructions will remain in the desired order. $ gcc -O2 -S -masm=intel foo.c $ cat foo.s ... mov eax, DWORD PTR _B add eax, 1 mov DWORD PTR _A, eax mov DWORD PTR _B, 0 ... Similarly, if we want to guarantee our sendMessage example works correctly, and we only care about single-processor systems , then at an absolute minimum, we must introduce compiler barriers here as well. Not only does the sending operation require a compiler barrier, to prevent the reordering of stores, but receiving side needs one between the loads as well. #define COMPILER_BARRIER() asm volatile(\"\" ::: \"memory\") int Value ; int IsPublished = 0 ; void sendValue ( int x ) { Value = x ; COMPILER_BARRIER (); // prevent reordering of stores IsPublished = 1 ; } int tryRecvValue () { if ( IsPublished ) { COMPILER_BARRIER (); // prevent reordering of loads return Value ; } return -1 ; // or some other value to mean not yet received } CPU fence instruction As I mentioned, compiler barriers are sufficient to prevent memory reordering on a single-processor system. But it\u2019s 2012, and these days, multicore computing is the norm. If we want to ensure our interactions happen in the desired order in a multiprocessor environment, and on any CPU architecture, then a compiler barrier is not enough. We need either to issue a CPU fence instruction, or perform any operation which acts as a memory barrier at runtime. I\u2019ll write more about those in the next post, Memory Barriers Are Like Source Control Operations . The Linux kernel exposes several CPU fence instructions through preprocessor macros such as smb_rmb , and those macros are reduced to simple compiler barriers when compiling for a single-processor system. Implied Compiler Barriers NOTE: \u672c\u8282\u4ecb\u7ecd\u7684\u9690\u5f0f\u7684Compiler Barriers\uff1b\u5176\u5b9e\uff0c\u672c\u8282\u4ecb\u7ecd\u7684CPU fence instruction\uff0c\u5f53\u6211\u4eec\u5728source code\u4e2d\uff0c\u4f7f\u7528CPU fence instruction\u7684\u65f6\u5019\uff0c\u5b83\u4eec\u4e5f\u4f1a\u6267\u884ccompile barrier\uff0c\u56e0\u4e3a\u9700\u8981\u9996\u5148\u907f\u514dcompiler\u8fdb\u884creorder\uff0c\u540e\u7eed\u4ece\u80fd\u591f\u5b9e\u73b0runtime\u7684recorder There are other ways to prevent compiler reordering. Indeed, the CPU fence instructions I just mentioned act as compiler barriers, too. Here\u2019s an example CPU fence instruction for PowerPC, defined as a macro in GCC: #define RELEASE_FENCE() asm volatile(\"lwsync\" ::: \"memory\") Anywhere we place RELEASE_FENCE throughout our code, it will prevent certain kinds of processor reordering in addition to compiler reordering. For example, it can be used to make our sendValue function safe in a multiprocessor environment. void sendValue ( int x ) { Value = x ; RELEASE_FENCE (); IsPublished = 1 ; } In the new C++11 (formerly known as C++0x) atomic library standard, every non-relaxed atomic operation acts as a compiler barrier as well. int Value ; std :: atomic < int > IsPublished ( 0 ); void sendValue ( int x ) { Value = x ; // <-- reordering is prevented here! IsPublished . store ( 1 , std :: memory_order_release ); } Every function containing a compiler barrier must act as a compiler barrier And as you might expect, every function containing a compiler barrier must act as a compiler barrier itself, even when the function is inlined. (However, Microsoft\u2019s documentation suggests that may not have been the case in earlier versions of the Visual C++ compiler. Tsk, tsk!) void doSomeStuff ( Foo * foo ) { foo -> bar = 5 ; sendValue ( 123 ); // prevents reordering of neighboring assignments foo -> bar2 = foo -> bar ; } Function calls act as compiler barriers In fact, the majority of function calls act as compiler barriers, whether they contain their own compiler barrier or not. This excludes inline functions, functions declared with the pure attribute , and cases where link-time code generation is used. Other than those cases, a call to an external function is even stronger than a compiler barrier, since the compiler has no idea what the function\u2019s side effects will be. It must forget any assumptions it made about memory that is potentially visible to that function. When you think about it, this makes perfect sense. In the above code snippet, suppose our implementation of sendValue exists in an external library. How does the compiler know that sendValue doesn\u2019t depend on the value of foo->bar ? How does it know sendValue will not modify foo->bar in memory? It doesn\u2019t. Therefore, to obey the cardinal rule of memory ordering, it must not reorder any memory operations around the external call to sendValue . Similarly, it must load a fresh value for foo->bar from memory after the call completes, rather than assuming it still equals 5, even with optimization enabled. $ gcc -O2 -S -masm=intel dosomestuff.c $ cat dosomestuff.s ... mov ebx, DWORD PTR [esp+32] mov DWORD PTR [ebx], 5 // Store 5 to foo->bar mov DWORD PTR [esp], 123 call sendValue // Call sendValue mov eax, DWORD PTR [ebx] // Load fresh value from foo->bar mov DWORD PTR [ebx+4], eax ... As you can see, there are many instances where compiler instruction reordering is prohibited(\u7981\u6b62), and even when the compiler must reload certain values from memory. I believe these hidden rules form a big part of the reason why people have long been saying that volatile data types in C are not usually necessary in correctly-written multithreaded code . Out-Of-Thin-Air Stores NOTE: \"Out-Of-Thin-Air \"\u7684\u610f\u601d\u662f\"\u65e0\u4e2d\u751f\u6709\u7684\" Think instruction reordering makes lock-free programming tricky? Before C++11 was standardized, there was technically no rule preventing the compiler from getting up to even worse tricks. In particular, compilers were free to introduce stores to shared memory in cases where there previously was none. Here\u2019s a very simplified example, inspired by the examples provided in multiple articles by Hans Boehm . NOTE: \"In particular, compilers were free to introduce stores to shared memory in cases where there previously was none\"\u7684\u610f\u601d\u662f\"\u7279\u522b\u662f\uff0c\u5728\u4ee5\u524d\u6ca1\u6709\u5171\u4eab\u5185\u5b58\u7684\u60c5\u51b5\u4e0b\uff0c\u7f16\u8bd1\u5668\u53ef\u4ee5\u81ea\u7531\u5730\u5c06\u5b58\u50a8\u5f15\u5165\u5171\u4eab\u5185\u5b58\u3002\" int A , B ; void foo () { if ( A ) B ++ ; } Though it\u2019s rather unlikely in practice, nothing prevents a compiler from promoting B to a register before checking A, resulting in machine code equivalent to the following: void foo () { register int r = B ; // Promote B to a register before checking A. if ( A ) r ++ ; B = r ; // Surprise! A new memory store where there previously was none. } Once again, the cardinal rule of memory ordering is still followed. A single-threaded application would be none the wiser(\u5355\u7ebf\u7a0b\u5e94\u7528\u7a0b\u5e8f\u5bf9\u6b64\u4e00\u65e0\u6240\u77e5). But in a multithreaded environment, we now have a function which can wipe out(\u64e6\u9664) any changes made concurrently to B in other threads \u2013 even when A is 0. The original code didn\u2019t do that. This type of obscure, technical non-impossibility is part of the reason why people have been saying that C++ doesn\u2019t support threads , despite the fact that we\u2019ve been happily writing multithreaded and lock-free code in C/C++ for decades. NOTE: 1\u3001\u8fd9\u4e2a\u4f8b\u5b50\u518d\u6b21\u9a8c\u8bc1\u4e86\u524d\u9762\u7684\u8bba\u70b9 2\u3001\u4e0a\u8ff0\u4f8b\u5b50\u662f\u5178\u578b\u7684overwrite I don\u2019t know anyone who ever fell victim to such \u201cout-of-thin-air\u201d stores in practice. Maybe it\u2019s just because for the type of lock-free code we tend to write, there aren\u2019t a whole lot of optimization opportunities fitting this pattern. I suppose if I ever caught this type of compiler transformation happening, I would search for a way to wrestle the compiler into submission. If it\u2019s happened to you, let me know in the comments. NOTE: \"\u6211\u60f3\uff0c\u5982\u679c\u6211\u53d1\u73b0\u53d1\u751f\u4e86\u8fd9\u79cd\u7f16\u8bd1\u5668\u8f6c\u6362\uff0c\u6211\u5c06\u5bfb\u627e\u4e00\u79cd\u65b9\u6cd5\u6765\u4f7f\u7f16\u8bd1\u5668\u5c48\u670d\u3002\" In any case, the new C++11 standard explictly prohibits such behavior from the compiler in cases where it would introduce a data race . The wording can be found in and around \u00a71.10.22 of the most recent C++11 working draft : Compiler transformations that introduce assignments to a potentially shared memory location that would not be modified by the abstract machine are generally precluded(\u963b\u6b62\u3001\u6392\u67e5) by this standard. Why Compiler Reordering? Performance optimization NOTE: \u9075\u5faaoptimization principle As I mentioned at the start, the compiler modifies the order of memory interactions for the same reason that the processor does it \u2013 performance optimization. Such optimizations are a direct consequence of modern CPU complexity. I may going out on a limb, but I somehow doubt that compilers did a whole lot of instruction reordering in the early 80\u2019s, when CPUs had only a few hundred thousand transistors at most. I don\u2019t think there would have been much point. But since then, Moore\u2019s Law has provided CPU designers with about 10000 times the number of transistors to play with, and those transistors have been spent on tricks such as pipelining, memory prefetching, ILP and more recently, multicore. As a result of some of those features, we\u2019ve seen architectures where the order of instructions in a program can make a significant difference in performance. The first Intel Pentium released in 1993, with its so-called U and V-pipes, was the first processor where I really remember people talking about pipelining and the significance of instruction ordering . More recently, though, when I step through x86 disassembly in Visual Studio, I\u2019m actually surprised how little instruction reordering there is. On the other hand, out of the times I\u2019ve stepped through SPU disassembly on Playstation 3, I\u2019ve found that the compiler really went to town. These are just anecdotal experiences; it may not reflect the experience of others, and certainly should not influence the way we enforce memory ordering in our lock-free code.","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/04-Memory-Ordering-at-Compile-Time/#preshing#memory#ordering#at#compile#time","text":"Between the time you type in some C/C++ source code and the time it executes on a CPU, the memory interactions of that code may be reordered according to certain rules. Changes to memory ordering are made both by the compiler (at compile time) and by the processor (at run time), all in the name of making your code run faster. NOTE: optimization principle The cardinal(\u4e3b\u8981\u7684) rule of memory reordering , which is universally followed by compiler developers and CPU vendors, could be phrased as follows: Thou shalt not modify the behavior of a single-threaded program. NOTE: \u4e0d\u80fd\u4fee\u6539\u5355\u7ebf\u7a0b\u7a0b\u5e8f\u7684\u884c\u4e3a\u3002 As a result of this rule, memory reordering goes largely unnoticed by programmers writing single-threaded code. It often goes unnoticed in multithreaded programming, too, since mutexes , semaphores and events are all designed to prevent memory reordering around their call sites. It\u2019s only when lock-free techniques are used \u2013 when memory is shared between threads without any kind of mutual exclusion \u2013 that the cat is finally out of the bag, and the effects of memory reordering can be plainly observed . NOTE: \u53ea\u6709\u5f53\u4f7f\u7528\u4e86\u65e0\u9501\u6280\u672f\u65f6\u2014\u2014\u5f53\u5185\u5b58\u5728\u7ebf\u7a0b\u4e4b\u95f4\u5171\u4eab\u800c\u4e0d\u5b58\u5728\u4efb\u4f55\u4e92\u65a5\u73b0\u8c61\u65f6\u2014\u2014\u95ee\u9898\u624d\u6700\u7ec8\u5f97\u5230\u89e3\u51b3\uff0c\u5e76\u4e14\u5185\u5b58\u91cd\u65b0\u6392\u5e8f\u7684\u6548\u679c\u53ef\u4ee5\u6e05\u695a\u5730\u89c2\u5bdf\u5230\u3002 Mind you, it is possible to write lock-free code for multicore platforms without the hassles(\u9ebb\u70e6\u4e8b) of memory reordering . As I mentioned in my introduction to lock-free programming , one can take advantage of sequentially consistent types , such as volatile variables in Java or C++11 atomics \u2013 possibly at the price of a little performance. I won\u2019t go into detail about those here. In this post, I\u2019ll focus on the impact of the compiler on memory ordering for regular, non-sequentially-consistent types .","title":"preshing Memory Ordering at Compile Time"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/04-Memory-Ordering-at-Compile-Time/#compiler#instruction#reordering","text":"As you know, the job of a compiler is to convert human-readable source code into machine-readable code for the CPU. During this conversion, the compiler is free to take many liberties. Once such liberty is the reordering of instructions \u2013 again, only in cases where single-threaded program behavior does not change. Such instruction reordering typically happens only when compiler optimizations are enabled. Consider the following function: int A , B ; void foo () { A = B + 1 ; B = 0 ; } If we compile this function using GCC 4.6.1 without compiler optimization, it generates the following machine code, which we can view as an assembly listing using the -S option. The memory store to global variable B occurs right after the store to A , just as it does in the original source code. $ gcc - S - masm = intel foo . c $ cat foo . s ... mov eax , DWORD PTR _B ( redo this at home ...) add eax , 1 mov DWORD PTR _A , eax mov DWORD PTR _B , 0 ... Compare that to the resulting assembly listing when optimizations are enabled using -O2 : $ gcc -O2 -S -masm=intel foo.c $ cat foo.s ... mov eax, DWORD PTR B mov DWORD PTR B, 0 add eax, 1 mov DWORD PTR A, eax ... This time, the compiler has chosen to exercise its liberties, and reordered the store to B before the store to A . And why shouldn\u2019t it? The cardinal rule of memory ordering is not broken. A single-threaded program would never know the difference. On the other hand, such compiler reorderings can cause problems when writing lock-free code. Here\u2019s a commonly-cited example, where a shared flag is used to indicate that some other shared data has been published: int Value ; int IsPublished = 0 ; void sendValue ( int x ) { Value = x ; IsPublished = 1 ; } NOTE: \u8fd9\u662facquire-release semantic\u7684\u7ecf\u5178\u4f8b\u5b50 Imagine what would happen if the compiler reordered the store to IsPublished before the store to Value . Even on a single-processor system, we\u2019d have a problem: a thread could very well be pre-empted by the operating system between the two stores, leaving other threads to believe that Value has been updated when in fact, it hasn\u2019t. NOTE: access outside of object lifetime Of course, the compiler might not reorder those operations, and the resulting machine code would work fine as a lock-free operation on any multicore CPU having a strong memory model , such as an x86/64 \u2013 or in a single-processor environment, any type of CPU at all. If that\u2019s the case, we should consider ourselves lucky. Needless to say, it\u2019s much better practice to recognize the possibility of memory reordering for shared variables, and to ensure that the correct ordering is enforced.","title":"Compiler Instruction Reordering"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/04-Memory-Ordering-at-Compile-Time/#explicit#compiler#barriers","text":"NOTE: \u672c\u8282\u4ecb\u7ecd\u7684\u662f\u663e\u5f0f\u7684compiler barrier The minimalist approach to preventing compiler reordering is by using a special directive known as a compiler barrier. I\u2019ve already demonstrated compiler barriers in a previous post . The following is a full compiler barrier in GCC. In Microsoft Visual C++, _ReadWriteBarrier serves the same purpose. int A , B ; void foo () { A = B + 1 ; asm volatile ( \"\" ::: \"memory\" ); B = 0 ; } With this change, we can leave optimizations enabled, and the memory store instructions will remain in the desired order. $ gcc -O2 -S -masm=intel foo.c $ cat foo.s ... mov eax, DWORD PTR _B add eax, 1 mov DWORD PTR _A, eax mov DWORD PTR _B, 0 ... Similarly, if we want to guarantee our sendMessage example works correctly, and we only care about single-processor systems , then at an absolute minimum, we must introduce compiler barriers here as well. Not only does the sending operation require a compiler barrier, to prevent the reordering of stores, but receiving side needs one between the loads as well. #define COMPILER_BARRIER() asm volatile(\"\" ::: \"memory\") int Value ; int IsPublished = 0 ; void sendValue ( int x ) { Value = x ; COMPILER_BARRIER (); // prevent reordering of stores IsPublished = 1 ; } int tryRecvValue () { if ( IsPublished ) { COMPILER_BARRIER (); // prevent reordering of loads return Value ; } return -1 ; // or some other value to mean not yet received }","title":"Explicit Compiler Barriers"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/04-Memory-Ordering-at-Compile-Time/#cpu#fence#instruction","text":"As I mentioned, compiler barriers are sufficient to prevent memory reordering on a single-processor system. But it\u2019s 2012, and these days, multicore computing is the norm. If we want to ensure our interactions happen in the desired order in a multiprocessor environment, and on any CPU architecture, then a compiler barrier is not enough. We need either to issue a CPU fence instruction, or perform any operation which acts as a memory barrier at runtime. I\u2019ll write more about those in the next post, Memory Barriers Are Like Source Control Operations . The Linux kernel exposes several CPU fence instructions through preprocessor macros such as smb_rmb , and those macros are reduced to simple compiler barriers when compiling for a single-processor system.","title":"CPU fence instruction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/04-Memory-Ordering-at-Compile-Time/#implied#compiler#barriers","text":"NOTE: \u672c\u8282\u4ecb\u7ecd\u7684\u9690\u5f0f\u7684Compiler Barriers\uff1b\u5176\u5b9e\uff0c\u672c\u8282\u4ecb\u7ecd\u7684CPU fence instruction\uff0c\u5f53\u6211\u4eec\u5728source code\u4e2d\uff0c\u4f7f\u7528CPU fence instruction\u7684\u65f6\u5019\uff0c\u5b83\u4eec\u4e5f\u4f1a\u6267\u884ccompile barrier\uff0c\u56e0\u4e3a\u9700\u8981\u9996\u5148\u907f\u514dcompiler\u8fdb\u884creorder\uff0c\u540e\u7eed\u4ece\u80fd\u591f\u5b9e\u73b0runtime\u7684recorder There are other ways to prevent compiler reordering. Indeed, the CPU fence instructions I just mentioned act as compiler barriers, too. Here\u2019s an example CPU fence instruction for PowerPC, defined as a macro in GCC: #define RELEASE_FENCE() asm volatile(\"lwsync\" ::: \"memory\") Anywhere we place RELEASE_FENCE throughout our code, it will prevent certain kinds of processor reordering in addition to compiler reordering. For example, it can be used to make our sendValue function safe in a multiprocessor environment. void sendValue ( int x ) { Value = x ; RELEASE_FENCE (); IsPublished = 1 ; } In the new C++11 (formerly known as C++0x) atomic library standard, every non-relaxed atomic operation acts as a compiler barrier as well. int Value ; std :: atomic < int > IsPublished ( 0 ); void sendValue ( int x ) { Value = x ; // <-- reordering is prevented here! IsPublished . store ( 1 , std :: memory_order_release ); }","title":"Implied Compiler Barriers"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/04-Memory-Ordering-at-Compile-Time/#every#function#containing#a#compiler#barrier#must#act#as#a#compiler#barrier","text":"And as you might expect, every function containing a compiler barrier must act as a compiler barrier itself, even when the function is inlined. (However, Microsoft\u2019s documentation suggests that may not have been the case in earlier versions of the Visual C++ compiler. Tsk, tsk!) void doSomeStuff ( Foo * foo ) { foo -> bar = 5 ; sendValue ( 123 ); // prevents reordering of neighboring assignments foo -> bar2 = foo -> bar ; }","title":"Every function containing a compiler barrier must act as a compiler barrier"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/04-Memory-Ordering-at-Compile-Time/#function#calls#act#as#compiler#barriers","text":"In fact, the majority of function calls act as compiler barriers, whether they contain their own compiler barrier or not. This excludes inline functions, functions declared with the pure attribute , and cases where link-time code generation is used. Other than those cases, a call to an external function is even stronger than a compiler barrier, since the compiler has no idea what the function\u2019s side effects will be. It must forget any assumptions it made about memory that is potentially visible to that function. When you think about it, this makes perfect sense. In the above code snippet, suppose our implementation of sendValue exists in an external library. How does the compiler know that sendValue doesn\u2019t depend on the value of foo->bar ? How does it know sendValue will not modify foo->bar in memory? It doesn\u2019t. Therefore, to obey the cardinal rule of memory ordering, it must not reorder any memory operations around the external call to sendValue . Similarly, it must load a fresh value for foo->bar from memory after the call completes, rather than assuming it still equals 5, even with optimization enabled. $ gcc -O2 -S -masm=intel dosomestuff.c $ cat dosomestuff.s ... mov ebx, DWORD PTR [esp+32] mov DWORD PTR [ebx], 5 // Store 5 to foo->bar mov DWORD PTR [esp], 123 call sendValue // Call sendValue mov eax, DWORD PTR [ebx] // Load fresh value from foo->bar mov DWORD PTR [ebx+4], eax ... As you can see, there are many instances where compiler instruction reordering is prohibited(\u7981\u6b62), and even when the compiler must reload certain values from memory. I believe these hidden rules form a big part of the reason why people have long been saying that volatile data types in C are not usually necessary in correctly-written multithreaded code .","title":"Function calls act as compiler barriers"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/04-Memory-Ordering-at-Compile-Time/#out-of-thin-air#stores","text":"NOTE: \"Out-Of-Thin-Air \"\u7684\u610f\u601d\u662f\"\u65e0\u4e2d\u751f\u6709\u7684\" Think instruction reordering makes lock-free programming tricky? Before C++11 was standardized, there was technically no rule preventing the compiler from getting up to even worse tricks. In particular, compilers were free to introduce stores to shared memory in cases where there previously was none. Here\u2019s a very simplified example, inspired by the examples provided in multiple articles by Hans Boehm . NOTE: \"In particular, compilers were free to introduce stores to shared memory in cases where there previously was none\"\u7684\u610f\u601d\u662f\"\u7279\u522b\u662f\uff0c\u5728\u4ee5\u524d\u6ca1\u6709\u5171\u4eab\u5185\u5b58\u7684\u60c5\u51b5\u4e0b\uff0c\u7f16\u8bd1\u5668\u53ef\u4ee5\u81ea\u7531\u5730\u5c06\u5b58\u50a8\u5f15\u5165\u5171\u4eab\u5185\u5b58\u3002\" int A , B ; void foo () { if ( A ) B ++ ; } Though it\u2019s rather unlikely in practice, nothing prevents a compiler from promoting B to a register before checking A, resulting in machine code equivalent to the following: void foo () { register int r = B ; // Promote B to a register before checking A. if ( A ) r ++ ; B = r ; // Surprise! A new memory store where there previously was none. } Once again, the cardinal rule of memory ordering is still followed. A single-threaded application would be none the wiser(\u5355\u7ebf\u7a0b\u5e94\u7528\u7a0b\u5e8f\u5bf9\u6b64\u4e00\u65e0\u6240\u77e5). But in a multithreaded environment, we now have a function which can wipe out(\u64e6\u9664) any changes made concurrently to B in other threads \u2013 even when A is 0. The original code didn\u2019t do that. This type of obscure, technical non-impossibility is part of the reason why people have been saying that C++ doesn\u2019t support threads , despite the fact that we\u2019ve been happily writing multithreaded and lock-free code in C/C++ for decades. NOTE: 1\u3001\u8fd9\u4e2a\u4f8b\u5b50\u518d\u6b21\u9a8c\u8bc1\u4e86\u524d\u9762\u7684\u8bba\u70b9 2\u3001\u4e0a\u8ff0\u4f8b\u5b50\u662f\u5178\u578b\u7684overwrite I don\u2019t know anyone who ever fell victim to such \u201cout-of-thin-air\u201d stores in practice. Maybe it\u2019s just because for the type of lock-free code we tend to write, there aren\u2019t a whole lot of optimization opportunities fitting this pattern. I suppose if I ever caught this type of compiler transformation happening, I would search for a way to wrestle the compiler into submission. If it\u2019s happened to you, let me know in the comments. NOTE: \"\u6211\u60f3\uff0c\u5982\u679c\u6211\u53d1\u73b0\u53d1\u751f\u4e86\u8fd9\u79cd\u7f16\u8bd1\u5668\u8f6c\u6362\uff0c\u6211\u5c06\u5bfb\u627e\u4e00\u79cd\u65b9\u6cd5\u6765\u4f7f\u7f16\u8bd1\u5668\u5c48\u670d\u3002\" In any case, the new C++11 standard explictly prohibits such behavior from the compiler in cases where it would introduce a data race . The wording can be found in and around \u00a71.10.22 of the most recent C++11 working draft : Compiler transformations that introduce assignments to a potentially shared memory location that would not be modified by the abstract machine are generally precluded(\u963b\u6b62\u3001\u6392\u67e5) by this standard.","title":"Out-Of-Thin-Air Stores"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/04-Memory-Ordering-at-Compile-Time/#why#compiler#reordering","text":"","title":"Why Compiler Reordering?"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/04-Memory-Ordering-at-Compile-Time/#performance#optimization","text":"NOTE: \u9075\u5faaoptimization principle As I mentioned at the start, the compiler modifies the order of memory interactions for the same reason that the processor does it \u2013 performance optimization. Such optimizations are a direct consequence of modern CPU complexity. I may going out on a limb, but I somehow doubt that compilers did a whole lot of instruction reordering in the early 80\u2019s, when CPUs had only a few hundred thousand transistors at most. I don\u2019t think there would have been much point. But since then, Moore\u2019s Law has provided CPU designers with about 10000 times the number of transistors to play with, and those transistors have been spent on tricks such as pipelining, memory prefetching, ILP and more recently, multicore. As a result of some of those features, we\u2019ve seen architectures where the order of instructions in a program can make a significant difference in performance. The first Intel Pentium released in 1993, with its so-called U and V-pipes, was the first processor where I really remember people talking about pipelining and the significance of instruction ordering . More recently, though, when I step through x86 disassembly in Visual Studio, I\u2019m actually surprised how little instruction reordering there is. On the other hand, out of the times I\u2019ve stepped through SPU disassembly on Playstation 3, I\u2019ve found that the compiler really went to town. These are just anecdotal experiences; it may not reflect the experience of others, and certainly should not influence the way we enforce memory ordering in our lock-free code.","title":"Performance optimization"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/05-Memory-Barriers-Are-Like-Source-Control-Operations/","text":"preshing Memory Barriers Are Like Source Control Operations NOTE: 1\u3001\u5728\u8fd9\u4e00\u8282\uff0c\u4f5c\u8005\u8fdb\u884c\u4e86\u975e\u5e38\u597d\u7684\u6bd4\u55bb 2\u3001C++ \u7684\u5b9e\u73b0 a\u3001 atomic_thread_fence (C++11) b\u3001 atomic_signal_fence (C++11) 3\u3001advanced programming\uff0c\u6bd4\u5982 C++\uff0c\u4f1a\u63d0\u4f9b\u66f4\u52a0\u62bd\u8c61\u7684\u6982\u5ff5\uff0c\u8fd9\u4e9b\u62bd\u8c61\u6982\u5ff5\u7684\u5b9e\u73b0\u6700\u7ec8\u8fd8\u662f\u4f9d\u8d56\u4e8e\u672c\u8282\u4ecb\u7ecd\u7684\u5404\u79cdmemory barrier instruction Memory ordering at compile time VS at runtime In my last post, I wrote about memory ordering at compile time , which forms one half of the memory ordering puzzle. This post is about the other half: memory ordering at runtime, on the processor itself. Like compiler reordering, processor reordering is invisible to a single-threaded program. It only becomes apparent when lock-free techniques are used \u2013 that is, when shared memory is manipulated without any mutual exclusion between threads. However, unlike compiler reordering, the effects of processor reordering are only visible in multicore and multiprocessor systems . NOTE: \u6700\u540e\u4e00\u6bb5\u8bdd\u662f\u6709\u8bef\u7684\uff0c\u5728\u4e00\u4e9b\u6587\u7ae0\u4e2d\u652f\u6301\uff0c\u5728\u5355\u6838system\u4e2d\uff0c\u4e5f\u53ef\u80fd\u4f1a\u51fa\u73b0 What is memory barrier? You can enforce correct memory ordering on the processor by issuing any instruction which acts as a memory barrier . In some ways, this is the only technique you need to know, because when you use such instructions, compiler ordering is taken care of automatically. Examples of instructions which act as memory barriers include (but are not limited to) the following: 1\u3001Certain inline assembly directives in GCC, such as the PowerPC-specific asm volatile(\"lwsync\" ::: \"memory\") 2\u3001Any Win32 Interlocked operation , except on Xbox 360 3\u3001Many operations on C++11 atomic types , such as load(std::memory_order_acquire) 4\u3001Operations on POSIX mutexes, such as pthread_mutex_lock Just as there are many instructions which act as memory barriers, there are many different types of memory barriers to know about. Indeed, not all of the above instructions produce the same kind of memory barrier \u2013 leading to another possible area of confusion when writing lock-free code. In an attempt to clear things up to some extent, I\u2019d like to offer an analogy which I\u2019ve found helpful in understanding the vast majority (but not all) of possible memory barrier types. Architecture of a typical multicore system NOTE: \u6b64\u5904\u7ed9\u51fa\u4e86multicore system\u7684\u975e\u5e38\u597d\u7684\u7c7b\u6bd4/\u6bd4\u55bb To begin with, consider the architecture of a typical multicore system. Here\u2019s a device with two cores, each having 32 KiB of private L1 data cache. There\u2019s 1 MiB of L2 cache shared between both cores, and 512 MiB of main memory. \u7c7b\u6bd4/\u6bd4\u55bb NOTE: 1\u3001RAM\u5bf9\u5e94\u7684\u662fcentral repository 2\u3001cache\u5bf9\u5e94\u7684\u662flocal repository A multicore system is a bit like a group of programmers collaborating on a project using a bizarre kind of source control strategy. For example, the above dual-core system corresponds to a scenario with just two programmers. Let\u2019s name them Larry and Sergey. On the right, we have a shared, central repository \u2013 this represents a combination of main memory and the shared L2 cache. Larry has a complete working copy of the repository on his local machine, and so does Sergey \u2013 these (effectively) represent the L1 caches attached to each CPU core. There\u2019s also a scratch area on each machine, to privately keep track of registers and/or local variables. Our two programmers sit there, feverishly(\u5fd9\u4e71\u7684) editing their working copy and scratch area, all while making decisions about what to do next based on the data they see \u2013 much like a thread of execution running on that core. Source control strategy Leak into the respository Which brings us to the source control strategy. In this analogy, the source control strategy is very strange indeed. As Larry and Sergey modify their working copies of the repository, their modifications are constantly leaking (\u6cc4\u9732) in the background, to and from the central repository, at totally random times. Once Larry edits the file X, his change will leak to the central repository , but there\u2019s no guarantee about when it will happen. It might happen immediately, or it might happen much, much later. He might go on to edit other files, say Y and Z, and those modifications might leak into the respository before X gets leaked. In this manner, stores are effectively reordered on their way to the repository. Leak back from the repository into his working copy Similarly, on Sergey\u2019s machine, there\u2019s no guarantee about the timing or the order in which those changes leak back from the repository into his working copy. In this manner, loads are effectively reordered on their way out of the repository. Now, if each programmer works on completely separate parts of the repository, neither programmer will be aware of these background leaks going on, or even of the other programmer\u2019s existence. That would be analogous to running two independent, single-threaded processes. In this case, the cardinal rule of memory ordering is upheld(\u652f\u6301). Classic example The analogy becomes more useful once our programmers start working on the same parts of the repository. Let\u2019s revisit the example I gave in an earlier post . X and Y are global variables, both initially 0: Think of X and Y as files which exist on Larry\u2019s working copy of the repository, Sergey\u2019s working copy, and the central repository itself. Larry writes 1 to his working copy of X and Sergey writes 1 to his working copy of Y at roughly the same time. If neither modification has time to leak to the repository and back before each programmer looks up his working copy of the other file, they\u2019ll end up with both r1 = 0 and r2 = 0. This result, which may have seemed counterintuitive(\u8fdd\u53cd\u76f4\u89c9\u7684) at first, actually becomes pretty obvious in the source control analogy. Types of Memory Barrier NOTE: \u5728\u5de5\u7a0bhardware\u7684 Memory-ordering \u4e2d\u7ed9\u51fa\u7684\u603b\u7ed3\u5982\u4e0b: reordering \u542b\u4e49 memory barrier/fence load-load(read-read) acquire semantic store-store(write-write) release semantic load-store release semantic\u3001acquire semantic store-load sequential consistency 1\u3001\u542b\u4e49\u8fd9\u4e00\u5217\u7701\u7565\u4e86\uff0c\u53c2\u89c1 preshing Memory Barriers Are Like Source Control Operations \uff0c\u5176\u4e2d\u6709\u975e\u5e38\u597d\u7684\u63cf\u8ff0\u3002 2\u3001\u6700\u540e\u4e00\u5217\u7684\u542b\u4e49\u662f: \u5bf9\u4e8e\u6bcf\u4e00\u79cdreordering\uff0c\u90fd\u6709\u5bf9\u5e94\u7684memory barrier\u6765\u963b\u6b62\u5b83\uff0c\u6dfb\u52a0\u4e86\u5bf9\u5e94\u7684memory barrier\uff0c\u5c31\u80fd\u591f\u4fdd\u8bc1\u5bf9\u5e94\u7684semantic\u3002 Fence instructions, which act as memory barriers Fortunately, Larry and Sergey are not entirely at the mercy of these random, unpredictable leaks happening in the background. They also have the ability to issue special instructions, called fence instructions, which act as memory barriers . For this analogy, it\u2019s sufficient to define four types of memory barrier , and thus four different fence instructions. Each type of memory barrier is named after the type of memory reordering it\u2019s designed to prevent: for example, #StoreLoad is designed to prevent the reordering of a store followed by a load. As Doug Lea points out , these four categories map pretty well to specific instructions on real CPUs \u2013 though not exactly. Most of the time, a real CPU instruction acts as some combination of the above barrier types, possibly in addition to other effects. In any case, once you understand these four types of memory barriers in the source control analogy, you\u2019re in a good position to understand a large number of instructions on real CPUs, as well as several higher-level programming language constructs. #LoadLoad A LoadLoad barrier effectively prevents reordering of loads performed before the barrier with loads performed after the barrier. NOTE: \u6ce8\u610f\u7bad\u5934\u65b9\u5411 Prevent seeing stale data This may sound like a weak guarantee, but it\u2019s still a perfectly good way to prevent seeing stale(\u9648\u65e7\u7684) data. Consider the classic example, where Sergey checks a shared flag to see if some data has been published by Larry. If the flag is true, he issues a #LoadLoad barrier before reading the published value: NOTE: 1\u3001\u8fd9\u4e2a\u4f8b\u5b50\u5176\u5b9e\u662f\u7ecf\u5e38\u7528\u6765\u63cf\u8ff0acquire-release semantic\u7684\u7ecf\u5178\u4f8b\u5b50 2\u3001\u5b83\u5bf9\u5e94\u7684\u662facquire semantic if ( IsPublished ) // Load and check shared flag { LOADLOAD_FENCE (); // Prevent reordering of loads return Value ; // Load published value } Obviously, this example depends on having the IsPublished flag leak into Sergey\u2019s working copy by itself. It doesn\u2019t matter exactly when that happens; once the leaked flag has been observed, he issues a #LoadLoad fence to prevent reading some value of Value which is older than the flag itself. #StoreStore A StoreStore barrier effectively prevents reordering of stores performed before the barrier with stores performed after the barrier. In our analogy, the #StoreStore fence instruction corresponds to a push to the central repository. Think git push , hg push , p4 submit , svn commit or cvs commit , all acting on the entire repository. As an added twist, let\u2019s suppose that #StoreStore instructions are not instant . They\u2019re performed in a delayed, asynchronous manner. So, even though Larry executes a #StoreStore , we can\u2019t make any assumptions about when all his previous stores finally become visible in the central repository. Prevent seeing stale data This, too, may sound like a weak guarantee, but again, it\u2019s perfectly sufficient to prevent Sergey from seeing any stale data published by Larry. Returning to the same example as above, Larry needs only to publish some data to shared memory, issue a #StoreStore barrier, then set the shared flag to true: NOTE: 1\u3001\u8fd9\u4e2a\u4f8b\u5b50\uff0c\u5176\u5b9e\u5bf9\u5e94\u7684release semantic 2\u3001StoreStore barrier \u80fd\u591f\u4fdd\u8bc1\u5148\u63d0\u4ea4\u7684write\u88ab\u5199\u5165\u5230central repository\u4e2d\uff1f\u5373\u5b83\u662f\u5426\u80fd\u591f\u5b9e\u73b0release semantic\u7684\u6548\u679c\uff1f Value = x ; // Publish some data STORESTORE_FENCE (); IsPublished = 1 ; // Set shared flag to indicate availability of data Again, we\u2019re counting on the value of IsPublished to leak from Larry\u2019s working copy over to Sergey\u2019s, all by itself. Once Sergey detects that, he can be confident he\u2019ll see the correct value of Value . What\u2019s interesting is that, for this pattern to work, Value does not even need to be an atomic type; it could just as well be a huge structure with lots of elements. NOTE: 1\u3001 IsPublished \u4f5c\u4e3a\u7ebf\u7a0b\u95f4synchronization \u7684 guard variable \uff0c Value \u662f**payload**\uff0c\u8fd9\u5728preshing The Synchronizes-With Relation \u4e2d\u8fdb\u884c\u4e86\u8bf4\u660e #LoadStore Unlike #LoadLoad and #StoreStore , there\u2019s no clever metaphor for #LoadStore in terms of source control operations. The best way to understand a #LoadStore barrier is, quite simply, in terms of instruction reordering. Metaphor: \u624b\u5de5reorder Imagine Larry has a set of instructions to follow. Some instructions make him load data from his private working copy into a register, and some make him store data from a register back into the working copy. Larry has the ability to juggle instructions, but only in specific cases. Whenever he encounters a load, he looks ahead at any stores that are coming up after that; if the stores are completely unrelated to the current load, then he\u2019s allowed to skip ahead, do the stores first, then come back afterwards to finish up the load. In such cases, the cardinal rule of memory ordering \u2013 never modify the behavior of a single-threaded program \u2013 is still followed. NOTE: \u59cb\u7ec8\u9075\u5faacardinal rule of memory ordering \u73b0\u5b9eCPU\u4e2dload\u3001store\u4f8b\u5b50 On a real CPU, such instruction reordering might happen on certain processors if, say, there is a cache miss on the load followed by a cache hit on the store. NOTE: Fortunately, this is a relatively inexpensive type of reordering to prevent; when Larry encounters a #LoadStore barrier, he simply refrains(\u907f\u514d) from such reordering around that barrier. In our analogy, it\u2019s valid for Larry to perform this kind of LoadStore reordering even when there is a #LoadLoad or #StoreStore barrier between the load and the store. However, on a real CPU, instructions which act as a #LoadStore barrier typically act as at least one of those other two barrier types. #StoreLoad NOTE: 1\u3001StoreLoad barrier \u7528\u4e8e\u4fdd\u8bc1 sequential consistency A StoreLoad barrier ensures that all stores performed before the barrier are visible to other processors, and that all loads performed after the barrier receive the latest value that is visible at the time of the barrier. In other words, it effectively prevents reordering of all stores before the barrier against all loads after the barrier, respecting the way a sequentially consistent multiprocessor would perform those operations. #StoreLoad is unique. It\u2019s the only type of memory barrier that will prevent the result r1 = r2 = 0 in the example given in Memory Reordering Caught in the Act ; the same example I\u2019ve repeated earlier in this post. #StoreLoad VS #StoreStore + #LoadLoad If you\u2019ve been following closely, you might wonder: How is #StoreLoad different from a #StoreStore followed by a #LoadLoad ? After all, a #StoreStore pushes changes to the central repository, while #LoadLoad pulls remote changes back. However, those two barrier types are insufficient. Remember, the push operation may be delayed for an arbitrary number of instructions, and the pull operation might not pull from the head revision. This hints(\u6697\u793a) at why the PowerPC\u2019s lwsync instruction \u2013 which acts as all three #LoadLoad , #LoadStore and #StoreStore memory barriers, but not #StoreLoad \u2013 is insufficient to prevent r1 = r2 = 0 in that example. In terms of the analogy, a #StoreLoad barrier could be achieved by pushing all local changes to the central repostitory, waiting for that operation to complete, then pulling the absolute latest head revision of the repository. On most processors, instructions that act as a #StoreLoad barrier tend to be more expensive than instructions acting as the other barrier types. Full memory fence If we throw a #LoadStore barrier into that operation, which shouldn\u2019t be a big deal, then what we get is a full memory fence \u2013 acting as all four barrier types at once. As Doug Lea also points out , it just so happens that on all current processors, every instruction which acts as a #StoreLoad barrier also acts as a full memory fence. How Far Does This Analogy Get You? As I\u2019ve mentioned previously, every processor has different habits when it comes to memory ordering. The x86/64 family, in particular, has a strong memory model ; it\u2019s known to keep memory reordering to a minimum. PowerPC and ARM have weaker memory models , and the Alpha is famous for being in a league(\u8054\u76df) of its own. Fortunately, the analogy presented in this post corresponds to a weak memory model . If you can wrap your head around it, and enforce correct memory ordering using the fence instructions given here, you should be able to handle most CPUs. The analogy also corresponds pretty well to the abstract machine targeted by both C++11 (formerly known as C++0x) and C11. Therefore, if you write lock-free code using the standard library of those languages while keeping the above analogy in mind, it\u2019s more likely to function correctly on any platform. In this analogy, I\u2019ve said that each programmer represents a single thread of execution running on a separate core. On a real operating system, threads tend to move between different cores over the course of their lifetime, but the analogy still works. I\u2019ve also alternated(\u4ea4\u66ff) between examples in machine language and examples written in C/C++. Obviously, we\u2019d prefer to stick with C/C++, or another high-level language; this is possible because again, any operation which acts as a memory barrier also prevents compiler reordering . NOTE: \u5728 preshing Memory Ordering at Compile Time \u4e2d\uff0c\u5c06\u6b64\u6210\u4e3a \"Implied Compiler Barriers\"\u3002 I haven\u2019t written about every type of memory barrier yet. For instance, there are also data dependency barriers . I\u2019ll describe those further in a future post. Still, the four types given here are the big ones. If you\u2019re interested in how CPUs work under the hood \u2013 things like stores buffers, cache coherency protocols and other hardware implementation details \u2013 and why they perform memory reordering in the first place, I\u2019d recommend the fine work of Paul McKenney & David Howells. Indeed, I suspect most programmers who have successfully written lock-free code have at least a passing familiarity with such hardware details.","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/05-Memory-Barriers-Are-Like-Source-Control-Operations/#preshing#memory#barriers#are#like#source#control#operations","text":"NOTE: 1\u3001\u5728\u8fd9\u4e00\u8282\uff0c\u4f5c\u8005\u8fdb\u884c\u4e86\u975e\u5e38\u597d\u7684\u6bd4\u55bb 2\u3001C++ \u7684\u5b9e\u73b0 a\u3001 atomic_thread_fence (C++11) b\u3001 atomic_signal_fence (C++11) 3\u3001advanced programming\uff0c\u6bd4\u5982 C++\uff0c\u4f1a\u63d0\u4f9b\u66f4\u52a0\u62bd\u8c61\u7684\u6982\u5ff5\uff0c\u8fd9\u4e9b\u62bd\u8c61\u6982\u5ff5\u7684\u5b9e\u73b0\u6700\u7ec8\u8fd8\u662f\u4f9d\u8d56\u4e8e\u672c\u8282\u4ecb\u7ecd\u7684\u5404\u79cdmemory barrier instruction","title":"preshing Memory Barriers Are Like Source Control Operations"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/05-Memory-Barriers-Are-Like-Source-Control-Operations/#memory#ordering#at#compile#time#vs#at#runtime","text":"In my last post, I wrote about memory ordering at compile time , which forms one half of the memory ordering puzzle. This post is about the other half: memory ordering at runtime, on the processor itself. Like compiler reordering, processor reordering is invisible to a single-threaded program. It only becomes apparent when lock-free techniques are used \u2013 that is, when shared memory is manipulated without any mutual exclusion between threads. However, unlike compiler reordering, the effects of processor reordering are only visible in multicore and multiprocessor systems . NOTE: \u6700\u540e\u4e00\u6bb5\u8bdd\u662f\u6709\u8bef\u7684\uff0c\u5728\u4e00\u4e9b\u6587\u7ae0\u4e2d\u652f\u6301\uff0c\u5728\u5355\u6838system\u4e2d\uff0c\u4e5f\u53ef\u80fd\u4f1a\u51fa\u73b0","title":"Memory ordering at compile time VS at runtime"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/05-Memory-Barriers-Are-Like-Source-Control-Operations/#what#is#memory#barrier","text":"You can enforce correct memory ordering on the processor by issuing any instruction which acts as a memory barrier . In some ways, this is the only technique you need to know, because when you use such instructions, compiler ordering is taken care of automatically. Examples of instructions which act as memory barriers include (but are not limited to) the following: 1\u3001Certain inline assembly directives in GCC, such as the PowerPC-specific asm volatile(\"lwsync\" ::: \"memory\") 2\u3001Any Win32 Interlocked operation , except on Xbox 360 3\u3001Many operations on C++11 atomic types , such as load(std::memory_order_acquire) 4\u3001Operations on POSIX mutexes, such as pthread_mutex_lock Just as there are many instructions which act as memory barriers, there are many different types of memory barriers to know about. Indeed, not all of the above instructions produce the same kind of memory barrier \u2013 leading to another possible area of confusion when writing lock-free code. In an attempt to clear things up to some extent, I\u2019d like to offer an analogy which I\u2019ve found helpful in understanding the vast majority (but not all) of possible memory barrier types.","title":"What is memory barrier?"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/05-Memory-Barriers-Are-Like-Source-Control-Operations/#architecture#of#a#typical#multicore#system","text":"NOTE: \u6b64\u5904\u7ed9\u51fa\u4e86multicore system\u7684\u975e\u5e38\u597d\u7684\u7c7b\u6bd4/\u6bd4\u55bb To begin with, consider the architecture of a typical multicore system. Here\u2019s a device with two cores, each having 32 KiB of private L1 data cache. There\u2019s 1 MiB of L2 cache shared between both cores, and 512 MiB of main memory.","title":"Architecture of a typical multicore system"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/05-Memory-Barriers-Are-Like-Source-Control-Operations/#_1","text":"NOTE: 1\u3001RAM\u5bf9\u5e94\u7684\u662fcentral repository 2\u3001cache\u5bf9\u5e94\u7684\u662flocal repository A multicore system is a bit like a group of programmers collaborating on a project using a bizarre kind of source control strategy. For example, the above dual-core system corresponds to a scenario with just two programmers. Let\u2019s name them Larry and Sergey. On the right, we have a shared, central repository \u2013 this represents a combination of main memory and the shared L2 cache. Larry has a complete working copy of the repository on his local machine, and so does Sergey \u2013 these (effectively) represent the L1 caches attached to each CPU core. There\u2019s also a scratch area on each machine, to privately keep track of registers and/or local variables. Our two programmers sit there, feverishly(\u5fd9\u4e71\u7684) editing their working copy and scratch area, all while making decisions about what to do next based on the data they see \u2013 much like a thread of execution running on that core.","title":"\u7c7b\u6bd4/\u6bd4\u55bb"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/05-Memory-Barriers-Are-Like-Source-Control-Operations/#source#control#strategy","text":"","title":"Source control strategy"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/05-Memory-Barriers-Are-Like-Source-Control-Operations/#leak#into#the#respository","text":"Which brings us to the source control strategy. In this analogy, the source control strategy is very strange indeed. As Larry and Sergey modify their working copies of the repository, their modifications are constantly leaking (\u6cc4\u9732) in the background, to and from the central repository, at totally random times. Once Larry edits the file X, his change will leak to the central repository , but there\u2019s no guarantee about when it will happen. It might happen immediately, or it might happen much, much later. He might go on to edit other files, say Y and Z, and those modifications might leak into the respository before X gets leaked. In this manner, stores are effectively reordered on their way to the repository.","title":"Leak into the respository"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/05-Memory-Barriers-Are-Like-Source-Control-Operations/#leak#back#from#the#repository#into#his#working#copy","text":"Similarly, on Sergey\u2019s machine, there\u2019s no guarantee about the timing or the order in which those changes leak back from the repository into his working copy. In this manner, loads are effectively reordered on their way out of the repository. Now, if each programmer works on completely separate parts of the repository, neither programmer will be aware of these background leaks going on, or even of the other programmer\u2019s existence. That would be analogous to running two independent, single-threaded processes. In this case, the cardinal rule of memory ordering is upheld(\u652f\u6301).","title":"Leak back from the repository into his working copy"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/05-Memory-Barriers-Are-Like-Source-Control-Operations/#classic#example","text":"The analogy becomes more useful once our programmers start working on the same parts of the repository. Let\u2019s revisit the example I gave in an earlier post . X and Y are global variables, both initially 0: Think of X and Y as files which exist on Larry\u2019s working copy of the repository, Sergey\u2019s working copy, and the central repository itself. Larry writes 1 to his working copy of X and Sergey writes 1 to his working copy of Y at roughly the same time. If neither modification has time to leak to the repository and back before each programmer looks up his working copy of the other file, they\u2019ll end up with both r1 = 0 and r2 = 0. This result, which may have seemed counterintuitive(\u8fdd\u53cd\u76f4\u89c9\u7684) at first, actually becomes pretty obvious in the source control analogy.","title":"Classic example"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/05-Memory-Barriers-Are-Like-Source-Control-Operations/#types#of#memory#barrier","text":"NOTE: \u5728\u5de5\u7a0bhardware\u7684 Memory-ordering \u4e2d\u7ed9\u51fa\u7684\u603b\u7ed3\u5982\u4e0b: reordering \u542b\u4e49 memory barrier/fence load-load(read-read) acquire semantic store-store(write-write) release semantic load-store release semantic\u3001acquire semantic store-load sequential consistency 1\u3001\u542b\u4e49\u8fd9\u4e00\u5217\u7701\u7565\u4e86\uff0c\u53c2\u89c1 preshing Memory Barriers Are Like Source Control Operations \uff0c\u5176\u4e2d\u6709\u975e\u5e38\u597d\u7684\u63cf\u8ff0\u3002 2\u3001\u6700\u540e\u4e00\u5217\u7684\u542b\u4e49\u662f: \u5bf9\u4e8e\u6bcf\u4e00\u79cdreordering\uff0c\u90fd\u6709\u5bf9\u5e94\u7684memory barrier\u6765\u963b\u6b62\u5b83\uff0c\u6dfb\u52a0\u4e86\u5bf9\u5e94\u7684memory barrier\uff0c\u5c31\u80fd\u591f\u4fdd\u8bc1\u5bf9\u5e94\u7684semantic\u3002","title":"Types of Memory Barrier"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/05-Memory-Barriers-Are-Like-Source-Control-Operations/#fence#instructions#which#act#as#memory#barriers","text":"Fortunately, Larry and Sergey are not entirely at the mercy of these random, unpredictable leaks happening in the background. They also have the ability to issue special instructions, called fence instructions, which act as memory barriers . For this analogy, it\u2019s sufficient to define four types of memory barrier , and thus four different fence instructions. Each type of memory barrier is named after the type of memory reordering it\u2019s designed to prevent: for example, #StoreLoad is designed to prevent the reordering of a store followed by a load. As Doug Lea points out , these four categories map pretty well to specific instructions on real CPUs \u2013 though not exactly. Most of the time, a real CPU instruction acts as some combination of the above barrier types, possibly in addition to other effects. In any case, once you understand these four types of memory barriers in the source control analogy, you\u2019re in a good position to understand a large number of instructions on real CPUs, as well as several higher-level programming language constructs.","title":"Fence instructions, which act as memory barriers"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/05-Memory-Barriers-Are-Like-Source-Control-Operations/#loadload","text":"A LoadLoad barrier effectively prevents reordering of loads performed before the barrier with loads performed after the barrier. NOTE: \u6ce8\u610f\u7bad\u5934\u65b9\u5411","title":"#LoadLoad"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/05-Memory-Barriers-Are-Like-Source-Control-Operations/#prevent#seeing#stale#data","text":"This may sound like a weak guarantee, but it\u2019s still a perfectly good way to prevent seeing stale(\u9648\u65e7\u7684) data. Consider the classic example, where Sergey checks a shared flag to see if some data has been published by Larry. If the flag is true, he issues a #LoadLoad barrier before reading the published value: NOTE: 1\u3001\u8fd9\u4e2a\u4f8b\u5b50\u5176\u5b9e\u662f\u7ecf\u5e38\u7528\u6765\u63cf\u8ff0acquire-release semantic\u7684\u7ecf\u5178\u4f8b\u5b50 2\u3001\u5b83\u5bf9\u5e94\u7684\u662facquire semantic if ( IsPublished ) // Load and check shared flag { LOADLOAD_FENCE (); // Prevent reordering of loads return Value ; // Load published value } Obviously, this example depends on having the IsPublished flag leak into Sergey\u2019s working copy by itself. It doesn\u2019t matter exactly when that happens; once the leaked flag has been observed, he issues a #LoadLoad fence to prevent reading some value of Value which is older than the flag itself.","title":"Prevent seeing stale data"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/05-Memory-Barriers-Are-Like-Source-Control-Operations/#storestore","text":"A StoreStore barrier effectively prevents reordering of stores performed before the barrier with stores performed after the barrier. In our analogy, the #StoreStore fence instruction corresponds to a push to the central repository. Think git push , hg push , p4 submit , svn commit or cvs commit , all acting on the entire repository. As an added twist, let\u2019s suppose that #StoreStore instructions are not instant . They\u2019re performed in a delayed, asynchronous manner. So, even though Larry executes a #StoreStore , we can\u2019t make any assumptions about when all his previous stores finally become visible in the central repository.","title":"#StoreStore"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/05-Memory-Barriers-Are-Like-Source-Control-Operations/#prevent#seeing#stale#data_1","text":"This, too, may sound like a weak guarantee, but again, it\u2019s perfectly sufficient to prevent Sergey from seeing any stale data published by Larry. Returning to the same example as above, Larry needs only to publish some data to shared memory, issue a #StoreStore barrier, then set the shared flag to true: NOTE: 1\u3001\u8fd9\u4e2a\u4f8b\u5b50\uff0c\u5176\u5b9e\u5bf9\u5e94\u7684release semantic 2\u3001StoreStore barrier \u80fd\u591f\u4fdd\u8bc1\u5148\u63d0\u4ea4\u7684write\u88ab\u5199\u5165\u5230central repository\u4e2d\uff1f\u5373\u5b83\u662f\u5426\u80fd\u591f\u5b9e\u73b0release semantic\u7684\u6548\u679c\uff1f Value = x ; // Publish some data STORESTORE_FENCE (); IsPublished = 1 ; // Set shared flag to indicate availability of data Again, we\u2019re counting on the value of IsPublished to leak from Larry\u2019s working copy over to Sergey\u2019s, all by itself. Once Sergey detects that, he can be confident he\u2019ll see the correct value of Value . What\u2019s interesting is that, for this pattern to work, Value does not even need to be an atomic type; it could just as well be a huge structure with lots of elements. NOTE: 1\u3001 IsPublished \u4f5c\u4e3a\u7ebf\u7a0b\u95f4synchronization \u7684 guard variable \uff0c Value \u662f**payload**\uff0c\u8fd9\u5728preshing The Synchronizes-With Relation \u4e2d\u8fdb\u884c\u4e86\u8bf4\u660e","title":"Prevent seeing stale data"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/05-Memory-Barriers-Are-Like-Source-Control-Operations/#loadstore","text":"Unlike #LoadLoad and #StoreStore , there\u2019s no clever metaphor for #LoadStore in terms of source control operations. The best way to understand a #LoadStore barrier is, quite simply, in terms of instruction reordering.","title":"#LoadStore"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/05-Memory-Barriers-Are-Like-Source-Control-Operations/#metaphor#reorder","text":"Imagine Larry has a set of instructions to follow. Some instructions make him load data from his private working copy into a register, and some make him store data from a register back into the working copy. Larry has the ability to juggle instructions, but only in specific cases. Whenever he encounters a load, he looks ahead at any stores that are coming up after that; if the stores are completely unrelated to the current load, then he\u2019s allowed to skip ahead, do the stores first, then come back afterwards to finish up the load. In such cases, the cardinal rule of memory ordering \u2013 never modify the behavior of a single-threaded program \u2013 is still followed. NOTE: \u59cb\u7ec8\u9075\u5faacardinal rule of memory ordering","title":"Metaphor: \u624b\u5de5reorder"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/05-Memory-Barriers-Are-Like-Source-Control-Operations/#cpuloadstore","text":"On a real CPU, such instruction reordering might happen on certain processors if, say, there is a cache miss on the load followed by a cache hit on the store. NOTE: Fortunately, this is a relatively inexpensive type of reordering to prevent; when Larry encounters a #LoadStore barrier, he simply refrains(\u907f\u514d) from such reordering around that barrier. In our analogy, it\u2019s valid for Larry to perform this kind of LoadStore reordering even when there is a #LoadLoad or #StoreStore barrier between the load and the store. However, on a real CPU, instructions which act as a #LoadStore barrier typically act as at least one of those other two barrier types.","title":"\u73b0\u5b9eCPU\u4e2dload\u3001store\u4f8b\u5b50"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/05-Memory-Barriers-Are-Like-Source-Control-Operations/#storeload","text":"NOTE: 1\u3001StoreLoad barrier \u7528\u4e8e\u4fdd\u8bc1 sequential consistency A StoreLoad barrier ensures that all stores performed before the barrier are visible to other processors, and that all loads performed after the barrier receive the latest value that is visible at the time of the barrier. In other words, it effectively prevents reordering of all stores before the barrier against all loads after the barrier, respecting the way a sequentially consistent multiprocessor would perform those operations. #StoreLoad is unique. It\u2019s the only type of memory barrier that will prevent the result r1 = r2 = 0 in the example given in Memory Reordering Caught in the Act ; the same example I\u2019ve repeated earlier in this post.","title":"#StoreLoad"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/05-Memory-Barriers-Are-Like-Source-Control-Operations/#storeload#vs#storestore#loadload","text":"If you\u2019ve been following closely, you might wonder: How is #StoreLoad different from a #StoreStore followed by a #LoadLoad ? After all, a #StoreStore pushes changes to the central repository, while #LoadLoad pulls remote changes back. However, those two barrier types are insufficient. Remember, the push operation may be delayed for an arbitrary number of instructions, and the pull operation might not pull from the head revision. This hints(\u6697\u793a) at why the PowerPC\u2019s lwsync instruction \u2013 which acts as all three #LoadLoad , #LoadStore and #StoreStore memory barriers, but not #StoreLoad \u2013 is insufficient to prevent r1 = r2 = 0 in that example. In terms of the analogy, a #StoreLoad barrier could be achieved by pushing all local changes to the central repostitory, waiting for that operation to complete, then pulling the absolute latest head revision of the repository. On most processors, instructions that act as a #StoreLoad barrier tend to be more expensive than instructions acting as the other barrier types.","title":"#StoreLoad VS #StoreStore + #LoadLoad"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/05-Memory-Barriers-Are-Like-Source-Control-Operations/#full#memory#fence","text":"If we throw a #LoadStore barrier into that operation, which shouldn\u2019t be a big deal, then what we get is a full memory fence \u2013 acting as all four barrier types at once. As Doug Lea also points out , it just so happens that on all current processors, every instruction which acts as a #StoreLoad barrier also acts as a full memory fence.","title":"Full memory fence"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/05-Memory-Barriers-Are-Like-Source-Control-Operations/#how#far#does#this#analogy#get#you","text":"As I\u2019ve mentioned previously, every processor has different habits when it comes to memory ordering. The x86/64 family, in particular, has a strong memory model ; it\u2019s known to keep memory reordering to a minimum. PowerPC and ARM have weaker memory models , and the Alpha is famous for being in a league(\u8054\u76df) of its own. Fortunately, the analogy presented in this post corresponds to a weak memory model . If you can wrap your head around it, and enforce correct memory ordering using the fence instructions given here, you should be able to handle most CPUs. The analogy also corresponds pretty well to the abstract machine targeted by both C++11 (formerly known as C++0x) and C11. Therefore, if you write lock-free code using the standard library of those languages while keeping the above analogy in mind, it\u2019s more likely to function correctly on any platform. In this analogy, I\u2019ve said that each programmer represents a single thread of execution running on a separate core. On a real operating system, threads tend to move between different cores over the course of their lifetime, but the analogy still works. I\u2019ve also alternated(\u4ea4\u66ff) between examples in machine language and examples written in C/C++. Obviously, we\u2019d prefer to stick with C/C++, or another high-level language; this is possible because again, any operation which acts as a memory barrier also prevents compiler reordering . NOTE: \u5728 preshing Memory Ordering at Compile Time \u4e2d\uff0c\u5c06\u6b64\u6210\u4e3a \"Implied Compiler Barriers\"\u3002 I haven\u2019t written about every type of memory barrier yet. For instance, there are also data dependency barriers . I\u2019ll describe those further in a future post. Still, the four types given here are the big ones. If you\u2019re interested in how CPUs work under the hood \u2013 things like stores buffers, cache coherency protocols and other hardware implementation details \u2013 and why they perform memory reordering in the first place, I\u2019d recommend the fine work of Paul McKenney & David Howells. Indeed, I suspect most programmers who have successfully written lock-free code have at least a passing familiarity with such hardware details.","title":"How Far Does This Analogy Get You?"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/06-Acquire-and-Release-Semantics/","text":"preshing Acquire and Release Semantics Generally speaking, in lock-free programming , there are two ways in which threads can manipulate shared memory: 1\u3001They can compete with each other for a resource, or 2\u3001they can pass information co-operatively from one thread to another. Acquire and release semantics are crucial(\u81f3\u5173\u91cd\u8981\u7684) for the latter: reliable passing of information between threads. NOTE: 1\u3001\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u610f\u601d\u662f: \u4f7f\u7528**Acquire and release semantics** \u5bf9\u4e8e\u5b9e\u73b0thread\u4e4b\u95f4pass information co-operatively to access shared memory\u81f3\u5173\u91cd\u8981\uff0c\u4e5f\u5c31\u662f: \u6211\u4eec\u4f7f\u7528**Acquire and release semantics** \u6765\u5b9e\u73b0lock free In fact, I would venture(\u5192\u8fdb\u7684) to guess that incorrect or missing acquire and release semantics is the #1 type of lock-free programming error. In this post, I\u2019ll demonstrate various ways to achieve acquire and release semantics in C++. I\u2019ll touch upon the C++11 atomic library standard in an introductory way, so you don\u2019t already need to know it. And to be clear from the start, the information here pertains(\u9002\u5408\u3001\u5c5e\u4e8e) to lock-free programming without sequential consistency . We\u2019re dealing directly with memory ordering in a multicore or multiprocessor environment. Unfortunately, the terms acquire and release semantics appear to be in even worse shape than the term lock-free , in that the more you scour(\u641c\u7d22) the web, the more seemingly contradictory(\u77db\u76fe\u7684) definitions you\u2019ll find. Bruce Dawson offers a couple of good definitions (credited to Herb Sutter) about halfway through this white paper . I\u2019d like to offer a couple of definitions of my own, staying close to the principles behind C++11 atomics: Definition NOTE: \u539f\u6587\u5e76\u6ca1\u6709\u672c\u8282\u7684\u6807\u9898\u3002 1\u3001\u672c\u8282\u63cf\u8ff0Acquire semantics\u3001Release semantics\u7684\u542b\u4e49 2\u3001Acquire semantics\u3001Release semantics\u662f\u76ee\u6807\u3001\u8981\u6c42\uff0c\u540e\u9762Implementation\u7ae0\u8282\u63cf\u8ff0\u5982\u4f55\u6765\u5b9e\u73b0\u5b83\u4eec Acquire semantics Acquire semantics is a property that can only apply to operations that read from shared memory, whether they are read-modify-write operations or plain loads. The operation is then considered a read-acquire . Acquire semantics prevent memory reordering of the read-acquire with any read or write operation that follows it in program order. NOTE: \u5c06 Acquire semantics \u5e94\u7528\u4e8e\u4e00\u4e2aread operation\uff0c\u5219\u5c31\u662f: read-acquire \uff0c**read-acquire**\u5c31\u76f8\u5f53\u4e8e\u4e00\u6761instruction\uff1b**read-acquire**\u5bf9memory reordering\u6709\u5982\u4e0b\u8981\u6c42: \" Acquire semantics prevent memory reordering of the read-acquire with any read or write operation that follows it in program order.\" \u4e0b\u9762\u7684write-release\u4f9d\u6b21\u7c7b\u63a8 Release semantics Release semantics is a property that can only apply to operations that write to shared memory, whether they are read-modify-write operations or plain stores. The operation is then considered a write-release . Release semantics prevent memory reordering of the write-release with any read or write operation that precedes it in program order. Implementation NOTE: \u672c\u8282\u63cf\u8ff0\u5982\u4f55\u6765\u5b9e\u73b0Acquire semantics\u3001Release semantics Once you digest the above definitions, it\u2019s not hard to see that acquire and release semantics can be achieved using simple combinations of the memory barrier types I described at length in my previous post . The barriers must (somehow) be placed after the read-acquire operation , but before the write-release . [Update: Please note that these barriers are technically more strict than what\u2019s required for acquire and release semantics on a single memory operation, but they do achieve the desired effect.] NOTE: \u4e0b\u9762\u7684\"With Explicit Platform-Specific Fence Instructions\"\u7ae0\u8282\u7684\u4f8b\u5b50\u975e\u5e38\u5f62\u8c61\u5730\u8bf4\u660e\u4e86\u4e0a\u9762\u8fd9\u4e00\u6bb5\u7684\u5185\u5bb9\uff1b NOTE: \u4e0a\u9762\u8fd9\u5f20\u56fe\u8981\u5982\u4f55\u6765\u7406\u89e3\uff1f acquire semantic: \u4f7f\u7528 loadload\u3001loadstore\uff0c\u56e0\u4e3a: \" Acquire semantics prevent memory reordering of the read-acquire with any read or write operation that follows it in program order.\" release semantic: \u4f7f\u7528loadstore\u3001storestore\uff0c\u56e0\u4e3a: \" Release semantics prevent memory reordering of the write-release with any read or write operation that precedes it in program order.\" What\u2019s cool is that neither acquire nor release semantics requires the use of a #StoreLoad barrier, which is often a more expensive memory barrier type. For example, on PowerPC, the lwsync (short for \u201clightweight sync\u201d) instruction acts as all three #LoadLoad , #LoadStore and #StoreStore barriers at the same time, yet is less expensive than the sync instruction, which includes a #StoreLoad barrier. NOTE: \u4e0a\u9762\u7ed9\u51fa\u7684powerpc\u7684\u5b9e\u73b0\u662f\u975e\u5e38\u6613\u61c2\u7684\uff0c\u540e\u9762\u5c31\u662f\u4ee5\u5b83\u4e3a\u4f8b With Explicit Platform-Specific Fence Instructions One way to obtain the desired memory barriers is by issuing explicit fence instructions. Let\u2019s start with a simple example. Suppose we\u2019re coding for PowerPC, and __lwsync() is a compiler intrinsic(\u56fa\u6709\u7684) function that emits the lwsync instruction. Since lwsync provides so many barrier types, we can use it in the following code to establish either acquire or release semantics as needed. In Thread 1, the store to Ready turns into a write-release , and in Thread 2, the load from Ready becomes a read-acquire . If we let both threads run and find that r1 == 1 , that serves as confirmation that the value of A assigned in Thread 1 was passed successfully to Thread 2. As such, we are guaranteed that r2 == 42 . In my previous post, I already gave a lengthy analogy for #LoadLoad and #StoreStore to illustrate how this works, so I won\u2019t rehash that explanation here. synchronized-with In formal terms, we say that the store to Ready synchronized-with the load. I\u2019ve written a separate post about synchronizes-with here . For now, suffice to say that for this technique to work in general, the acquire and release semantics must apply to the same variable \u2013 in this case, Ready \u2013 and both the load and store must be atomic operations. Here, Ready is a simple aligned int , so the operations are already atomic on PowerPC. NOTE: \u8fd9\u6bb5\u89e3\u91ca\u975e\u5e38\u597d\uff0c\u7ed3\u5408\u524d\u9762\u7684\u5185\u5bb9\uff0c\u6211\u4eec\u53ef\u4ee5\u603b\u7ed3\u5b9e\u73b0: 1\u3001 the acquire and release semantics must apply to the same variable \u2013 flag variable 2\u3001The barriers must (somehow) be placed after the read-acquire operation , but before the write-release . \u6211\u4eec\u5c06\u8fd9\u79cd\u5b9e\u73b0\u65b9\u5f0f\u7b80\u8bb0\u4e3a: write-release-flag-notify-read-acquire-model\uff0c\u663e\u7136\u5b83\u80fd\u591f\u5b9e\u73b0 synchronizes-with \uff0c\u80fd\u591f\u5b9e\u73b0: \u5199\u6210\u529f\u4e4b\u540e\u624d\u53bb\u8bfb \uff0c\u7ed3\u5408\u4e0b\u9762\u7684\u56fe\u6765\u7406\u89e3**\u5199\u6210\u529f\u4e4b\u540e\u624d\u53bb\u8bfb**: thread1 \u5148write\uff0cthread2\u7136\u540eread\uff1b With Fences in Portable C++11 The above example is compiler- and processor-specific. One approach for supporting multiple platforms is to convert the code to C++11. All C++11 identifiers exist in the std namespace, so to keep the following examples brief, let\u2019s assume the statement using namespace std; was placed somewhere earlier in the code. atomic_thread_fence C++11\u2019s atomic library standard defines a portable function atomic_thread_fence() that takes a single argument to specify the type of fence. There are several possible values for this argument, but the values we\u2019re most interested in here are memory_order_acquire and memory_order_release . We\u2019ll use this function in place of __lwsync() . There\u2019s one more change to make before this example is complete. On PowerPC, we knew that both operations on Ready were atomic, but we can\u2019t make that assumption about every platform. To ensure atomicity on all platforms, we\u2019ll change the type of Ready from int to atomic<int> . I know, it\u2019s kind of a silly change, considering that aligned loads and stores of int are already atomic on every modern CPU that exists today. I\u2019ll write more about this in the post on synchronizes-with , but for now, let\u2019s do it for the warm fuzzy feeling of 100% correctness in theory. No changes to A are necessary. The memory_order_relaxed arguments above mean \u201censure these operations are atomic, but don\u2019t impose any ordering constraints/memory barriers that aren\u2019t already there.\u201d Implementation Once again, both of the above atomic_thread_fence() calls can be (and hopefully are) implemented as lwsync on PowerPC. Similarly, they could both emit a dmb instruction on ARM, which I believe is at least as effective as PowerPC\u2019s lwsync . On x86/64, both atomic_thread_fence() calls can simply be implemented as compiler barriers , since usually , every load on x86/64 already implies acquire semantics and every store implies release semantics . This is why x86/64 is often said to be strongly ordered . NOTE:\u8fd9\u6bb5\u603b\u7ed3\u975e\u5e38\u597d Without Fences in Portable C++11 In C++11, it\u2019s possible to achieve acquire and release semantics on Ready without issuing explicit fence instructions. You just need to specify memory ordering constraints directly on the operations on Ready : Think of it as rolling each fence instruction into the operations on Ready themselves. [Update: Please note that this form is not exactly the same as the version using standalone fences; technically, it\u2019s less strict.] The compiler will emit any instructions necessary to obtain the required barrier effects. In particular, on Itanium, each operation can be easily implemented as a single instruction: ld.acq and st.rel . Just as before, r1 == 1 indicates a synchronizes-with relationship, serving as confirmation that r2 == 42 . This is actually the preferred way to express acquire and release semantics in C++11. In fact, the atomic_thread_fence() function used in the previous example was added relatively late in the creation of the standard. Acquire and Release While Locking NOTE: \u8fd9\u6bb5\u603b\u7ed3\u4e86: 1\u3001Acquire and Release \u547d\u540d\u7684\u6765\u6e90\u548c mutex \u4e4b\u95f4\u7684\u5173\u8054 2\u3001mutex \u7684 \u5b9e\u73b0 As you can see, none of the examples in this post took advantage of the #LoadStore barriers provided by acquire and release semantics. Really, only the #LoadLoad and #StoreStore parts were necessary. That\u2019s just because in this post, I chose a simple example to let us focus on API and syntax. One case in which the #LoadStore part becomes essential is when using acquire and release semantics to implement a (mutex) lock. In fact, this is where the names come from: acquiring a lock implies acquire semantics, while releasing a lock implies release semantics! All the memory operations in between are contained inside a nice little barrier sandwich, preventing any undesireable memory reordering across the boundaries. Here, acquire and release semantics ensure that all modifications made while holding the lock will propagate fully to the next thread that obtains the lock. Every implementation of a lock, even one you roll on your own , should provide these guarantees. Again, it\u2019s all about passing information reliably between threads, especially in a multicore or multiprocessor environment. In a followup post, I\u2019ll show a working demonstration of C++11 code, running on real hardware, which can be plainly observed to break if acquire and release semantics are not used.","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/06-Acquire-and-Release-Semantics/#preshing#acquire#and#release#semantics","text":"Generally speaking, in lock-free programming , there are two ways in which threads can manipulate shared memory: 1\u3001They can compete with each other for a resource, or 2\u3001they can pass information co-operatively from one thread to another. Acquire and release semantics are crucial(\u81f3\u5173\u91cd\u8981\u7684) for the latter: reliable passing of information between threads. NOTE: 1\u3001\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u610f\u601d\u662f: \u4f7f\u7528**Acquire and release semantics** \u5bf9\u4e8e\u5b9e\u73b0thread\u4e4b\u95f4pass information co-operatively to access shared memory\u81f3\u5173\u91cd\u8981\uff0c\u4e5f\u5c31\u662f: \u6211\u4eec\u4f7f\u7528**Acquire and release semantics** \u6765\u5b9e\u73b0lock free In fact, I would venture(\u5192\u8fdb\u7684) to guess that incorrect or missing acquire and release semantics is the #1 type of lock-free programming error. In this post, I\u2019ll demonstrate various ways to achieve acquire and release semantics in C++. I\u2019ll touch upon the C++11 atomic library standard in an introductory way, so you don\u2019t already need to know it. And to be clear from the start, the information here pertains(\u9002\u5408\u3001\u5c5e\u4e8e) to lock-free programming without sequential consistency . We\u2019re dealing directly with memory ordering in a multicore or multiprocessor environment. Unfortunately, the terms acquire and release semantics appear to be in even worse shape than the term lock-free , in that the more you scour(\u641c\u7d22) the web, the more seemingly contradictory(\u77db\u76fe\u7684) definitions you\u2019ll find. Bruce Dawson offers a couple of good definitions (credited to Herb Sutter) about halfway through this white paper . I\u2019d like to offer a couple of definitions of my own, staying close to the principles behind C++11 atomics:","title":"preshing Acquire and Release Semantics"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/06-Acquire-and-Release-Semantics/#definition","text":"NOTE: \u539f\u6587\u5e76\u6ca1\u6709\u672c\u8282\u7684\u6807\u9898\u3002 1\u3001\u672c\u8282\u63cf\u8ff0Acquire semantics\u3001Release semantics\u7684\u542b\u4e49 2\u3001Acquire semantics\u3001Release semantics\u662f\u76ee\u6807\u3001\u8981\u6c42\uff0c\u540e\u9762Implementation\u7ae0\u8282\u63cf\u8ff0\u5982\u4f55\u6765\u5b9e\u73b0\u5b83\u4eec","title":"Definition"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/06-Acquire-and-Release-Semantics/#acquire#semantics","text":"Acquire semantics is a property that can only apply to operations that read from shared memory, whether they are read-modify-write operations or plain loads. The operation is then considered a read-acquire . Acquire semantics prevent memory reordering of the read-acquire with any read or write operation that follows it in program order. NOTE: \u5c06 Acquire semantics \u5e94\u7528\u4e8e\u4e00\u4e2aread operation\uff0c\u5219\u5c31\u662f: read-acquire \uff0c**read-acquire**\u5c31\u76f8\u5f53\u4e8e\u4e00\u6761instruction\uff1b**read-acquire**\u5bf9memory reordering\u6709\u5982\u4e0b\u8981\u6c42: \" Acquire semantics prevent memory reordering of the read-acquire with any read or write operation that follows it in program order.\" \u4e0b\u9762\u7684write-release\u4f9d\u6b21\u7c7b\u63a8","title":"Acquire semantics"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/06-Acquire-and-Release-Semantics/#release#semantics","text":"Release semantics is a property that can only apply to operations that write to shared memory, whether they are read-modify-write operations or plain stores. The operation is then considered a write-release . Release semantics prevent memory reordering of the write-release with any read or write operation that precedes it in program order.","title":"Release semantics"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/06-Acquire-and-Release-Semantics/#implementation","text":"NOTE: \u672c\u8282\u63cf\u8ff0\u5982\u4f55\u6765\u5b9e\u73b0Acquire semantics\u3001Release semantics Once you digest the above definitions, it\u2019s not hard to see that acquire and release semantics can be achieved using simple combinations of the memory barrier types I described at length in my previous post . The barriers must (somehow) be placed after the read-acquire operation , but before the write-release . [Update: Please note that these barriers are technically more strict than what\u2019s required for acquire and release semantics on a single memory operation, but they do achieve the desired effect.] NOTE: \u4e0b\u9762\u7684\"With Explicit Platform-Specific Fence Instructions\"\u7ae0\u8282\u7684\u4f8b\u5b50\u975e\u5e38\u5f62\u8c61\u5730\u8bf4\u660e\u4e86\u4e0a\u9762\u8fd9\u4e00\u6bb5\u7684\u5185\u5bb9\uff1b NOTE: \u4e0a\u9762\u8fd9\u5f20\u56fe\u8981\u5982\u4f55\u6765\u7406\u89e3\uff1f acquire semantic: \u4f7f\u7528 loadload\u3001loadstore\uff0c\u56e0\u4e3a: \" Acquire semantics prevent memory reordering of the read-acquire with any read or write operation that follows it in program order.\" release semantic: \u4f7f\u7528loadstore\u3001storestore\uff0c\u56e0\u4e3a: \" Release semantics prevent memory reordering of the write-release with any read or write operation that precedes it in program order.\" What\u2019s cool is that neither acquire nor release semantics requires the use of a #StoreLoad barrier, which is often a more expensive memory barrier type. For example, on PowerPC, the lwsync (short for \u201clightweight sync\u201d) instruction acts as all three #LoadLoad , #LoadStore and #StoreStore barriers at the same time, yet is less expensive than the sync instruction, which includes a #StoreLoad barrier. NOTE: \u4e0a\u9762\u7ed9\u51fa\u7684powerpc\u7684\u5b9e\u73b0\u662f\u975e\u5e38\u6613\u61c2\u7684\uff0c\u540e\u9762\u5c31\u662f\u4ee5\u5b83\u4e3a\u4f8b","title":"Implementation"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/06-Acquire-and-Release-Semantics/#with#explicit#platform-specific#fence#instructions","text":"One way to obtain the desired memory barriers is by issuing explicit fence instructions. Let\u2019s start with a simple example. Suppose we\u2019re coding for PowerPC, and __lwsync() is a compiler intrinsic(\u56fa\u6709\u7684) function that emits the lwsync instruction. Since lwsync provides so many barrier types, we can use it in the following code to establish either acquire or release semantics as needed. In Thread 1, the store to Ready turns into a write-release , and in Thread 2, the load from Ready becomes a read-acquire . If we let both threads run and find that r1 == 1 , that serves as confirmation that the value of A assigned in Thread 1 was passed successfully to Thread 2. As such, we are guaranteed that r2 == 42 . In my previous post, I already gave a lengthy analogy for #LoadLoad and #StoreStore to illustrate how this works, so I won\u2019t rehash that explanation here.","title":"With Explicit Platform-Specific Fence Instructions"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/06-Acquire-and-Release-Semantics/#synchronized-with","text":"In formal terms, we say that the store to Ready synchronized-with the load. I\u2019ve written a separate post about synchronizes-with here . For now, suffice to say that for this technique to work in general, the acquire and release semantics must apply to the same variable \u2013 in this case, Ready \u2013 and both the load and store must be atomic operations. Here, Ready is a simple aligned int , so the operations are already atomic on PowerPC. NOTE: \u8fd9\u6bb5\u89e3\u91ca\u975e\u5e38\u597d\uff0c\u7ed3\u5408\u524d\u9762\u7684\u5185\u5bb9\uff0c\u6211\u4eec\u53ef\u4ee5\u603b\u7ed3\u5b9e\u73b0: 1\u3001 the acquire and release semantics must apply to the same variable \u2013 flag variable 2\u3001The barriers must (somehow) be placed after the read-acquire operation , but before the write-release . \u6211\u4eec\u5c06\u8fd9\u79cd\u5b9e\u73b0\u65b9\u5f0f\u7b80\u8bb0\u4e3a: write-release-flag-notify-read-acquire-model\uff0c\u663e\u7136\u5b83\u80fd\u591f\u5b9e\u73b0 synchronizes-with \uff0c\u80fd\u591f\u5b9e\u73b0: \u5199\u6210\u529f\u4e4b\u540e\u624d\u53bb\u8bfb \uff0c\u7ed3\u5408\u4e0b\u9762\u7684\u56fe\u6765\u7406\u89e3**\u5199\u6210\u529f\u4e4b\u540e\u624d\u53bb\u8bfb**: thread1 \u5148write\uff0cthread2\u7136\u540eread\uff1b","title":"synchronized-with"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/06-Acquire-and-Release-Semantics/#with#fences#in#portable#c11","text":"The above example is compiler- and processor-specific. One approach for supporting multiple platforms is to convert the code to C++11. All C++11 identifiers exist in the std namespace, so to keep the following examples brief, let\u2019s assume the statement using namespace std; was placed somewhere earlier in the code.","title":"With Fences in Portable C++11"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/06-Acquire-and-Release-Semantics/#atomic_thread_fence","text":"C++11\u2019s atomic library standard defines a portable function atomic_thread_fence() that takes a single argument to specify the type of fence. There are several possible values for this argument, but the values we\u2019re most interested in here are memory_order_acquire and memory_order_release . We\u2019ll use this function in place of __lwsync() . There\u2019s one more change to make before this example is complete. On PowerPC, we knew that both operations on Ready were atomic, but we can\u2019t make that assumption about every platform. To ensure atomicity on all platforms, we\u2019ll change the type of Ready from int to atomic<int> . I know, it\u2019s kind of a silly change, considering that aligned loads and stores of int are already atomic on every modern CPU that exists today. I\u2019ll write more about this in the post on synchronizes-with , but for now, let\u2019s do it for the warm fuzzy feeling of 100% correctness in theory. No changes to A are necessary. The memory_order_relaxed arguments above mean \u201censure these operations are atomic, but don\u2019t impose any ordering constraints/memory barriers that aren\u2019t already there.\u201d","title":"atomic_thread_fence"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/06-Acquire-and-Release-Semantics/#implementation_1","text":"Once again, both of the above atomic_thread_fence() calls can be (and hopefully are) implemented as lwsync on PowerPC. Similarly, they could both emit a dmb instruction on ARM, which I believe is at least as effective as PowerPC\u2019s lwsync . On x86/64, both atomic_thread_fence() calls can simply be implemented as compiler barriers , since usually , every load on x86/64 already implies acquire semantics and every store implies release semantics . This is why x86/64 is often said to be strongly ordered . NOTE:\u8fd9\u6bb5\u603b\u7ed3\u975e\u5e38\u597d","title":"Implementation"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/06-Acquire-and-Release-Semantics/#without#fences#in#portable#c11","text":"In C++11, it\u2019s possible to achieve acquire and release semantics on Ready without issuing explicit fence instructions. You just need to specify memory ordering constraints directly on the operations on Ready : Think of it as rolling each fence instruction into the operations on Ready themselves. [Update: Please note that this form is not exactly the same as the version using standalone fences; technically, it\u2019s less strict.] The compiler will emit any instructions necessary to obtain the required barrier effects. In particular, on Itanium, each operation can be easily implemented as a single instruction: ld.acq and st.rel . Just as before, r1 == 1 indicates a synchronizes-with relationship, serving as confirmation that r2 == 42 . This is actually the preferred way to express acquire and release semantics in C++11. In fact, the atomic_thread_fence() function used in the previous example was added relatively late in the creation of the standard.","title":"Without Fences in Portable C++11"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/06-Acquire-and-Release-Semantics/#acquire#and#release#while#locking","text":"NOTE: \u8fd9\u6bb5\u603b\u7ed3\u4e86: 1\u3001Acquire and Release \u547d\u540d\u7684\u6765\u6e90\u548c mutex \u4e4b\u95f4\u7684\u5173\u8054 2\u3001mutex \u7684 \u5b9e\u73b0 As you can see, none of the examples in this post took advantage of the #LoadStore barriers provided by acquire and release semantics. Really, only the #LoadLoad and #StoreStore parts were necessary. That\u2019s just because in this post, I chose a simple example to let us focus on API and syntax. One case in which the #LoadStore part becomes essential is when using acquire and release semantics to implement a (mutex) lock. In fact, this is where the names come from: acquiring a lock implies acquire semantics, while releasing a lock implies release semantics! All the memory operations in between are contained inside a nice little barrier sandwich, preventing any undesireable memory reordering across the boundaries. Here, acquire and release semantics ensure that all modifications made while holding the lock will propagate fully to the next thread that obtains the lock. Every implementation of a lock, even one you roll on your own , should provide these guarantees. Again, it\u2019s all about passing information reliably between threads, especially in a multicore or multiprocessor environment. In a followup post, I\u2019ll show a working demonstration of C++11 code, running on real hardware, which can be plainly observed to break if acquire and release semantics are not used.","title":"Acquire and Release While Locking"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/07-Weak-vs-Strong-Memory-Models/","text":"preshing Weak vs. Strong Memory Models There are many types of memory reordering, and not all types of reordering occur equally often. It all depends on processor you\u2019re targeting and/or the toolchain you\u2019re using for development. NOTE: \u4f5c\u4e3aprogrammer\uff0c\u6211\u4eec\u671f\u671b\u6211\u4eec\u7684program\u6700\u7ec8\u662fcross-plateform\u7684\uff0c\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cdcross-plateform technique\uff0cC++ 11 atomic\u6b63\u662f\u8fd9\u6837\u7684\u4e00\u79cd Memory model\u7684\u542b\u4e49 A memory model tells you, for a given processor or toolchain, exactly what types of memory reordering to expect at runtime relative to a given source code listing. Keep in mind that the effects of memory reordering can only be observed when lock-free programming techniques are used. Classification NOTE: \u5206\u7c7b\u65b9\u6cd5\u662f\u5178\u578b\u7684level\uff0c\u53c2\u89c1\u6587\u7ae0Level-in-computer-science After studying memory models for a while \u2013 mostly by reading various online sources and verifying through experimentation \u2013 I\u2019ve gone ahead and organized them into the following four categories. Below, each memory model makes all the guarantees of the ones to the left, plus some additional ones. I\u2019ve drawn a clear line between weak memory models and strong ones, to capture the way most people appear to use these terms. Read on for my justification(\u7406\u7531) for doing so. Hardware memory model Each physical device pictured above represents a hardware memory model. A hardware memory model tells you what kind of memory ordering to expect at runtime relative to an assembly (or machine) code listing. Every processor family has different habits when it comes to memory reordering, and those habits can only be observed in multicore or multiprocessor configurations. Given that multicore is now mainstream , it\u2019s worth having some familiarity with them. NOTE: 1\u3001 multicore is now mainstream \uff0c\u8ba9\u6211\u60f3\u5230\u4e86 tendency: parallel computing Software memory models There are software memory models as well. Technically, once you\u2019ve written (and debugged) portable lock-free code in C11, C++11 or Java, only the software memory model is supposed to matter. Nonetheless, a general understanding of hardware memory models may come in handy. It can help you explain unexpected behavior while debugging, and \u2014 perhaps just as importantly \u2014 appreciate how incorrect code may function correctly on a specific processor and toolchain out of luck. Weak Memory Models In the weakest memory model, it\u2019s possible to experience all four types of memory reordering I described using a source control analogy in a previous post. Any load or store operation can effectively be reordered with any other load or store operation, as long as it would never modify the behavior of a single, isolated thread. In reality, the reordering may be due to either compiler reordering of instructions, or memory reordering on the processor itself. When a processor has a weak hardware memory model, we tend to say it\u2019s weakly-ordered or that it has weak ordering . We may also say it has a relaxed memory model. The venerable DEC Alpha is everybody\u2019s favorite example of a weakly-ordered processor. There\u2019s really no mainstream processor with weaker ordering. memory_order_relaxed The C11 and C++11 programming languages expose a weak software memory model which was in many ways influenced by the Alpha. When using low-level atomic operations in these languages, it doesn\u2019t matter if you\u2019re actually targeting a strong processor family such as x86/64. As I demonstrated previously, you must still specify the correct memory ordering constraints , if only to prevent compiler reordering. Weak With Data Dependency Ordering Though the Alpha has become less relevant with time, we still have several modern CPU families which carry on in the same tradition of weak hardware ordering: 1\u3001 ARM , which is currently found in hundreds of millions of smartphones and tablets, and is increasingly popular in multicore configurations. 2\u3001 PowerPC , which the Xbox 360 in particular has already delivered to 70 million living rooms in a multicore configuration. 3\u3001 Itanium , which Microsoft no longer supports in Windows, but which is still supported in Linux and found in HP servers. These families have memory models which are, in various ways, almost as weak as the Alpha\u2019s, except for one common detail of particular interest to programmers: they maintain data dependency ordering . What does that mean? It means that if you write A->B in C/C++, you are always guaranteed to load a value of B which is at least as new as the value of A . The Alpha doesn\u2019t guarantee that. I won\u2019t dwell(\u7ec6\u60f3\u67d0\u4e8b) on data dependency ordering too much here, except to mention that the Linux RCU mechanism relies on it heavily. Strong Memory Models Let\u2019s look at hardware memory models first. What, exactly, is the difference between a strong one and a weak one? There is actually a little disagreement over this question, but my feeling is that in 80% of the cases, most people mean the same thing. Therefore, I\u2019d like to propose the following definition: A strong hardware memory model is one in which every machine instruction comes implicitly with acquire and release semantics . As a result, when one CPU core performs a sequence of writes, every other CPU core sees those values change in the same order that they were written. It\u2019s not too hard to visualize. Just imagine a refinement(\u6539\u826f) of the source control analogy where all modifications are committed to shared memory in-order (no StoreStore reordering ), pulled from shared memory in-order (no LoadLoad reordering ), and instructions are always executed in-order (no LoadStore reordering). StoreLoad reordering, however, still remains possible . NOTE: \u8fd9\u6bb5\u5bf9strong memory model\u7684\u4ecb\u7ecd\u662f\u975e\u5e38\u597d\u7684 Under the above definition, the x86/64 family of processors is usually strongly-ordered. There are certain cases in which some of x86/64\u2019s strong ordering guarantees are lost , but for the most part, as application programmers, we can ignore those cases. It\u2019s true that a x86/64 processor can execute instructions out-of-order , but that\u2019s a hardware implementation detail \u2013 what matters is that it still keeps its memory interactions in-order, so in a multicore environment, we can still consider it strongly-ordered. Historically, there has also been a little confusion due to evolving specs . Apparently SPARC processors, when running in TSO mode, are another example of a strong hardware ordering. TSO stands for \u201ctotal store order\u201d, which in a subtle way, is different from the definition I gave above. It means that there is always a single, global order of writes to shared memory from all cores. The x86/64 has this property too: See Volume 3, \u00a78.2.3.6-8 of Intel\u2019s x86/64 Architecture Specification for some examples. From what I can tell, the TSO property isn\u2019t usually of direct interest to low-level lock-free programmers, but it is a step towards sequential consistency. Sequential Consistency In a sequentially consistent memory model, there is no memory reordering. It\u2019s as if the entire program execution is reduced to a sequential interleaving of instructions from each thread. In particular, the result r1 = r2 = 0 from Memory Reordering Caught in the Act becomes impossible. These days, you won\u2019t easily find a modern multicore device which guarantees sequential consistency at the hardware level. However, it seems at least one sequentially consistent, dual-processor machine existed back in 1989: The 386-based Compaq SystemPro . According to Intel\u2019s docs, the 386 wasn\u2019t advanced enough to perform any memory reordering at runtime. In any case, sequential consistency only really becomes interesting as a software memory model, when working in higher-level programming languages. In Java 5 and higher, you can declare shared variables as volatile . In C++11, you can use the default ordering constraint, memory_order_seq_cst , when performing operations on atomic library types. If you do those things, the toolchain will restrict compiler reordering and emit CPU-specific instructions which act as the appropriate memory barrier types. In this way, a sequentially consistent memory model can be \u201cemulated\u201d even on weakly-ordered multicore devices. If you read Herlihy & Shavit\u2019s The Art of Multiprocessor Programming , be aware that most of their examples assume a sequentially consistent software memory model. Further Details There are many other subtle details filling out the spectrum of memory models, but in my experience, they haven\u2019t proved quite as interesting when writing lock-free code at the application level. There are things like control dependencies, causal consistency(\u56e0\u679c\u4e00\u81f4\u6027), and different memory types. Still, most discussions come back the four main categories I\u2019ve outlined here. If you really want to nitpick the fine details of processor memory models, and you enjoy eating formal logic for breakfast, you can check out the admirably detailed work done at the University of Cambridge. Paul McKenney has written an accessible overview of some of their work and its associated tools.","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/07-Weak-vs-Strong-Memory-Models/#preshing#weak#vs#strong#memory#models","text":"There are many types of memory reordering, and not all types of reordering occur equally often. It all depends on processor you\u2019re targeting and/or the toolchain you\u2019re using for development. NOTE: \u4f5c\u4e3aprogrammer\uff0c\u6211\u4eec\u671f\u671b\u6211\u4eec\u7684program\u6700\u7ec8\u662fcross-plateform\u7684\uff0c\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cdcross-plateform technique\uff0cC++ 11 atomic\u6b63\u662f\u8fd9\u6837\u7684\u4e00\u79cd","title":"preshing Weak vs. Strong Memory Models"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/07-Weak-vs-Strong-Memory-Models/#memory#model","text":"A memory model tells you, for a given processor or toolchain, exactly what types of memory reordering to expect at runtime relative to a given source code listing. Keep in mind that the effects of memory reordering can only be observed when lock-free programming techniques are used.","title":"Memory model\u7684\u542b\u4e49"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/07-Weak-vs-Strong-Memory-Models/#classification","text":"NOTE: \u5206\u7c7b\u65b9\u6cd5\u662f\u5178\u578b\u7684level\uff0c\u53c2\u89c1\u6587\u7ae0Level-in-computer-science After studying memory models for a while \u2013 mostly by reading various online sources and verifying through experimentation \u2013 I\u2019ve gone ahead and organized them into the following four categories. Below, each memory model makes all the guarantees of the ones to the left, plus some additional ones. I\u2019ve drawn a clear line between weak memory models and strong ones, to capture the way most people appear to use these terms. Read on for my justification(\u7406\u7531) for doing so.","title":"Classification"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/07-Weak-vs-Strong-Memory-Models/#hardware#memory#model","text":"Each physical device pictured above represents a hardware memory model. A hardware memory model tells you what kind of memory ordering to expect at runtime relative to an assembly (or machine) code listing. Every processor family has different habits when it comes to memory reordering, and those habits can only be observed in multicore or multiprocessor configurations. Given that multicore is now mainstream , it\u2019s worth having some familiarity with them. NOTE: 1\u3001 multicore is now mainstream \uff0c\u8ba9\u6211\u60f3\u5230\u4e86 tendency: parallel computing","title":"Hardware memory model"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/07-Weak-vs-Strong-Memory-Models/#software#memory#models","text":"There are software memory models as well. Technically, once you\u2019ve written (and debugged) portable lock-free code in C11, C++11 or Java, only the software memory model is supposed to matter. Nonetheless, a general understanding of hardware memory models may come in handy. It can help you explain unexpected behavior while debugging, and \u2014 perhaps just as importantly \u2014 appreciate how incorrect code may function correctly on a specific processor and toolchain out of luck.","title":"Software memory models"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/07-Weak-vs-Strong-Memory-Models/#weak#memory#models","text":"In the weakest memory model, it\u2019s possible to experience all four types of memory reordering I described using a source control analogy in a previous post. Any load or store operation can effectively be reordered with any other load or store operation, as long as it would never modify the behavior of a single, isolated thread. In reality, the reordering may be due to either compiler reordering of instructions, or memory reordering on the processor itself. When a processor has a weak hardware memory model, we tend to say it\u2019s weakly-ordered or that it has weak ordering . We may also say it has a relaxed memory model. The venerable DEC Alpha is everybody\u2019s favorite example of a weakly-ordered processor. There\u2019s really no mainstream processor with weaker ordering.","title":"Weak Memory Models"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/07-Weak-vs-Strong-Memory-Models/#memory_order_relaxed","text":"The C11 and C++11 programming languages expose a weak software memory model which was in many ways influenced by the Alpha. When using low-level atomic operations in these languages, it doesn\u2019t matter if you\u2019re actually targeting a strong processor family such as x86/64. As I demonstrated previously, you must still specify the correct memory ordering constraints , if only to prevent compiler reordering.","title":"memory_order_relaxed"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/07-Weak-vs-Strong-Memory-Models/#weak#with#data#dependency#ordering","text":"Though the Alpha has become less relevant with time, we still have several modern CPU families which carry on in the same tradition of weak hardware ordering: 1\u3001 ARM , which is currently found in hundreds of millions of smartphones and tablets, and is increasingly popular in multicore configurations. 2\u3001 PowerPC , which the Xbox 360 in particular has already delivered to 70 million living rooms in a multicore configuration. 3\u3001 Itanium , which Microsoft no longer supports in Windows, but which is still supported in Linux and found in HP servers. These families have memory models which are, in various ways, almost as weak as the Alpha\u2019s, except for one common detail of particular interest to programmers: they maintain data dependency ordering . What does that mean? It means that if you write A->B in C/C++, you are always guaranteed to load a value of B which is at least as new as the value of A . The Alpha doesn\u2019t guarantee that. I won\u2019t dwell(\u7ec6\u60f3\u67d0\u4e8b) on data dependency ordering too much here, except to mention that the Linux RCU mechanism relies on it heavily.","title":"Weak With Data Dependency Ordering"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/07-Weak-vs-Strong-Memory-Models/#strong#memory#models","text":"Let\u2019s look at hardware memory models first. What, exactly, is the difference between a strong one and a weak one? There is actually a little disagreement over this question, but my feeling is that in 80% of the cases, most people mean the same thing. Therefore, I\u2019d like to propose the following definition: A strong hardware memory model is one in which every machine instruction comes implicitly with acquire and release semantics . As a result, when one CPU core performs a sequence of writes, every other CPU core sees those values change in the same order that they were written. It\u2019s not too hard to visualize. Just imagine a refinement(\u6539\u826f) of the source control analogy where all modifications are committed to shared memory in-order (no StoreStore reordering ), pulled from shared memory in-order (no LoadLoad reordering ), and instructions are always executed in-order (no LoadStore reordering). StoreLoad reordering, however, still remains possible . NOTE: \u8fd9\u6bb5\u5bf9strong memory model\u7684\u4ecb\u7ecd\u662f\u975e\u5e38\u597d\u7684 Under the above definition, the x86/64 family of processors is usually strongly-ordered. There are certain cases in which some of x86/64\u2019s strong ordering guarantees are lost , but for the most part, as application programmers, we can ignore those cases. It\u2019s true that a x86/64 processor can execute instructions out-of-order , but that\u2019s a hardware implementation detail \u2013 what matters is that it still keeps its memory interactions in-order, so in a multicore environment, we can still consider it strongly-ordered. Historically, there has also been a little confusion due to evolving specs . Apparently SPARC processors, when running in TSO mode, are another example of a strong hardware ordering. TSO stands for \u201ctotal store order\u201d, which in a subtle way, is different from the definition I gave above. It means that there is always a single, global order of writes to shared memory from all cores. The x86/64 has this property too: See Volume 3, \u00a78.2.3.6-8 of Intel\u2019s x86/64 Architecture Specification for some examples. From what I can tell, the TSO property isn\u2019t usually of direct interest to low-level lock-free programmers, but it is a step towards sequential consistency.","title":"Strong Memory Models"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/07-Weak-vs-Strong-Memory-Models/#sequential#consistency","text":"In a sequentially consistent memory model, there is no memory reordering. It\u2019s as if the entire program execution is reduced to a sequential interleaving of instructions from each thread. In particular, the result r1 = r2 = 0 from Memory Reordering Caught in the Act becomes impossible. These days, you won\u2019t easily find a modern multicore device which guarantees sequential consistency at the hardware level. However, it seems at least one sequentially consistent, dual-processor machine existed back in 1989: The 386-based Compaq SystemPro . According to Intel\u2019s docs, the 386 wasn\u2019t advanced enough to perform any memory reordering at runtime. In any case, sequential consistency only really becomes interesting as a software memory model, when working in higher-level programming languages. In Java 5 and higher, you can declare shared variables as volatile . In C++11, you can use the default ordering constraint, memory_order_seq_cst , when performing operations on atomic library types. If you do those things, the toolchain will restrict compiler reordering and emit CPU-specific instructions which act as the appropriate memory barrier types. In this way, a sequentially consistent memory model can be \u201cemulated\u201d even on weakly-ordered multicore devices. If you read Herlihy & Shavit\u2019s The Art of Multiprocessor Programming , be aware that most of their examples assume a sequentially consistent software memory model.","title":"Sequential Consistency"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/07-Weak-vs-Strong-Memory-Models/#further#details","text":"There are many other subtle details filling out the spectrum of memory models, but in my experience, they haven\u2019t proved quite as interesting when writing lock-free code at the application level. There are things like control dependencies, causal consistency(\u56e0\u679c\u4e00\u81f4\u6027), and different memory types. Still, most discussions come back the four main categories I\u2019ve outlined here. If you really want to nitpick the fine details of processor memory models, and you enjoy eating formal logic for breakfast, you can check out the admirably detailed work done at the University of Cambridge. Paul McKenney has written an accessible overview of some of their work and its associated tools.","title":"Further Details"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/08-This-Is-Why-They-Call-It-a-Weakly-Ordered-CPU/","text":"preshing This Is Why They Call It a Weakly-Ordered CPU","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/08-This-Is-Why-They-Call-It-a-Weakly-Ordered-CPU/#preshing#this#is#why#they#call#it#a#weakly-ordered#cpu","text":"","title":"preshing This Is Why They Call It a Weakly-Ordered CPU"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/09-Atomic-vs-Non-Atomic-Operations/","text":"preshing Atomic vs. Non-Atomic Operations NOTE: \u8fd9\u7bc7\u6587\u7ae0\u5199\u5f97\u975e\u5e38\u597d Much has already been written about atomic operations on the web, usually with a focus on atomic read-modify-write (RMW) operations. However, those aren\u2019t the only kinds of atomic operations . There are also atomic loads and stores , which are equally important. In this post, I\u2019ll compare atomic loads and stores to their non-atomic counterparts at both the processor level and the C/C++ language level . Along the way, we\u2019ll clarify the C++11 concept of a \u201cdata race\u201d. An operation acting on shared memory is atomic if it completes in a single step relative to other threads(\u4ece\u7ebf\u7a0b\u7684\u89d2\u5ea6\u6765\u770b\u5f85atomic). When an atomic store is performed on a shared variable , no other thread can observe the modification half-complete. When an atomic load is performed on a shared variable , it reads the entire value as it appeared at a single moment in time. Non-atomic loads and stores do not make those guarantees. Without those guarantees, lock-free programming would be impossible, since you could never let different threads manipulate a shared variable at the same time. We can formulate it as a rule: Any time two threads operate on a shared variable concurrently, and one of those operations performs a write, both threads must use atomic operations. NOTE: \u4e0a\u9762\u8fd9\u4e9b\u90fd\u662f\u53ef\u4ee5\u4f7f\u7528multiple-model\u6765\u8fdb\u884c\u5206\u6790\u7684\uff0c\u5c24\u5176\u662f\u4eceread-write\u7684\u89d2\u5ea6\u6765\u8fdb\u884c\u5206\u6790\u7684 If you violate this rule, and either thread uses a non-atomic operation, you\u2019ll have what the C++11 standard refers to as a data race (not to be confused with Java\u2019s concept of a data race, which is different, or the more general race condition ). The C++11 standard doesn\u2019t tell you why data races are bad; only that if you have one, \u201cundefined behavior\u201d will result ( \u00a71.10.21 ). The real reason why such data races are bad is actually quite simple: They result in torn reads and torn writes . NOTE: \"torn\"\u7684\u610f\u601d\u662f\"\u6495\u88c2\u7684\" A memory operation can be non-atomic because it uses multiple CPU instructions , non-atomic even when using a single CPU instruction, or non-atomic because you\u2019re writing portable code and you simply can\u2019t make the assumption. Let\u2019s look at a few examples. Non-Atomic Due to Multiple CPU Instructions Suppose you have a 64-bit global variable, initially zero. uint64_t sharedValue = 0 ; At some point, you assign a 64-bit value to this variable. void storeValue () { sharedValue = 0x100000002 ; //16\u8fdb\u5236 } When you compile this function for 32-bit x86 using GCC, it generates the following machine code. NOTE: \u9700\u8981\u6ce8\u610f\u7684\u662f: uint64_t \u662f64-bit\uff0c\u5b9e\u9a8c\u73af\u5883\u662f: 32-bit $ gcc -O2 -S -masm=intel test.c $ cat test.s ... mov DWORD PTR sharedValue, 2 mov DWORD PTR sharedValue+4, 1 ret ... As you can see, the compiler implemented the 64-bit assignment using two separate machine instructions . The first instruction sets the lower 32 bits to 0x00000002 , and the second sets the upper 32 bits to 0x00000001 . Clearly, this assignment operation is not atomic. If sharedValue is accessed concurrently by different threads, several things can now go wrong: 1\u3001If a thread calling storeValue is preempted between the two machine instructions, it will leave the value of 0x0000000000000002 in memory \u2013 a torn write . At this point, if another thread reads sharedValue , it will receive this completely bogus(\u4f2a\u9020\u7684) value which nobody intended to store. NOTE: \u5982\u679c\u8c03\u7528 storeValue \u7684\u7ebf\u7a0b\u5728\u4e24\u4e2a\u673a\u5668\u6307\u4ee4\u4e4b\u95f4\u88ab\u62a2\u5360\uff0c\u5b83\u5c06\u5728\u5185\u5b58\u4e2d\u4fdd\u7559 0x0000000000000002 \u7684\u503c - \u4e00\u4e2a\u6495\u88c2\u7684\u5199\u5165\u3002 \u6b64\u65f6\uff0c\u5982\u679c\u53e6\u4e00\u4e2a\u7ebf\u7a0b\u8bfb\u53d6 sharedValue \uff0c\u5b83\u5c06\u6536\u5230\u8fd9\u4e2a\u5b8c\u5168\u865a\u5047\u7684\u503c\uff0c\u6ca1\u6709\u4eba\u6253\u7b97\u5b58\u50a8\u3002 2\u3001Even worse, if a thread is preempted between the two instructions, and another thread modifies sharedValue before the first thread resumes, it will result in a permanently torn write : the upper 32 bits from one thread, the lower 32 bits from another. NOTE: \u66f4\u7cdf\u7cd5\u7684\u662f\uff0c\u5982\u679c\u4e00\u4e2a\u7ebf\u7a0b\u5728\u4e24\u4e2a\u6307\u4ee4\u4e4b\u95f4\u88ab\u62a2\u5360\uff0c\u800c\u53e6\u4e00\u4e2a\u7ebf\u7a0b\u5728\u7b2c\u4e00\u4e2a\u7ebf\u7a0b\u6062\u590d\u4e4b\u524d\u4fee\u6539\u4e86 sharedValue \uff0c\u5219\u4f1a\u5bfc\u81f4\u6c38\u4e45\u6027\u7684\u5199\u5165\uff1a\u4e00\u4e2a\u7ebf\u7a0b\u7684\u9ad832\u4f4d\uff0c\u53e6\u4e00\u4e2a\u4f4e32\u4f4d\uff08\u7b2c\u4e00\u4e2a\u7ebf\u7a0b\u5148\u5199\u5165\u7684\u4e00\u534a\u5185\u5bb9\u4f1a\u88ab\u7b2c\u4e8c\u4e2a\u7ebf\u7a0b\u8986\u76d6\uff0c\u7136\u540e\u7b2c\u4e00\u4e2a\u7ebf\u7a0bresume\u540e\uff0c\u5b83\u4f1a\u5199\u5165\u540e\u534a\u90e8\u5206\uff0c\u8fd9\u5c31\u8986\u76d6\u4e86\u7b2c\u4e8c\u4e2a\u7ebf\u7a0b\u4e4b\u524d\u5199\u5165\u7684\u5185\u5bb9\uff0c\u663e\u7136\u5b83\u4eec\u76f8\u4e92\u8986\u76d6\uff0c\u5bfc\u81f4\u4e86\u6700\u7ec8\u5404\u81ea\u90fd\u53ea\u5199\u5165\u4e86\u4e00\u534a\uff09 3\u3001On multicore devices, it isn\u2019t even necessary to preempt one of the threads to have a torn write. When a thread calls storeValue , any thread executing on a different core could read sharedValue at a moment when only half the change is visible. NOTE: \u663e\u7136\uff0cmulticore\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u80fd\u6bcf\u4e2athread\u4f1a\u5360\u7528\u4e00\u4e2acore\uff0c\u4f46\u662f\u5b83\u4eec\u5171\u4eabprocess\u7684memory\uff0c\u5b83\u4eec\u80fd\u591f\u540c\u65f6access process\u7684memory\u4e2d\u7684\u540c\u4e00\u4e2a\u5730\u5740\uff1b\u5982\u679c\u5b83\u4eec\u5bf9shared memory\u7684access\u4e0d\u6309\u7167\u4e92\u65a5\u539f\u5219\u6765\u8fdb\u884c\uff0c\u5373\u6bcf\u6b21\u5728access shared memory\u4e4b\u524d\uff0c\u5148lock\uff1b\u5219\u5c31\u4f1a\u5bfc\u81f4\u5bf9shared memory\u7684access\u662f\u65e0\u5e8f\u7684\uff1b\u663e\u7136\u4e92\u65a5\u539f\u5219\u5373\u80fd\u591f\u4fdd\u8bc1\u5728multicore\u60c5\u51b5\u4e0b\u7684\u5b89\u5168\uff0c\u4e5f\u4fdd\u8bc1\u4e86\u5728single core\u60c5\u51b5\u4e0b\uff0c\u5373\u4f7fpreempted\uff0c\u4e5f\u80fd\u591f\u5b89\u5168\uff1b Reading concurrently from sharedValue brings its own set of problems: uint64_t loadValue () { return sharedValue ; } $ gcc - O2 - S - masm = intel test . c $ cat test . s ... mov eax , DWORD PTR sharedValue mov edx , DWORD PTR sharedValue + 4 ret ... Here too, the compiler has implemented the load operation using two machine instructions : The first reads the lower 32 bits into eax , and the second reads the upper 32 bits into edx . In this case, if a concurrent store to sharedValue becomes visible between the two instructions, it will result in a torn read \u2013 even if the concurrent store was atomic. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u610f\u601d\u662f\u5728\u4e24\u4e2amachine instruction\u4e4b\u95f4\uff0c\u53e6\u5916\u4e00\u4e2a\u7ebf\u7a0bwrite to sharedValue \uff0c\u8fd9\u5c31\u5bfc\u81f4\u4e4b\u524dread\u7684\u503c\u662f\u4e00\u534a\u4e00\u534a\u7684\uff1b These problems are not just theoretical. Mintomic \u2019s test suite includes a test case called test_load_store_64_fail , in which one thread stores a bunch of 64-bit values to a single variable using a plain assignment operator, while another thread repeatedly performs a plain load from the same variable, validating each result. On a multicore x86, this test fails consistently, as expected. Non-Atomic CPU Instructions A memory operation can be non-atomic even when performed by a single CPU instruction. For example, the ARMv7 instruction set includes the strd instruction, which stores the contents of two 32-bit source registers to a single 64-bit value in memory. strd r0, r1, [r2] On some ARMv7 processors, this instruction is not atomic. When the processor sees this instruction, it actually performs two separate 32-bit stores under the hood ( \u00a7A3.5.3 ). Once again, another thread running on a separate core has the possibility of observing a torn write . Interestingly, a torn write is even possible on a single-core device: A system interrupt \u2013 say, for a scheduled thread context switch \u2013 can actually occur between the two internal 32-bit stores! In this case, when the thread resumes from the interrupt, it will restart the strd instruction all over again. As another example, it\u2019s well-known that on x86, a 32-bit mov instruction is atomic if the memory operand is naturally aligned, but non-atomic otherwise. In other words, atomicity is only guaranteed when the 32-bit integer is located at an address which is an exact multiple of 4. Mintomic comes with another test case, test_load_store_32_fail , which verifies this guarantee. As it\u2019s written, this test always succeeds on x86, but if you modify the test to force sharedInt to certain unaligned addresses, it will fail. On my Core 2 Quad Q6600, the test fails when sharedInt crosses a cache line boundary: // Force sharedInt to cross a cache line boundary: #pragma pack(2) MINT_DECL_ALIGNED ( static struct , 64 ) { char padding [ 62 ]; mint_atomic32_t sharedInt ; } g_wrapper ; Those are enough processor-specific details for now. Let\u2019s look at atomicity at the C/C++ language level. NOTE: mov is not atomic\u7684\u60c5\u51b5\u8fd8\u6ca1\u6709\u641e\u61c2\uff0c\u9700\u8981Google: mov is not atomic when address is not aligned All C/C++ Operations Are Presumed\uff08\u5047\u5b9a\uff09 Non-Atomic In C and C++ , every operation is presumed non-atomic unless otherwise specified by the compiler or hardware vendor \u2013 even plain 32-bit integer assignment. uint32_t foo = 0 ; void storeFoo () { foo = 0x80286 ; } The language standards have nothing to say about atomicity in this case. Maybe integer assignment is atomic, maybe it isn\u2019t. Since non-atomic operations don\u2019t make any guarantees, plain integer assignment in C is non-atomic by definition. In practice, we usually know more about our target platforms than that. For example, it\u2019s common knowledge that on all modern x86, x64, Itanium, SPARC, ARM and PowerPC processors, plain 32-bit integer assignment is atomic as long as the target variable is naturally aligned. You can verify it by consulting your processor manual and/or compiler documentation. In the games industry, I can tell you that a lot of 32-bit integer assignments rely on this particular guarantee. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u8ba9\u6211\u60f3\u8d77\u6765 alignment \u7684\u91cd\u8981\u4ef7\u503c Nonetheless(\u5c3d\u7ba1\u5982\u6b64), when writing truly portable C and C++, there\u2019s a long-standing tradition of pretending that we don\u2019t know anything more than what the language standards tell us. Portable C and C++ is designed to run on every possible computing device past, present and imaginary (\u8fd9\u53e5\u8bdd\u8bf4\u660e\u4e86portable\u7684\u542b\u4e49). NOTE: \u4e0a\u9762\u5728\u8fd9\u6bb5\u8bdd\uff0c\u5176\u5b9e\u603b\u7ed3\u4e86\u4e66\u5199cross-plateform/portable C and C++ program\u7684\u7cbe\u795e\u8981\u4e49: \u5bf9\u4e8e language standards \u672a\u8fdb\u884c\u6807\u51c6\u5316\u7684\uff0c\u6211\u4eec\u5e94\u8be5\u5047\u88c5\u4e0d\u77e5\u9053\uff0c\u5373\u4e0d\u505a\u4efb\u4f55\u5047\u8bbe\u3002 Personally, I like to imagine a machine where memory can only be changed by mixing it up first: On such a machine, you definitely wouldn\u2019t want to perform a concurrent read at the same time as a plain assignment; you could end up reading a completely random value. In C++11 , there is finally a way to perform truly portable atomic loads and stores: the C++11 atomic library. Atomic loads and stores performed using the C++11 atomic library would even work on the imaginary computer above \u2013 even if it means the C++11 atomic library must secretly lock a mutex to make each operation atomic. There\u2019s also the Mintomic library which I released last month , which doesn\u2019t support as many platforms, but works on several older compilers, is hand-optimized and is guaranteed to be lock-free. NOTE: C++11 atomic library\u5bf9atomic load\u3001store\u8fdb\u884c\u4e86\u6807\u51c6\u5316\u3002 Relaxed Atomic Operations Let\u2019s return to the original sharedValue example from earlier in this post. We\u2019ll rewrite it using Mintomic so that all operations are performed atomically on every platform Mintomic supports. First, we must declare sharedValue as one of Mintomic\u2019s atomic data types . #include <mintomic/mintomic.h> mint_atomic64_t sharedValue = { 0 }; The mint_atomic64_t type guarantees correct memory alignment for atomic access on each platform. This is important because, for example, the GCC 4.2 compiler for ARM bundled with Xcode 3.2.5 doesn\u2019t guarantee that plain uint64_t will be 8-byte aligned. In storeValue , instead of performing a plain, non-atomic assignment, we must call mint_store_64_relaxed . void storeValue () { mint_store_64_relaxed ( & sharedValue , 0x100000002 ); } Similarly, in loadValue , we call mint_load_64_relaxed . uint64_t loadValue () { return mint_load_64_relaxed ( & sharedValue ); } Using C++11 \u2019s terminology, these functions are now data race-free . When executing concurrently, there is absolutely no possibility of a torn read or write, whether the code runs on ARMv6/ARMv7 (Thumb or ARM mode), x86, x64 or PowerPC. If you\u2019re curious how mint_load_64_relaxed and mint_store_64_relaxed actually work, both functions expand to an inline cmpxchg8b instruction on x86; for other platforms, consult Mintomic\u2019s implementation . Here\u2019s the exact same thing written in C++11 instead: #include <atomic> std :: atomic < uint64_t > sharedValue ( 0 ); void storeValue () { sharedValue . store ( 0x100000002 , std :: memory_order_relaxed ); } uint64_t loadValue () { return sharedValue . load ( std :: memory_order_relaxed ); } You\u2019ll notice that both the Mintomic and C++11 examples use relaxed atomics , as evidenced by the _relaxed suffix on various identifiers. The _relaxed suffix is a reminder that few guarantees are made about memory ordering . In particular, it is still legal for the memory effects of a relaxed atomic operation to be reordered with respect to instructions which follow or precede it in program order, either due to compiler reordering or memory reordering on the processor itself. The compiler could even perform optimizations on redundant relaxed atomic operations, just as with non-atomic operations. In all cases, the operation remains atomic. When manipulating shared memory concurrently, I think it\u2019s good practice to always use Mintomic or C++11 atomic library functions, even in cases where you know that a plain load or store would already be atomic on your target platform. An atomic library function serves as a reminder that elsewhere, the variable is the target of concurrent data access. Hopefully, it\u2019s now a bit more clear why the World\u2019s Simplest Lock-Free Hash Table uses Mintomic library functions to manipulate shared memory concurrently from different threads. THINKING : Compare-and-swap Read\u2013modify\u2013write Data structure alignment how does compiler ensure alignment Coding for Performance: Data alignment and structures Data Structure Alignment : Linker or Compiler relaxed atomic","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/09-Atomic-vs-Non-Atomic-Operations/#preshing#atomic#vs#non-atomic#operations","text":"NOTE: \u8fd9\u7bc7\u6587\u7ae0\u5199\u5f97\u975e\u5e38\u597d Much has already been written about atomic operations on the web, usually with a focus on atomic read-modify-write (RMW) operations. However, those aren\u2019t the only kinds of atomic operations . There are also atomic loads and stores , which are equally important. In this post, I\u2019ll compare atomic loads and stores to their non-atomic counterparts at both the processor level and the C/C++ language level . Along the way, we\u2019ll clarify the C++11 concept of a \u201cdata race\u201d. An operation acting on shared memory is atomic if it completes in a single step relative to other threads(\u4ece\u7ebf\u7a0b\u7684\u89d2\u5ea6\u6765\u770b\u5f85atomic). When an atomic store is performed on a shared variable , no other thread can observe the modification half-complete. When an atomic load is performed on a shared variable , it reads the entire value as it appeared at a single moment in time. Non-atomic loads and stores do not make those guarantees. Without those guarantees, lock-free programming would be impossible, since you could never let different threads manipulate a shared variable at the same time. We can formulate it as a rule: Any time two threads operate on a shared variable concurrently, and one of those operations performs a write, both threads must use atomic operations. NOTE: \u4e0a\u9762\u8fd9\u4e9b\u90fd\u662f\u53ef\u4ee5\u4f7f\u7528multiple-model\u6765\u8fdb\u884c\u5206\u6790\u7684\uff0c\u5c24\u5176\u662f\u4eceread-write\u7684\u89d2\u5ea6\u6765\u8fdb\u884c\u5206\u6790\u7684 If you violate this rule, and either thread uses a non-atomic operation, you\u2019ll have what the C++11 standard refers to as a data race (not to be confused with Java\u2019s concept of a data race, which is different, or the more general race condition ). The C++11 standard doesn\u2019t tell you why data races are bad; only that if you have one, \u201cundefined behavior\u201d will result ( \u00a71.10.21 ). The real reason why such data races are bad is actually quite simple: They result in torn reads and torn writes . NOTE: \"torn\"\u7684\u610f\u601d\u662f\"\u6495\u88c2\u7684\" A memory operation can be non-atomic because it uses multiple CPU instructions , non-atomic even when using a single CPU instruction, or non-atomic because you\u2019re writing portable code and you simply can\u2019t make the assumption. Let\u2019s look at a few examples.","title":"preshing Atomic vs. Non-Atomic Operations"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/09-Atomic-vs-Non-Atomic-Operations/#non-atomic#due#to#multiple#cpu#instructions","text":"Suppose you have a 64-bit global variable, initially zero. uint64_t sharedValue = 0 ; At some point, you assign a 64-bit value to this variable. void storeValue () { sharedValue = 0x100000002 ; //16\u8fdb\u5236 } When you compile this function for 32-bit x86 using GCC, it generates the following machine code. NOTE: \u9700\u8981\u6ce8\u610f\u7684\u662f: uint64_t \u662f64-bit\uff0c\u5b9e\u9a8c\u73af\u5883\u662f: 32-bit $ gcc -O2 -S -masm=intel test.c $ cat test.s ... mov DWORD PTR sharedValue, 2 mov DWORD PTR sharedValue+4, 1 ret ... As you can see, the compiler implemented the 64-bit assignment using two separate machine instructions . The first instruction sets the lower 32 bits to 0x00000002 , and the second sets the upper 32 bits to 0x00000001 . Clearly, this assignment operation is not atomic. If sharedValue is accessed concurrently by different threads, several things can now go wrong: 1\u3001If a thread calling storeValue is preempted between the two machine instructions, it will leave the value of 0x0000000000000002 in memory \u2013 a torn write . At this point, if another thread reads sharedValue , it will receive this completely bogus(\u4f2a\u9020\u7684) value which nobody intended to store. NOTE: \u5982\u679c\u8c03\u7528 storeValue \u7684\u7ebf\u7a0b\u5728\u4e24\u4e2a\u673a\u5668\u6307\u4ee4\u4e4b\u95f4\u88ab\u62a2\u5360\uff0c\u5b83\u5c06\u5728\u5185\u5b58\u4e2d\u4fdd\u7559 0x0000000000000002 \u7684\u503c - \u4e00\u4e2a\u6495\u88c2\u7684\u5199\u5165\u3002 \u6b64\u65f6\uff0c\u5982\u679c\u53e6\u4e00\u4e2a\u7ebf\u7a0b\u8bfb\u53d6 sharedValue \uff0c\u5b83\u5c06\u6536\u5230\u8fd9\u4e2a\u5b8c\u5168\u865a\u5047\u7684\u503c\uff0c\u6ca1\u6709\u4eba\u6253\u7b97\u5b58\u50a8\u3002 2\u3001Even worse, if a thread is preempted between the two instructions, and another thread modifies sharedValue before the first thread resumes, it will result in a permanently torn write : the upper 32 bits from one thread, the lower 32 bits from another. NOTE: \u66f4\u7cdf\u7cd5\u7684\u662f\uff0c\u5982\u679c\u4e00\u4e2a\u7ebf\u7a0b\u5728\u4e24\u4e2a\u6307\u4ee4\u4e4b\u95f4\u88ab\u62a2\u5360\uff0c\u800c\u53e6\u4e00\u4e2a\u7ebf\u7a0b\u5728\u7b2c\u4e00\u4e2a\u7ebf\u7a0b\u6062\u590d\u4e4b\u524d\u4fee\u6539\u4e86 sharedValue \uff0c\u5219\u4f1a\u5bfc\u81f4\u6c38\u4e45\u6027\u7684\u5199\u5165\uff1a\u4e00\u4e2a\u7ebf\u7a0b\u7684\u9ad832\u4f4d\uff0c\u53e6\u4e00\u4e2a\u4f4e32\u4f4d\uff08\u7b2c\u4e00\u4e2a\u7ebf\u7a0b\u5148\u5199\u5165\u7684\u4e00\u534a\u5185\u5bb9\u4f1a\u88ab\u7b2c\u4e8c\u4e2a\u7ebf\u7a0b\u8986\u76d6\uff0c\u7136\u540e\u7b2c\u4e00\u4e2a\u7ebf\u7a0bresume\u540e\uff0c\u5b83\u4f1a\u5199\u5165\u540e\u534a\u90e8\u5206\uff0c\u8fd9\u5c31\u8986\u76d6\u4e86\u7b2c\u4e8c\u4e2a\u7ebf\u7a0b\u4e4b\u524d\u5199\u5165\u7684\u5185\u5bb9\uff0c\u663e\u7136\u5b83\u4eec\u76f8\u4e92\u8986\u76d6\uff0c\u5bfc\u81f4\u4e86\u6700\u7ec8\u5404\u81ea\u90fd\u53ea\u5199\u5165\u4e86\u4e00\u534a\uff09 3\u3001On multicore devices, it isn\u2019t even necessary to preempt one of the threads to have a torn write. When a thread calls storeValue , any thread executing on a different core could read sharedValue at a moment when only half the change is visible. NOTE: \u663e\u7136\uff0cmulticore\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u80fd\u6bcf\u4e2athread\u4f1a\u5360\u7528\u4e00\u4e2acore\uff0c\u4f46\u662f\u5b83\u4eec\u5171\u4eabprocess\u7684memory\uff0c\u5b83\u4eec\u80fd\u591f\u540c\u65f6access process\u7684memory\u4e2d\u7684\u540c\u4e00\u4e2a\u5730\u5740\uff1b\u5982\u679c\u5b83\u4eec\u5bf9shared memory\u7684access\u4e0d\u6309\u7167\u4e92\u65a5\u539f\u5219\u6765\u8fdb\u884c\uff0c\u5373\u6bcf\u6b21\u5728access shared memory\u4e4b\u524d\uff0c\u5148lock\uff1b\u5219\u5c31\u4f1a\u5bfc\u81f4\u5bf9shared memory\u7684access\u662f\u65e0\u5e8f\u7684\uff1b\u663e\u7136\u4e92\u65a5\u539f\u5219\u5373\u80fd\u591f\u4fdd\u8bc1\u5728multicore\u60c5\u51b5\u4e0b\u7684\u5b89\u5168\uff0c\u4e5f\u4fdd\u8bc1\u4e86\u5728single core\u60c5\u51b5\u4e0b\uff0c\u5373\u4f7fpreempted\uff0c\u4e5f\u80fd\u591f\u5b89\u5168\uff1b Reading concurrently from sharedValue brings its own set of problems: uint64_t loadValue () { return sharedValue ; } $ gcc - O2 - S - masm = intel test . c $ cat test . s ... mov eax , DWORD PTR sharedValue mov edx , DWORD PTR sharedValue + 4 ret ... Here too, the compiler has implemented the load operation using two machine instructions : The first reads the lower 32 bits into eax , and the second reads the upper 32 bits into edx . In this case, if a concurrent store to sharedValue becomes visible between the two instructions, it will result in a torn read \u2013 even if the concurrent store was atomic. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u610f\u601d\u662f\u5728\u4e24\u4e2amachine instruction\u4e4b\u95f4\uff0c\u53e6\u5916\u4e00\u4e2a\u7ebf\u7a0bwrite to sharedValue \uff0c\u8fd9\u5c31\u5bfc\u81f4\u4e4b\u524dread\u7684\u503c\u662f\u4e00\u534a\u4e00\u534a\u7684\uff1b These problems are not just theoretical. Mintomic \u2019s test suite includes a test case called test_load_store_64_fail , in which one thread stores a bunch of 64-bit values to a single variable using a plain assignment operator, while another thread repeatedly performs a plain load from the same variable, validating each result. On a multicore x86, this test fails consistently, as expected.","title":"Non-Atomic Due to Multiple CPU Instructions"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/09-Atomic-vs-Non-Atomic-Operations/#non-atomic#cpu#instructions","text":"A memory operation can be non-atomic even when performed by a single CPU instruction. For example, the ARMv7 instruction set includes the strd instruction, which stores the contents of two 32-bit source registers to a single 64-bit value in memory. strd r0, r1, [r2] On some ARMv7 processors, this instruction is not atomic. When the processor sees this instruction, it actually performs two separate 32-bit stores under the hood ( \u00a7A3.5.3 ). Once again, another thread running on a separate core has the possibility of observing a torn write . Interestingly, a torn write is even possible on a single-core device: A system interrupt \u2013 say, for a scheduled thread context switch \u2013 can actually occur between the two internal 32-bit stores! In this case, when the thread resumes from the interrupt, it will restart the strd instruction all over again. As another example, it\u2019s well-known that on x86, a 32-bit mov instruction is atomic if the memory operand is naturally aligned, but non-atomic otherwise. In other words, atomicity is only guaranteed when the 32-bit integer is located at an address which is an exact multiple of 4. Mintomic comes with another test case, test_load_store_32_fail , which verifies this guarantee. As it\u2019s written, this test always succeeds on x86, but if you modify the test to force sharedInt to certain unaligned addresses, it will fail. On my Core 2 Quad Q6600, the test fails when sharedInt crosses a cache line boundary: // Force sharedInt to cross a cache line boundary: #pragma pack(2) MINT_DECL_ALIGNED ( static struct , 64 ) { char padding [ 62 ]; mint_atomic32_t sharedInt ; } g_wrapper ; Those are enough processor-specific details for now. Let\u2019s look at atomicity at the C/C++ language level. NOTE: mov is not atomic\u7684\u60c5\u51b5\u8fd8\u6ca1\u6709\u641e\u61c2\uff0c\u9700\u8981Google: mov is not atomic when address is not aligned","title":"Non-Atomic CPU Instructions"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/09-Atomic-vs-Non-Atomic-Operations/#all#cc#operations#are#presumed#non-atomic","text":"In C and C++ , every operation is presumed non-atomic unless otherwise specified by the compiler or hardware vendor \u2013 even plain 32-bit integer assignment. uint32_t foo = 0 ; void storeFoo () { foo = 0x80286 ; } The language standards have nothing to say about atomicity in this case. Maybe integer assignment is atomic, maybe it isn\u2019t. Since non-atomic operations don\u2019t make any guarantees, plain integer assignment in C is non-atomic by definition. In practice, we usually know more about our target platforms than that. For example, it\u2019s common knowledge that on all modern x86, x64, Itanium, SPARC, ARM and PowerPC processors, plain 32-bit integer assignment is atomic as long as the target variable is naturally aligned. You can verify it by consulting your processor manual and/or compiler documentation. In the games industry, I can tell you that a lot of 32-bit integer assignments rely on this particular guarantee. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u8ba9\u6211\u60f3\u8d77\u6765 alignment \u7684\u91cd\u8981\u4ef7\u503c Nonetheless(\u5c3d\u7ba1\u5982\u6b64), when writing truly portable C and C++, there\u2019s a long-standing tradition of pretending that we don\u2019t know anything more than what the language standards tell us. Portable C and C++ is designed to run on every possible computing device past, present and imaginary (\u8fd9\u53e5\u8bdd\u8bf4\u660e\u4e86portable\u7684\u542b\u4e49). NOTE: \u4e0a\u9762\u5728\u8fd9\u6bb5\u8bdd\uff0c\u5176\u5b9e\u603b\u7ed3\u4e86\u4e66\u5199cross-plateform/portable C and C++ program\u7684\u7cbe\u795e\u8981\u4e49: \u5bf9\u4e8e language standards \u672a\u8fdb\u884c\u6807\u51c6\u5316\u7684\uff0c\u6211\u4eec\u5e94\u8be5\u5047\u88c5\u4e0d\u77e5\u9053\uff0c\u5373\u4e0d\u505a\u4efb\u4f55\u5047\u8bbe\u3002 Personally, I like to imagine a machine where memory can only be changed by mixing it up first: On such a machine, you definitely wouldn\u2019t want to perform a concurrent read at the same time as a plain assignment; you could end up reading a completely random value. In C++11 , there is finally a way to perform truly portable atomic loads and stores: the C++11 atomic library. Atomic loads and stores performed using the C++11 atomic library would even work on the imaginary computer above \u2013 even if it means the C++11 atomic library must secretly lock a mutex to make each operation atomic. There\u2019s also the Mintomic library which I released last month , which doesn\u2019t support as many platforms, but works on several older compilers, is hand-optimized and is guaranteed to be lock-free. NOTE: C++11 atomic library\u5bf9atomic load\u3001store\u8fdb\u884c\u4e86\u6807\u51c6\u5316\u3002","title":"All C/C++ Operations Are Presumed\uff08\u5047\u5b9a\uff09 Non-Atomic"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/09-Atomic-vs-Non-Atomic-Operations/#relaxed#atomic#operations","text":"Let\u2019s return to the original sharedValue example from earlier in this post. We\u2019ll rewrite it using Mintomic so that all operations are performed atomically on every platform Mintomic supports. First, we must declare sharedValue as one of Mintomic\u2019s atomic data types . #include <mintomic/mintomic.h> mint_atomic64_t sharedValue = { 0 }; The mint_atomic64_t type guarantees correct memory alignment for atomic access on each platform. This is important because, for example, the GCC 4.2 compiler for ARM bundled with Xcode 3.2.5 doesn\u2019t guarantee that plain uint64_t will be 8-byte aligned. In storeValue , instead of performing a plain, non-atomic assignment, we must call mint_store_64_relaxed . void storeValue () { mint_store_64_relaxed ( & sharedValue , 0x100000002 ); } Similarly, in loadValue , we call mint_load_64_relaxed . uint64_t loadValue () { return mint_load_64_relaxed ( & sharedValue ); } Using C++11 \u2019s terminology, these functions are now data race-free . When executing concurrently, there is absolutely no possibility of a torn read or write, whether the code runs on ARMv6/ARMv7 (Thumb or ARM mode), x86, x64 or PowerPC. If you\u2019re curious how mint_load_64_relaxed and mint_store_64_relaxed actually work, both functions expand to an inline cmpxchg8b instruction on x86; for other platforms, consult Mintomic\u2019s implementation . Here\u2019s the exact same thing written in C++11 instead: #include <atomic> std :: atomic < uint64_t > sharedValue ( 0 ); void storeValue () { sharedValue . store ( 0x100000002 , std :: memory_order_relaxed ); } uint64_t loadValue () { return sharedValue . load ( std :: memory_order_relaxed ); } You\u2019ll notice that both the Mintomic and C++11 examples use relaxed atomics , as evidenced by the _relaxed suffix on various identifiers. The _relaxed suffix is a reminder that few guarantees are made about memory ordering . In particular, it is still legal for the memory effects of a relaxed atomic operation to be reordered with respect to instructions which follow or precede it in program order, either due to compiler reordering or memory reordering on the processor itself. The compiler could even perform optimizations on redundant relaxed atomic operations, just as with non-atomic operations. In all cases, the operation remains atomic. When manipulating shared memory concurrently, I think it\u2019s good practice to always use Mintomic or C++11 atomic library functions, even in cases where you know that a plain load or store would already be atomic on your target platform. An atomic library function serves as a reminder that elsewhere, the variable is the target of concurrent data access. Hopefully, it\u2019s now a bit more clear why the World\u2019s Simplest Lock-Free Hash Table uses Mintomic library functions to manipulate shared memory concurrently from different threads.","title":"Relaxed Atomic Operations"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/09-Atomic-vs-Non-Atomic-Operations/#thinking","text":"Compare-and-swap Read\u2013modify\u2013write Data structure alignment","title":"THINKING :"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/09-Atomic-vs-Non-Atomic-Operations/#how#does#compiler#ensure#alignment","text":"Coding for Performance: Data alignment and structures Data Structure Alignment : Linker or Compiler","title":"how does compiler ensure alignment"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/09-Atomic-vs-Non-Atomic-Operations/#relaxed#atomic","text":"","title":"relaxed atomic"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/10-The-Happens-Before-Relation/","text":"preshing The Happens-Before Relation NOTE: \u5728cppreference std::memory_order \u7ae0\u8282\u4e2d\uff0c\u6709\u5173\u4e8ehappens-before\u7684\u51c6\u786e\u5b9a\u4e49: Happens-before Regardless of threads, evaluation A happens-before evaluation B if any of the following is true: 1) A is sequenced-before B 2) A inter-thread happens before B \u5728\u672c\u6587\u4e2d\uff0c\u5bf9\u8fd9\u4e24\u79cdhappens-before\u90fd\u6709\u63cf\u8ff0: 1\u3001\" Happens-Before Does Not Imply Happening Before\"\u7ae0\u8282\u63cf\u8ff0\u7684\u662fsequenced-before\u7684\u4f8b\u5b50 2\u3001\"Happening Before Does Not Imply Happens-Before \"\u7ae0\u8282\u63cf\u8ff0\u7684\u662f*inter-thread happens before*\u7684\u4f8b\u5b50 What is Happens-before relation? Happens-before is a modern computer science term which is instrumental in describing the software memory models behind C++11, Java, Go and even LLVM. Definition Roughly speaking, the common definition can be stated as follows: Let A and B represent operations performed by a multithreaded process. If A happens-before B, then the memory effects of A effectively become visible to the thread performing B before B is performed. NOTE: \u4e00\u3001\u4e0a\u9762\u63cf\u8ff0\u7684\u5176\u5b9e\u662finter-thread happens before \u4e8c\u3001\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\"memory effects\"\u3001\"visible\"\u7b49\u5176\u5b9e\u8fd8\u662f\u6bd4\u8f83\u62bd\u8c61\uff0c\u6211\u4eec\u521d\u8bfb\u8fd8\u662f\u4e0d\u5bb9\u6613\u7406\u89e3\uff0c\u90a3\u6709\u4ec0\u4e48\u5177\u4f53\u6848\u4f8b\u5417\uff1f\u4e0b\u9762\u662f\u6848\u4f8b: 1\u3001\u5728\"Happening Before Does Not Imply Happens-Before ( inter-thread happens before )\"\u4e2d\uff0c\u5c31\u5217\u4e3e\u4e86\u975e\u5e38\u597d\u7684\u6848\u4f8b\uff0c\u5728\u6b64\u6211\u4eec\u53ef\u4ee5\u7b80\u5355\u5148\u4ecb\u7ecd\u4e00\u4e0b: a\u3001 (2) \u548c (3) \u5206\u522b\u6709\u4e0d\u540c\u7684thread\u6267\u884c b\u3001\u5982\u679c (2) happens-before (3) -\u8574\u542b-> (3) reads the value written by (2) \u5728\u4e0b\u4e00\u7bc7 preshing The Synchronizes-With Relation \u4e2d\uff0c\u5bf9 happens-before \u7684\u89e3\u91ca\u4e3a: safely propagate modifications from one thread to another once they\u2019re complete. That\u2019s where the synchronizes-with relation comes in When you consider the various ways in which memory reordering can complicate lock-free programming, the guarantee that A happens-before B is a desirable one. NOTE: make it computational ordering There are several ways to obtain this guarantee, differing slightly from one programming language to next \u2013 though obviously, all languages must rely on the same mechanisms at the processor level. Sequenced-before NOTE: \u672c\u8282\u6240\u63cf\u8ff0\u7684\u662fProgrammer\u4ecesource code\u7684\u89d2\u5ea6\u6765\u7406\u89e3*Happens-before* relation\uff0c\u6309\u7167cppreference std::memory_order \u4e2d\u7684\u8bf4\u6cd5\uff0c\u8fd9\u5176\u5b9e\u662fSequenced-before\u3002 No matter which programming language you use, they all have one thing in common: If operations A and B are performed by the same thread, and A\u2019s statement comes before B\u2019s statement in program order, then A happens-before B. This is basically a formalization of the \u201ccardinal(\u57fa\u672c\u7684) rule of memory ordering\u201d I mentioned in an earlier post . NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7ed9\u51fa\u7684\u6700\u7ec8\u7ed3\u8bba\"A happens-before B\"\u662f\u6709\u524d\u63d0\u7684: 1\u3001\"operation A\u548coperation B are performed by the same thread \" \u5355\u7ebf\u7a0b\u6267\u884c\uff0c\u8fd9\u662f\u975e\u5e38\u91cd\u8981\u7684\u4e00\u4e2a\u524d\u63d0 2\u3001\"A\u2019s statement comes before B\u2019s statement in program order\" \u5728\u9605\u8bfb\u4e86\u8fd9\u6bb5\u8bdd\u540e\uff0c\u6211\u60f3\u5230\u4e86compiler\u4f1a\u6267\u884coptimization\uff0c\u53ef\u80fd\u9020\u6210 instruction reordering \uff0c\u90a3\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u63cf\u8ff0\u7684\u5c82\u4e0d\u662f\u9519\u8bef\u7684\uff0c\u540e\u6765\u53cd\u590d\u601d\u8003\uff0c\u5224\u65ad\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u5e76\u6ca1\u6709\u9519\u8bef\uff0c\u539f\u56e0\u5982\u4e0b: 1\u3001compiler\u7684optimization\u662f\u9075\u5faaAS-IF rule\u7684\uff0c\u5e76\u4e0d\u4f1a\u66f4\u6539\u8fd9\u4e9b\u57fa\u672c\u7684effect\uff0c\u4e5f\u5c31\u662fcompiler optimization\u5e76\u4e0d\u4f1a\u6539\u53d8happens-before relation\uff0c\u5373\u5b9e\u9645\u7684\u6267\u884c\u4e0d\u4f1a\u6539\u53d8happens-before relation\uff0c\u56e0\u6b64\u53ef\u4ee5\u8ba4\u4e3a\u4e0a\u8ff0\u65ad\u8a00\u662f\u6b63\u786e\u7684\uff1b NOTE: abstraction and optimization 2\u3001\u8fd9\u518d\u6b64\u63d0\u9192\u4e86\u6211\u4eec: happens-before relation\u662f\u57fa\u4e8eeffect\u800c\u5b9a\u4e49\u7684\uff0c\u800c\u4e0d\u662f\u57fa\u4e8estatement in programmer order\u800c\u5b9a\u4e49\u7684\uff0c\u8fd9\u6837\u7684\u8bbe\u7f6e\uff0c\u5141\u8bb8CPU\u3001compiler\u8fdb\u884c\u6700\u5927\u5316\u7684optimization(\u53ea\u8981\u5b83\u4fdd\u8bc1effect\u5373\u53ef)\uff0c\u8fd9\u4e2a\u8ba4\u77e5\u975e\u5e38\u91cd\u8981\uff0c\u4e0b\u9762\u7684\" Happens-Before Does Not Imply Happening Before\"\u5176\u5b9e\u5c31\u662f\u8bba\u8bc1\u8fd9\u4e2a\u89c2\u70b9\u7684\u6700\u597d\u7684\u4f8b\u5b50\u3002compiler optimization\u540e\u751f\u6210\u7684code\u80fd\u591f\u4fdd\u8bc1single thread\u7684\u6b63\u786e\uff0c\u4f46\u662f\u5e76\u4e0d\u80fd\u591f\u4fdd\u8bc1multithread\u3001multicore\u7684\u6b63\u786e\uff1b 3\u3001sequenced-before\u662fC++\u7684\u9ed8\u8ba4\u884c\u4e3a\uff0c\u5b83\u662f\u7b80\u5355\u76f4\u89c2\u7684: \u987a\u5e8f\u7684\uff0c\u5b83\u8ba9programmer\u80fd\u591f\u4ecesource code\u7684\u89d2\u5ea6\u5bf9program\u7684\u8fd0\u884c\u7ed3\u679c\u8fdb\u884c\u6709\u6548\u7684\u63a8\u6d4b\uff1bprogrammer\u63a8\u6d4b\u7684\u8fc7\u7a0b\u5176\u5b9e\u53ef\u4ee5\u8fd9\u6837\u6765\u7406\u89e3: programmer\u6a21\u62df\u4e86\u4e00\u4e2aabstract machine\uff0c\u8fd9\u4e2aabstract machine\u987a\u5e8f\u6267\u884c\u6211\u4eec\u7684program\uff0c\u7136\u540e\u8f93\u51fa\u6211\u4eec\u671f\u671b\u7684\u7ed3\u679c 4\u3001\u5982\u679c\u8981\u5b9e\u73b0inter-thread happens-before\uff0c\u9700\u8981\u7531programmer\u6dfb\u52a0\u989d\u5916\u7684\u63a7\u5236\uff0c\u4f5c\u8005\u5728\u4e0b\u4e00\u7bc7\u4e2d\u8fdb\u884c\u4e86\u4ecb\u7ecd int A , B ; void foo () { // This store to A ... A = 5 ; // ... effectively becomes visible before the following loads. Duh! B = A * A ; } That\u2019s not the only way to achieve a happens-before relation. The C++11 standard states that, among other methods, you also can achieve it between operations in different threads using acquire and release semantics . I\u2019ll talk about that more in the next post about synchronizes-with . happens-before VS happening before NOTE: happens-before \u662f\u57fa\u4e8eeffect\u800c\u5b9a\u4e49\u7684\uff0c\u5b83\u4e0d\u662f\u57fa\u4e8etime\u800c\u5b9a\u4e49\u7684\uff0c\u4e0d\u5bb9\u6613\u7406\u89e3\uff0c\u4e0e\u76f4\u89c9\u76f8\u5f02 happening before \u662f\u57fa\u4e8e\u5b9e\u9645\u6267\u884c\u987a\u5e8f\u800c\u5b9a\u4e49\u7684\uff0c\u5b83\u662f\u57fa\u4e8etime\u800c\u5b9a\u4e49\u7684\uff0c\u5bb9\u6613\u7406\u89e3\uff0c\u4e0e\u76f4\u89c9\u76f8\u540c \u9020\u6210 \"Ambiguity between the happens-before relation and the actual order of operations(happening before) \"\u7684\u6839\u672c\u539f\u56e0\u662fmemory reordering\uff0c\u5b83\u53ef\u80fd\u53d1\u751f\u4e8ecompile-time\uff0c\u4e5f\u53ef\u80fd\u53d1\u751f\u4e8erun-time\u3002 \u5728\u4e0b\u9762\u7684\"Ambiguity between the happens-before relation and the actual order of operations(happening before) \"\u7684: 1\u3001\" Happens-Before Does Not Imply Happening Before\" 2\u3001Happening Before Does Not Imply Happens-Before \u5176\u5b9e\u5c31\u662f\u5bf9\u4e24\u8005\u4e4b\u95f4\u7684Ambiguity\u8fdb\u884c\u975e\u5e38\u8be6\u7ec6\u7684\u5206\u6790\uff1b \u5728\" Ambiguity between the happens-before relation and the actual order of operations(happening before) \" \u7ae0\u8282\uff0c\u4f5c\u8005\u8fdb\u884c\u4e86\u603b\u7ed3 I\u2019m pretty sure that the name of this relation may lead to confusion for some. It\u2019s worth clearing up (\u6f84\u6e05) right away: The happens-before relation, under the definition given above, is not the same as A actually happening before B! In particular, 1\u3001A happens-before B does not imply A happening before B. 2\u3001A happening before B does not imply A happens-before B. happens-before exists independently of the concept of time NOTE: \u8fd9\u6bb5\u662f\u5bf9*happens-before*\u4e0d\u662f\u57fa\u4e8e\u65f6\u95f4\u5b9a\u4e49\u7684\u8fdb\u884c\u8bf4\u660e\uff0c\u4e0b\u9762\u4f1a\u7ed9\u51fa\u4f8b\u5b50\u8fdb\u884c\u8bba\u8bc1 These statements might appear paradoxical(\u77db\u76fe\u7684\u3001\u4f3c\u662f\u800c\u975e\u7684), but they\u2019re not. I\u2019ll try to explain them in the following sections. Remember, happens-before is a formal relation between operations, defined by a family of language specifications; it exists independently of the concept of time . This is different from what we usually mean when we say that \u201cA happens before B\u201d; referring the order, in time, of real-world events. Throughout this post, I\u2019ve been careful to always hyphenate(\u8fde\u5b57\u7b26\u53f7) the former term happens-before , in order to distinguish it from the latter. Happens-Before Does Not Imply Happening Before(Sequenced-before) NOTE: \u4f5c\u8005\u672c\u5730\u7ed9\u51fa\u4e86\u4e00\u4e2a\u4f8b\u5b50\u6765\u8bba\u8bc1\" Happens-Before Does Not Imply Happening Before\"\uff1b\u4f5c\u8005\u7684\u8fd9\u4e2a\u4f8b\u5b50\u5c55\u793a\u4e86 instruction reordering \uff0c\u867d\u7136\u53d1\u751f\u4e86 instruction reordering \uff0c\u4f46\u662f\u5e76\u6ca1\u6709\u8fdd\u80cc Happens-Before relation\uff0c A \u3001 B \u7684 value \u8fd8\u662f\u88ab\u6b63\u786e\u7684\u66f4\u65b0\u4e86\uff1b\u8fd9\u4e2a\u4f8b\u5b50\u6bd4\u8f83\u7275\u5f3a\u7684\u5730\u65b9\u5728\u4e8e: A \u548c B\u4e4b\u95f4\u5e76\u6ca1\u6709\u4f9d\u8d56\u5173\u7cfb\uff1b \u8fd9\u4e2a\u4f8b\u5b50\u8ba9\u6211\u8ba4\u77e5\u4e86 instruction reordering \u7684\u5e7f\u6cdb\u5b58\u5728\uff0c\u8054\u60f3 aristeia-C++and-the-Perils-of-Double-Checked-Locking \uff0c\u663e\u7136\u5728multithread\u3001multicore\u7684\u60c5\u51b5\u4e0b\uff0c instruction reordering \u4f1a\u9020\u6210\u8bf8\u591a\u95ee\u9898\u3002 Here\u2019s an example of operations having a happens-before relationship without actually happening in that order. The following code performs (1) a store to A , followed by (2) a store to B . According to the rule of program ordering, (1) happens-before (2). int A = 0 ; int B = 0 ; void foo () { A = B + 1 ; // (1) B = 1 ; // (2) } However, if we compile this code with -O2 using GCC , the compiler performs some instruction reordering . As a result, when we step through the resulting code at the disassembly level in the debugger, we clearly see that after the second machine instruction, the store to B has completed, but the store to A has not. In other words, (1) doesn\u2019t actually happen before (2)! Has the happens-before relation been violated? Let\u2019s see. According to the definition, the memory effects of (1) must effectively be visible before (2) is performed. In other words, the store to A must have a chance to influence the store to B . In this case, though, the store to A doesn\u2019t actually influence the store to B . (2) still behaves the same as it would have even if the effects of (1) had been visible, which is effectively the same as (1)\u2019s effects being visible. Therefore, this doesn\u2019t count as a violation of the happens-before rule. I\u2019ll admit, this explanation is a bit dicey(\u4e0d\u51c6\u786e\u7684), but I\u2019m fairly confident it\u2019s consistent with the meaning of happen-before in all those language specifications. Happening Before Does Not Imply Happens-Before ( inter-thread happens before ) NOTE: Happening Before \u5e76\u4e0d\u80fd\u4fdd\u8bc1 happens-before Here\u2019s an example of operations which clearly happen in a specific order without constituting(\u6784\u6210) a happens-before relationship. In the following code, imagine that one thread calls publishMessage , while another thread calls consumeMessage . Since we\u2019re manipulating shared variables concurrently, let\u2019s keep it simple and assume that plain loads and stores of int are atomic . Because of program ordering, there is a happens-before relation between (1) and (2), and another happens-before relation between (3) and (4). NOTE: \u9700\u8981\u6ce8\u610f\u7684\u662f: \"Because of program ordering, there is a happens-before relation between (1) and (2), and another happens-before relation between (3) and (4).\" \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7528\u7684\u662f*happens-before*\uff0c\u5373effect\uff1b int isReady = 0 ; int answer = 0 ; void publishMessage () { answer = 42 ; // (1) isReady = 1 ; // (2) } void consumeMessage () { if ( isReady ) // (3) <-- Let's suppose this line reads 1 printf ( \"%d \\n \" , answer ); // (4) } (2) and (3): \u8bf4\u660eHappening Before Does Not Imply Happens-Before NOTE: \u4e0b\u9762\u63cf\u8ff0\u7684(2) and (3)\u7684\u573a\u666f\u5c31\u53ef\u4ee5\u8bf4\u660e: Happening Before Does Not Imply Happens-Before Furthermore, let\u2019s suppose that at runtime, the line marked (3) ends up reading 1, the value that was stored at line (2) in the other thread. In this case, we know that (2) must have happened before (3). But that doesn\u2019t mean there is a happens-before relationship between (2) and (3)! NOTE: \u5728\u8fd9\u4e00\u6bb5\u63cf\u8ff0\u7684\u573a\u666f\u4e2d: (2) have happened before (3)\uff0c\u4f46\u662f\u8fd9\u5e76\u4e0d\u610f\u5473\"there is a happens-before relationship between (2) and (3)\"\u3002 The happens-before relationship only exists where the language standards say it exists. And since these are plain loads and stores, the C++11 standard has no rule which introduces a happens-before relation between (2) and (3), even when (3) reads the value written by (2). NOTE: 1\u3001\u770b\u4e86\u4e0a\u9762\u8fd9\u4e00\u6bb5\u7684\u6700\u540e\u4e00\u53e5\u8bdd\uff0c\u6211\u660e\u767d\u4e86happens-before\u7684\u6548\u679c: \u5982\u679c (2) happens-before (3) -\u8574\u542b-> (3) reads the value written by (2) 2\u3001\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u610f\u601d\u662f: \u6309\u7167C++\u6807\u51c6\u7684\u5b9a\u4e49\uff0c(2) and (3)\u4e4b\u95f4\u5e76\u4e0d\u5b58\u5728happens-before\u5173\u7cfb\uff0c\u867d\u7136\u5b9e\u9645\u8fd0\u884c\u6548\u679c\"(3) reads the value written by (2)\"\uff0c\u662f\u548chappens-before\u5173\u7cfb\u4e00\u81f4\u7684\uff0c\u4f46\u662f\u5b83\u5e76\u4e0d\u80fd\u591f\u8bf4\u660e(2) and (3)\u4e4b\u95f4\u5b58\u5728happens-before\u5173\u7cfb (1) and (4) Furthermore, because there is no happens-before relation between (2) and (3), there is no happens-before relation between (1) and (4), either. Therefore, the memory interactions of (1) and (4) can be reordered, either due to compiler instruction reordering or memory reordering on the processor itself, such that (4) ends up printing \u201c0\u201d, even though (3) reads 1. Access outside of object lifetime \u4e0a\u8ff0(4) \u5982\u679cread/load\u7684\u503c\u662f0\u7684\u8bdd\uff0c\u5176\u5b9e\u5c31\u76f8\u5f53\u4e8e\"access outside of object lifetime\"\uff0c\u5373access\u4e86\u4e3a\u521d\u59cb\u5316\u7684value\uff1b This post hasn\u2019t really shown anything new. We already knew that memory interactions can be reordered when executing lock-free code. We\u2019ve simply examined a term used in C++11, Java, Go and LLVM to formally specify the cases when memory reordering can be observed in those languages. Even Mintomic , a library I published several weeks ago, relies on the guarantees of happens-before , since it mimics the behavior of specific C++11 atomic library functions. Ambiguity between the happens-before and happening before I believe the ambiguity that exists between the happens-before relation and the actual order of operations is part of what makes low-level lock-free programming so tricky. If nothing else, this post should have demonstrated that happens-before is a useful guarantee which doesn\u2019t come cheaply between threads. I\u2019ll expand on that further in the next post.","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/10-The-Happens-Before-Relation/#preshing#the#happens-before#relation","text":"NOTE: \u5728cppreference std::memory_order \u7ae0\u8282\u4e2d\uff0c\u6709\u5173\u4e8ehappens-before\u7684\u51c6\u786e\u5b9a\u4e49:","title":"preshing The Happens-Before Relation"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/10-The-Happens-Before-Relation/#happens-before","text":"Regardless of threads, evaluation A happens-before evaluation B if any of the following is true: 1) A is sequenced-before B 2) A inter-thread happens before B \u5728\u672c\u6587\u4e2d\uff0c\u5bf9\u8fd9\u4e24\u79cdhappens-before\u90fd\u6709\u63cf\u8ff0: 1\u3001\" Happens-Before Does Not Imply Happening Before\"\u7ae0\u8282\u63cf\u8ff0\u7684\u662fsequenced-before\u7684\u4f8b\u5b50 2\u3001\"Happening Before Does Not Imply Happens-Before \"\u7ae0\u8282\u63cf\u8ff0\u7684\u662f*inter-thread happens before*\u7684\u4f8b\u5b50","title":"Happens-before"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/10-The-Happens-Before-Relation/#what#is#happens-before#relation","text":"Happens-before is a modern computer science term which is instrumental in describing the software memory models behind C++11, Java, Go and even LLVM.","title":"What is Happens-before relation?"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/10-The-Happens-Before-Relation/#definition","text":"Roughly speaking, the common definition can be stated as follows: Let A and B represent operations performed by a multithreaded process. If A happens-before B, then the memory effects of A effectively become visible to the thread performing B before B is performed. NOTE: \u4e00\u3001\u4e0a\u9762\u63cf\u8ff0\u7684\u5176\u5b9e\u662finter-thread happens before \u4e8c\u3001\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\"memory effects\"\u3001\"visible\"\u7b49\u5176\u5b9e\u8fd8\u662f\u6bd4\u8f83\u62bd\u8c61\uff0c\u6211\u4eec\u521d\u8bfb\u8fd8\u662f\u4e0d\u5bb9\u6613\u7406\u89e3\uff0c\u90a3\u6709\u4ec0\u4e48\u5177\u4f53\u6848\u4f8b\u5417\uff1f\u4e0b\u9762\u662f\u6848\u4f8b: 1\u3001\u5728\"Happening Before Does Not Imply Happens-Before ( inter-thread happens before )\"\u4e2d\uff0c\u5c31\u5217\u4e3e\u4e86\u975e\u5e38\u597d\u7684\u6848\u4f8b\uff0c\u5728\u6b64\u6211\u4eec\u53ef\u4ee5\u7b80\u5355\u5148\u4ecb\u7ecd\u4e00\u4e0b: a\u3001 (2) \u548c (3) \u5206\u522b\u6709\u4e0d\u540c\u7684thread\u6267\u884c b\u3001\u5982\u679c (2) happens-before (3) -\u8574\u542b-> (3) reads the value written by (2) \u5728\u4e0b\u4e00\u7bc7 preshing The Synchronizes-With Relation \u4e2d\uff0c\u5bf9 happens-before \u7684\u89e3\u91ca\u4e3a: safely propagate modifications from one thread to another once they\u2019re complete. That\u2019s where the synchronizes-with relation comes in When you consider the various ways in which memory reordering can complicate lock-free programming, the guarantee that A happens-before B is a desirable one. NOTE: make it computational ordering There are several ways to obtain this guarantee, differing slightly from one programming language to next \u2013 though obviously, all languages must rely on the same mechanisms at the processor level.","title":"Definition"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/10-The-Happens-Before-Relation/#sequenced-before","text":"NOTE: \u672c\u8282\u6240\u63cf\u8ff0\u7684\u662fProgrammer\u4ecesource code\u7684\u89d2\u5ea6\u6765\u7406\u89e3*Happens-before* relation\uff0c\u6309\u7167cppreference std::memory_order \u4e2d\u7684\u8bf4\u6cd5\uff0c\u8fd9\u5176\u5b9e\u662fSequenced-before\u3002 No matter which programming language you use, they all have one thing in common: If operations A and B are performed by the same thread, and A\u2019s statement comes before B\u2019s statement in program order, then A happens-before B. This is basically a formalization of the \u201ccardinal(\u57fa\u672c\u7684) rule of memory ordering\u201d I mentioned in an earlier post . NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7ed9\u51fa\u7684\u6700\u7ec8\u7ed3\u8bba\"A happens-before B\"\u662f\u6709\u524d\u63d0\u7684: 1\u3001\"operation A\u548coperation B are performed by the same thread \" \u5355\u7ebf\u7a0b\u6267\u884c\uff0c\u8fd9\u662f\u975e\u5e38\u91cd\u8981\u7684\u4e00\u4e2a\u524d\u63d0 2\u3001\"A\u2019s statement comes before B\u2019s statement in program order\" \u5728\u9605\u8bfb\u4e86\u8fd9\u6bb5\u8bdd\u540e\uff0c\u6211\u60f3\u5230\u4e86compiler\u4f1a\u6267\u884coptimization\uff0c\u53ef\u80fd\u9020\u6210 instruction reordering \uff0c\u90a3\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u63cf\u8ff0\u7684\u5c82\u4e0d\u662f\u9519\u8bef\u7684\uff0c\u540e\u6765\u53cd\u590d\u601d\u8003\uff0c\u5224\u65ad\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u5e76\u6ca1\u6709\u9519\u8bef\uff0c\u539f\u56e0\u5982\u4e0b: 1\u3001compiler\u7684optimization\u662f\u9075\u5faaAS-IF rule\u7684\uff0c\u5e76\u4e0d\u4f1a\u66f4\u6539\u8fd9\u4e9b\u57fa\u672c\u7684effect\uff0c\u4e5f\u5c31\u662fcompiler optimization\u5e76\u4e0d\u4f1a\u6539\u53d8happens-before relation\uff0c\u5373\u5b9e\u9645\u7684\u6267\u884c\u4e0d\u4f1a\u6539\u53d8happens-before relation\uff0c\u56e0\u6b64\u53ef\u4ee5\u8ba4\u4e3a\u4e0a\u8ff0\u65ad\u8a00\u662f\u6b63\u786e\u7684\uff1b NOTE: abstraction and optimization 2\u3001\u8fd9\u518d\u6b64\u63d0\u9192\u4e86\u6211\u4eec: happens-before relation\u662f\u57fa\u4e8eeffect\u800c\u5b9a\u4e49\u7684\uff0c\u800c\u4e0d\u662f\u57fa\u4e8estatement in programmer order\u800c\u5b9a\u4e49\u7684\uff0c\u8fd9\u6837\u7684\u8bbe\u7f6e\uff0c\u5141\u8bb8CPU\u3001compiler\u8fdb\u884c\u6700\u5927\u5316\u7684optimization(\u53ea\u8981\u5b83\u4fdd\u8bc1effect\u5373\u53ef)\uff0c\u8fd9\u4e2a\u8ba4\u77e5\u975e\u5e38\u91cd\u8981\uff0c\u4e0b\u9762\u7684\" Happens-Before Does Not Imply Happening Before\"\u5176\u5b9e\u5c31\u662f\u8bba\u8bc1\u8fd9\u4e2a\u89c2\u70b9\u7684\u6700\u597d\u7684\u4f8b\u5b50\u3002compiler optimization\u540e\u751f\u6210\u7684code\u80fd\u591f\u4fdd\u8bc1single thread\u7684\u6b63\u786e\uff0c\u4f46\u662f\u5e76\u4e0d\u80fd\u591f\u4fdd\u8bc1multithread\u3001multicore\u7684\u6b63\u786e\uff1b 3\u3001sequenced-before\u662fC++\u7684\u9ed8\u8ba4\u884c\u4e3a\uff0c\u5b83\u662f\u7b80\u5355\u76f4\u89c2\u7684: \u987a\u5e8f\u7684\uff0c\u5b83\u8ba9programmer\u80fd\u591f\u4ecesource code\u7684\u89d2\u5ea6\u5bf9program\u7684\u8fd0\u884c\u7ed3\u679c\u8fdb\u884c\u6709\u6548\u7684\u63a8\u6d4b\uff1bprogrammer\u63a8\u6d4b\u7684\u8fc7\u7a0b\u5176\u5b9e\u53ef\u4ee5\u8fd9\u6837\u6765\u7406\u89e3: programmer\u6a21\u62df\u4e86\u4e00\u4e2aabstract machine\uff0c\u8fd9\u4e2aabstract machine\u987a\u5e8f\u6267\u884c\u6211\u4eec\u7684program\uff0c\u7136\u540e\u8f93\u51fa\u6211\u4eec\u671f\u671b\u7684\u7ed3\u679c 4\u3001\u5982\u679c\u8981\u5b9e\u73b0inter-thread happens-before\uff0c\u9700\u8981\u7531programmer\u6dfb\u52a0\u989d\u5916\u7684\u63a7\u5236\uff0c\u4f5c\u8005\u5728\u4e0b\u4e00\u7bc7\u4e2d\u8fdb\u884c\u4e86\u4ecb\u7ecd int A , B ; void foo () { // This store to A ... A = 5 ; // ... effectively becomes visible before the following loads. Duh! B = A * A ; } That\u2019s not the only way to achieve a happens-before relation. The C++11 standard states that, among other methods, you also can achieve it between operations in different threads using acquire and release semantics . I\u2019ll talk about that more in the next post about synchronizes-with .","title":"Sequenced-before"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/10-The-Happens-Before-Relation/#happens-before#vs#happening#before","text":"NOTE: happens-before \u662f\u57fa\u4e8eeffect\u800c\u5b9a\u4e49\u7684\uff0c\u5b83\u4e0d\u662f\u57fa\u4e8etime\u800c\u5b9a\u4e49\u7684\uff0c\u4e0d\u5bb9\u6613\u7406\u89e3\uff0c\u4e0e\u76f4\u89c9\u76f8\u5f02 happening before \u662f\u57fa\u4e8e\u5b9e\u9645\u6267\u884c\u987a\u5e8f\u800c\u5b9a\u4e49\u7684\uff0c\u5b83\u662f\u57fa\u4e8etime\u800c\u5b9a\u4e49\u7684\uff0c\u5bb9\u6613\u7406\u89e3\uff0c\u4e0e\u76f4\u89c9\u76f8\u540c \u9020\u6210 \"Ambiguity between the happens-before relation and the actual order of operations(happening before) \"\u7684\u6839\u672c\u539f\u56e0\u662fmemory reordering\uff0c\u5b83\u53ef\u80fd\u53d1\u751f\u4e8ecompile-time\uff0c\u4e5f\u53ef\u80fd\u53d1\u751f\u4e8erun-time\u3002 \u5728\u4e0b\u9762\u7684\"Ambiguity between the happens-before relation and the actual order of operations(happening before) \"\u7684: 1\u3001\" Happens-Before Does Not Imply Happening Before\" 2\u3001Happening Before Does Not Imply Happens-Before \u5176\u5b9e\u5c31\u662f\u5bf9\u4e24\u8005\u4e4b\u95f4\u7684Ambiguity\u8fdb\u884c\u975e\u5e38\u8be6\u7ec6\u7684\u5206\u6790\uff1b \u5728\" Ambiguity between the happens-before relation and the actual order of operations(happening before) \" \u7ae0\u8282\uff0c\u4f5c\u8005\u8fdb\u884c\u4e86\u603b\u7ed3 I\u2019m pretty sure that the name of this relation may lead to confusion for some. It\u2019s worth clearing up (\u6f84\u6e05) right away: The happens-before relation, under the definition given above, is not the same as A actually happening before B! In particular, 1\u3001A happens-before B does not imply A happening before B. 2\u3001A happening before B does not imply A happens-before B.","title":"happens-before VS happening before"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/10-The-Happens-Before-Relation/#happens-before#exists#independently#of#the#concept#of#time","text":"NOTE: \u8fd9\u6bb5\u662f\u5bf9*happens-before*\u4e0d\u662f\u57fa\u4e8e\u65f6\u95f4\u5b9a\u4e49\u7684\u8fdb\u884c\u8bf4\u660e\uff0c\u4e0b\u9762\u4f1a\u7ed9\u51fa\u4f8b\u5b50\u8fdb\u884c\u8bba\u8bc1 These statements might appear paradoxical(\u77db\u76fe\u7684\u3001\u4f3c\u662f\u800c\u975e\u7684), but they\u2019re not. I\u2019ll try to explain them in the following sections. Remember, happens-before is a formal relation between operations, defined by a family of language specifications; it exists independently of the concept of time . This is different from what we usually mean when we say that \u201cA happens before B\u201d; referring the order, in time, of real-world events. Throughout this post, I\u2019ve been careful to always hyphenate(\u8fde\u5b57\u7b26\u53f7) the former term happens-before , in order to distinguish it from the latter.","title":"happens-before exists independently of the concept of time"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/10-The-Happens-Before-Relation/#happens-before#does#not#imply#happening#beforesequenced-before","text":"NOTE: \u4f5c\u8005\u672c\u5730\u7ed9\u51fa\u4e86\u4e00\u4e2a\u4f8b\u5b50\u6765\u8bba\u8bc1\" Happens-Before Does Not Imply Happening Before\"\uff1b\u4f5c\u8005\u7684\u8fd9\u4e2a\u4f8b\u5b50\u5c55\u793a\u4e86 instruction reordering \uff0c\u867d\u7136\u53d1\u751f\u4e86 instruction reordering \uff0c\u4f46\u662f\u5e76\u6ca1\u6709\u8fdd\u80cc Happens-Before relation\uff0c A \u3001 B \u7684 value \u8fd8\u662f\u88ab\u6b63\u786e\u7684\u66f4\u65b0\u4e86\uff1b\u8fd9\u4e2a\u4f8b\u5b50\u6bd4\u8f83\u7275\u5f3a\u7684\u5730\u65b9\u5728\u4e8e: A \u548c B\u4e4b\u95f4\u5e76\u6ca1\u6709\u4f9d\u8d56\u5173\u7cfb\uff1b \u8fd9\u4e2a\u4f8b\u5b50\u8ba9\u6211\u8ba4\u77e5\u4e86 instruction reordering \u7684\u5e7f\u6cdb\u5b58\u5728\uff0c\u8054\u60f3 aristeia-C++and-the-Perils-of-Double-Checked-Locking \uff0c\u663e\u7136\u5728multithread\u3001multicore\u7684\u60c5\u51b5\u4e0b\uff0c instruction reordering \u4f1a\u9020\u6210\u8bf8\u591a\u95ee\u9898\u3002 Here\u2019s an example of operations having a happens-before relationship without actually happening in that order. The following code performs (1) a store to A , followed by (2) a store to B . According to the rule of program ordering, (1) happens-before (2). int A = 0 ; int B = 0 ; void foo () { A = B + 1 ; // (1) B = 1 ; // (2) } However, if we compile this code with -O2 using GCC , the compiler performs some instruction reordering . As a result, when we step through the resulting code at the disassembly level in the debugger, we clearly see that after the second machine instruction, the store to B has completed, but the store to A has not. In other words, (1) doesn\u2019t actually happen before (2)! Has the happens-before relation been violated? Let\u2019s see. According to the definition, the memory effects of (1) must effectively be visible before (2) is performed. In other words, the store to A must have a chance to influence the store to B . In this case, though, the store to A doesn\u2019t actually influence the store to B . (2) still behaves the same as it would have even if the effects of (1) had been visible, which is effectively the same as (1)\u2019s effects being visible. Therefore, this doesn\u2019t count as a violation of the happens-before rule. I\u2019ll admit, this explanation is a bit dicey(\u4e0d\u51c6\u786e\u7684), but I\u2019m fairly confident it\u2019s consistent with the meaning of happen-before in all those language specifications.","title":"Happens-Before Does Not Imply Happening Before(Sequenced-before)"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/10-The-Happens-Before-Relation/#happening#before#does#not#imply#happens-before#inter-thread#happens#before","text":"NOTE: Happening Before \u5e76\u4e0d\u80fd\u4fdd\u8bc1 happens-before Here\u2019s an example of operations which clearly happen in a specific order without constituting(\u6784\u6210) a happens-before relationship. In the following code, imagine that one thread calls publishMessage , while another thread calls consumeMessage . Since we\u2019re manipulating shared variables concurrently, let\u2019s keep it simple and assume that plain loads and stores of int are atomic . Because of program ordering, there is a happens-before relation between (1) and (2), and another happens-before relation between (3) and (4). NOTE: \u9700\u8981\u6ce8\u610f\u7684\u662f: \"Because of program ordering, there is a happens-before relation between (1) and (2), and another happens-before relation between (3) and (4).\" \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7528\u7684\u662f*happens-before*\uff0c\u5373effect\uff1b int isReady = 0 ; int answer = 0 ; void publishMessage () { answer = 42 ; // (1) isReady = 1 ; // (2) } void consumeMessage () { if ( isReady ) // (3) <-- Let's suppose this line reads 1 printf ( \"%d \\n \" , answer ); // (4) }","title":"Happening Before Does Not Imply Happens-Before( inter-thread happens before )"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/10-The-Happens-Before-Relation/#2#and#3#happening#before#does#not#imply#happens-before","text":"NOTE: \u4e0b\u9762\u63cf\u8ff0\u7684(2) and (3)\u7684\u573a\u666f\u5c31\u53ef\u4ee5\u8bf4\u660e: Happening Before Does Not Imply Happens-Before Furthermore, let\u2019s suppose that at runtime, the line marked (3) ends up reading 1, the value that was stored at line (2) in the other thread. In this case, we know that (2) must have happened before (3). But that doesn\u2019t mean there is a happens-before relationship between (2) and (3)! NOTE: \u5728\u8fd9\u4e00\u6bb5\u63cf\u8ff0\u7684\u573a\u666f\u4e2d: (2) have happened before (3)\uff0c\u4f46\u662f\u8fd9\u5e76\u4e0d\u610f\u5473\"there is a happens-before relationship between (2) and (3)\"\u3002 The happens-before relationship only exists where the language standards say it exists. And since these are plain loads and stores, the C++11 standard has no rule which introduces a happens-before relation between (2) and (3), even when (3) reads the value written by (2). NOTE: 1\u3001\u770b\u4e86\u4e0a\u9762\u8fd9\u4e00\u6bb5\u7684\u6700\u540e\u4e00\u53e5\u8bdd\uff0c\u6211\u660e\u767d\u4e86happens-before\u7684\u6548\u679c: \u5982\u679c (2) happens-before (3) -\u8574\u542b-> (3) reads the value written by (2) 2\u3001\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u610f\u601d\u662f: \u6309\u7167C++\u6807\u51c6\u7684\u5b9a\u4e49\uff0c(2) and (3)\u4e4b\u95f4\u5e76\u4e0d\u5b58\u5728happens-before\u5173\u7cfb\uff0c\u867d\u7136\u5b9e\u9645\u8fd0\u884c\u6548\u679c\"(3) reads the value written by (2)\"\uff0c\u662f\u548chappens-before\u5173\u7cfb\u4e00\u81f4\u7684\uff0c\u4f46\u662f\u5b83\u5e76\u4e0d\u80fd\u591f\u8bf4\u660e(2) and (3)\u4e4b\u95f4\u5b58\u5728happens-before\u5173\u7cfb","title":"(2) and (3): \u8bf4\u660eHappening Before Does Not Imply Happens-Before"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/10-The-Happens-Before-Relation/#1#and#4","text":"Furthermore, because there is no happens-before relation between (2) and (3), there is no happens-before relation between (1) and (4), either. Therefore, the memory interactions of (1) and (4) can be reordered, either due to compiler instruction reordering or memory reordering on the processor itself, such that (4) ends up printing \u201c0\u201d, even though (3) reads 1.","title":"(1) and (4)"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/10-The-Happens-Before-Relation/#access#outside#of#object#lifetime","text":"\u4e0a\u8ff0(4) \u5982\u679cread/load\u7684\u503c\u662f0\u7684\u8bdd\uff0c\u5176\u5b9e\u5c31\u76f8\u5f53\u4e8e\"access outside of object lifetime\"\uff0c\u5373access\u4e86\u4e3a\u521d\u59cb\u5316\u7684value\uff1b This post hasn\u2019t really shown anything new. We already knew that memory interactions can be reordered when executing lock-free code. We\u2019ve simply examined a term used in C++11, Java, Go and LLVM to formally specify the cases when memory reordering can be observed in those languages. Even Mintomic , a library I published several weeks ago, relies on the guarantees of happens-before , since it mimics the behavior of specific C++11 atomic library functions.","title":"Access outside of object lifetime"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/10-The-Happens-Before-Relation/#ambiguity#between#the#happens-before#and#happening#before","text":"I believe the ambiguity that exists between the happens-before relation and the actual order of operations is part of what makes low-level lock-free programming so tricky. If nothing else, this post should have demonstrated that happens-before is a useful guarantee which doesn\u2019t come cheaply between threads. I\u2019ll expand on that further in the next post.","title":"Ambiguity between the happens-before and happening before"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/11-The-Synchronizes-With-Relation/","text":"preshing The Synchronizes-With Relation NOTE: 1\u3001\u6309\u7167 cppreference std::memory_order \u7ae0\u8282\u4e2d\u7684\u8bf4\u6cd5: Inter-thread happens-before Between threads, evaluation A inter-thread happens before evaluation B if any of the following is true 1) A synchronizes-with B \u663e\u7136 \u672c\u8282\u4ecb\u7ecd\u7684 synchronization-with\u662finter-thread happens-before\u7684\u4e00\u79cd\u3002 \u672c\u6587\u4fa7\u91cd\u4e8e\u4ecb\u7ecd\u5982\u4f55\u5b9e\u73b0\"synchronizes-with relation\"\u3002 2\u3001\u5728 \"csdn \u5728 C++ memory order\u5faa\u5e8f\u6e10\u8fdb\uff08\u4e8c\uff09\u2014\u2014 C++ memory order\u57fa\u672c\u5b9a\u4e49\u548c\u5f62\u5f0f\u5316\u63cf\u8ff0\u6240\u9700\u672f\u8bed\u5173\u7cfb\u8be6\u89e3 2.5 Synchronizes-with\" \u4e2d\uff0c\u5f15\u7528\u4e86\u8fd9\u7bc7\u6587\u7ae0\uff0c\u5176\u4e2d\u5bf9\u8fd9\u7bc7\u6587\u7ae0\u7684\u5185\u5bb9\u8fdb\u884c\u4e86\u4e00\u4e9b\u6ce8\u89e3\uff0c\u8fd9\u662f\u6709\u52a9\u4e8e\u7406\u89e3\u672c\u6587\u7684\u5185\u5bb9\u7684 What is and why need \u201d Synchronizes-with \u201d? NOTE: \u8fd9\u6bb5\u8bdd\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48\u9700\u8981\"synchronizes-with\"\uff0c\u5176\u5b9e\u8fd9\u662f\u8001\u8c03\u5e38\u8c08\u7684\u95ee\u9898\uff0c\u5728 preshing The Happens-Before Relation \u4e2d\u5df2\u7ecf\u8ba8\u8bba\u4e86\u8fd9\u4e2a\u8bdd\u9898\uff1b In an earlier post, I explained how atomic operations let you manipulate shared variables concurrently without any torn reads or torn writes. Quite often, though, a thread only modifies a shared variable when there are no concurrent readers or writers. In such cases, atomic operations are unnecessary. We just need a way to safely propagate modifications from one thread to another once they\u2019re complete. That\u2019s where the synchronizes-with relation comes in. NOTE: \"safely propagate modifications from one thread to another once they\u2019re complete\" \u975e\u5e38\u597d\u7684\u89e3\u91ca \u201d Synchronizes-with \u201d is a term invented by language designers to describe ways in which the memory effects of source-level operations \u2013 even non-atomic operations \u2013 are guaranteed to become visible to other threads. This is a desirable guarantee when writing lock-free code, since you can use it to avoid unwelcome surprises caused by memory reordering. \u201d Synchronizes-with \u201d is a fairly modern computer science term. You\u2019ll find it in the specifications of C++11, Java 5+ and LLVM, all of which were published within the last 10 years. Each specification defines this term, then uses it to make formal guarantees to the programmer. One thing they have in common is that whenever there\u2019s a synchronizes-with relationship between two operations, typically on different threads, there\u2019s a happens-before relationship between those operations as well. Guard variable and the payload Before digging deeper, I\u2019ll let you in on a small insight: In every synchronizes-with relationship, you should be able to identify two key ingredients, which I like to call the guard variable and the payload . The payload is the set of data being propagated between threads, while the guard variable protects access to the payload. I\u2019ll point out these ingredients as we go. Now let\u2019s look at a familiar example using C++11 atomics. A Write-Release Can Synchronize-With a Read-Acquire NOTE: \u672c\u8282\u4ecb\u7ecd\u7684\u4f8b\u5b50\u672c\u8d28\u4e0a\u548c\u4e0a\u4e00\u7bc7\u4e2d\u7684\u4f8b\u5b50\u76f8\u540c\uff0c\u6211\u4eec\u4f7f\u7528\u4e0a\u4e00\u7bc7\u7684\u4f8b\u5b50\uff0c\u80fd\u591f\u8f7b\u677e\u5730\u7406\u89e3\u4e0b\u9762\u7684\u4f8b\u5b50: 1\u3001\u5b83\u6240\u9762\u4e34\u7684\u95ee\u9898 2\u3001\u89e3\u51b3\u65b9\u6cd5 Suppose we have a Message structure which is produced by one thread and consumed by another. It has the following fields: struct Message { clock_t tick ; const char * str ; void * param ; }; We\u2019ll pass an instance of Message between threads by placing it in a shared global variable. This shared variable acts as the payload. Message g_payload ; Now, there\u2019s no portable way to fill in g_payload using a single atomic operation. So we won\u2019t try. Instead, we\u2019ll define a separate atomic variable, g_guard , to indicate whether g_payload is ready. As you might guess, g_guard acts as our guard variable. The guard variable must be manipulated using atomic operations , since two threads will operate on it concurrently, and one of those threads performs a write. std :: atomic < int > g_guard ( 0 ); To pass g_payload safely between threads, we\u2019ll use acquire and release semantics, a subject I\u2019ve written about previously using an example very similar to this one. If you\u2019ve already read that post, you\u2019ll recognize the final line of the following function as a write-release operation on g_guard . void SendTestMessage ( void * param ) { // Copy to shared memory using non-atomic stores. g_payload . tick = clock (); g_payload . str = \"TestMessage\" ; g_payload . param = param ; // Perform an atomic write-release to indicate that the message is ready. g_guard . store ( 1 , std :: memory_order_release ); } While the first thread calls SendTestMessage , the second thread calls TryReceiveMessage intermittently, retrying until it sees a return value of true . You\u2019ll recognize the first line of this function as a read-acquire operation on g_guard . bool TryReceiveMessage ( Message & result ) { // Perform an atomic read-acquire to check whether the message is ready. int ready = g_guard . load ( std :: memory_order_acquire ); if ( ready != 0 ) { // Yes. Copy from shared memory using non-atomic loads. result . tick = g_payload . tick ; result . str = g_payload . str ; result . param = g_payload . param ; return true ; } // No. return false ; } If you\u2019ve been following this blog for a while, you already know that this example works reliably (though it\u2019s only capable of passing a single message). I\u2019ve already explained how acquire and release semantics introduce memory barriers, and given a detailed example of acquire and release semantics in a working C++11 application. The C++11 standard, on the other hand, doesn\u2019t explain anything. That\u2019s because a standard is meant to serve as a contract or an agreement, not as a tutorial. It simply makes the promise that this example will work, without going into any further detail. The promise is made in \u00a729.3.2 of working draft N3337 : An atomic operation A that performs a release operation on an atomic object M synchronizes with an atomic operation B that performs an acquire operation on M and takes its value from any side effect in the release sequence headed by A. It\u2019s worth breaking this down. In our example: 1\u3001 Atomic operation A is the write-release performed in SendTestMessage . 2\u3001 Atomic object M is the guard variable, g_guard . 3\u3001 Atomic operation B is the read-acquire performed in TryReceiveMessage . \u975e\u5e38\u597d\u7684\u89e3\u91ca As for the condition that the read-acquire must \u201ctake its value from any side effect\u201d \u2013 let\u2019s just say it\u2019s sufficient for the read-acquire to read the value written by the write-release. If that happens, the synchronized-with relationship is complete, and we\u2019ve achieved the coveted(\u68a6\u5bd0\u4ee5\u6c42\u7684) happens-before relationship between threads. Some people like to call this a synchronize-with or happens-before \u201cedge\u201d. Most importantly, the standard guarantees (in \u00a71.10.11-12 ) that whenever there\u2019s a synchronizes-with edge, the happens-before relationship extends to neighboring operations, too. This includes all operations before the edge in Thread 1, and all operations after the edge in Thread 2. In the example above, it ensures that all the modifications to g_payload are visible by the time the other thread reads them. Compiler implementation Compiler vendors, if they wish to claim C++11 compliance, must adhere(\u9075\u5faa) to this guarantee. At first, it might seem mysterious how they do it. But in fact, compilers fulfill this promise using the same old tricks which programmers technically had to use long before C++11 came along. For example, in this post , we saw how an ARMv7 compiler implements these operations using a pair of dmb instructions. A PowerPC compiler could implement them using lwsync , while an x86 compiler could simply use a compiler barrier, thanks to x86\u2019s relatively strong hardware memory model . Java volatile Of course, acquire and release semantics are not unique to C++11. For example, in Java version 5 onward, every store to a volatile variable is a write-release, while every load from a volatile variable is a read-acquire. Therefore, any volatile variable in Java can act as a guard variable, and can be used to propagate a payload of any size between threads. Jeremy Manson explains this in his blog post on volatile variables in Java . He even uses a diagram very similar to the one shown above, calling it the \u201ctwo cones\u201d diagram. It\u2019s a Runtime Relationship In the previous example, we saw how the last line of SendTestMessage synchronized-with the first line of TryReceiveMessage . But don\u2019t fall into the trap of thinking that synchronizes-with is a relationship between statements in your source code. It isn\u2019t! It\u2019s a relationship between operations which occur at runtime, based on those statements. This distinction is important, and should really be obvious when you think about it. A single source code statement can execute any number of times in a running process. And if TryReceiveMessage is called too early \u2013 before Thread 1\u2019s store to g_guard is visible \u2013 there will be no synchronizes-with relationship whatsoever. It all depends on whether the read-acquire sees the value written by the write-release, or not. That\u2019s what the C++11 standard means when it says that atomic operation B must \u201ctake its value\u201d from atomic operation A. Other Ways to Achieve Synchronizes-With Just as synchronizes-with is not only way to achieve a happens-before relationship, a pair of write-release/read-acquire operations is not the only way to achieve synchronizes-with ; nor are C++11 atomics the only way to achieve acquire and release semantics. I\u2019ve organized a few other ways into the following chart. Keep in mind that this chart is by no means exhaustive. The example in this post generates lock-free code (on virtually all modern compilers and processors), but C++11 and Java expose blocking operations which introduce synchronize-with edges as well. For instance, unlocking a mutex always synchronizes-with a subsequent lock of that mutex. The language specifications are pretty clear about that one, and as programmers, we naturally expect it. You can consider the mutex itself to be the guard, and the protected variables as the payload. IBM even published an article on Java\u2019s updated memory model in 2004 which contains a \u201ctwo cones\u201d diagram showing a pair of lock/unlock operations synchronizing-with each other. As I\u2019ve shown previously , acquire and release semantics can also be implemented using standalone, explicit fence instructions. In other words, it\u2019s possible for a release fence to synchronize-with an acquire fence, provided that the right conditions are met. In fact, explicit fence instructions are the only available option in Mintomic , my own portable API for lock-free programming. I think that acquire and release fences are woefully misunderstood on the web right now, so I\u2019ll probably write a dedicated post about them next. The bottom line is that the synchronizes-with relationship only exists where the language and API specifications say it exists. It\u2019s their job to define the conditions of their own guarantees at the source code level. Therefore, when using low-level ordering constraints in C++11 atomics, you can\u2019t just slap std::memory_order_acquire and release on some operations and hope things magically work out. You need to identify which atomic variable is the guard, what\u2019s the payload, and in which codepaths a synchronizes-with relationship is ensured. Interestingly, the Go programming language is a bit of convention breaker. Go\u2019s memory model is well specified , but the specification does not bother using the term \u201c synchronizes-with \u201d anywhere. It simply sticks with the term \u201c happens-before \u201d, which is just as good, since obviously, happens-before can fill the role anywhere that synchronizes-with would. Perhaps Go\u2019s authors chose a reduced vocabulary because \u201c synchronizes-with \u201d is normally used to describe operations on different threads, and Go doesn\u2019t expose the concept of threads.","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/11-The-Synchronizes-With-Relation/#preshing#the#synchronizes-with#relation","text":"NOTE: 1\u3001\u6309\u7167 cppreference std::memory_order \u7ae0\u8282\u4e2d\u7684\u8bf4\u6cd5:","title":"preshing The Synchronizes-With Relation"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/11-The-Synchronizes-With-Relation/#inter-thread#happens-before","text":"Between threads, evaluation A inter-thread happens before evaluation B if any of the following is true 1) A synchronizes-with B \u663e\u7136 \u672c\u8282\u4ecb\u7ecd\u7684 synchronization-with\u662finter-thread happens-before\u7684\u4e00\u79cd\u3002 \u672c\u6587\u4fa7\u91cd\u4e8e\u4ecb\u7ecd\u5982\u4f55\u5b9e\u73b0\"synchronizes-with relation\"\u3002 2\u3001\u5728 \"csdn \u5728 C++ memory order\u5faa\u5e8f\u6e10\u8fdb\uff08\u4e8c\uff09\u2014\u2014 C++ memory order\u57fa\u672c\u5b9a\u4e49\u548c\u5f62\u5f0f\u5316\u63cf\u8ff0\u6240\u9700\u672f\u8bed\u5173\u7cfb\u8be6\u89e3 2.5 Synchronizes-with\" \u4e2d\uff0c\u5f15\u7528\u4e86\u8fd9\u7bc7\u6587\u7ae0\uff0c\u5176\u4e2d\u5bf9\u8fd9\u7bc7\u6587\u7ae0\u7684\u5185\u5bb9\u8fdb\u884c\u4e86\u4e00\u4e9b\u6ce8\u89e3\uff0c\u8fd9\u662f\u6709\u52a9\u4e8e\u7406\u89e3\u672c\u6587\u7684\u5185\u5bb9\u7684","title":"Inter-thread happens-before"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/11-The-Synchronizes-With-Relation/#what#is#and#why#need#synchronizes-with","text":"NOTE: \u8fd9\u6bb5\u8bdd\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48\u9700\u8981\"synchronizes-with\"\uff0c\u5176\u5b9e\u8fd9\u662f\u8001\u8c03\u5e38\u8c08\u7684\u95ee\u9898\uff0c\u5728 preshing The Happens-Before Relation \u4e2d\u5df2\u7ecf\u8ba8\u8bba\u4e86\u8fd9\u4e2a\u8bdd\u9898\uff1b In an earlier post, I explained how atomic operations let you manipulate shared variables concurrently without any torn reads or torn writes. Quite often, though, a thread only modifies a shared variable when there are no concurrent readers or writers. In such cases, atomic operations are unnecessary. We just need a way to safely propagate modifications from one thread to another once they\u2019re complete. That\u2019s where the synchronizes-with relation comes in. NOTE: \"safely propagate modifications from one thread to another once they\u2019re complete\" \u975e\u5e38\u597d\u7684\u89e3\u91ca \u201d Synchronizes-with \u201d is a term invented by language designers to describe ways in which the memory effects of source-level operations \u2013 even non-atomic operations \u2013 are guaranteed to become visible to other threads. This is a desirable guarantee when writing lock-free code, since you can use it to avoid unwelcome surprises caused by memory reordering. \u201d Synchronizes-with \u201d is a fairly modern computer science term. You\u2019ll find it in the specifications of C++11, Java 5+ and LLVM, all of which were published within the last 10 years. Each specification defines this term, then uses it to make formal guarantees to the programmer. One thing they have in common is that whenever there\u2019s a synchronizes-with relationship between two operations, typically on different threads, there\u2019s a happens-before relationship between those operations as well.","title":"What is and why need \u201dSynchronizes-with\u201d?"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/11-The-Synchronizes-With-Relation/#guard#variable#and#the#payload","text":"Before digging deeper, I\u2019ll let you in on a small insight: In every synchronizes-with relationship, you should be able to identify two key ingredients, which I like to call the guard variable and the payload . The payload is the set of data being propagated between threads, while the guard variable protects access to the payload. I\u2019ll point out these ingredients as we go. Now let\u2019s look at a familiar example using C++11 atomics.","title":"Guard variable and the payload"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/11-The-Synchronizes-With-Relation/#a#write-release#can#synchronize-with#a#read-acquire","text":"NOTE: \u672c\u8282\u4ecb\u7ecd\u7684\u4f8b\u5b50\u672c\u8d28\u4e0a\u548c\u4e0a\u4e00\u7bc7\u4e2d\u7684\u4f8b\u5b50\u76f8\u540c\uff0c\u6211\u4eec\u4f7f\u7528\u4e0a\u4e00\u7bc7\u7684\u4f8b\u5b50\uff0c\u80fd\u591f\u8f7b\u677e\u5730\u7406\u89e3\u4e0b\u9762\u7684\u4f8b\u5b50: 1\u3001\u5b83\u6240\u9762\u4e34\u7684\u95ee\u9898 2\u3001\u89e3\u51b3\u65b9\u6cd5 Suppose we have a Message structure which is produced by one thread and consumed by another. It has the following fields: struct Message { clock_t tick ; const char * str ; void * param ; }; We\u2019ll pass an instance of Message between threads by placing it in a shared global variable. This shared variable acts as the payload. Message g_payload ; Now, there\u2019s no portable way to fill in g_payload using a single atomic operation. So we won\u2019t try. Instead, we\u2019ll define a separate atomic variable, g_guard , to indicate whether g_payload is ready. As you might guess, g_guard acts as our guard variable. The guard variable must be manipulated using atomic operations , since two threads will operate on it concurrently, and one of those threads performs a write. std :: atomic < int > g_guard ( 0 ); To pass g_payload safely between threads, we\u2019ll use acquire and release semantics, a subject I\u2019ve written about previously using an example very similar to this one. If you\u2019ve already read that post, you\u2019ll recognize the final line of the following function as a write-release operation on g_guard . void SendTestMessage ( void * param ) { // Copy to shared memory using non-atomic stores. g_payload . tick = clock (); g_payload . str = \"TestMessage\" ; g_payload . param = param ; // Perform an atomic write-release to indicate that the message is ready. g_guard . store ( 1 , std :: memory_order_release ); } While the first thread calls SendTestMessage , the second thread calls TryReceiveMessage intermittently, retrying until it sees a return value of true . You\u2019ll recognize the first line of this function as a read-acquire operation on g_guard . bool TryReceiveMessage ( Message & result ) { // Perform an atomic read-acquire to check whether the message is ready. int ready = g_guard . load ( std :: memory_order_acquire ); if ( ready != 0 ) { // Yes. Copy from shared memory using non-atomic loads. result . tick = g_payload . tick ; result . str = g_payload . str ; result . param = g_payload . param ; return true ; } // No. return false ; } If you\u2019ve been following this blog for a while, you already know that this example works reliably (though it\u2019s only capable of passing a single message). I\u2019ve already explained how acquire and release semantics introduce memory barriers, and given a detailed example of acquire and release semantics in a working C++11 application. The C++11 standard, on the other hand, doesn\u2019t explain anything. That\u2019s because a standard is meant to serve as a contract or an agreement, not as a tutorial. It simply makes the promise that this example will work, without going into any further detail. The promise is made in \u00a729.3.2 of working draft N3337 : An atomic operation A that performs a release operation on an atomic object M synchronizes with an atomic operation B that performs an acquire operation on M and takes its value from any side effect in the release sequence headed by A. It\u2019s worth breaking this down. In our example: 1\u3001 Atomic operation A is the write-release performed in SendTestMessage . 2\u3001 Atomic object M is the guard variable, g_guard . 3\u3001 Atomic operation B is the read-acquire performed in TryReceiveMessage .","title":"A Write-Release Can Synchronize-With a Read-Acquire"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/11-The-Synchronizes-With-Relation/#_1","text":"As for the condition that the read-acquire must \u201ctake its value from any side effect\u201d \u2013 let\u2019s just say it\u2019s sufficient for the read-acquire to read the value written by the write-release. If that happens, the synchronized-with relationship is complete, and we\u2019ve achieved the coveted(\u68a6\u5bd0\u4ee5\u6c42\u7684) happens-before relationship between threads. Some people like to call this a synchronize-with or happens-before \u201cedge\u201d. Most importantly, the standard guarantees (in \u00a71.10.11-12 ) that whenever there\u2019s a synchronizes-with edge, the happens-before relationship extends to neighboring operations, too. This includes all operations before the edge in Thread 1, and all operations after the edge in Thread 2. In the example above, it ensures that all the modifications to g_payload are visible by the time the other thread reads them.","title":"\u975e\u5e38\u597d\u7684\u89e3\u91ca"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/11-The-Synchronizes-With-Relation/#compiler#implementation","text":"Compiler vendors, if they wish to claim C++11 compliance, must adhere(\u9075\u5faa) to this guarantee. At first, it might seem mysterious how they do it. But in fact, compilers fulfill this promise using the same old tricks which programmers technically had to use long before C++11 came along. For example, in this post , we saw how an ARMv7 compiler implements these operations using a pair of dmb instructions. A PowerPC compiler could implement them using lwsync , while an x86 compiler could simply use a compiler barrier, thanks to x86\u2019s relatively strong hardware memory model .","title":"Compiler implementation"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/11-The-Synchronizes-With-Relation/#java#volatile","text":"Of course, acquire and release semantics are not unique to C++11. For example, in Java version 5 onward, every store to a volatile variable is a write-release, while every load from a volatile variable is a read-acquire. Therefore, any volatile variable in Java can act as a guard variable, and can be used to propagate a payload of any size between threads. Jeremy Manson explains this in his blog post on volatile variables in Java . He even uses a diagram very similar to the one shown above, calling it the \u201ctwo cones\u201d diagram.","title":"Java volatile"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/11-The-Synchronizes-With-Relation/#its#a#runtime#relationship","text":"In the previous example, we saw how the last line of SendTestMessage synchronized-with the first line of TryReceiveMessage . But don\u2019t fall into the trap of thinking that synchronizes-with is a relationship between statements in your source code. It isn\u2019t! It\u2019s a relationship between operations which occur at runtime, based on those statements. This distinction is important, and should really be obvious when you think about it. A single source code statement can execute any number of times in a running process. And if TryReceiveMessage is called too early \u2013 before Thread 1\u2019s store to g_guard is visible \u2013 there will be no synchronizes-with relationship whatsoever. It all depends on whether the read-acquire sees the value written by the write-release, or not. That\u2019s what the C++11 standard means when it says that atomic operation B must \u201ctake its value\u201d from atomic operation A.","title":"It\u2019s a Runtime Relationship"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/11-The-Synchronizes-With-Relation/#other#ways#to#achieve#synchronizes-with","text":"Just as synchronizes-with is not only way to achieve a happens-before relationship, a pair of write-release/read-acquire operations is not the only way to achieve synchronizes-with ; nor are C++11 atomics the only way to achieve acquire and release semantics. I\u2019ve organized a few other ways into the following chart. Keep in mind that this chart is by no means exhaustive. The example in this post generates lock-free code (on virtually all modern compilers and processors), but C++11 and Java expose blocking operations which introduce synchronize-with edges as well. For instance, unlocking a mutex always synchronizes-with a subsequent lock of that mutex. The language specifications are pretty clear about that one, and as programmers, we naturally expect it. You can consider the mutex itself to be the guard, and the protected variables as the payload. IBM even published an article on Java\u2019s updated memory model in 2004 which contains a \u201ctwo cones\u201d diagram showing a pair of lock/unlock operations synchronizing-with each other. As I\u2019ve shown previously , acquire and release semantics can also be implemented using standalone, explicit fence instructions. In other words, it\u2019s possible for a release fence to synchronize-with an acquire fence, provided that the right conditions are met. In fact, explicit fence instructions are the only available option in Mintomic , my own portable API for lock-free programming. I think that acquire and release fences are woefully misunderstood on the web right now, so I\u2019ll probably write a dedicated post about them next. The bottom line is that the synchronizes-with relationship only exists where the language and API specifications say it exists. It\u2019s their job to define the conditions of their own guarantees at the source code level. Therefore, when using low-level ordering constraints in C++11 atomics, you can\u2019t just slap std::memory_order_acquire and release on some operations and hope things magically work out. You need to identify which atomic variable is the guard, what\u2019s the payload, and in which codepaths a synchronizes-with relationship is ensured. Interestingly, the Go programming language is a bit of convention breaker. Go\u2019s memory model is well specified , but the specification does not bother using the term \u201c synchronizes-with \u201d anywhere. It simply sticks with the term \u201c happens-before \u201d, which is just as good, since obviously, happens-before can fill the role anywhere that synchronizes-with would. Perhaps Go\u2019s authors chose a reduced vocabulary because \u201c synchronizes-with \u201d is normally used to describe operations on different threads, and Go doesn\u2019t expose the concept of threads.","title":"Other Ways to Achieve Synchronizes-With"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/12-Acquire-and-Release-Fences/","text":"","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/13-Double-Checked-Locking-is-Fixed-In-C%2B%2B11/","text":"Double-Checked Locking is Fixed In C++11 What Is Double-Checked Locking? Singleton * Singleton :: getInstance () { Singleton * tmp = m_instance ; ... // insert memory barrier if ( tmp == NULL ) { Lock lock ; tmp = m_instance ; if ( tmp == NULL ) { tmp = new Singleton ; ... // insert memory barrier m_instance = tmp ; } } return tmp ; } Using C++11 Acquire and Release Fences std :: atomic < Singleton *> Singleton :: m_instance ; std :: mutex Singleton :: m_mutex ; Singleton * Singleton :: getInstance () { Singleton * tmp = m_instance . load ( std :: memory_order_relaxed ); std :: atomic_thread_fence ( std :: memory_order_acquire ); if ( tmp == nullptr ) { std :: lock_guard < std :: mutex > lock ( m_mutex ); tmp = m_instance . load ( std :: memory_order_relaxed ); if ( tmp == nullptr ) { tmp = new Singleton ; std :: atomic_thread_fence ( std :: memory_order_release ); m_instance . store ( tmp , std :: memory_order_relaxed ); } } return tmp ; } Using Mintomic Fences mint_atomicPtr_t Singleton :: m_instance = { 0 }; mint_mutex_t Singleton :: m_mutex ; Singleton * Singleton :: getInstance () { Singleton * tmp = ( Singleton * ) mint_load_ptr_relaxed ( & m_instance ); mint_thread_fence_acquire (); if ( tmp == NULL ) { mint_mutex_lock ( & m_mutex ); tmp = ( Singleton * ) mint_load_ptr_relaxed ( & m_instance ); if ( tmp == NULL ) { tmp = new Singleton ; mint_thread_fence_release (); mint_store_ptr_relaxed ( & m_instance , tmp ); } mint_mutex_unlock ( & m_mutex ); } return tmp ; } Using C++11 Low-Level Ordering Constraints std :: atomic < Singleton *> Singleton :: m_instance ; std :: mutex Singleton :: m_mutex ; Singleton * Singleton :: getInstance () { Singleton * tmp = m_instance . load ( std :: memory_order_acquire ); if ( tmp == nullptr ) { std :: lock_guard < std :: mutex > lock ( m_mutex ); tmp = m_instance . load ( std :: memory_order_relaxed ); if ( tmp == nullptr ) { tmp = new Singleton ; m_instance . store ( tmp , std :: memory_order_release ); } } return tmp ; } Using C++11 Sequentially Consistent Atomics std :: atomic < Singleton *> Singleton :: m_instance ; std :: mutex Singleton :: m_mutex ; Singleton * Singleton :: getInstance () { Singleton * tmp = m_instance . load (); if ( tmp == nullptr ) { std :: lock_guard < std :: mutex > lock ( m_mutex ); tmp = m_instance . load (); if ( tmp == nullptr ) { tmp = new Singleton ; m_instance . store ( tmp ); } } return tmp ; }","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/13-Double-Checked-Locking-is-Fixed-In-C%2B%2B11/#double-checked#locking#is#fixed#in#c11","text":"","title":"Double-Checked Locking is Fixed In C++11"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/13-Double-Checked-Locking-is-Fixed-In-C%2B%2B11/#what#is#double-checked#locking","text":"Singleton * Singleton :: getInstance () { Singleton * tmp = m_instance ; ... // insert memory barrier if ( tmp == NULL ) { Lock lock ; tmp = m_instance ; if ( tmp == NULL ) { tmp = new Singleton ; ... // insert memory barrier m_instance = tmp ; } } return tmp ; }","title":"What Is Double-Checked Locking?"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/13-Double-Checked-Locking-is-Fixed-In-C%2B%2B11/#using#c11#acquire#and#release#fences","text":"std :: atomic < Singleton *> Singleton :: m_instance ; std :: mutex Singleton :: m_mutex ; Singleton * Singleton :: getInstance () { Singleton * tmp = m_instance . load ( std :: memory_order_relaxed ); std :: atomic_thread_fence ( std :: memory_order_acquire ); if ( tmp == nullptr ) { std :: lock_guard < std :: mutex > lock ( m_mutex ); tmp = m_instance . load ( std :: memory_order_relaxed ); if ( tmp == nullptr ) { tmp = new Singleton ; std :: atomic_thread_fence ( std :: memory_order_release ); m_instance . store ( tmp , std :: memory_order_relaxed ); } } return tmp ; }","title":"Using C++11 Acquire and Release Fences"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/13-Double-Checked-Locking-is-Fixed-In-C%2B%2B11/#using#mintomic#fences","text":"mint_atomicPtr_t Singleton :: m_instance = { 0 }; mint_mutex_t Singleton :: m_mutex ; Singleton * Singleton :: getInstance () { Singleton * tmp = ( Singleton * ) mint_load_ptr_relaxed ( & m_instance ); mint_thread_fence_acquire (); if ( tmp == NULL ) { mint_mutex_lock ( & m_mutex ); tmp = ( Singleton * ) mint_load_ptr_relaxed ( & m_instance ); if ( tmp == NULL ) { tmp = new Singleton ; mint_thread_fence_release (); mint_store_ptr_relaxed ( & m_instance , tmp ); } mint_mutex_unlock ( & m_mutex ); } return tmp ; }","title":"Using Mintomic Fences"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/13-Double-Checked-Locking-is-Fixed-In-C%2B%2B11/#using#c11#low-level#ordering#constraints","text":"std :: atomic < Singleton *> Singleton :: m_instance ; std :: mutex Singleton :: m_mutex ; Singleton * Singleton :: getInstance () { Singleton * tmp = m_instance . load ( std :: memory_order_acquire ); if ( tmp == nullptr ) { std :: lock_guard < std :: mutex > lock ( m_mutex ); tmp = m_instance . load ( std :: memory_order_relaxed ); if ( tmp == nullptr ) { tmp = new Singleton ; m_instance . store ( tmp , std :: memory_order_release ); } } return tmp ; }","title":"Using C++11 Low-Level Ordering Constraints"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/13-Double-Checked-Locking-is-Fixed-In-C%2B%2B11/#using#c11#sequentially#consistent#atomics","text":"std :: atomic < Singleton *> Singleton :: m_instance ; std :: mutex Singleton :: m_mutex ; Singleton * Singleton :: getInstance () { Singleton * tmp = m_instance . load (); if ( tmp == nullptr ) { std :: lock_guard < std :: mutex > lock ( m_mutex ); tmp = m_instance . load (); if ( tmp == nullptr ) { tmp = new Singleton ; m_instance . store ( tmp ); } } return tmp ; }","title":"Using C++11 Sequentially Consistent Atomics"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/14-Acquire-and-Release-Fences-Don%E2%80%99t-Work-the-Way-You%E2%80%99d-Expect/","text":"","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/15-The-Purpose-of-memory_order_consume-in-C%2B%2B11/","text":"preshing The Purpose of memory_order_consume in C++11","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/15-The-Purpose-of-memory_order_consume-in-C%2B%2B11/#preshing#the#purpose#of#memory_order_consume#in#c11","text":"","title":"preshing The Purpose of memory_order_consume in C++11"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/16-You-Can-Do-Any-Kind-of-Atomic-Read-Modify-Write-Operation/","text":"preshing You Can Do Any Kind of Atomic Read-Modify-Write Operation Atomic read-modify-write operations \u2013 or \u201cRMWs\u201d \u2013 are more sophisticated than atomic loads and stores . They let you read from a variable in shared memory and simultaneously write a different value in its place. In the C++11 atomic library, all of the following functions perform an RMW: std :: atomic <>:: fetch_add () std :: atomic <>:: fetch_sub () std :: atomic <>:: fetch_and () std :: atomic <>:: fetch_or () std :: atomic <>:: fetch_xor () std :: atomic <>:: exchange () std :: atomic <>:: compare_exchange_strong () std :: atomic <>:: compare_exchange_weak () fetch_add , for example, reads from a shared variable, adds another value to it, and writes the result back \u2013 all in one indivisible step. You can accomplish the same thing using a mutex, but a mutex-based version wouldn\u2019t be lock-free . RMW operations, on the other hand, are designed to be lock-free. They\u2019ll take advantage of lock-free CPU instructions whenever possible, such as ldrex / strex on ARMv7. A novice programmer might look at the above list of functions and ask, \u201cWhy does C++11 offer so few RMW operations? Why is there an atomic fetch_add , but no atomic fetch_multiply , no fetch_divide and no fetch_shift_left ?\u201d There are two reasons: 1\u3001Because there is very little need for those RMW operations in practice. Try not to get the wrong impression of how RMWs are used. You can\u2019t write safe multithreaded code by taking a single-threaded algorithm and turning each step into an RMW. 2\u3001Because if you do need those operations, you can easily implement them yourself. As the title says, you can do any kind of RMW operation! NOTE: \u5982\u4f55\u5b9e\u73b0? \u8fd9\u662f\u672c\u6587\u7684\u91cd\u70b9 Compare-and-Swap: The Mother of All RMWs NOTE: \u8fd9\u6bb5\u8bdd\u7684\u610f\u601d\u662f: \u53ef\u4ee5\u4f7f\u7528\"Compare-and-Swap\"\u6765\u751f\u6210\u6240\u6709\u7684\u5176\u4ed6\u7684RMW Out of all the available RMW operations in C++11, the only one that is absolutely essential is compare_exchange_weak . Every other RMW operation can be implemented using that one. It takes a minimum of two arguments: shared . compare_exchange_weak ( T & expected , T desired , ...); This function attempts to store the desired value to shared , but only if the current value of shared matches expected . It returns true if successful. If it fails, it loads the current value of shared back into expected , which despite its name, is an in/out parameter. This is called a compare-and-swap operation, and it all happens in one atomic, indivisible step. So, suppose you really need an atomic fetch_multiply operation, though I can\u2019t imagine why. Here\u2019s one way to implement it: uint32_t fetch_multiply ( std :: atomic < uint32_t >& shared , uint32_t multiplier ) { uint32_t oldValue = shared . load (); while ( ! shared . compare_exchange_weak ( oldValue , oldValue * multiplier )) { } return oldValue ; } Compare-and-swap loop, or CAS loop This is known as a compare-and-swap loop, or CAS loop . The function repeatedly tries to exchange oldValue with oldValue * multiplier until it succeeds. If no concurrent modifications happen in other threads, compare_exchange_weak will usually succeed on the first try. On the other hand, if shared is concurrently modified by another thread, it\u2019s totally possible for its value to change between the call to load and the call to compare_exchange_weak , causing the compare-and-swap operation to fail. In that case, oldValue will be updated with the most recent value of shared , and the loop will try again. NOTE: \u4e0a\u9762\u5206\u6790\u4e86\u4e24\u79cd\u53ef\u80fd\u7684\u60c5\u51b5\uff0c\u5728\u8fd9\u4e24\u79cd\u60c5\u51b5\u4e0b\uff0c fetch_multiply \u90fd\u4e0d\u4f1a\u9677\u5165dead loop Atomic and lock-free The above implementation of fetch_multiply is both atomic and lock-free. It\u2019s atomic even though the CAS loop may take an indeterminate number of tries, because when the loop finally does modify shared , it does so atomically. It\u2019s lock-free because if a single iteration of the CAS loop fails, it\u2019s usually because some other thread modified shared successfully. That last statement hinges on the assumption that compare_exchange_weak actually compiles to lock-free machine code \u2013 more on that below. It also ignores the fact that compare_exchange_weak can fail spuriously on certain platforms, but that\u2019s a rare event. You Can Combine Several Steps Into One RMW fetch_multiply just replaces the value of shared with a multiple of the same value. What if we want to perform a more elaborate kind of RMW? Can we still make the operation atomic and lock-free? Sure we can. To offer a somewhat convoluted(\u590d\u6742\u7684) example, here\u2019s a function that loads a shared variable, decrements the value if odd, divides it in half if even, and stores the result back only if it\u2019s greater than or equal to 10, all in a single atomic, lock-free operation: uint32_t atomicDecrementOrHalveWithLimit ( std :: atomic < uint32_t >& shared ) { uint32_t oldValue = shared . load (); uint32_t newValue ; do { if ( oldValue % 2 == 1 ) newValue = oldValue - 1 ; else newValue = oldValue / 2 ; if ( newValue < 10 ) break ; } while ( ! shared . compare_exchange_weak ( oldValue , newValue )); return oldValue ; } It\u2019s the same idea as before: If compare_exchange_weak fails \u2013 usually due to a modification performed by another thread \u2013 oldValue is updated with a more recent value, and the loop tries again. If, during any attempt, we find that newValue is less than 10, the CAS loop terminates early, effectively turning the RMW operation into a no-op. The point is that you can put anything inside the CAS loop. Think of the body of the CAS loop as a critical section. Normally, we protect a critical section using a mutex. With a CAS loop, we simply retry the entire transaction until it succeeds. This is obviously a synthetic(\u5408\u6210\u7684) example. A more practical example can be seen in the AutoResetEvent class described in my earlier post about semaphores . It uses a CAS loop with multiple steps to atomically increment a shared variable up to a limit of 1. You Can Combine Several Variables Into One RMW So far, we\u2019ve only looked at examples that perform an atomic operation on a single shared variable. What if we want to perform an atomic operation on multiple variables? Normally, we\u2019d protect those variables using a mutex: std :: mutex mutex ; uint32_t x ; uint32_t y ; void atomicFibonacciStep () { std :: lock_guard < std :: mutex > lock ( mutex ); int t = y ; y = x + y ; x = t ; } This mutex-based approach is atomic, but obviously not lock-free. That may very well be good enough , but for the sake of illustration, let\u2019s go ahead and convert it to a CAS loop just like the other examples. std::atomic<> is a template, so we can actually pack both shared variables into a struct and apply the same pattern as before: struct Terms { uint32_t x ; uint32_t y ; }; std :: atomic < Terms > terms ; void atomicFibonacciStep () { Terms oldTerms = terms . load (); Terms newTerms ; do { newTerms . x = oldTerms . y ; newTerms . y = oldTerms . x + oldTerms . y ; } while ( ! terms . compare_exchange_weak ( oldTerms , newTerms )); } Is this operation lock-free? Now we\u2019re venturing(\u5192\u9669) into dicey territory(\u9886\u571f). As I wrote at the start, C++11 atomic operations are designed take advantage of lock-free CPU instructions \u201cwhenever possible\u201d \u2013 admittedly(\u516c\u8ba4\u7684) a loose definition. In this case, we\u2019ve wrapped std::atomic<> around a struct, Terms . Let\u2019s see how GCC 4.9.2 compiles it for x64: NOTE: \u73b0\u5728\u6211\u4eec\u6b63\u5728\u5192\u9669\u8fdb\u5165\u4e00\u4e2a\u5371\u9669\u7684\u9886\u57df\u3002 We got lucky. The compiler was clever enough to see that Terms fits inside a single 64-bit register, and implemented compare_exchange_weak using lock cmpxchg . The compiled code is lock-free. This brings up an interesting point: In general, the C++11 standard does not guarantee that atomic operations will be lock-free. There are simply too many CPU architectures to support and too many ways to specialize the std::atomic<> template. You need to check with your compiler to make absolutely sure. In practice, though, it\u2019s pretty safe to assume that atomic operations are lock-free when all of the following conditions are true: 1\u3001The compiler is a recent version MSVC, GCC or Clang. 2\u3001The target processor is x86, x64 or ARMv7 (and possibly others). 3\u3001The atomic type is std::atomic<uint32_t> , std::atomic<uint64_t> or std::atomic<T*> for some type T . As a personal preference, I like to hang my hat on that third point, and limit myself to specializations of the std::atomic<> template that use explicit integer or pointer types. The safe bitfield technique I described in the previous post gives us a convenient way to rewrite the above function using an explicit integer specialization, std::atomic<uint64_t> : NOTE: \u4f5c\u4e3a\u4e2a\u4eba\u504f\u597d\uff0c\u6211\u559c\u6b22\u5f3a\u8c03\u7b2c\u4e09\u70b9\uff0c\u5e76\u5c06\u81ea\u5df1\u9650\u5236\u5728\u4f7f\u7528\u663e\u5f0f\u6574\u6570\u6216\u6307\u9488\u7c7b\u578b\u7684' std::atomic<> '\u6a21\u677f\u7684\u4e13\u95e8\u5316\u4e2d\u3002 BEGIN_BITFIELD_TYPE ( Terms , uint64_t ) ADD_BITFIELD_MEMBER ( x , 0 , 32 ) ADD_BITFIELD_MEMBER ( y , 32 , 32 ) END_BITFIELD_TYPE () std :: atomic < uint64_t > terms ; void atomicFibonacciStep () { Terms oldTerms = terms . load (); Terms newTerms ; do { newTerms . x = oldTerms . y ; newTerms . y = ( uint32_t ) ( oldTerms . x + oldTerms . y ); } while ( ! terms . compare_exchange_weak ( oldTerms , newTerms )); } Some real-world examples where we pack several values into an atomic bitfield include: 1\u3001Implementing tagged pointers as a workaround for the ABA problem . 2\u3001Implementing a lightweight read-write lock, which I touched upon briefly in a previous post . In general, any time you have a small amount of data protected by a mutex, and you can pack that data entirely into a 32- or 64-bit integer type, you can always convert your mutex-based operations into lock-free RMW operations, no matter what those operations actually do! That\u2019s the principle I exploited in my Semaphores are Surprisingly Versatile post, to implement a bunch of lightweight synchronization primitives. Of course, this technique is not unique to the C++11 atomic library. I\u2019m just using C++11 atomics because they\u2019re quite widely available now, and compiler support is pretty good. You can implement a custom RMW operation using any library that exposes a compare-and-swap function, such as Win32 , the Mach kernel API , the Linux kernel API , GCC atomic builtins or Mintomic . In the interest of brevity, I didn\u2019t discuss memory ordering concerns in this post, but it\u2019s critical to consider the guarantees made by your atomic library. In particular, if your custom RMW operation is intended to pass non-atomic information between threads, then at a minimum, you should ensure that there is the equivalent of a synchronizes-with relationship somewhere.","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/16-You-Can-Do-Any-Kind-of-Atomic-Read-Modify-Write-Operation/#preshing#you#can#do#any#kind#of#atomic#read-modify-write#operation","text":"Atomic read-modify-write operations \u2013 or \u201cRMWs\u201d \u2013 are more sophisticated than atomic loads and stores . They let you read from a variable in shared memory and simultaneously write a different value in its place. In the C++11 atomic library, all of the following functions perform an RMW: std :: atomic <>:: fetch_add () std :: atomic <>:: fetch_sub () std :: atomic <>:: fetch_and () std :: atomic <>:: fetch_or () std :: atomic <>:: fetch_xor () std :: atomic <>:: exchange () std :: atomic <>:: compare_exchange_strong () std :: atomic <>:: compare_exchange_weak () fetch_add , for example, reads from a shared variable, adds another value to it, and writes the result back \u2013 all in one indivisible step. You can accomplish the same thing using a mutex, but a mutex-based version wouldn\u2019t be lock-free . RMW operations, on the other hand, are designed to be lock-free. They\u2019ll take advantage of lock-free CPU instructions whenever possible, such as ldrex / strex on ARMv7. A novice programmer might look at the above list of functions and ask, \u201cWhy does C++11 offer so few RMW operations? Why is there an atomic fetch_add , but no atomic fetch_multiply , no fetch_divide and no fetch_shift_left ?\u201d There are two reasons: 1\u3001Because there is very little need for those RMW operations in practice. Try not to get the wrong impression of how RMWs are used. You can\u2019t write safe multithreaded code by taking a single-threaded algorithm and turning each step into an RMW. 2\u3001Because if you do need those operations, you can easily implement them yourself. As the title says, you can do any kind of RMW operation! NOTE: \u5982\u4f55\u5b9e\u73b0? \u8fd9\u662f\u672c\u6587\u7684\u91cd\u70b9","title":"preshing You Can Do Any Kind of Atomic Read-Modify-Write Operation"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/16-You-Can-Do-Any-Kind-of-Atomic-Read-Modify-Write-Operation/#compare-and-swap#the#mother#of#all#rmws","text":"NOTE: \u8fd9\u6bb5\u8bdd\u7684\u610f\u601d\u662f: \u53ef\u4ee5\u4f7f\u7528\"Compare-and-Swap\"\u6765\u751f\u6210\u6240\u6709\u7684\u5176\u4ed6\u7684RMW Out of all the available RMW operations in C++11, the only one that is absolutely essential is compare_exchange_weak . Every other RMW operation can be implemented using that one. It takes a minimum of two arguments: shared . compare_exchange_weak ( T & expected , T desired , ...); This function attempts to store the desired value to shared , but only if the current value of shared matches expected . It returns true if successful. If it fails, it loads the current value of shared back into expected , which despite its name, is an in/out parameter. This is called a compare-and-swap operation, and it all happens in one atomic, indivisible step. So, suppose you really need an atomic fetch_multiply operation, though I can\u2019t imagine why. Here\u2019s one way to implement it: uint32_t fetch_multiply ( std :: atomic < uint32_t >& shared , uint32_t multiplier ) { uint32_t oldValue = shared . load (); while ( ! shared . compare_exchange_weak ( oldValue , oldValue * multiplier )) { } return oldValue ; }","title":"Compare-and-Swap: The Mother of All RMWs"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/16-You-Can-Do-Any-Kind-of-Atomic-Read-Modify-Write-Operation/#compare-and-swap#loop#or#cas#loop","text":"This is known as a compare-and-swap loop, or CAS loop . The function repeatedly tries to exchange oldValue with oldValue * multiplier until it succeeds. If no concurrent modifications happen in other threads, compare_exchange_weak will usually succeed on the first try. On the other hand, if shared is concurrently modified by another thread, it\u2019s totally possible for its value to change between the call to load and the call to compare_exchange_weak , causing the compare-and-swap operation to fail. In that case, oldValue will be updated with the most recent value of shared , and the loop will try again. NOTE: \u4e0a\u9762\u5206\u6790\u4e86\u4e24\u79cd\u53ef\u80fd\u7684\u60c5\u51b5\uff0c\u5728\u8fd9\u4e24\u79cd\u60c5\u51b5\u4e0b\uff0c fetch_multiply \u90fd\u4e0d\u4f1a\u9677\u5165dead loop","title":"Compare-and-swap loop, or CAS loop"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/16-You-Can-Do-Any-Kind-of-Atomic-Read-Modify-Write-Operation/#atomic#and#lock-free","text":"The above implementation of fetch_multiply is both atomic and lock-free. It\u2019s atomic even though the CAS loop may take an indeterminate number of tries, because when the loop finally does modify shared , it does so atomically. It\u2019s lock-free because if a single iteration of the CAS loop fails, it\u2019s usually because some other thread modified shared successfully. That last statement hinges on the assumption that compare_exchange_weak actually compiles to lock-free machine code \u2013 more on that below. It also ignores the fact that compare_exchange_weak can fail spuriously on certain platforms, but that\u2019s a rare event.","title":"Atomic and lock-free"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/16-You-Can-Do-Any-Kind-of-Atomic-Read-Modify-Write-Operation/#you#can#combine#several#steps#into#one#rmw","text":"fetch_multiply just replaces the value of shared with a multiple of the same value. What if we want to perform a more elaborate kind of RMW? Can we still make the operation atomic and lock-free? Sure we can. To offer a somewhat convoluted(\u590d\u6742\u7684) example, here\u2019s a function that loads a shared variable, decrements the value if odd, divides it in half if even, and stores the result back only if it\u2019s greater than or equal to 10, all in a single atomic, lock-free operation: uint32_t atomicDecrementOrHalveWithLimit ( std :: atomic < uint32_t >& shared ) { uint32_t oldValue = shared . load (); uint32_t newValue ; do { if ( oldValue % 2 == 1 ) newValue = oldValue - 1 ; else newValue = oldValue / 2 ; if ( newValue < 10 ) break ; } while ( ! shared . compare_exchange_weak ( oldValue , newValue )); return oldValue ; } It\u2019s the same idea as before: If compare_exchange_weak fails \u2013 usually due to a modification performed by another thread \u2013 oldValue is updated with a more recent value, and the loop tries again. If, during any attempt, we find that newValue is less than 10, the CAS loop terminates early, effectively turning the RMW operation into a no-op. The point is that you can put anything inside the CAS loop. Think of the body of the CAS loop as a critical section. Normally, we protect a critical section using a mutex. With a CAS loop, we simply retry the entire transaction until it succeeds. This is obviously a synthetic(\u5408\u6210\u7684) example. A more practical example can be seen in the AutoResetEvent class described in my earlier post about semaphores . It uses a CAS loop with multiple steps to atomically increment a shared variable up to a limit of 1.","title":"You Can Combine Several Steps Into One RMW"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Lock-Free-Programming/16-You-Can-Do-Any-Kind-of-Atomic-Read-Modify-Write-Operation/#you#can#combine#several#variables#into#one#rmw","text":"So far, we\u2019ve only looked at examples that perform an atomic operation on a single shared variable. What if we want to perform an atomic operation on multiple variables? Normally, we\u2019d protect those variables using a mutex: std :: mutex mutex ; uint32_t x ; uint32_t y ; void atomicFibonacciStep () { std :: lock_guard < std :: mutex > lock ( mutex ); int t = y ; y = x + y ; x = t ; } This mutex-based approach is atomic, but obviously not lock-free. That may very well be good enough , but for the sake of illustration, let\u2019s go ahead and convert it to a CAS loop just like the other examples. std::atomic<> is a template, so we can actually pack both shared variables into a struct and apply the same pattern as before: struct Terms { uint32_t x ; uint32_t y ; }; std :: atomic < Terms > terms ; void atomicFibonacciStep () { Terms oldTerms = terms . load (); Terms newTerms ; do { newTerms . x = oldTerms . y ; newTerms . y = oldTerms . x + oldTerms . y ; } while ( ! terms . compare_exchange_weak ( oldTerms , newTerms )); } Is this operation lock-free? Now we\u2019re venturing(\u5192\u9669) into dicey territory(\u9886\u571f). As I wrote at the start, C++11 atomic operations are designed take advantage of lock-free CPU instructions \u201cwhenever possible\u201d \u2013 admittedly(\u516c\u8ba4\u7684) a loose definition. In this case, we\u2019ve wrapped std::atomic<> around a struct, Terms . Let\u2019s see how GCC 4.9.2 compiles it for x64: NOTE: \u73b0\u5728\u6211\u4eec\u6b63\u5728\u5192\u9669\u8fdb\u5165\u4e00\u4e2a\u5371\u9669\u7684\u9886\u57df\u3002 We got lucky. The compiler was clever enough to see that Terms fits inside a single 64-bit register, and implemented compare_exchange_weak using lock cmpxchg . The compiled code is lock-free. This brings up an interesting point: In general, the C++11 standard does not guarantee that atomic operations will be lock-free. There are simply too many CPU architectures to support and too many ways to specialize the std::atomic<> template. You need to check with your compiler to make absolutely sure. In practice, though, it\u2019s pretty safe to assume that atomic operations are lock-free when all of the following conditions are true: 1\u3001The compiler is a recent version MSVC, GCC or Clang. 2\u3001The target processor is x86, x64 or ARMv7 (and possibly others). 3\u3001The atomic type is std::atomic<uint32_t> , std::atomic<uint64_t> or std::atomic<T*> for some type T . As a personal preference, I like to hang my hat on that third point, and limit myself to specializations of the std::atomic<> template that use explicit integer or pointer types. The safe bitfield technique I described in the previous post gives us a convenient way to rewrite the above function using an explicit integer specialization, std::atomic<uint64_t> : NOTE: \u4f5c\u4e3a\u4e2a\u4eba\u504f\u597d\uff0c\u6211\u559c\u6b22\u5f3a\u8c03\u7b2c\u4e09\u70b9\uff0c\u5e76\u5c06\u81ea\u5df1\u9650\u5236\u5728\u4f7f\u7528\u663e\u5f0f\u6574\u6570\u6216\u6307\u9488\u7c7b\u578b\u7684' std::atomic<> '\u6a21\u677f\u7684\u4e13\u95e8\u5316\u4e2d\u3002 BEGIN_BITFIELD_TYPE ( Terms , uint64_t ) ADD_BITFIELD_MEMBER ( x , 0 , 32 ) ADD_BITFIELD_MEMBER ( y , 32 , 32 ) END_BITFIELD_TYPE () std :: atomic < uint64_t > terms ; void atomicFibonacciStep () { Terms oldTerms = terms . load (); Terms newTerms ; do { newTerms . x = oldTerms . y ; newTerms . y = ( uint32_t ) ( oldTerms . x + oldTerms . y ); } while ( ! terms . compare_exchange_weak ( oldTerms , newTerms )); } Some real-world examples where we pack several values into an atomic bitfield include: 1\u3001Implementing tagged pointers as a workaround for the ABA problem . 2\u3001Implementing a lightweight read-write lock, which I touched upon briefly in a previous post . In general, any time you have a small amount of data protected by a mutex, and you can pack that data entirely into a 32- or 64-bit integer type, you can always convert your mutex-based operations into lock-free RMW operations, no matter what those operations actually do! That\u2019s the principle I exploited in my Semaphores are Surprisingly Versatile post, to implement a bunch of lightweight synchronization primitives. Of course, this technique is not unique to the C++11 atomic library. I\u2019m just using C++11 atomics because they\u2019re quite widely available now, and compiler support is pretty good. You can implement a custom RMW operation using any library that exposes a compare-and-swap function, such as Win32 , the Mach kernel API , the Linux kernel API , GCC atomic builtins or Mintomic . In the interest of brevity, I didn\u2019t discuss memory ordering concerns in this post, but it\u2019s critical to consider the guarantees made by your atomic library. In particular, if your custom RMW operation is intended to pass non-atomic information between threads, then at a minimum, you should ensure that there is the equivalent of a synchronizes-with relationship somewhere.","title":"You Can Combine Several Variables Into One RMW"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Multithread/","text":"","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Multithread/01-Locks-Aren%27t-Slow-Lock-Contention-Is/","text":"preshing Locks Aren't Slow; Lock Contention Is NOTE: Locks (also known as mutexes ) have a history of being misjudged(\u88ab\u8bef\u5224). Back in 1986, in a Usenet discussion on multithreading, Matthew Dillon wrote , \u201cMost people have the misconception that locks are slow.\u201d 25 years later, this misconception still seems to pop up once in a while. It\u2019s true that locking is slow on some platforms, or when the lock is highly contended. And when you\u2019re developing a multithreaded application, it\u2019s very common to find a huge performance bottleneck caused by a single lock. But that doesn\u2019t mean all locks are slow. As I\u2019ll show in this post, sometimes a **locking strateg**y achieves excellent performance. NOTE: lock\u662f\u4e3a\u4e86\u5b9e\u73b0Mutual-exclusion\uff0c\u4e0d\u540c\u7684lock\uff0c\u91c7\u7528\u4e0d\u540c\u7684locking strategy\u3001\u8fd0\u884c\u673a\u5236\uff0c\u8fd9\u5bfc\u81f4\u5b83\u4eec\u7684performance\u4e5f\u4e0d\u540c\uff0cprogrammer\u9700\u8981\u6839\u636e\u81ea\u5df1\u7684\u9700\u6c42\u6765\u9009\u62e9lock; \u65e0\u8bba\u9009\u62e9\u54ea\u79cdlock\u65b9\u5f0f\uff0c\u5f53lock\u88ab\u5360\u7528\u7684\u65f6\u5019\uff0c\u5b83\u90fd\u9700\u8981wait\uff0c\u8fd9\u5c31\u662fLock Contention\uff0c\u5728\u540e\u9762\u7684\"Lock Contention Benchmark\"\u7ae0\u8282\u4e2d\uff0c\u7ed9\u51fa\u4e86definition\uff1b\u4e0d\u540c\u7684lock\uff0c\u5b9e\u73b0wait\u7684\u673a\u5236\u4e5f\u662f\u4e0d\u540c\u7684\uff1b\u663e\u7136\u5bfc\u81f4lock contention\u662fslow\u7684\u539f\u56e0\uff1b \u5982\u679c\u4f7f\u7528kernel mutex\uff0c\u5219\u5f53\u53d1\u751fLock Contention\u7684\u65f6\u5019\uff0cOS \u901a\u5e38\u4f1a\u5c06thread block\uff0c\u7136\u540e\u5f53\u53ef\u7528\u7684\u65f6\u5019\uff0c\u518d\u5c06\u5b83\u5524\u9192\uff0c\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u662f\u6bd4\u8f83\u7f13\u6162\u7684\uff1b \u56e0\u6b64\uff0c\u8fd9\u7bc7\u6587\u7ae0\u5176\u5b9e\u4e3b\u8981\u662f\u5728\u5bf9\u6bd4kernel mutex\u548clight weight mutex\uff1b Perhaps the most easily-overlooked source of this misconception: Not all programmers may be aware of the difference between a lightweight mutex and a \u201c kernel mutex \u201d. I\u2019ll talk about that in my next post, Always Use a Lightweight Mutex . For now, let\u2019s just say that if you\u2019re programming in C/C++ on Windows, the Critical Section object is the one you want. NOTE: light weight mutex\u548ckernel mutex\u662f\u4f5c\u8005\u4e3b\u8981\u8fdb\u884c\u5bf9\u6bd4\u7684\u5185\u5bb9 Other times, the conclusion that locks are slow is supported by a benchmark. For example, this post measures the performance of a lock under heavy conditions: each thread must hold the lock to do any work ( high contention ), and the lock is held for an extremely short interval of time ( high frequency ). It\u2019s a good read, but in a real application, you generally want to avoid using locks in that way. To put things in context, I\u2019ve devised a benchmark which includes both best-case and worst-case usage scenarios for locks. NOTE: \u5728high contention\u3001high frequency\u7684\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528lock\uff0c\u5219\u975e\u5e38\u5bb9\u6613\u53d1\u751flock contention\u3002 Locks may be frowned upon for other reasons. There\u2019s a whole other family of techniques out there known as lock-free (or lockless ) programming. Lock-free programming is extremely challenging, but delivers huge performance gains in a lot of real-world scenarios. I know programmers who spent days, even weeks fine-tuning a lock-free algorithm, subjecting it to a battery of tests, only to discover hidden timing bugs several months later. The combination of danger and reward can be very enticing to a certain kind of programmer \u2013 and this includes me, as you\u2019ll see in future posts ! With lock-free techniques beckoning us to use them, locks can begin to feel boring, slow and busted. NOTE: \u7b2c\u4e00\u53e5\u8bdd\"Locks may be frowned upon for other reasons.\"\u7684\u610f\u601d\u662f: lock\u6709\u8bf8\u591a\u7f3a\u70b9\uff0c\u56e0\u6b64\u51fa\u73b0\u4e86lockless programming\u3002\u867d\u7136lock\u6709\u7740\u8bf8\u591a\u7f3a\u70b9\uff0c\u4f46\u662f\u5e76\u4e0d\u662f\u8bf4\u5b83\u662f\u4e00\u65e0\u662f\u5904\u7684\uff0c\u5728\u4e0b\u4e00\u6bb5\u4e2d\uff0c\u4f5c\u8005\u7ed9\u51fa\u4e86\u9700\u8981\u4f7f\u7528lock\u7684\u60c5\u51b5 But don\u2019t disregard(\u5ffd\u89c6) locks yet. One good example of a place where locks perform admirably(\u6781\u597d\u7684), in real software, is when protecting the memory allocator. Doug Lea\u2019s Malloc is a popular memory allocator in video game development, but it\u2019s single threaded, so we need to protect it using a lock. During gameplay, it\u2019s not uncommon to see multiple threads hammering(\u6376\u6253\uff0c\u6b64\u5904\u5f15\u7533\u4e49: \u4f7f\u7528\u3001\u8c03\u7528) the memory allocator, say around 15000 times per second. While loading, this figure can climb to 100000 times per second or more. It\u2019s not a big problem, though. As you\u2019ll see, locks handle the workload like a champ. Lock Contention Benchmark In this test, we spawn a thread which generates random numbers, using a custom Mersenne Twister implementation. Every once in a while, it acquires and releases a lock. The lengths of time between acquiring and releasing the lock are random, but they tend towards average values which we decide ahead of time. For example, suppose we want to acquire the lock 15000 times per second, and keep it held 50% of the time. Here\u2019s what part of the timeline would look like. Red means the lock is held, grey means it\u2019s released: This is essentially a Poisson process(\u6cca\u677e\u8fc7\u7a0b). If we know the average amount of time to generate a single random number \u2013 6.349 ns on a 2.66 GHz quad-core Xeon \u2013 we can measure time in work units , rather than seconds. We can then use the technique described in my previous post, How to Generate Random Timings for a Poisson Process , to decide how many work units to perform between acquiring and releasing the lock. Here\u2019s the implementation in C++. I\u2019ve left out a few details, but if you like, you can download the complete source code here . NOTE: \u4f5c\u8005\u5ea6\u91cf\u65f6\u95f4\u7684\u5355\u4f4d\u662fwork unit\uff0c\u800c\u4e0d\u662fsecond\uff0c\u4e00\u4e2awork unit\u5bf9\u5e94\u7684\u662f\u4f7f\u7528Poisson process(\u6cca\u677e\u8fc7\u7a0b)\u751f\u6210\u4e00\u4e2arandom number\uff0c\u4f5c\u8005\u8ba4\u4e3awork unit\u7684\u8017\u65f6\u662f\u56fa\u5b9a\u7684\uff1b \u4f5c\u8005\u8fdb\u884cbenchmark\u7684\u601d\u8def\u662f\u8fd9\u6837\u7684: 1\u3001process\u7684\u8fd0\u884c\u65f6\u95f4\u56fa\u5b9a 2\u3001\u4e00\u4e2aprocess\u53ef\u4ee5\u7531\u591a\u4e2athread\u7ec4\u6210\uff0c\u6bcf\u4e2athread\u6267\u884c\u5982\u4e0bfunction\uff1b 3\u3001 workDone \u8bb0\u5f55\u4e86process\u662f\u8fd0\u884c\u65f6\u95f4\u5185\u5b8c\u6210\u7684work unit 4\u3001\u901a\u8fc7\u6bd4\u8f83work unit\u6765\u5bf9\u6bd4performance QueryPerformanceCounter ( & start ); for (;;) { // Do some work without holding the lock workunits = ( int ) ( random . poissonInterval ( averageUnlockedCount ) + 0.5f ); for ( int i = 1 ; i < workunits ; i ++ ) random . integer (); // Do one work unit workDone += workunits ; QueryPerformanceCounter ( & end ); elapsedTime = ( end . QuadPart - start . QuadPart ) * ooFreq ; if ( elapsedTime >= timeLimit ) break ; // Do some work while holding the lock EnterCriticalSection ( & criticalSection ); workunits = ( int ) ( random . poissonInterval ( averageLockedCount ) + 0.5f ); for ( int i = 1 ; i < workunits ; i ++ ) random . integer (); // Do one work unit workDone += workunits ; LeaveCriticalSection ( & criticalSection ); QueryPerformanceCounter ( & end ); elapsedTime = ( end . QuadPart - start . QuadPart ) * ooFreq ; if ( elapsedTime >= timeLimit ) break ; } Now suppose we launch two such threads, each running on a different core. Each thread will hold the lock during 50% of the time when it can perform work , but if one thread tries to acquire the lock while the other thread is holding it, it will be forced to wait. This is known as lock contention . NOTE: \u4ece\u4e0a\u8ff0\u4ee3\u7801\u53ef\u4ee5\u770b\u51fa\uff0c\u5b9e\u73b0 \"50%\" \u7684\u65b9\u6cd5\u662f\u975e\u5e38\u7b80\u5355\u7684: \u91cd\u590d\u6267\u884c\u4e24\u6b21\uff0c\u4e00\u6b21\u4e0dlock\uff0c\u4e00\u6b21lock In my opinion, this is a pretty good simulation of the way a lock might be used in a real application. When we run the above scenario, we find that each thread spends roughly 25% of its time waiting , and 75% of its time doing actual work. Together, both threads achieve a net performance of 1.5x compared to the single-threaded case. NOTE: **1.5x**\u662f\u8fd9\u6837\u7b97\u51fa\u6765\u7684: 75% * 2 = 1.5 I ran several variations of the test on a 2.66 GHz quad-core Xeon, from 1 thread, 2 threads, all the way up to 4 threads, each running on its own core. I also varied the duration of the lock, from the trivial case where the the lock is never held, all the way up to the maximum where each thread must hold the lock for 100% of its workload. In all cases, the lock frequency remained constant \u2013 threads acquired the lock 15000 times for each second of work performed. The results were interesting. For short lock durations, up to say 10%, the system achieved very high parallelism. Not perfect parallelism, but close. Locks are fast! To put the results in perspective, I analyzed the memory allocator lock in a multithreaded game engine using this profiler . During gameplay, with 15000 locks per second coming from 3 threads, the lock duration was in the neighborhood of just 2% . That\u2019s well within the comfort zone on the left side of the diagram. These results also show that once the lock duration passes 90%, there\u2019s no point using multiple threads anymore. A single thread performs better. Most surprising is the way the performance of 4 threads drops off a cliff around the 60% mark! This looked like an anomaly, so I re-ran the tests several additional times, even trying a different testing order. The same behavior happened consistently. My best hypothesis is that the experiment hits some kind of snag in the Windows scheduler, but I didn\u2019t investigate further. Lock Frequency Benchmark","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Multithread/01-Locks-Aren%27t-Slow-Lock-Contention-Is/#preshing#locks#arent#slow#lock#contention#is","text":"NOTE: Locks (also known as mutexes ) have a history of being misjudged(\u88ab\u8bef\u5224). Back in 1986, in a Usenet discussion on multithreading, Matthew Dillon wrote , \u201cMost people have the misconception that locks are slow.\u201d 25 years later, this misconception still seems to pop up once in a while. It\u2019s true that locking is slow on some platforms, or when the lock is highly contended. And when you\u2019re developing a multithreaded application, it\u2019s very common to find a huge performance bottleneck caused by a single lock. But that doesn\u2019t mean all locks are slow. As I\u2019ll show in this post, sometimes a **locking strateg**y achieves excellent performance. NOTE: lock\u662f\u4e3a\u4e86\u5b9e\u73b0Mutual-exclusion\uff0c\u4e0d\u540c\u7684lock\uff0c\u91c7\u7528\u4e0d\u540c\u7684locking strategy\u3001\u8fd0\u884c\u673a\u5236\uff0c\u8fd9\u5bfc\u81f4\u5b83\u4eec\u7684performance\u4e5f\u4e0d\u540c\uff0cprogrammer\u9700\u8981\u6839\u636e\u81ea\u5df1\u7684\u9700\u6c42\u6765\u9009\u62e9lock; \u65e0\u8bba\u9009\u62e9\u54ea\u79cdlock\u65b9\u5f0f\uff0c\u5f53lock\u88ab\u5360\u7528\u7684\u65f6\u5019\uff0c\u5b83\u90fd\u9700\u8981wait\uff0c\u8fd9\u5c31\u662fLock Contention\uff0c\u5728\u540e\u9762\u7684\"Lock Contention Benchmark\"\u7ae0\u8282\u4e2d\uff0c\u7ed9\u51fa\u4e86definition\uff1b\u4e0d\u540c\u7684lock\uff0c\u5b9e\u73b0wait\u7684\u673a\u5236\u4e5f\u662f\u4e0d\u540c\u7684\uff1b\u663e\u7136\u5bfc\u81f4lock contention\u662fslow\u7684\u539f\u56e0\uff1b \u5982\u679c\u4f7f\u7528kernel mutex\uff0c\u5219\u5f53\u53d1\u751fLock Contention\u7684\u65f6\u5019\uff0cOS \u901a\u5e38\u4f1a\u5c06thread block\uff0c\u7136\u540e\u5f53\u53ef\u7528\u7684\u65f6\u5019\uff0c\u518d\u5c06\u5b83\u5524\u9192\uff0c\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u662f\u6bd4\u8f83\u7f13\u6162\u7684\uff1b \u56e0\u6b64\uff0c\u8fd9\u7bc7\u6587\u7ae0\u5176\u5b9e\u4e3b\u8981\u662f\u5728\u5bf9\u6bd4kernel mutex\u548clight weight mutex\uff1b Perhaps the most easily-overlooked source of this misconception: Not all programmers may be aware of the difference between a lightweight mutex and a \u201c kernel mutex \u201d. I\u2019ll talk about that in my next post, Always Use a Lightweight Mutex . For now, let\u2019s just say that if you\u2019re programming in C/C++ on Windows, the Critical Section object is the one you want. NOTE: light weight mutex\u548ckernel mutex\u662f\u4f5c\u8005\u4e3b\u8981\u8fdb\u884c\u5bf9\u6bd4\u7684\u5185\u5bb9 Other times, the conclusion that locks are slow is supported by a benchmark. For example, this post measures the performance of a lock under heavy conditions: each thread must hold the lock to do any work ( high contention ), and the lock is held for an extremely short interval of time ( high frequency ). It\u2019s a good read, but in a real application, you generally want to avoid using locks in that way. To put things in context, I\u2019ve devised a benchmark which includes both best-case and worst-case usage scenarios for locks. NOTE: \u5728high contention\u3001high frequency\u7684\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528lock\uff0c\u5219\u975e\u5e38\u5bb9\u6613\u53d1\u751flock contention\u3002 Locks may be frowned upon for other reasons. There\u2019s a whole other family of techniques out there known as lock-free (or lockless ) programming. Lock-free programming is extremely challenging, but delivers huge performance gains in a lot of real-world scenarios. I know programmers who spent days, even weeks fine-tuning a lock-free algorithm, subjecting it to a battery of tests, only to discover hidden timing bugs several months later. The combination of danger and reward can be very enticing to a certain kind of programmer \u2013 and this includes me, as you\u2019ll see in future posts ! With lock-free techniques beckoning us to use them, locks can begin to feel boring, slow and busted. NOTE: \u7b2c\u4e00\u53e5\u8bdd\"Locks may be frowned upon for other reasons.\"\u7684\u610f\u601d\u662f: lock\u6709\u8bf8\u591a\u7f3a\u70b9\uff0c\u56e0\u6b64\u51fa\u73b0\u4e86lockless programming\u3002\u867d\u7136lock\u6709\u7740\u8bf8\u591a\u7f3a\u70b9\uff0c\u4f46\u662f\u5e76\u4e0d\u662f\u8bf4\u5b83\u662f\u4e00\u65e0\u662f\u5904\u7684\uff0c\u5728\u4e0b\u4e00\u6bb5\u4e2d\uff0c\u4f5c\u8005\u7ed9\u51fa\u4e86\u9700\u8981\u4f7f\u7528lock\u7684\u60c5\u51b5 But don\u2019t disregard(\u5ffd\u89c6) locks yet. One good example of a place where locks perform admirably(\u6781\u597d\u7684), in real software, is when protecting the memory allocator. Doug Lea\u2019s Malloc is a popular memory allocator in video game development, but it\u2019s single threaded, so we need to protect it using a lock. During gameplay, it\u2019s not uncommon to see multiple threads hammering(\u6376\u6253\uff0c\u6b64\u5904\u5f15\u7533\u4e49: \u4f7f\u7528\u3001\u8c03\u7528) the memory allocator, say around 15000 times per second. While loading, this figure can climb to 100000 times per second or more. It\u2019s not a big problem, though. As you\u2019ll see, locks handle the workload like a champ.","title":"preshing Locks Aren't Slow; Lock Contention Is"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Multithread/01-Locks-Aren%27t-Slow-Lock-Contention-Is/#lock#contention#benchmark","text":"In this test, we spawn a thread which generates random numbers, using a custom Mersenne Twister implementation. Every once in a while, it acquires and releases a lock. The lengths of time between acquiring and releasing the lock are random, but they tend towards average values which we decide ahead of time. For example, suppose we want to acquire the lock 15000 times per second, and keep it held 50% of the time. Here\u2019s what part of the timeline would look like. Red means the lock is held, grey means it\u2019s released: This is essentially a Poisson process(\u6cca\u677e\u8fc7\u7a0b). If we know the average amount of time to generate a single random number \u2013 6.349 ns on a 2.66 GHz quad-core Xeon \u2013 we can measure time in work units , rather than seconds. We can then use the technique described in my previous post, How to Generate Random Timings for a Poisson Process , to decide how many work units to perform between acquiring and releasing the lock. Here\u2019s the implementation in C++. I\u2019ve left out a few details, but if you like, you can download the complete source code here . NOTE: \u4f5c\u8005\u5ea6\u91cf\u65f6\u95f4\u7684\u5355\u4f4d\u662fwork unit\uff0c\u800c\u4e0d\u662fsecond\uff0c\u4e00\u4e2awork unit\u5bf9\u5e94\u7684\u662f\u4f7f\u7528Poisson process(\u6cca\u677e\u8fc7\u7a0b)\u751f\u6210\u4e00\u4e2arandom number\uff0c\u4f5c\u8005\u8ba4\u4e3awork unit\u7684\u8017\u65f6\u662f\u56fa\u5b9a\u7684\uff1b \u4f5c\u8005\u8fdb\u884cbenchmark\u7684\u601d\u8def\u662f\u8fd9\u6837\u7684: 1\u3001process\u7684\u8fd0\u884c\u65f6\u95f4\u56fa\u5b9a 2\u3001\u4e00\u4e2aprocess\u53ef\u4ee5\u7531\u591a\u4e2athread\u7ec4\u6210\uff0c\u6bcf\u4e2athread\u6267\u884c\u5982\u4e0bfunction\uff1b 3\u3001 workDone \u8bb0\u5f55\u4e86process\u662f\u8fd0\u884c\u65f6\u95f4\u5185\u5b8c\u6210\u7684work unit 4\u3001\u901a\u8fc7\u6bd4\u8f83work unit\u6765\u5bf9\u6bd4performance QueryPerformanceCounter ( & start ); for (;;) { // Do some work without holding the lock workunits = ( int ) ( random . poissonInterval ( averageUnlockedCount ) + 0.5f ); for ( int i = 1 ; i < workunits ; i ++ ) random . integer (); // Do one work unit workDone += workunits ; QueryPerformanceCounter ( & end ); elapsedTime = ( end . QuadPart - start . QuadPart ) * ooFreq ; if ( elapsedTime >= timeLimit ) break ; // Do some work while holding the lock EnterCriticalSection ( & criticalSection ); workunits = ( int ) ( random . poissonInterval ( averageLockedCount ) + 0.5f ); for ( int i = 1 ; i < workunits ; i ++ ) random . integer (); // Do one work unit workDone += workunits ; LeaveCriticalSection ( & criticalSection ); QueryPerformanceCounter ( & end ); elapsedTime = ( end . QuadPart - start . QuadPart ) * ooFreq ; if ( elapsedTime >= timeLimit ) break ; } Now suppose we launch two such threads, each running on a different core. Each thread will hold the lock during 50% of the time when it can perform work , but if one thread tries to acquire the lock while the other thread is holding it, it will be forced to wait. This is known as lock contention . NOTE: \u4ece\u4e0a\u8ff0\u4ee3\u7801\u53ef\u4ee5\u770b\u51fa\uff0c\u5b9e\u73b0 \"50%\" \u7684\u65b9\u6cd5\u662f\u975e\u5e38\u7b80\u5355\u7684: \u91cd\u590d\u6267\u884c\u4e24\u6b21\uff0c\u4e00\u6b21\u4e0dlock\uff0c\u4e00\u6b21lock In my opinion, this is a pretty good simulation of the way a lock might be used in a real application. When we run the above scenario, we find that each thread spends roughly 25% of its time waiting , and 75% of its time doing actual work. Together, both threads achieve a net performance of 1.5x compared to the single-threaded case. NOTE: **1.5x**\u662f\u8fd9\u6837\u7b97\u51fa\u6765\u7684: 75% * 2 = 1.5 I ran several variations of the test on a 2.66 GHz quad-core Xeon, from 1 thread, 2 threads, all the way up to 4 threads, each running on its own core. I also varied the duration of the lock, from the trivial case where the the lock is never held, all the way up to the maximum where each thread must hold the lock for 100% of its workload. In all cases, the lock frequency remained constant \u2013 threads acquired the lock 15000 times for each second of work performed. The results were interesting. For short lock durations, up to say 10%, the system achieved very high parallelism. Not perfect parallelism, but close. Locks are fast! To put the results in perspective, I analyzed the memory allocator lock in a multithreaded game engine using this profiler . During gameplay, with 15000 locks per second coming from 3 threads, the lock duration was in the neighborhood of just 2% . That\u2019s well within the comfort zone on the left side of the diagram. These results also show that once the lock duration passes 90%, there\u2019s no point using multiple threads anymore. A single thread performs better. Most surprising is the way the performance of 4 threads drops off a cliff around the 60% mark! This looked like an anomaly, so I re-ran the tests several additional times, even trying a different testing order. The same behavior happened consistently. My best hypothesis is that the experiment hits some kind of snag in the Windows scheduler, but I didn\u2019t investigate further.","title":"Lock Contention Benchmark"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Multithread/01-Locks-Aren%27t-Slow-Lock-Contention-Is/#lock#frequency#benchmark","text":"","title":"Lock Frequency Benchmark"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Multithread/02-Always-Use-a-Lightweight-Mutex/","text":"Always Use a Lightweight Mutex In multithreaded programming, we often speak of locks (also known as mutexes). But a lock is only a concept. To actually use that concept, you need an implementation. As it turns out, there are many ways to implement a lock, and those implementations vary wildly in performance. NOTE: \u8fd9\u4e2a\u89c2\u70b9\uff0c\u5728\u4e0a\u4e00\u7bc7\u7684note\u4e2d\u5df2\u7ecf\u603b\u7ed3\u4e86\uff0c\u8fd9\u4e00\u7bc7\uff0c\u4f5c\u8005\u5bf9\u8fd9\u4e2a\u89c2\u70b9\u8fdb\u884c\u9a8c\u8bc1\u3002\u4f5c\u8005\u4f7f\u7528\u7684\u662fWindows\u7684 The Windows SDK provides two lock implementations for C/C++: the Mutex and the Critical Section . (As Ned Batchelder points out , Critical Section is probably not the best name to give to the lock itself, but we\u2019ll forgive that here.) The Windows Critical Section is what we call a lightweight mutex . It\u2019s optimized for the case when there are no other threads competing for the lock. To demonstrate using a simple example, here\u2019s a single thread which locks and unlocks a Windows Mutex exactly one million times. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7684\"It\u2019s optimized for the case when there are no other threads competing for the lock\"\u7684\u610f\u601d\u662f: \u5f53\"no other threads competing for the lock\"\u65f6\uff0c Windows Critical Section \u88ab\u4f18\u5316\u4e86: \u5b83\u4e0d\u4f1a\u8fdb\u5165kernel space HANDLE mutex = CreateMutex ( NULL , FALSE , NULL ); for ( int i = 0 ; i < 1000000 ; i ++ ) { WaitForSingleObject ( mutex , INFINITE ); ReleaseMutex ( mutex ); } CloseHandle ( mutex ); Here\u2019s the same experiment using a Windows Critical Section. CRITICAL_SECTION critSec ; InitializeCriticalSection ( & critSec ); for ( int i = 0 ; i < 1000000 ; i ++ ) { EnterCriticalSection ( & critSec ); LeaveCriticalSection ( & critSec ); } DeleteCriticalSection ( & critSec ); If you insert some timing code around the inner loop, and divide the result by one million, you\u2019ll find the average time required for a pair of lock/unlock operations in both cases. I did that, and ran the experiment on two different processors. The results: The Critical Section is 25 times faster. As Larry Osterman explains , the Windows Mutex enters the kernel every time you use it, while the Critical Section does not. The tradeoff is that you can\u2019t share a Critical Section between processes. But who cares? Most of the time, you just want to protect some data within a single process. (It is actually possible to share a lightweight mutex between processes - just not using a Critical Section. See Roll Your Own Lightweight Mutex for example.) NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u89e3\u91ca\u4e86Windows critical section\u4f18\u4e8eWindows mutex\u7684\u539f\u56e0: **Windows Mutex**\u4e0d\u7ba1\u4ec0\u4e48\u60c5\u51b5\u90fd\u8fdb\u5165kernel\uff0c\u800cCritical Section \u91c7\u7528\u4e86\u66f4\u52a0\u4f18\u5316\u7684\u7b56\u7565\u3002 Other Platforms In MacOS 10.6.6, a lock implementation is provided using the POSIX Threads API. It\u2019s a lightweight mutex which doesn\u2019t enter the kernel unless there\u2019s contention. A pair of uncontended calls to pthread_mutex_lock and pthread_mutex_unlock takes about 92 ns on my 1.86 GHz Core 2 Duo. Interestingly, it detects when there\u2019s only one thread running, and in that case switches to a trivial codepath taking only 38 ns. Naturally, Ubuntu 11.10 provides a lock implementation using the POSIX Threads API as well. It\u2019s another lightweight mutex, based on a Linux-specific construct known as a futex . A pair of pthread_mutex_lock / pthread_mutex_unlock calls takes about 66 ns on my Core 2 Duo. You can even share this implementation between processes, but I didn\u2019t test that. Some of you old-timers may point out ancient platforms where a heavy lock was the only implementation available, or when a semaphore had to be used for the job. But it seems all modern platforms offer a lightweight mutex. And even if they didn\u2019t, you could write your own lightweight mutex at the application level, even sharing it between processes, provided you\u2019re willing to live with certain caveats. You\u2019ll find one example in my followup post, Roll Your Own Lightweight Mutex .","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Multithread/02-Always-Use-a-Lightweight-Mutex/#always#use#a#lightweight#mutex","text":"In multithreaded programming, we often speak of locks (also known as mutexes). But a lock is only a concept. To actually use that concept, you need an implementation. As it turns out, there are many ways to implement a lock, and those implementations vary wildly in performance. NOTE: \u8fd9\u4e2a\u89c2\u70b9\uff0c\u5728\u4e0a\u4e00\u7bc7\u7684note\u4e2d\u5df2\u7ecf\u603b\u7ed3\u4e86\uff0c\u8fd9\u4e00\u7bc7\uff0c\u4f5c\u8005\u5bf9\u8fd9\u4e2a\u89c2\u70b9\u8fdb\u884c\u9a8c\u8bc1\u3002\u4f5c\u8005\u4f7f\u7528\u7684\u662fWindows\u7684 The Windows SDK provides two lock implementations for C/C++: the Mutex and the Critical Section . (As Ned Batchelder points out , Critical Section is probably not the best name to give to the lock itself, but we\u2019ll forgive that here.) The Windows Critical Section is what we call a lightweight mutex . It\u2019s optimized for the case when there are no other threads competing for the lock. To demonstrate using a simple example, here\u2019s a single thread which locks and unlocks a Windows Mutex exactly one million times. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7684\"It\u2019s optimized for the case when there are no other threads competing for the lock\"\u7684\u610f\u601d\u662f: \u5f53\"no other threads competing for the lock\"\u65f6\uff0c Windows Critical Section \u88ab\u4f18\u5316\u4e86: \u5b83\u4e0d\u4f1a\u8fdb\u5165kernel space HANDLE mutex = CreateMutex ( NULL , FALSE , NULL ); for ( int i = 0 ; i < 1000000 ; i ++ ) { WaitForSingleObject ( mutex , INFINITE ); ReleaseMutex ( mutex ); } CloseHandle ( mutex ); Here\u2019s the same experiment using a Windows Critical Section. CRITICAL_SECTION critSec ; InitializeCriticalSection ( & critSec ); for ( int i = 0 ; i < 1000000 ; i ++ ) { EnterCriticalSection ( & critSec ); LeaveCriticalSection ( & critSec ); } DeleteCriticalSection ( & critSec ); If you insert some timing code around the inner loop, and divide the result by one million, you\u2019ll find the average time required for a pair of lock/unlock operations in both cases. I did that, and ran the experiment on two different processors. The results: The Critical Section is 25 times faster. As Larry Osterman explains , the Windows Mutex enters the kernel every time you use it, while the Critical Section does not. The tradeoff is that you can\u2019t share a Critical Section between processes. But who cares? Most of the time, you just want to protect some data within a single process. (It is actually possible to share a lightweight mutex between processes - just not using a Critical Section. See Roll Your Own Lightweight Mutex for example.) NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u89e3\u91ca\u4e86Windows critical section\u4f18\u4e8eWindows mutex\u7684\u539f\u56e0: **Windows Mutex**\u4e0d\u7ba1\u4ec0\u4e48\u60c5\u51b5\u90fd\u8fdb\u5165kernel\uff0c\u800cCritical Section \u91c7\u7528\u4e86\u66f4\u52a0\u4f18\u5316\u7684\u7b56\u7565\u3002","title":"Always Use a Lightweight Mutex"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Multithread/02-Always-Use-a-Lightweight-Mutex/#other#platforms","text":"In MacOS 10.6.6, a lock implementation is provided using the POSIX Threads API. It\u2019s a lightweight mutex which doesn\u2019t enter the kernel unless there\u2019s contention. A pair of uncontended calls to pthread_mutex_lock and pthread_mutex_unlock takes about 92 ns on my 1.86 GHz Core 2 Duo. Interestingly, it detects when there\u2019s only one thread running, and in that case switches to a trivial codepath taking only 38 ns. Naturally, Ubuntu 11.10 provides a lock implementation using the POSIX Threads API as well. It\u2019s another lightweight mutex, based on a Linux-specific construct known as a futex . A pair of pthread_mutex_lock / pthread_mutex_unlock calls takes about 66 ns on my Core 2 Duo. You can even share this implementation between processes, but I didn\u2019t test that. Some of you old-timers may point out ancient platforms where a heavy lock was the only implementation available, or when a semaphore had to be used for the job. But it seems all modern platforms offer a lightweight mutex. And even if they didn\u2019t, you could write your own lightweight mutex at the application level, even sharing it between processes, provided you\u2019re willing to live with certain caveats. You\u2019ll find one example in my followup post, Roll Your Own Lightweight Mutex .","title":"Other Platforms"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Multithread/03-A-C%2B%2BProfiling-Module-for-Multithreaded-APIs/","text":"preshing A C++ Profiling Module for Multithreaded APIs NOTE: \u8fd9\u7bc7\u6587\u7ae0\u91cd\u8981\u8ba8\u8bbaWindows system\uff0c\u6682\u65f6\u4e0d\u8003\u8651\u9605\u8bfb","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Multithread/03-A-C%2B%2BProfiling-Module-for-Multithreaded-APIs/#preshing#a#c#profiling#module#for#multithreaded#apis","text":"NOTE: \u8fd9\u7bc7\u6587\u7ae0\u91cd\u8981\u8ba8\u8bbaWindows system\uff0c\u6682\u65f6\u4e0d\u8003\u8651\u9605\u8bfb","title":"preshing A C++ Profiling Module for Multithreaded APIs"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Multithread/04-A-Look-Back-at-Single-Threaded-CPU-Performance/","text":"A Look Back at Single-Threaded CPU Performance NOTE: \u4fc3\u4f7f\u6211\u6536\u5f55\u8fd9\u7bc7\u6587\u7ae0\u7684\u539f\u56e0\u662f: \u5b83\u5bf9multicore parallel computing\u53d1\u5c55\u8d8b\u52bf\u7684\u603b\u7ed3\uff0c\u5728\u5de5\u7a0bhardware\u7684 CPU\\Parallel-computing \u7ae0\u8282\u4e2d\u8ba8\u8bba\u4e86\u8fd9\u4e2a\u4e3b\u9898\u3002 Perhaps the turning point came in May 2004, when Intel canceled its latest single-core development effort to focus on multicore designs. Later that year, Herb Sutter wrote his now-famous article, The Free Lunch Is Over . Not all software will run remarkably faster year-over-year anymore, he warned us. Concurrent software would continue its meteoric(\u6781\u901f\u7684) rise, but single-threaded software was about to get left in the dust.","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Multithread/04-A-Look-Back-at-Single-Threaded-CPU-Performance/#a#look#back#at#single-threaded#cpu#performance","text":"NOTE: \u4fc3\u4f7f\u6211\u6536\u5f55\u8fd9\u7bc7\u6587\u7ae0\u7684\u539f\u56e0\u662f: \u5b83\u5bf9multicore parallel computing\u53d1\u5c55\u8d8b\u52bf\u7684\u603b\u7ed3\uff0c\u5728\u5de5\u7a0bhardware\u7684 CPU\\Parallel-computing \u7ae0\u8282\u4e2d\u8ba8\u8bba\u4e86\u8fd9\u4e2a\u4e3b\u9898\u3002 Perhaps the turning point came in May 2004, when Intel canceled its latest single-core development effort to focus on multicore designs. Later that year, Herb Sutter wrote his now-famous article, The Free Lunch Is Over . Not all software will run remarkably faster year-over-year anymore, he warned us. Concurrent software would continue its meteoric(\u6781\u901f\u7684) rise, but single-threaded software was about to get left in the dust.","title":"A Look Back at Single-Threaded CPU Performance"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Multithread/05-Roll-Your-Own-Lightweight-Mutex/","text":"preshing Roll Your Own Lightweight Mutex In an earlier post, I pointed out the importance of using a lightweight mutex . I also mentioned it was possible to write your own, provided you can live with certain limitations. Why would you do such a thing? Well, in the past, some platforms (like BeOS) didn\u2019t provide a lightweight mutex in the native API. Today, that\u2019s not really a concern. I\u2019m mainly showing this because it\u2019s an interesting look at implementing synchronization primitives in general. As a curiosity, it just so happens this implementation shaves almost 50% off the overhead of the Windows Critical Section in the uncontended case. [ Update: A closer look shows that under very high contention, a Windows Critical Section still performs much better. ] For the record, there are numerous ways to write your own mutex \u2013 or lock \u2013 entirely in user space, each with its own tradeoffs: 1\u3001 Spin locks . These employ a busy-wait strategy which has the potential to waste CPU time, and in the worst case, can lead to livelock when competing threads run on the same core. Still, some programmers have found measurable speed improvements switching to spin locks in certain cases. 2\u3001 Peterson\u2019s algorithm is like a spinlock for two threads. A neat trick, but seems useless on today\u2019s platforms. I find it noteworthy because Bartosz Milewski used this algorithm as a case study while discussing the finer points of the x86 memory model . Others have discussed Peterson\u2019s lock in similar contexts . 3\u3001Charles Bloom has a long writeup describing various mutex implementations . Excellent information, but possibly greek to anyone unfamiliar with C++11\u2019s atomics library and Relacy \u2019s ($) notation.","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Multithread/05-Roll-Your-Own-Lightweight-Mutex/#preshing#roll#your#own#lightweight#mutex","text":"In an earlier post, I pointed out the importance of using a lightweight mutex . I also mentioned it was possible to write your own, provided you can live with certain limitations. Why would you do such a thing? Well, in the past, some platforms (like BeOS) didn\u2019t provide a lightweight mutex in the native API. Today, that\u2019s not really a concern. I\u2019m mainly showing this because it\u2019s an interesting look at implementing synchronization primitives in general. As a curiosity, it just so happens this implementation shaves almost 50% off the overhead of the Windows Critical Section in the uncontended case. [ Update: A closer look shows that under very high contention, a Windows Critical Section still performs much better. ] For the record, there are numerous ways to write your own mutex \u2013 or lock \u2013 entirely in user space, each with its own tradeoffs: 1\u3001 Spin locks . These employ a busy-wait strategy which has the potential to waste CPU time, and in the worst case, can lead to livelock when competing threads run on the same core. Still, some programmers have found measurable speed improvements switching to spin locks in certain cases. 2\u3001 Peterson\u2019s algorithm is like a spinlock for two threads. A neat trick, but seems useless on today\u2019s platforms. I find it noteworthy because Bartosz Milewski used this algorithm as a case study while discussing the finer points of the x86 memory model . Others have discussed Peterson\u2019s lock in similar contexts . 3\u3001Charles Bloom has a long writeup describing various mutex implementations . Excellent information, but possibly greek to anyone unfamiliar with C++11\u2019s atomics library and Relacy \u2019s ($) notation.","title":"preshing Roll Your Own Lightweight Mutex"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Multithread/05-Roll-Your-Own-Lightweight-Mutex/Benaphore/","text":"Benaphore \u5728\u9605\u8bfb preshing Roll Your Own Lightweight Mutex \u65f6\uff0c\u53d1\u73b0\u7684\u3002 haiku-os Be Engineering Insights: Benaphores stackoverflow Are \u201cbenaphores\u201d worth implementing on modern OS's? Implementation 1\u3001preshing Roll Your Own Lightweight Mutex 2\u3001 preshing / cpp11-on-multicore","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Multithread/05-Roll-Your-Own-Lightweight-Mutex/Benaphore/#benaphore","text":"\u5728\u9605\u8bfb preshing Roll Your Own Lightweight Mutex \u65f6\uff0c\u53d1\u73b0\u7684\u3002","title":"Benaphore"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Multithread/05-Roll-Your-Own-Lightweight-Mutex/Benaphore/#haiku-os#be#engineering#insights#benaphores","text":"","title":"haiku-os Be Engineering Insights: Benaphores"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Multithread/05-Roll-Your-Own-Lightweight-Mutex/Benaphore/#stackoverflow#are#benaphores#worth#implementing#on#modern#oss","text":"","title":"stackoverflow Are \u201cbenaphores\u201d worth implementing on modern OS's?"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Multithread/05-Roll-Your-Own-Lightweight-Mutex/Benaphore/#implementation","text":"1\u3001preshing Roll Your Own Lightweight Mutex 2\u3001 preshing / cpp11-on-multicore","title":"Implementation"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Multithread/06-Implementing-a-Recursive-Mutex/","text":"preshing Implementing a Recursive Mutex NOTE: \u8bb2\u8ff0\u5982\u4f55\u5b9e\u73b0\u4e00\u4e2arecursive mutex\uff0c\u5176\u4e2d\u63cf\u8ff0\u7684\u4f7f\u7528recursive mutex\u7684\u573a\u666f\u662f\u503c\u5f97\u501f\u9274\u7684","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Multithread/06-Implementing-a-Recursive-Mutex/#preshing#implementing#a#recursive#mutex","text":"NOTE: \u8bb2\u8ff0\u5982\u4f55\u5b9e\u73b0\u4e00\u4e2arecursive mutex\uff0c\u5176\u4e2d\u63cf\u8ff0\u7684\u4f7f\u7528recursive mutex\u7684\u573a\u666f\u662f\u503c\u5f97\u501f\u9274\u7684","title":"preshing Implementing a Recursive Mutex"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Multithread/07-Lightweight%20In-Memory%20Logging/","text":"preshing Lightweight In-Memory Logging NOTE: \u8fd9\u7bc7\u6587\u7ae0\u63cf\u8ff0\u7684debug multithread\u7684\u65b9\u5f0f\u662f\u975e\u5e38\u503c\u5f97\u501f\u9274\u7684 When debugging multithreaded code, it\u2019s not always easy to determine which codepath was taken. You can\u2019t always reproduce the bug while stepping through the debugger, nor can you always sprinkle(\u6492\u3001\u6563\u64ad) printf s throughout the code, as you might in a single-threaded program. There might be millions of events before the bug occurs, and printf can easily slow the application to a crawl, mask the bug, or create a spam fest in the output log. One way of attacking such problems is to instrument(\u88c5\u914d) the code so that events are logged to a circular buffer in memory. This is similar to adding printf s, except that only the most recent events are kept in the log, and the performance overhead can be made very low using lock-free techniques. NOTE: lockless circular buffer #include <windows.h> #include <intrin.h> namespace Logger { struct Event { DWORD tid ; // Thread ID const char * msg ; // Message string DWORD param ; // A parameter which can mean anything you want }; static const int BUFFER_SIZE = 65536 ; // Must be a power of 2 extern Event g_events [ BUFFER_SIZE ]; extern LONG g_pos ; inline void Log ( const char * msg , DWORD param ) { // Get next event index LONG index = _InterlockedIncrement ( & g_pos ); // Write an event at this index Event * e = g_events + ( index & ( BUFFER_SIZE - 1 )); // Wrap to buffer size e -> tid = (( DWORD * ) __readfsdword ( 24 ))[ 9 ]; // Get thread ID e -> msg = msg ; e -> param = param ; } } #define LOG(m, p) Logger::Log(m, p) NOTE: \u4e00\u4e2a\u95ee\u9898: why static const int BUFFER_SIZE = 65536; // Must be a power of 2 \uff1f \u89e3\u7b54\u5982\u4e0b: 1\u300165536\u662f 2^{16} 2^{16} \uff0c\u6b63\u597d\u662f\"a power of 2\" 2\u3001\u5b83\u5176\u5b9e\u6267\u884c\u7684\u662fModulo operation: index \\% BUFFER\\_SIZE index \\% BUFFER\\_SIZE \uff0c\u8fd9\u662f\u4f7f\u7528\u7684\u4e00\u4e2aoptimization technique\uff0c\u5728 wikipedia Modulo operation#Performance issues \u4e2d\u7ed9\u51fa\u4e86\u89e3\u7b54 And you must place the following in a .cpp file. namespace Logger { Event g_events [ BUFFER_SIZE ]; LONG g_pos = -1 ; } This is perhaps one of the simplest examples of lock-free programming which actually does something useful. There\u2019s a single macro LOG , which writes to the log. It uses _InterlockedIncrement , an atomic operation which I\u2019ve talked about in previous posts , for thread safety. There are no readers. You are meant to be the reader when you inspect the process in the debugger, such as when the program crashes, or when the bug is otherwise caught. NOTE: \u4e0a\u8ff0\u4ee3\u7801\u7684lock-free programming\u7684\u5b9e\u73b0\u662f\u975e\u5e38\u503c\u5f97\u501f\u9274\u7684\uff0c\u5b83\u6709\u5982\u4e0b\u7279\u6027: 1\u3001memory pool/buffer: g_events \uff0c\u663e\u7136\u5b83\u662fproallocation\u7684 2\u3001\u591a\u4e2athread\u540c\u65f6\u4f7f\u7528shared data g_events \uff0c\u7531\u4e8e\u6bcf\u4e2athread\u90fd\u4f1athread-safe\u5730\u83b7\u5f97\u4e00\u4e2a\u72ec\u7acb\u7684 index \uff0c\u56e0\u6b64\u5728 e \u4e0a\u4e0d\u5b58\u5728race\uff0c\u56e0\u6b64\u5b83\u662f\u5141\u8bb8\u591a\u4e2athread\u540c\u65f6\u4f7f\u7528 g_events 3\u3001\u591a\u4e2athread\u540c\u65f6\u4f7f\u7528 g_pos \u662f\u5b58\u5728race condition\u7684\uff0c\u56e0\u6b64\u5b83\u4f7f\u7528 _InterlockedIncrement 4\u3001\u4f7f\u7528multiple model\u7684read and write\u6765\u8fdb\u884c\u5206\u6790: multiple writer\uff0c\u6ca1\u6709reader\uff0c\u56e0\u6b64\u5c31\u65e0\u9700\u8fdb\u884clock Using It to Debug My Previous Post NOTE: \u7565 What Makes This Lightweight? NOTE: \u7565 Implementation preshing / cpp11-on-multicore / common / inmemorylogger.h","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Multithread/07-Lightweight%20In-Memory%20Logging/#preshing#lightweight#in-memory#logging","text":"NOTE: \u8fd9\u7bc7\u6587\u7ae0\u63cf\u8ff0\u7684debug multithread\u7684\u65b9\u5f0f\u662f\u975e\u5e38\u503c\u5f97\u501f\u9274\u7684 When debugging multithreaded code, it\u2019s not always easy to determine which codepath was taken. You can\u2019t always reproduce the bug while stepping through the debugger, nor can you always sprinkle(\u6492\u3001\u6563\u64ad) printf s throughout the code, as you might in a single-threaded program. There might be millions of events before the bug occurs, and printf can easily slow the application to a crawl, mask the bug, or create a spam fest in the output log. One way of attacking such problems is to instrument(\u88c5\u914d) the code so that events are logged to a circular buffer in memory. This is similar to adding printf s, except that only the most recent events are kept in the log, and the performance overhead can be made very low using lock-free techniques. NOTE: lockless circular buffer #include <windows.h> #include <intrin.h> namespace Logger { struct Event { DWORD tid ; // Thread ID const char * msg ; // Message string DWORD param ; // A parameter which can mean anything you want }; static const int BUFFER_SIZE = 65536 ; // Must be a power of 2 extern Event g_events [ BUFFER_SIZE ]; extern LONG g_pos ; inline void Log ( const char * msg , DWORD param ) { // Get next event index LONG index = _InterlockedIncrement ( & g_pos ); // Write an event at this index Event * e = g_events + ( index & ( BUFFER_SIZE - 1 )); // Wrap to buffer size e -> tid = (( DWORD * ) __readfsdword ( 24 ))[ 9 ]; // Get thread ID e -> msg = msg ; e -> param = param ; } } #define LOG(m, p) Logger::Log(m, p) NOTE: \u4e00\u4e2a\u95ee\u9898: why static const int BUFFER_SIZE = 65536; // Must be a power of 2 \uff1f \u89e3\u7b54\u5982\u4e0b: 1\u300165536\u662f 2^{16} 2^{16} \uff0c\u6b63\u597d\u662f\"a power of 2\" 2\u3001\u5b83\u5176\u5b9e\u6267\u884c\u7684\u662fModulo operation: index \\% BUFFER\\_SIZE index \\% BUFFER\\_SIZE \uff0c\u8fd9\u662f\u4f7f\u7528\u7684\u4e00\u4e2aoptimization technique\uff0c\u5728 wikipedia Modulo operation#Performance issues \u4e2d\u7ed9\u51fa\u4e86\u89e3\u7b54 And you must place the following in a .cpp file. namespace Logger { Event g_events [ BUFFER_SIZE ]; LONG g_pos = -1 ; } This is perhaps one of the simplest examples of lock-free programming which actually does something useful. There\u2019s a single macro LOG , which writes to the log. It uses _InterlockedIncrement , an atomic operation which I\u2019ve talked about in previous posts , for thread safety. There are no readers. You are meant to be the reader when you inspect the process in the debugger, such as when the program crashes, or when the bug is otherwise caught. NOTE: \u4e0a\u8ff0\u4ee3\u7801\u7684lock-free programming\u7684\u5b9e\u73b0\u662f\u975e\u5e38\u503c\u5f97\u501f\u9274\u7684\uff0c\u5b83\u6709\u5982\u4e0b\u7279\u6027: 1\u3001memory pool/buffer: g_events \uff0c\u663e\u7136\u5b83\u662fproallocation\u7684 2\u3001\u591a\u4e2athread\u540c\u65f6\u4f7f\u7528shared data g_events \uff0c\u7531\u4e8e\u6bcf\u4e2athread\u90fd\u4f1athread-safe\u5730\u83b7\u5f97\u4e00\u4e2a\u72ec\u7acb\u7684 index \uff0c\u56e0\u6b64\u5728 e \u4e0a\u4e0d\u5b58\u5728race\uff0c\u56e0\u6b64\u5b83\u662f\u5141\u8bb8\u591a\u4e2athread\u540c\u65f6\u4f7f\u7528 g_events 3\u3001\u591a\u4e2athread\u540c\u65f6\u4f7f\u7528 g_pos \u662f\u5b58\u5728race condition\u7684\uff0c\u56e0\u6b64\u5b83\u4f7f\u7528 _InterlockedIncrement 4\u3001\u4f7f\u7528multiple model\u7684read and write\u6765\u8fdb\u884c\u5206\u6790: multiple writer\uff0c\u6ca1\u6709reader\uff0c\u56e0\u6b64\u5c31\u65e0\u9700\u8fdb\u884clock","title":"preshing Lightweight In-Memory Logging"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Multithread/07-Lightweight%20In-Memory%20Logging/#using#it#to#debug#my#previous#post","text":"NOTE: \u7565","title":"Using It to Debug My Previous Post"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Multithread/07-Lightweight%20In-Memory%20Logging/#what#makes#this#lightweight","text":"NOTE: \u7565","title":"What Makes This Lightweight?"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Multithread/07-Lightweight%20In-Memory%20Logging/#implementation","text":"preshing / cpp11-on-multicore / common / inmemorylogger.h","title":"Implementation"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Performance/The-Cost-of-Enabling-Exception-Handling/","text":"preshing The Cost of Enabling Exception Handling NOTE: \u8fd9\u7bc7\u6587\u7ae0\u8ba9\u6211\u60f3\u5230\u4e86C++ nonthrow","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/Performance/The-Cost-of-Enabling-Exception-Handling/#preshing#the#cost#of#enabling#exception#handling","text":"NOTE: \u8fd9\u7bc7\u6587\u7ae0\u8ba9\u6211\u60f3\u5230\u4e86C++ nonthrow","title":"preshing The Cost of Enabling Exception Handling"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/TODO-Preshing-Safe-Bitfields-in-C%2B%2B/","text":"preshing Safe Bitfields in C++","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/TODO-Preshing-Safe-Bitfields-in-C%2B%2B/#preshing#safe#bitfields#in#c","text":"","title":"preshing Safe Bitfields in C++"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/TODO-Preshing-Semaphores-are-Surprisingly-Versatile/","text":"preshing Semaphores are Surprisingly Versatile In multithreaded programming, it\u2019s important to make threads wait. They must wait for exclusive access to a resource. They must wait when there\u2019s no work available. One way to make threads wait \u2013 and put them to sleep inside the kernel, so that they no longer take any CPU time \u2013 is with a semaphore . NOTE: \u5178\u578b\u7684wait-notify Implement Lightweight primitives NOTE: \u8fd9\u662f\u5178\u578b\u7684: avoid-system-call-lightweight-userspace \u4f18\u5316\u601d\u8def My opinion changed once I realized that, using only semaphores and atomic operations, it\u2019s possible to implement all of the following primitives: 1\u3001A Lightweight Mutex 2\u3001A Lightweight Auto-Reset Event Object 3\u3001A Lightweight Read-Write Lock 4\u3001Another Solution to the Dining Philosophers Problem 5\u3001A Lightweight Semaphore With Partial Spinning Not only that, but these implementations share some desirable properties. They\u2019re lightweight , in the sense that some operations happen entirely in userspace, and they can (optionally) spin for a short period before sleeping in the kernel. You\u2019ll find all of the C++11 source code on GitHub . Since the standard C++11 library does not include semaphores, I\u2019ve also provided a portable Semaphore class that maps directly to native semaphores on Windows, MacOS, iOS, Linux and other POSIX environments. You should be able to drop any of these primitives into almost any existing C++11 project. A Semaphore Is Like a Bouncer NOTE: \"Bouncer\"\u4fdd\u9556 Imagine a set of waiting threads, lined up in a queue \u2013 much like a lineup(\u4e00\u961f\u4eba) in front of a busy nightclub or theatre. A semaphore is like a bouncer at the front of the lineup. He only allows threads to proceed when instructed to do so. Each thread decides for itself when to join the queue. Dijkstra called this the P operation. P originally stood for some funny-sounding Dutch word, but in a modern semaphore implementation, you\u2019re more likely to see this operation called wait . Basically, when a thread calls the semaphore\u2019s wait operation, it enters the lineup. signal The bouncer, himself, only needs to understand a single instruction. Originally, Dijkstra called this the V operation. Nowadays, the operation goes by various names, such as post , release or signal . I prefer signal . Any running thread can call signal at any time, and when it does, the bouncer releases exactly one waiting thread from the queue. (Not necessarily in the same order they arrived.) Now, what happens if some thread calls signal before there are any threads waiting in line? No problem: As soon as the next thread arrives in the lineup, the bouncer will let it pass directly through. And if signal is called, say, 3 times on an empty lineup, the bouncer will let the next 3 threads to arrive pass directly through. Of course, the bouncer needs to keep track of this number, which is why all semaphores maintain an integer counter . signal increments the counter, and wait decrements it. The beauty of this strategy is that if wait is called some number of times, and signal is called some number of times, the outcome is always the same: The bouncer will always release the same number of threads, and there will always be the same number of threads left waiting in line, regardless of the order in which those wait and signal calls occurred. 1. A Lightweight Mutex I\u2019ve already shown how to implement a lightweight mutex in an earlier post . I didn\u2019t know it at the time, but that post was just one example of a reusable pattern. The trick is to build another mechanism in front of the semaphore, which I like to call the box office . The box office is where the real decisions are made. Should the current thread wait in line? Should it bypass the queue entirely? Should another thread be released from the queue? The box office cannot directly check how many threads are waiting on the semaphore, nor can it check the semaphore\u2019s current signal count. Instead, the box office must somehow keep track of its own previous decisions. In the case of a lightweight mutex, all it needs is an atomic counter. I\u2019ll call this counter m_contention , since it keeps track of how many threads are simultaneously contending for the mutex. class LightweightMutex { private : std :: atomic < int > m_contention ; // The \"box office\" Semaphore m_semaphore ; // The \"bouncer\" lock() When a thread decides to lock the mutex, it first visits the box office to increment m_contention . public : void lock () { if ( m_contention . fetch_add ( 1 , std :: memory_order_acquire ) > 0 ) // Visit the box office { m_semaphore . wait (); // Enter the wait queue } } If the previous value was 0, that means no other thread has contended for the mutex yet. As such, the current thread immediately considers itself the new owner, bypasses the semaphore, returns from lock and proceeds into whatever code the mutex is intended to protect. Otherwise, if the previous value was greater than 0, that means another thread is already considered to own the mutex. In that case, the current thread must wait in line for its turn. unlock() When the previous thread unlocks the mutex, it visits the box office to decrement the counter: void unlock () { if ( m_contention . fetch_sub ( 1 , std :: memory_order_release ) > 1 ) // Visit the box office { m_semaphore . signal (); // Release a waiting thread from the queue } } If the previous counter value was 1, that means no other threads arrived in the meantime, so there\u2019s nothing else to do. m_contention is simply left at 0. Otherwise, if the previous counter value was greater than 1, another thread has attempted to lock the mutex, and is therefore waiting in the queue. As such, we alert the bouncer that it\u2019s now safe to release the next thread. That thread will be considered the new owner. Every visit to the box office is an indivisible, atomic operation. Therefore, even if multiple threads call lock and unlock concurrently, they will always visit the box office one at a time. Furthermore, the behavior of the mutex is completely determined by the decisions made at the box office. After they visit the box office, they may operate on the semaphore in an unpredictable order, but that\u2019s OK. As I\u2019ve already explained, the outcome will remain valid regardless of the order in which those semaphore operations occur. (In the worst case, some threads may trade places in line.) lightweight This class is considered \u201clightweight\u201d because it bypasses(\u7ed5\u8fc7) the semaphore when there\u2019s no contention, thereby avoiding system calls. I\u2019ve published it to GitHub as NonRecursiveBenaphore along with a recursive version . However, there\u2019s no need to use these classes in practice. Most available mutex implementations are already lightweight . Nonetheless(\u5c3d\u7ba1\u5982\u6b64), they\u2019re noteworthy for serving as inspiration for the rest of the primitives described here. 2. A Lightweight Auto-Reset Event Object You don\u2019t hear autoreset event objects discussed very often, but as I mentioned in my CppCon 2014 talk , they\u2019re widely used in game engines. Most often, they\u2019re used to notify a single other thread (possibly sleeping) of available work. NOTE: \u901a\u8fc7\u8c03\u7528 signal \u65b9\u6cd5\u6765\u5b9e\u73b0notify\u3002\u9700\u8981\u6ce8\u610f\u7684\u662f: 1\u3001\u53ea\u6709\u4e00\u4e2aconsumer thread\uff1f\u4e0d\u662f\u7684\uff0c\u662f\u53ef\u4ee5\u6709\u591a\u4e2aconsumer\u7684 Ignores redundant signals An autoreset event object is basically a semaphore that ignores redundant signals. In other words, when signal is called multiple times, the event object\u2019s signal count will never exceed 1. That means you can go ahead and publish work units somewhere, blindly calling signal after each one. It\u2019s a flexible technique that works even when you publish work units to some data structure other than a queue. NOTE: \"An autoreset event object is basically a semaphore that ignores redundant signals.\"\u89e3\u91ca\u4e86\"autoreset event object\"\u7684\u672c\u8d28 Windows has native support for event objects, but its SetEvent function \u2013 the equivalent of signal \u2013 can be expensive. One one machine, I timed it at 700 ns per call, even when the event was already signaled. If you\u2019re publishing thousands of work units between threads, the overhead for each SetEvent can quickly add up. box office/bouncer pattern Luckily, the box office/bouncer pattern reduces this overhead significantly. All of the autoreset event logic can be implemented at the box office using atomic operations, and the box office will invoke the semaphore only when it\u2019s absolutely necessary for threads to wait. I\u2019ve published the implementation as AutoResetEvent . This time, the box office has a different way to keep track of how many threads have been sent to wait in the queue. When m_status is negative, its magnitude(\u91cf\u7ea7\u3001\u5927\u5c0f) indicates how many threads are waiting: class AutoResetEvent { private : // m_status == 1: Event object is signaled. // m_status == 0: Event object is reset and no threads are waiting. // m_status == -N: Event object is reset and N threads are waiting. std :: atomic < int > m_status ; Semaphore m_sema ; NOTE: \u4e00\u4e2aobject\u53ea\u6709\u662f\u5426\u6709signal status: a\u3001\u6709signal b\u3001\u6ca1\u6709signal \u56e0\u6b64\u5b83\u5e76\u4e0d\u5173\u6ce8signal\u7684\u4e2a\u6570\uff0c\u5b83\u53ea\u5173\u6ce8\u662f\u5426\u6709signal\uff1b\u56e0\u6b64 m_status == 1 \u8868\u793a\u6709signal\uff0c\u5176\u4ed6\u503c\uff0c\u8868\u793a\u6ca1\u6709signal\uff1b 1\u3001reset\u8981\u5982\u4f55\u7406\u89e3\uff1f \u5f53\u6ca1\u6709signal\u7684\u65f6\u5019\uff0c\u5c31\u662f\"Event object is reset\" \u901a\u8fc7\u548c\u524d\u9762\u7684\"1. A Lightweight Mutex\"\u7684\u5b9e\u73b0\u5bf9\u6bd4\u53ef\u4ee5\u53d1\u73b0\uff0c\u5b83\u4eec\u7684\u5b9e\u73b0\uff0c\u5176\u5b9e\u975e\u5e38\u7c7b\u4f3c\uff1b signal operation \u901a\u8fc7etednal operation, we increment m_status` atomically, up to the limit of 1: public : void signal () { int oldStatus = m_status . load ( std :: memory_order_relaxed ); for (;;) // Increment m_status atomically via CAS loop. { assert ( oldStatus <= 1 ); int newStatus = oldStatus < 1 ? oldStatus + 1 : 1 ; if ( m_status . compare_exchange_weak ( oldStatus , newStatus , std :: memory_order_release , std :: memory_order_relaxed )) break ; // The compare-exchange failed, likely because another thread changed m_status. // oldStatus has been updated. Retry the CAS loop. } if ( oldStatus < 0 ) m_sema . signal (); // Release one waiting thread. } NOTE: 1\u3001 if (oldStatus < 0) \u8868\u793a\u6709thread\u5728waiting 2\u3001\u5f53\u6ca1\u6709thread waiting\u7684\u65f6\u5019\uff0c\u5219\u4e0d\u8c03\u7528 m_sema.signal(); 3\u3001\u4fdd\u8bc1 m_status \u4e0d\u4f1a\u8d85\u8fc71 Note that because the initial load from m_status is relaxed, it\u2019s important for the above code to call compare_exchange_weak even if m_status already equals 1. Thanks to commenter Tobias Br\u00fcll for pointing that out. See this README file for more information. 3. A Lightweight Read-Write Lock Using the same box office/bouncer pattern, it\u2019s possible to implement a pretty good read-write lock . This read-write lock is completely lock-free in the absence of writers , it\u2019s starvation-free for both readers and writers, and just like the other primitives, it can spin before putting threads to sleep. It requires two semaphores: one for waiting readers, and another for waiting writers. The code is available as NonRecursiveRWLock . 4. Another Solution to the Dining Philosophers Problem The box office/bouncer pattern can also solve Dijkstra\u2019s dining philosophers problem in a way that I haven\u2019t seen described elsewhere. If you\u2019re not familiar with this problem, it involves philosophers that share dinner forks with each other. Each philosopher must obtain two specific forks before he or she can eat. I don\u2019t believe this solution will prove useful to anybody, so I won\u2019t go into great detail. I\u2019m just including it as further demonstration of semaphores\u2019 versatility. In this solution, we assign each philosopher (thread) its own dedicated semaphore. The box office keeps track of which philosophers are eating, which ones have requested to eat, and the order in which those requests arrived. With that information, the box office is able to shepherd(\u5e26\u9886\uff0c\u62a4\u9001\uff1b\u6307\u5bfc\uff1b\u770b\u7ba1) all philosophers through their bouncers in an optimal way. I\u2019ve posted two implementations. One is DiningPhilosophers , which implements the box office using a mutex. The other is LockReducedDiningPhilosophers , in which every visit to the box office is lock-free. 5. A Lightweight Semaphore with Partial Spinning You read that right: It\u2019s possible to combine a semaphore with a box office to implement\u2026 another semaphore. Why would you do such a thing? Because you end up with a LightweightSemaphore . It becomes extremely cheap when the lineup is empty and the signal count climbs above zero, regardless of how the underlying semaphore is implemented. In such cases, the box office will rely entirely on atomic operations, leaving the underlying semaphore untouched. Not only that, but you can make threads wait in a spin loop for a short period of time before invoking the underlying semaphore. This trick helps avoid expensive system calls when the wait time ends up being short. In the GitHub repository , all of the other primitives are implemented on top of LightweightSemaphore , rather than using Semaphore directly. That\u2019s how they all inherit the ability to partially spin. LightweightSemaphore sits on top of Semaphore , which in turn encapsulates a platform-specific semaphore. The repository comes with a simple test suite, with each test case exercising a different primitive. It\u2019s possible to remove LightweightSemaphore and force all primitives to use Semaphore directly. Here are the resulting timings on my Windows PC: LightweightSemaphore Semaphore testBenaphore 375 ms 5503 ms testRecursiveBenaphore 393 ms 404 ms testAutoResetEvent 593 ms 4665 ms testRWLock 598 ms 7126 ms testDiningPhilosophers 309 ms 580 ms As you can see, the test suite benefits significantly from LightweightSemaphore in this environment. Having said that, I\u2019m pretty sure the current spinning strategy is not optimal for every environment. It simply spins a fixed number of 10000 times before falling back on Semaphore . I looked briefly into adaptive spinning, but the best approach wasn\u2019t obvious. Any suggestions? Comparison With Condition Variables With all of these applications, semaphores are more general-purpose than I originally thought \u2013 and this wasn\u2019t even a complete list. So why are semaphores absent from the standard C++11 library? For the same reason they\u2019re absent from Boost: a preference for mutexes and condition variables . From the library maintainers\u2019 point of view, conventional semaphore techniques are just too error prone . When you think about it, though, the box office/bouncer pattern shown here is really just an optimization for condition variables in a specific case \u2013 the case where all condition variable operations are performed at the end of the critical section. Consider the AutoResetEvent class described above. I\u2019ve implemented AutoResetEventCondVar , an equivalent class based on a condition variable, in the same repository. Its condition variable is always manipulated at the end of the critical section. void AutoResetEventCondVar :: signal () { // Increment m_status atomically via critical section. std :: unique_lock < std :: mutex > lock ( m_mutex ); int oldStatus = m_status ; if ( oldStatus == 1 ) return ; // Event object is already signaled. m_status ++ ; if ( oldStatus < 0 ) m_condition . notify_one (); // Release one waiting thread. } We can optimize AutoResetEventCondVar in two steps: 1\u3001Pull each condition variable outside of its critical section and convert it to a semaphore. The order-independence of semaphore operations makes this safe. After this step, we\u2019ve already implemented the box office/bouncer pattern. (In general, this step also lets us avoid a thundering herd when multiple threads are signaled at once.) 2\u3001Make the box office lock-free by converting all operations to CAS loops , greatly improving its scalability. This step results in AutoResetEvent . On my Windows PC, using AutoResetEvent in place of AutoResetEventCondVar makes the associated test case run 10x faster.","title":"Introduction"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/TODO-Preshing-Semaphores-are-Surprisingly-Versatile/#preshing#semaphores#are#surprisingly#versatile","text":"In multithreaded programming, it\u2019s important to make threads wait. They must wait for exclusive access to a resource. They must wait when there\u2019s no work available. One way to make threads wait \u2013 and put them to sleep inside the kernel, so that they no longer take any CPU time \u2013 is with a semaphore . NOTE: \u5178\u578b\u7684wait-notify","title":"preshing Semaphores are Surprisingly Versatile"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/TODO-Preshing-Semaphores-are-Surprisingly-Versatile/#implement#lightweight#primitives","text":"NOTE: \u8fd9\u662f\u5178\u578b\u7684: avoid-system-call-lightweight-userspace \u4f18\u5316\u601d\u8def My opinion changed once I realized that, using only semaphores and atomic operations, it\u2019s possible to implement all of the following primitives: 1\u3001A Lightweight Mutex 2\u3001A Lightweight Auto-Reset Event Object 3\u3001A Lightweight Read-Write Lock 4\u3001Another Solution to the Dining Philosophers Problem 5\u3001A Lightweight Semaphore With Partial Spinning Not only that, but these implementations share some desirable properties. They\u2019re lightweight , in the sense that some operations happen entirely in userspace, and they can (optionally) spin for a short period before sleeping in the kernel. You\u2019ll find all of the C++11 source code on GitHub . Since the standard C++11 library does not include semaphores, I\u2019ve also provided a portable Semaphore class that maps directly to native semaphores on Windows, MacOS, iOS, Linux and other POSIX environments. You should be able to drop any of these primitives into almost any existing C++11 project.","title":"Implement Lightweight primitives"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/TODO-Preshing-Semaphores-are-Surprisingly-Versatile/#a#semaphore#is#like#a#bouncer","text":"NOTE: \"Bouncer\"\u4fdd\u9556 Imagine a set of waiting threads, lined up in a queue \u2013 much like a lineup(\u4e00\u961f\u4eba) in front of a busy nightclub or theatre. A semaphore is like a bouncer at the front of the lineup. He only allows threads to proceed when instructed to do so. Each thread decides for itself when to join the queue. Dijkstra called this the P operation. P originally stood for some funny-sounding Dutch word, but in a modern semaphore implementation, you\u2019re more likely to see this operation called wait . Basically, when a thread calls the semaphore\u2019s wait operation, it enters the lineup.","title":"A Semaphore Is Like a Bouncer"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/TODO-Preshing-Semaphores-are-Surprisingly-Versatile/#signal","text":"The bouncer, himself, only needs to understand a single instruction. Originally, Dijkstra called this the V operation. Nowadays, the operation goes by various names, such as post , release or signal . I prefer signal . Any running thread can call signal at any time, and when it does, the bouncer releases exactly one waiting thread from the queue. (Not necessarily in the same order they arrived.) Now, what happens if some thread calls signal before there are any threads waiting in line? No problem: As soon as the next thread arrives in the lineup, the bouncer will let it pass directly through. And if signal is called, say, 3 times on an empty lineup, the bouncer will let the next 3 threads to arrive pass directly through. Of course, the bouncer needs to keep track of this number, which is why all semaphores maintain an integer counter . signal increments the counter, and wait decrements it. The beauty of this strategy is that if wait is called some number of times, and signal is called some number of times, the outcome is always the same: The bouncer will always release the same number of threads, and there will always be the same number of threads left waiting in line, regardless of the order in which those wait and signal calls occurred.","title":"signal"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/TODO-Preshing-Semaphores-are-Surprisingly-Versatile/#1#a#lightweight#mutex","text":"I\u2019ve already shown how to implement a lightweight mutex in an earlier post . I didn\u2019t know it at the time, but that post was just one example of a reusable pattern. The trick is to build another mechanism in front of the semaphore, which I like to call the box office . The box office is where the real decisions are made. Should the current thread wait in line? Should it bypass the queue entirely? Should another thread be released from the queue? The box office cannot directly check how many threads are waiting on the semaphore, nor can it check the semaphore\u2019s current signal count. Instead, the box office must somehow keep track of its own previous decisions. In the case of a lightweight mutex, all it needs is an atomic counter. I\u2019ll call this counter m_contention , since it keeps track of how many threads are simultaneously contending for the mutex. class LightweightMutex { private : std :: atomic < int > m_contention ; // The \"box office\" Semaphore m_semaphore ; // The \"bouncer\"","title":"1. A Lightweight Mutex"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/TODO-Preshing-Semaphores-are-Surprisingly-Versatile/#lock","text":"When a thread decides to lock the mutex, it first visits the box office to increment m_contention . public : void lock () { if ( m_contention . fetch_add ( 1 , std :: memory_order_acquire ) > 0 ) // Visit the box office { m_semaphore . wait (); // Enter the wait queue } } If the previous value was 0, that means no other thread has contended for the mutex yet. As such, the current thread immediately considers itself the new owner, bypasses the semaphore, returns from lock and proceeds into whatever code the mutex is intended to protect. Otherwise, if the previous value was greater than 0, that means another thread is already considered to own the mutex. In that case, the current thread must wait in line for its turn.","title":"lock()"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/TODO-Preshing-Semaphores-are-Surprisingly-Versatile/#unlock","text":"When the previous thread unlocks the mutex, it visits the box office to decrement the counter: void unlock () { if ( m_contention . fetch_sub ( 1 , std :: memory_order_release ) > 1 ) // Visit the box office { m_semaphore . signal (); // Release a waiting thread from the queue } } If the previous counter value was 1, that means no other threads arrived in the meantime, so there\u2019s nothing else to do. m_contention is simply left at 0. Otherwise, if the previous counter value was greater than 1, another thread has attempted to lock the mutex, and is therefore waiting in the queue. As such, we alert the bouncer that it\u2019s now safe to release the next thread. That thread will be considered the new owner. Every visit to the box office is an indivisible, atomic operation. Therefore, even if multiple threads call lock and unlock concurrently, they will always visit the box office one at a time. Furthermore, the behavior of the mutex is completely determined by the decisions made at the box office. After they visit the box office, they may operate on the semaphore in an unpredictable order, but that\u2019s OK. As I\u2019ve already explained, the outcome will remain valid regardless of the order in which those semaphore operations occur. (In the worst case, some threads may trade places in line.)","title":"unlock()"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/TODO-Preshing-Semaphores-are-Surprisingly-Versatile/#lightweight","text":"This class is considered \u201clightweight\u201d because it bypasses(\u7ed5\u8fc7) the semaphore when there\u2019s no contention, thereby avoiding system calls. I\u2019ve published it to GitHub as NonRecursiveBenaphore along with a recursive version . However, there\u2019s no need to use these classes in practice. Most available mutex implementations are already lightweight . Nonetheless(\u5c3d\u7ba1\u5982\u6b64), they\u2019re noteworthy for serving as inspiration for the rest of the primitives described here.","title":"lightweight"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/TODO-Preshing-Semaphores-are-Surprisingly-Versatile/#2#a#lightweight#auto-reset#event#object","text":"You don\u2019t hear autoreset event objects discussed very often, but as I mentioned in my CppCon 2014 talk , they\u2019re widely used in game engines. Most often, they\u2019re used to notify a single other thread (possibly sleeping) of available work. NOTE: \u901a\u8fc7\u8c03\u7528 signal \u65b9\u6cd5\u6765\u5b9e\u73b0notify\u3002\u9700\u8981\u6ce8\u610f\u7684\u662f: 1\u3001\u53ea\u6709\u4e00\u4e2aconsumer thread\uff1f\u4e0d\u662f\u7684\uff0c\u662f\u53ef\u4ee5\u6709\u591a\u4e2aconsumer\u7684","title":"2. A Lightweight Auto-Reset Event Object"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/TODO-Preshing-Semaphores-are-Surprisingly-Versatile/#ignores#redundant#signals","text":"An autoreset event object is basically a semaphore that ignores redundant signals. In other words, when signal is called multiple times, the event object\u2019s signal count will never exceed 1. That means you can go ahead and publish work units somewhere, blindly calling signal after each one. It\u2019s a flexible technique that works even when you publish work units to some data structure other than a queue. NOTE: \"An autoreset event object is basically a semaphore that ignores redundant signals.\"\u89e3\u91ca\u4e86\"autoreset event object\"\u7684\u672c\u8d28 Windows has native support for event objects, but its SetEvent function \u2013 the equivalent of signal \u2013 can be expensive. One one machine, I timed it at 700 ns per call, even when the event was already signaled. If you\u2019re publishing thousands of work units between threads, the overhead for each SetEvent can quickly add up.","title":"Ignores redundant signals"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/TODO-Preshing-Semaphores-are-Surprisingly-Versatile/#box#officebouncer#pattern","text":"Luckily, the box office/bouncer pattern reduces this overhead significantly. All of the autoreset event logic can be implemented at the box office using atomic operations, and the box office will invoke the semaphore only when it\u2019s absolutely necessary for threads to wait. I\u2019ve published the implementation as AutoResetEvent . This time, the box office has a different way to keep track of how many threads have been sent to wait in the queue. When m_status is negative, its magnitude(\u91cf\u7ea7\u3001\u5927\u5c0f) indicates how many threads are waiting: class AutoResetEvent { private : // m_status == 1: Event object is signaled. // m_status == 0: Event object is reset and no threads are waiting. // m_status == -N: Event object is reset and N threads are waiting. std :: atomic < int > m_status ; Semaphore m_sema ; NOTE: \u4e00\u4e2aobject\u53ea\u6709\u662f\u5426\u6709signal status: a\u3001\u6709signal b\u3001\u6ca1\u6709signal \u56e0\u6b64\u5b83\u5e76\u4e0d\u5173\u6ce8signal\u7684\u4e2a\u6570\uff0c\u5b83\u53ea\u5173\u6ce8\u662f\u5426\u6709signal\uff1b\u56e0\u6b64 m_status == 1 \u8868\u793a\u6709signal\uff0c\u5176\u4ed6\u503c\uff0c\u8868\u793a\u6ca1\u6709signal\uff1b 1\u3001reset\u8981\u5982\u4f55\u7406\u89e3\uff1f \u5f53\u6ca1\u6709signal\u7684\u65f6\u5019\uff0c\u5c31\u662f\"Event object is reset\" \u901a\u8fc7\u548c\u524d\u9762\u7684\"1. A Lightweight Mutex\"\u7684\u5b9e\u73b0\u5bf9\u6bd4\u53ef\u4ee5\u53d1\u73b0\uff0c\u5b83\u4eec\u7684\u5b9e\u73b0\uff0c\u5176\u5b9e\u975e\u5e38\u7c7b\u4f3c\uff1b","title":"box office/bouncer pattern"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/TODO-Preshing-Semaphores-are-Surprisingly-Versatile/#signal#operation","text":"\u901a\u8fc7etednal operation, we increment m_status` atomically, up to the limit of 1: public : void signal () { int oldStatus = m_status . load ( std :: memory_order_relaxed ); for (;;) // Increment m_status atomically via CAS loop. { assert ( oldStatus <= 1 ); int newStatus = oldStatus < 1 ? oldStatus + 1 : 1 ; if ( m_status . compare_exchange_weak ( oldStatus , newStatus , std :: memory_order_release , std :: memory_order_relaxed )) break ; // The compare-exchange failed, likely because another thread changed m_status. // oldStatus has been updated. Retry the CAS loop. } if ( oldStatus < 0 ) m_sema . signal (); // Release one waiting thread. } NOTE: 1\u3001 if (oldStatus < 0) \u8868\u793a\u6709thread\u5728waiting 2\u3001\u5f53\u6ca1\u6709thread waiting\u7684\u65f6\u5019\uff0c\u5219\u4e0d\u8c03\u7528 m_sema.signal(); 3\u3001\u4fdd\u8bc1 m_status \u4e0d\u4f1a\u8d85\u8fc71 Note that because the initial load from m_status is relaxed, it\u2019s important for the above code to call compare_exchange_weak even if m_status already equals 1. Thanks to commenter Tobias Br\u00fcll for pointing that out. See this README file for more information.","title":"signal operation"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/TODO-Preshing-Semaphores-are-Surprisingly-Versatile/#3#a#lightweight#read-write#lock","text":"Using the same box office/bouncer pattern, it\u2019s possible to implement a pretty good read-write lock . This read-write lock is completely lock-free in the absence of writers , it\u2019s starvation-free for both readers and writers, and just like the other primitives, it can spin before putting threads to sleep. It requires two semaphores: one for waiting readers, and another for waiting writers. The code is available as NonRecursiveRWLock .","title":"3. A Lightweight Read-Write Lock"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/TODO-Preshing-Semaphores-are-Surprisingly-Versatile/#4#another#solution#to#the#dining#philosophers#problem","text":"The box office/bouncer pattern can also solve Dijkstra\u2019s dining philosophers problem in a way that I haven\u2019t seen described elsewhere. If you\u2019re not familiar with this problem, it involves philosophers that share dinner forks with each other. Each philosopher must obtain two specific forks before he or she can eat. I don\u2019t believe this solution will prove useful to anybody, so I won\u2019t go into great detail. I\u2019m just including it as further demonstration of semaphores\u2019 versatility. In this solution, we assign each philosopher (thread) its own dedicated semaphore. The box office keeps track of which philosophers are eating, which ones have requested to eat, and the order in which those requests arrived. With that information, the box office is able to shepherd(\u5e26\u9886\uff0c\u62a4\u9001\uff1b\u6307\u5bfc\uff1b\u770b\u7ba1) all philosophers through their bouncers in an optimal way. I\u2019ve posted two implementations. One is DiningPhilosophers , which implements the box office using a mutex. The other is LockReducedDiningPhilosophers , in which every visit to the box office is lock-free.","title":"4. Another Solution to the Dining Philosophers Problem"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/TODO-Preshing-Semaphores-are-Surprisingly-Versatile/#5#a#lightweight#semaphore#with#partial#spinning","text":"You read that right: It\u2019s possible to combine a semaphore with a box office to implement\u2026 another semaphore. Why would you do such a thing? Because you end up with a LightweightSemaphore . It becomes extremely cheap when the lineup is empty and the signal count climbs above zero, regardless of how the underlying semaphore is implemented. In such cases, the box office will rely entirely on atomic operations, leaving the underlying semaphore untouched. Not only that, but you can make threads wait in a spin loop for a short period of time before invoking the underlying semaphore. This trick helps avoid expensive system calls when the wait time ends up being short. In the GitHub repository , all of the other primitives are implemented on top of LightweightSemaphore , rather than using Semaphore directly. That\u2019s how they all inherit the ability to partially spin. LightweightSemaphore sits on top of Semaphore , which in turn encapsulates a platform-specific semaphore. The repository comes with a simple test suite, with each test case exercising a different primitive. It\u2019s possible to remove LightweightSemaphore and force all primitives to use Semaphore directly. Here are the resulting timings on my Windows PC: LightweightSemaphore Semaphore testBenaphore 375 ms 5503 ms testRecursiveBenaphore 393 ms 404 ms testAutoResetEvent 593 ms 4665 ms testRWLock 598 ms 7126 ms testDiningPhilosophers 309 ms 580 ms As you can see, the test suite benefits significantly from LightweightSemaphore in this environment. Having said that, I\u2019m pretty sure the current spinning strategy is not optimal for every environment. It simply spins a fixed number of 10000 times before falling back on Semaphore . I looked briefly into adaptive spinning, but the best approach wasn\u2019t obvious. Any suggestions?","title":"5. A Lightweight Semaphore with Partial Spinning"},{"location":"Concurrent-computing/Expert-Jeff-Preshing/TODO-Preshing-Semaphores-are-Surprisingly-Versatile/#comparison#with#condition#variables","text":"With all of these applications, semaphores are more general-purpose than I originally thought \u2013 and this wasn\u2019t even a complete list. So why are semaphores absent from the standard C++11 library? For the same reason they\u2019re absent from Boost: a preference for mutexes and condition variables . From the library maintainers\u2019 point of view, conventional semaphore techniques are just too error prone . When you think about it, though, the box office/bouncer pattern shown here is really just an optimization for condition variables in a specific case \u2013 the case where all condition variable operations are performed at the end of the critical section. Consider the AutoResetEvent class described above. I\u2019ve implemented AutoResetEventCondVar , an equivalent class based on a condition variable, in the same repository. Its condition variable is always manipulated at the end of the critical section. void AutoResetEventCondVar :: signal () { // Increment m_status atomically via critical section. std :: unique_lock < std :: mutex > lock ( m_mutex ); int oldStatus = m_status ; if ( oldStatus == 1 ) return ; // Event object is already signaled. m_status ++ ; if ( oldStatus < 0 ) m_condition . notify_one (); // Release one waiting thread. } We can optimize AutoResetEventCondVar in two steps: 1\u3001Pull each condition variable outside of its critical section and convert it to a semaphore. The order-independence of semaphore operations makes this safe. After this step, we\u2019ve already implemented the box office/bouncer pattern. (In general, this step also lets us avoid a thundering herd when multiple threads are signaled at once.) 2\u3001Make the box office lock-free by converting all operations to CAS loops , greatly improving its scalability. This step results in AutoResetEvent . On my Windows PC, using AutoResetEvent in place of AutoResetEventCondVar makes the associated test case run 10x faster.","title":"Comparison With Condition Variables"},{"location":"Concurrent-computing/Expert-iter_zc/","text":"csdn iter_zc \u804a\u804a\u9ad8\u5e76\u53d1 \u8fd9\u4e2a\u7cfb\u5217\u7684\u6587\u7ae0\u4e0d\u9519","title":"Introduction"},{"location":"Concurrent-computing/Expert-iter_zc/#csdn#iter_zc","text":"","title":"csdn iter_zc"},{"location":"Concurrent-computing/Expert-iter_zc/#_1","text":"\u8fd9\u4e2a\u7cfb\u5217\u7684\u6587\u7ae0\u4e0d\u9519","title":"\u804a\u804a\u9ad8\u5e76\u53d1"},{"location":"Concurrent-computing/Expert-iter_zc/CSDN-%E8%81%8A%E8%81%8A%E9%AB%98%E5%B9%B6%E5%8F%91-5-%E7%90%86%E8%A7%A3%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE%E4%BB%A5%E5%8F%8A%E5%AF%B9%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%9A%84%E5%BD%B1%E5%93%8D/","text":"csdn \u804a\u804a\u9ad8\u5e76\u53d1\uff08\u4e94\uff09\u7406\u89e3\u7f13\u5b58\u4e00\u81f4\u6027\u534f\u8bae\u4ee5\u53ca\u5bf9\u5e76\u53d1\u7f16\u7a0b\u7684\u5f71\u54cd NOTE: 1\u3001\u4ecehardware\u7684\u89d2\u5ea6\u5206\u6790\u4e86\u5f71\u54cdconcurrency\u7684\u56e0\u7d20\uff0c\u8fd9\u662f\u975e\u5e38\u597d\u7684\uff0c\u8fd9\u662f\u4ece\u6839\u672c\u4e0a\u8fdb\u884c\u7684\u5206\u6790 \u4e00\u4e2a\u57fa\u672c\u7684CPU\u6267\u884c\u8ba1\u7b97\u7684\u8fc7\u7a0b\u5982\u4e0b\uff1a \u7a0b\u5e8f\u4ee5\u53ca\u6570\u636e\u88ab\u52a0\u8f7d\u5230\u4e3b\u5185\u5b58 \u6307\u4ee4\u548c\u6570\u636e\u88ab\u52a0\u8f7d\u5230CPU\u7684\u9ad8\u901f\u7f13\u5b58 CPU\u6267\u884c\u6307\u4ee4\uff0c\u628a\u7ed3\u679c\u5199\u5230\u9ad8\u901f\u7f13\u5b58 \u9ad8\u901f\u7f13\u5b58\u4e2d\u7684\u6570\u636e\u5199\u56de\u4e3b\u5185\u5b58 NOTE: 1\u3001\u76f8\u5f53\u4e8eload and store \u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u6709\u4e24\u4e2a\u95ee\u9898 1\u3001\u73b0\u4ee3\u7684\u8ba1\u7b97\u82af\u7247\u90fd\u4f1a\u96c6\u6210\u4e00\u4e2aL1\u9ad8\u901f\u7f13\u5b58\uff0c\u6211\u4eec\u53ef\u4ee5\u7406\u89e3\u4e3a\u6bcf\u4e2a\u82af\u7247\u90fd\u6709\u4e00\u4e2a\u79c1\u6709\u7684\u5b58\u50a8\u7a7a\u95f4\u3002\u90a3\u4e48\u5f53CPU\u7684\u4e0d\u540c\u8ba1\u7b97\u82af\u7247\u8981\u8bbf\u95ee\u540c\u4e00\u4e2a\u5185\u5b58\u5730\u5740\u65f6\uff0c\u8be5\u5185\u5b58\u5730\u5740\u7684\u503c\u4f1a\u5728CPU\u7684\u4e0d\u540c\u8ba1\u7b97\u82af\u7247\u4e4b\u95f4\u6709\u591a\u4e2a\u62f7\u8d1d\uff0c\u5982\u4f55\u540c\u6b65\u8fd9\u4e9b\u62f7\u8d1d\uff1f 2\u3001CPU\u8bfb\u5199\u662f\u76f4\u63a5\u548c\u9ad8\u901f\u7f13\u5b58\u6253\u4ea4\u9053\uff0c\u800c\u4e0d\u662f\u548c\u4e3b\u5185\u5b58\u76f4\u63a5\u6253\u4ea4\u9053\u3002\u56e0\u4e3a\u901a\u5e38\u4e00\u6b21\u4e3b\u5b58\u8bbf\u95ee\u5728\u51e0\u5341\u5230\u51e0\u767e\u4e2a\u65f6\u949f\u5468\u671f\uff0c\u800c\u4e00\u6b21L1\u9ad8\u901f\u7f13\u5b58\u7684\u8bfb\u5199\u53ea\u9700\u89811-2\u4e2a\u65f6\u949f\u5468\u671f\uff0c\u800c\u4e00\u6b21L2\u9ad8\u901f\u7f13\u5b58\u7684\u8bfb\u5199\u53ea\u9700\u8981\u6570\u5341\u4e2a\u65f6\u949f\u5468\u671f\u3002\u90a3\u4e48CPU\u5199\u5230\u9ad8\u901f\u7f13\u5b58\u7684\u503c\u4f55\u65f6\u5199\u56de\u5230\u4e3b\u5185\uff1f\u5982\u679c\u662f\u591a\u4e2a\u8ba1\u7b97\u82af\u7247\u5728\u5904\u7406\u540c\u4e00\u4e2a\u5185\u5b58\u5730\u5740\uff0c\u90a3\u4e48\u5982\u4f55\u5904\u7406\u8fd9\u4e2a\u65f6\u95f4\u5dee\u662f\u4e2a\u95ee\u9898\u3002 \u4e92\u8fde\u7ed3\u6784 \u548c \u4e92\u8fde\u7ebf \u4e92\u8fde\u7ebf \u5bf9\u4e8e\u7b2c\u4e00\u4e2a\u95ee\u9898\uff0c\u4e0d\u540c\u7684\u786c\u4ef6\u7ed3\u6784\u5904\u7406\u7684\u65b9\u5f0f\u4e0d\u4e00\u6837\u3002\u6211\u4eec\u6765\u7406\u89e3\u4e00\u4e0b**\u4e92\u8fde\u7ebf**\u7684\u6982\u5ff5\u3002 **\u4e92\u8fde\u7ebf**\u662f\u5904\u7406\u5668\u4e8e\u4e3b\u5b58\u4ee5\u53ca\u5904\u7406\u5668\u4e0e\u5904\u7406\u5668\u4e4b\u95f4\u8fdb\u884c\u901a\u4fe1\u7684\u5a92\u4ecb\uff0c\u6709\u4e24\u79cd\u57fa\u672c\u7684\u4e92\u8054\u7ed3\u6784\uff1aSMP\uff08symmetric multiprocessing \u5bf9\u79f0\u591a\u5904\u7406\uff09\u548cNUMA\uff08nonuniform memory access \u975e\u4e00\u81f4\u5185\u5b58\u8bbf\u95ee\uff09 SMP\u7cfb\u7edf\u7ed3\u6784 SMP\u7cfb\u7edf\u7ed3\u6784**\u975e\u5e38\u666e\u901a\uff0c\u56e0\u4e3a\u5b83\u4eec\u6700\u5bb9\u6613\u6784\u5efa\uff0c\u5f88\u591a\u5c0f\u578b\u670d\u52a1\u5668\u91c7\u7528\u8fd9\u79cd\u7ed3\u6784\u3002\u5904\u7406\u5668\u548c\u5b58\u50a8\u5668\u4e4b\u95f4\u91c7\u7528**\u603b\u7ebf**\u4e92\u8054\uff0c\u5904\u7406\u5668\u548c\u5b58\u50a8\u5668\u90fd\u6709\u8d1f\u8d23\u53d1\u9001\u548c\u76d1\u542c\u603b\u7ebf\u5e7f\u64ad\u7684\u4fe1\u606f\u7684**\u603b\u7ebf\u63a7\u5236\u5355\u5143 \u3002\u4f46\u662f\u540c\u4e00\u65f6\u523b\u53ea\u80fd\u6709\u4e00\u4e2a\u5904\u7406\u5668\uff08\u6216\u5b58\u50a8\u63a7\u5236\u5668\uff09\u5728\u603b\u7ebf\u4e0a\u5e7f\u64ad\uff0c\u6240\u6709\u7684\u5904\u7406\u5668\u90fd\u53ef\u4ee5\u76d1\u542c\u3002 \u5f88\u5bb9\u6613\u770b\u51fa\uff0c\u5bf9\u603b\u7ebf\u7684\u4f7f\u7528\u662f**SMP\u7ed3\u6784**\u7684\u74f6\u9888\u3002 NOTE: 1\u3001\u7ed3\u6784\u51b3\u5b9a\u6027\u8d28 NUMA NOTE: 1\u3001\u6ca1\u6709\u8bfb\u61c2 NUMP\u7cfb\u7edf\u7ed3\u6784\u4e2d\uff0c\u4e00\u7cfb\u5217\u8282\u70b9\u901a\u8fc7\u70b9\u5bf9\u70b9\u7f51\u7edc\u4e92\u8054\uff0c\u50cf\u4e00\u4e2a\u5c0f\u578b\u4e92\u8054\u7f51\uff0c\u6bcf\u4e2a\u8282\u70b9\u5305\u542b\u4e00\u4e2a\u6216\u591a\u4e2a\u5904\u7406\u5668\u548c\u4e00\u4e2a\u672c\u5730\u5b58\u50a8\u5668\u3002\u4e00\u4e2a\u8282\u70b9\u7684\u672c\u5730\u5b58\u50a8\u5bf9\u4e8e\u5176\u4ed6\u8282\u70b9\u662f\u53ef\u89c1\u7684\uff0c\u6240\u6709\u8282\u70b9\u7684\u672c\u5730\u5b58\u50a8\u4e00\u8d77\u5f62\u6210\u4e86\u4e00\u4e2a\u53ef\u4ee5\u88ab\u6240\u6709\u5904\u7406\u5668\u5171\u4eab\u7684\u5168\u5c40\u5b58\u50a8\u5668\u3002\u53ef\u4ee5\u770b\u51fa\uff0cNUMP\u7684\u672c\u5730\u5b58\u50a8\u662f\u5171\u4eab\u7684\uff0c\u800c\u4e0d\u662f\u79c1\u6709\u7684\uff0c\u8fd9\u70b9\u548cSMP\u662f\u4e0d\u540c\u7684\u3002NUMP\u7684\u95ee\u9898\u662f\u7f51\u7edc\u6bd4\u603b\u7ebf\u590d\u5236\uff0c\u9700\u8981\u66f4\u52a0\u590d\u6742\u7684\u534f\u8bae\uff0c\u5904\u7406\u5668\u8bbf\u95ee\u81ea\u5df1\u8282\u70b9\u7684\u5b58\u50a8\u5668\u901f\u5ea6\u5feb\u4e8e\u8bbf\u95ee\u5176\u4ed6\u8282\u70b9\u7684\u5b58\u50a8\u5668\u3002NUMP\u7684\u6269\u5c55\u6027\u5f88\u597d\uff0c\u6240\u4ee5\u76ee\u524d\u5f88\u591a\u5927\u4e2d\u578b\u7684\u670d\u52a1\u5668\u5728\u91c7\u7528NUMP\u7ed3\u6784\u3002 \u4e92\u8fde\u7ebf\u662f\u51b3\u5b9a\u56e0\u7d20 \u5bf9\u4e8e\u4e0a\u5c42\u7a0b\u5e8f\u5458\u6765\u8bf4\uff0c\u6700\u9700\u8981\u7406\u89e3\u7684\u662f\u4e92\u8fde\u7ebf\u662f\u4e00\u79cd\u91cd\u8981\u7684\u8d44\u6e90\uff0c\u4f7f\u7528\u7684\u597d\u574f\u4f1a\u76f4\u63a5\u5f71\u54cd\u7a0b\u5e8f\u7684\u6267\u884c\u6027\u80fd\u3002 \u7f13\u5b58\u4e00\u81f4\u6027\u534f\u8bae \u5927\u6982\u7406\u89e3\u4e86\u4e0d\u540c\u7684\u4e92\u8fde\u7ed3\u6784\u4e4b\u540e\uff0c\u6211\u4eec\u6765\u770b\u770b\u7f13\u5b58\u4e00\u81f4\u6027\u534f\u8bae\u3002\u5b83\u4e3b\u8981\u5c31\u662f\u5904\u7406\u591a\u4e2a\u5904\u7406\u5668\u5904\u7406\u540c\u4e00\u4e2a\u4e3b\u5b58\u5730\u5740\u7684\u95ee\u9898\u3002 \u7f13\u5b58\u4e00\u81f4\u6027\u6d41\u91cf\u98ce\u66b4 NOTE: 1\u3001\u5c06\u6b64\u79f0\u4e3aCAS-cache coherence flood 2\u3001How-Cache-Coherency-Impacts-Power-Performance \u7f13\u5b58\u4e00\u81f4\u6027\u534f\u8bae\u5b58\u5728\u7684\u4e00\u4e2a\u6700\u5927\u7684\u95ee\u9898\u662f\u53ef\u80fd\u5f15\u8d77\u7f13\u5b58\u4e00\u81f4\u6027\u6d41\u91cf\u98ce\u66b4\uff0c\u4e4b\u524d\u6211\u4eec\u770b\u5230\u603b\u7ebf\u5728\u540c\u4e00\u65f6\u523b\u53ea\u80fd\u88ab\u4e00\u4e2a\u5904\u7406\u5668\u4f7f\u7528\uff0c\u5f53\u6709\u5927\u91cf\u7f13\u5b58\u88ab\u4fee\u6539\uff0c\u6216\u8005\u540c\u4e00\u4e2a\u7f13\u5b58\u5757\u4e00\u76f4\u88ab\u4fee\u6539\u65f6\uff0c\u4f1a\u4ea7\u751f\u5927\u91cf\u7684\u7f13\u5b58\u4e00\u81f4\u6027\u6d41\u91cf\uff0c\u4ece\u800c\u5360\u7528\u603b\u7ebf\uff0c\u5f71\u54cd\u4e86\u5176\u4ed6\u6b63\u5e38\u7684\u8bfb\u5199\u8bf7\u6c42\u3002 \u4e00\u4e2a\u6700\u5e38\u89c1\u7684\u4f8b\u5b50\u5c31\u662f\u5982\u679c\u591a\u4e2a\u7ebf\u7a0b\u5bf9\u540c\u4e00\u4e2a\u53d8\u91cf\u4e00\u76f4\u4f7f\u7528CAS\u64cd\u4f5c\uff0c\u90a3\u4e48\u4f1a\u6709\u5927\u91cf\u4fee\u6539\u64cd\u4f5c\uff0c\u4ece\u800c\u4ea7\u751f\u5927\u91cf\u7684\u7f13\u5b58\u4e00\u81f4\u6027\u6d41\u91cf\uff0c\u56e0\u4e3a\u6bcf\u4e00\u6b21CAS\u64cd\u4f5c\u90fd\u4f1a\u53d1\u51fa\u5e7f\u64ad\u901a\u77e5\u5176\u4ed6\u5904\u7406\u5668\uff0c\u4ece\u800c\u5f71\u54cd\u7a0b\u5e8f\u7684\u6027\u80fd\u3002 \u540e\u9762\u6211\u4eec\u4f1a\u8bb2\u5982\u4f55\u4f18\u5316\u8fd9\u79cd\u4f7f\u7528\u65b9\u5f0f\u3002 \u5bf9\u4e8e\u7b2c\u4e8c\u4e2a\u95ee\u9898\uff0c\u5982\u4f55\u5904\u7406\u4fee\u6539\u6570\u636e\u4ece\u9ad8\u901f\u7f13\u5b58\u5230\u4e3b\u5185\u5b58\u7684\u65f6\u95f4\u5dee\uff0c\u901a\u5e38\u4f7f\u7528\u5185\u5b58\u5c4f\u969c\u6765\u5904\u7406\uff0c\u540e\u9762\u4f1a\u6709\u4e13\u95e8\u7684\u4e3b\u9898\u3002 \u8f6c\u8f7d\u8bf7\u6ce8\u660e\u6765\u6e90\uff1a http://blog.csdn.net/iter_zc","title":"Introduction"},{"location":"Concurrent-computing/Expert-iter_zc/CSDN-%E8%81%8A%E8%81%8A%E9%AB%98%E5%B9%B6%E5%8F%91-5-%E7%90%86%E8%A7%A3%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE%E4%BB%A5%E5%8F%8A%E5%AF%B9%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%9A%84%E5%BD%B1%E5%93%8D/#csdn","text":"NOTE: 1\u3001\u4ecehardware\u7684\u89d2\u5ea6\u5206\u6790\u4e86\u5f71\u54cdconcurrency\u7684\u56e0\u7d20\uff0c\u8fd9\u662f\u975e\u5e38\u597d\u7684\uff0c\u8fd9\u662f\u4ece\u6839\u672c\u4e0a\u8fdb\u884c\u7684\u5206\u6790 \u4e00\u4e2a\u57fa\u672c\u7684CPU\u6267\u884c\u8ba1\u7b97\u7684\u8fc7\u7a0b\u5982\u4e0b\uff1a \u7a0b\u5e8f\u4ee5\u53ca\u6570\u636e\u88ab\u52a0\u8f7d\u5230\u4e3b\u5185\u5b58 \u6307\u4ee4\u548c\u6570\u636e\u88ab\u52a0\u8f7d\u5230CPU\u7684\u9ad8\u901f\u7f13\u5b58 CPU\u6267\u884c\u6307\u4ee4\uff0c\u628a\u7ed3\u679c\u5199\u5230\u9ad8\u901f\u7f13\u5b58 \u9ad8\u901f\u7f13\u5b58\u4e2d\u7684\u6570\u636e\u5199\u56de\u4e3b\u5185\u5b58 NOTE: 1\u3001\u76f8\u5f53\u4e8eload and store \u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u6709\u4e24\u4e2a\u95ee\u9898 1\u3001\u73b0\u4ee3\u7684\u8ba1\u7b97\u82af\u7247\u90fd\u4f1a\u96c6\u6210\u4e00\u4e2aL1\u9ad8\u901f\u7f13\u5b58\uff0c\u6211\u4eec\u53ef\u4ee5\u7406\u89e3\u4e3a\u6bcf\u4e2a\u82af\u7247\u90fd\u6709\u4e00\u4e2a\u79c1\u6709\u7684\u5b58\u50a8\u7a7a\u95f4\u3002\u90a3\u4e48\u5f53CPU\u7684\u4e0d\u540c\u8ba1\u7b97\u82af\u7247\u8981\u8bbf\u95ee\u540c\u4e00\u4e2a\u5185\u5b58\u5730\u5740\u65f6\uff0c\u8be5\u5185\u5b58\u5730\u5740\u7684\u503c\u4f1a\u5728CPU\u7684\u4e0d\u540c\u8ba1\u7b97\u82af\u7247\u4e4b\u95f4\u6709\u591a\u4e2a\u62f7\u8d1d\uff0c\u5982\u4f55\u540c\u6b65\u8fd9\u4e9b\u62f7\u8d1d\uff1f 2\u3001CPU\u8bfb\u5199\u662f\u76f4\u63a5\u548c\u9ad8\u901f\u7f13\u5b58\u6253\u4ea4\u9053\uff0c\u800c\u4e0d\u662f\u548c\u4e3b\u5185\u5b58\u76f4\u63a5\u6253\u4ea4\u9053\u3002\u56e0\u4e3a\u901a\u5e38\u4e00\u6b21\u4e3b\u5b58\u8bbf\u95ee\u5728\u51e0\u5341\u5230\u51e0\u767e\u4e2a\u65f6\u949f\u5468\u671f\uff0c\u800c\u4e00\u6b21L1\u9ad8\u901f\u7f13\u5b58\u7684\u8bfb\u5199\u53ea\u9700\u89811-2\u4e2a\u65f6\u949f\u5468\u671f\uff0c\u800c\u4e00\u6b21L2\u9ad8\u901f\u7f13\u5b58\u7684\u8bfb\u5199\u53ea\u9700\u8981\u6570\u5341\u4e2a\u65f6\u949f\u5468\u671f\u3002\u90a3\u4e48CPU\u5199\u5230\u9ad8\u901f\u7f13\u5b58\u7684\u503c\u4f55\u65f6\u5199\u56de\u5230\u4e3b\u5185\uff1f\u5982\u679c\u662f\u591a\u4e2a\u8ba1\u7b97\u82af\u7247\u5728\u5904\u7406\u540c\u4e00\u4e2a\u5185\u5b58\u5730\u5740\uff0c\u90a3\u4e48\u5982\u4f55\u5904\u7406\u8fd9\u4e2a\u65f6\u95f4\u5dee\u662f\u4e2a\u95ee\u9898\u3002","title":"csdn \u804a\u804a\u9ad8\u5e76\u53d1\uff08\u4e94\uff09\u7406\u89e3\u7f13\u5b58\u4e00\u81f4\u6027\u534f\u8bae\u4ee5\u53ca\u5bf9\u5e76\u53d1\u7f16\u7a0b\u7684\u5f71\u54cd"},{"location":"Concurrent-computing/Expert-iter_zc/CSDN-%E8%81%8A%E8%81%8A%E9%AB%98%E5%B9%B6%E5%8F%91-5-%E7%90%86%E8%A7%A3%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE%E4%BB%A5%E5%8F%8A%E5%AF%B9%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%9A%84%E5%BD%B1%E5%93%8D/#_1","text":"","title":"\u4e92\u8fde\u7ed3\u6784 \u548c \u4e92\u8fde\u7ebf"},{"location":"Concurrent-computing/Expert-iter_zc/CSDN-%E8%81%8A%E8%81%8A%E9%AB%98%E5%B9%B6%E5%8F%91-5-%E7%90%86%E8%A7%A3%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE%E4%BB%A5%E5%8F%8A%E5%AF%B9%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%9A%84%E5%BD%B1%E5%93%8D/#_2","text":"\u5bf9\u4e8e\u7b2c\u4e00\u4e2a\u95ee\u9898\uff0c\u4e0d\u540c\u7684\u786c\u4ef6\u7ed3\u6784\u5904\u7406\u7684\u65b9\u5f0f\u4e0d\u4e00\u6837\u3002\u6211\u4eec\u6765\u7406\u89e3\u4e00\u4e0b**\u4e92\u8fde\u7ebf**\u7684\u6982\u5ff5\u3002 **\u4e92\u8fde\u7ebf**\u662f\u5904\u7406\u5668\u4e8e\u4e3b\u5b58\u4ee5\u53ca\u5904\u7406\u5668\u4e0e\u5904\u7406\u5668\u4e4b\u95f4\u8fdb\u884c\u901a\u4fe1\u7684\u5a92\u4ecb\uff0c\u6709\u4e24\u79cd\u57fa\u672c\u7684\u4e92\u8054\u7ed3\u6784\uff1aSMP\uff08symmetric multiprocessing \u5bf9\u79f0\u591a\u5904\u7406\uff09\u548cNUMA\uff08nonuniform memory access \u975e\u4e00\u81f4\u5185\u5b58\u8bbf\u95ee\uff09","title":"\u4e92\u8fde\u7ebf"},{"location":"Concurrent-computing/Expert-iter_zc/CSDN-%E8%81%8A%E8%81%8A%E9%AB%98%E5%B9%B6%E5%8F%91-5-%E7%90%86%E8%A7%A3%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE%E4%BB%A5%E5%8F%8A%E5%AF%B9%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%9A%84%E5%BD%B1%E5%93%8D/#smp","text":"SMP\u7cfb\u7edf\u7ed3\u6784**\u975e\u5e38\u666e\u901a\uff0c\u56e0\u4e3a\u5b83\u4eec\u6700\u5bb9\u6613\u6784\u5efa\uff0c\u5f88\u591a\u5c0f\u578b\u670d\u52a1\u5668\u91c7\u7528\u8fd9\u79cd\u7ed3\u6784\u3002\u5904\u7406\u5668\u548c\u5b58\u50a8\u5668\u4e4b\u95f4\u91c7\u7528**\u603b\u7ebf**\u4e92\u8054\uff0c\u5904\u7406\u5668\u548c\u5b58\u50a8\u5668\u90fd\u6709\u8d1f\u8d23\u53d1\u9001\u548c\u76d1\u542c\u603b\u7ebf\u5e7f\u64ad\u7684\u4fe1\u606f\u7684**\u603b\u7ebf\u63a7\u5236\u5355\u5143 \u3002\u4f46\u662f\u540c\u4e00\u65f6\u523b\u53ea\u80fd\u6709\u4e00\u4e2a\u5904\u7406\u5668\uff08\u6216\u5b58\u50a8\u63a7\u5236\u5668\uff09\u5728\u603b\u7ebf\u4e0a\u5e7f\u64ad\uff0c\u6240\u6709\u7684\u5904\u7406\u5668\u90fd\u53ef\u4ee5\u76d1\u542c\u3002 \u5f88\u5bb9\u6613\u770b\u51fa\uff0c\u5bf9\u603b\u7ebf\u7684\u4f7f\u7528\u662f**SMP\u7ed3\u6784**\u7684\u74f6\u9888\u3002 NOTE: 1\u3001\u7ed3\u6784\u51b3\u5b9a\u6027\u8d28","title":"SMP\u7cfb\u7edf\u7ed3\u6784"},{"location":"Concurrent-computing/Expert-iter_zc/CSDN-%E8%81%8A%E8%81%8A%E9%AB%98%E5%B9%B6%E5%8F%91-5-%E7%90%86%E8%A7%A3%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE%E4%BB%A5%E5%8F%8A%E5%AF%B9%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%9A%84%E5%BD%B1%E5%93%8D/#numa","text":"NOTE: 1\u3001\u6ca1\u6709\u8bfb\u61c2 NUMP\u7cfb\u7edf\u7ed3\u6784\u4e2d\uff0c\u4e00\u7cfb\u5217\u8282\u70b9\u901a\u8fc7\u70b9\u5bf9\u70b9\u7f51\u7edc\u4e92\u8054\uff0c\u50cf\u4e00\u4e2a\u5c0f\u578b\u4e92\u8054\u7f51\uff0c\u6bcf\u4e2a\u8282\u70b9\u5305\u542b\u4e00\u4e2a\u6216\u591a\u4e2a\u5904\u7406\u5668\u548c\u4e00\u4e2a\u672c\u5730\u5b58\u50a8\u5668\u3002\u4e00\u4e2a\u8282\u70b9\u7684\u672c\u5730\u5b58\u50a8\u5bf9\u4e8e\u5176\u4ed6\u8282\u70b9\u662f\u53ef\u89c1\u7684\uff0c\u6240\u6709\u8282\u70b9\u7684\u672c\u5730\u5b58\u50a8\u4e00\u8d77\u5f62\u6210\u4e86\u4e00\u4e2a\u53ef\u4ee5\u88ab\u6240\u6709\u5904\u7406\u5668\u5171\u4eab\u7684\u5168\u5c40\u5b58\u50a8\u5668\u3002\u53ef\u4ee5\u770b\u51fa\uff0cNUMP\u7684\u672c\u5730\u5b58\u50a8\u662f\u5171\u4eab\u7684\uff0c\u800c\u4e0d\u662f\u79c1\u6709\u7684\uff0c\u8fd9\u70b9\u548cSMP\u662f\u4e0d\u540c\u7684\u3002NUMP\u7684\u95ee\u9898\u662f\u7f51\u7edc\u6bd4\u603b\u7ebf\u590d\u5236\uff0c\u9700\u8981\u66f4\u52a0\u590d\u6742\u7684\u534f\u8bae\uff0c\u5904\u7406\u5668\u8bbf\u95ee\u81ea\u5df1\u8282\u70b9\u7684\u5b58\u50a8\u5668\u901f\u5ea6\u5feb\u4e8e\u8bbf\u95ee\u5176\u4ed6\u8282\u70b9\u7684\u5b58\u50a8\u5668\u3002NUMP\u7684\u6269\u5c55\u6027\u5f88\u597d\uff0c\u6240\u4ee5\u76ee\u524d\u5f88\u591a\u5927\u4e2d\u578b\u7684\u670d\u52a1\u5668\u5728\u91c7\u7528NUMP\u7ed3\u6784\u3002","title":"NUMA"},{"location":"Concurrent-computing/Expert-iter_zc/CSDN-%E8%81%8A%E8%81%8A%E9%AB%98%E5%B9%B6%E5%8F%91-5-%E7%90%86%E8%A7%A3%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE%E4%BB%A5%E5%8F%8A%E5%AF%B9%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%9A%84%E5%BD%B1%E5%93%8D/#_3","text":"\u5bf9\u4e8e\u4e0a\u5c42\u7a0b\u5e8f\u5458\u6765\u8bf4\uff0c\u6700\u9700\u8981\u7406\u89e3\u7684\u662f\u4e92\u8fde\u7ebf\u662f\u4e00\u79cd\u91cd\u8981\u7684\u8d44\u6e90\uff0c\u4f7f\u7528\u7684\u597d\u574f\u4f1a\u76f4\u63a5\u5f71\u54cd\u7a0b\u5e8f\u7684\u6267\u884c\u6027\u80fd\u3002","title":"\u4e92\u8fde\u7ebf\u662f\u51b3\u5b9a\u56e0\u7d20"},{"location":"Concurrent-computing/Expert-iter_zc/CSDN-%E8%81%8A%E8%81%8A%E9%AB%98%E5%B9%B6%E5%8F%91-5-%E7%90%86%E8%A7%A3%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE%E4%BB%A5%E5%8F%8A%E5%AF%B9%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%9A%84%E5%BD%B1%E5%93%8D/#_4","text":"\u5927\u6982\u7406\u89e3\u4e86\u4e0d\u540c\u7684\u4e92\u8fde\u7ed3\u6784\u4e4b\u540e\uff0c\u6211\u4eec\u6765\u770b\u770b\u7f13\u5b58\u4e00\u81f4\u6027\u534f\u8bae\u3002\u5b83\u4e3b\u8981\u5c31\u662f\u5904\u7406\u591a\u4e2a\u5904\u7406\u5668\u5904\u7406\u540c\u4e00\u4e2a\u4e3b\u5b58\u5730\u5740\u7684\u95ee\u9898\u3002","title":"\u7f13\u5b58\u4e00\u81f4\u6027\u534f\u8bae"},{"location":"Concurrent-computing/Expert-iter_zc/CSDN-%E8%81%8A%E8%81%8A%E9%AB%98%E5%B9%B6%E5%8F%91-5-%E7%90%86%E8%A7%A3%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE%E4%BB%A5%E5%8F%8A%E5%AF%B9%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%9A%84%E5%BD%B1%E5%93%8D/#_5","text":"NOTE: 1\u3001\u5c06\u6b64\u79f0\u4e3aCAS-cache coherence flood 2\u3001How-Cache-Coherency-Impacts-Power-Performance \u7f13\u5b58\u4e00\u81f4\u6027\u534f\u8bae\u5b58\u5728\u7684\u4e00\u4e2a\u6700\u5927\u7684\u95ee\u9898\u662f\u53ef\u80fd\u5f15\u8d77\u7f13\u5b58\u4e00\u81f4\u6027\u6d41\u91cf\u98ce\u66b4\uff0c\u4e4b\u524d\u6211\u4eec\u770b\u5230\u603b\u7ebf\u5728\u540c\u4e00\u65f6\u523b\u53ea\u80fd\u88ab\u4e00\u4e2a\u5904\u7406\u5668\u4f7f\u7528\uff0c\u5f53\u6709\u5927\u91cf\u7f13\u5b58\u88ab\u4fee\u6539\uff0c\u6216\u8005\u540c\u4e00\u4e2a\u7f13\u5b58\u5757\u4e00\u76f4\u88ab\u4fee\u6539\u65f6\uff0c\u4f1a\u4ea7\u751f\u5927\u91cf\u7684\u7f13\u5b58\u4e00\u81f4\u6027\u6d41\u91cf\uff0c\u4ece\u800c\u5360\u7528\u603b\u7ebf\uff0c\u5f71\u54cd\u4e86\u5176\u4ed6\u6b63\u5e38\u7684\u8bfb\u5199\u8bf7\u6c42\u3002 \u4e00\u4e2a\u6700\u5e38\u89c1\u7684\u4f8b\u5b50\u5c31\u662f\u5982\u679c\u591a\u4e2a\u7ebf\u7a0b\u5bf9\u540c\u4e00\u4e2a\u53d8\u91cf\u4e00\u76f4\u4f7f\u7528CAS\u64cd\u4f5c\uff0c\u90a3\u4e48\u4f1a\u6709\u5927\u91cf\u4fee\u6539\u64cd\u4f5c\uff0c\u4ece\u800c\u4ea7\u751f\u5927\u91cf\u7684\u7f13\u5b58\u4e00\u81f4\u6027\u6d41\u91cf\uff0c\u56e0\u4e3a\u6bcf\u4e00\u6b21CAS\u64cd\u4f5c\u90fd\u4f1a\u53d1\u51fa\u5e7f\u64ad\u901a\u77e5\u5176\u4ed6\u5904\u7406\u5668\uff0c\u4ece\u800c\u5f71\u54cd\u7a0b\u5e8f\u7684\u6027\u80fd\u3002 \u540e\u9762\u6211\u4eec\u4f1a\u8bb2\u5982\u4f55\u4f18\u5316\u8fd9\u79cd\u4f7f\u7528\u65b9\u5f0f\u3002 \u5bf9\u4e8e\u7b2c\u4e8c\u4e2a\u95ee\u9898\uff0c\u5982\u4f55\u5904\u7406\u4fee\u6539\u6570\u636e\u4ece\u9ad8\u901f\u7f13\u5b58\u5230\u4e3b\u5185\u5b58\u7684\u65f6\u95f4\u5dee\uff0c\u901a\u5e38\u4f7f\u7528\u5185\u5b58\u5c4f\u969c\u6765\u5904\u7406\uff0c\u540e\u9762\u4f1a\u6709\u4e13\u95e8\u7684\u4e3b\u9898\u3002 \u8f6c\u8f7d\u8bf7\u6ce8\u660e\u6765\u6e90\uff1a http://blog.csdn.net/iter_zc","title":"\u7f13\u5b58\u4e00\u81f4\u6027\u6d41\u91cf\u98ce\u66b4"},{"location":"Concurrent-computing/Expert-iter_zc/CSDN-%E8%81%8A%E8%81%8A%E9%AB%98%E5%B9%B6%E5%8F%91-6-%E5%AE%9E%E7%8E%B0%E5%87%A0%E7%A7%8D%E8%87%AA%E6%97%8B%E9%94%81%E4%B8%80/","text":"\u804a\u804a\u9ad8\u5e76\u53d1\uff08\u516d\uff09\u5b9e\u73b0\u51e0\u79cd\u81ea\u65cb\u9501\uff08\u4e00\uff09","title":"Introduction"},{"location":"Concurrent-computing/Expert-iter_zc/CSDN-%E8%81%8A%E8%81%8A%E9%AB%98%E5%B9%B6%E5%8F%91-6-%E5%AE%9E%E7%8E%B0%E5%87%A0%E7%A7%8D%E8%87%AA%E6%97%8B%E9%94%81%E4%B8%80/#_1","text":"","title":"\u804a\u804a\u9ad8\u5e76\u53d1\uff08\u516d\uff09\u5b9e\u73b0\u51e0\u79cd\u81ea\u65cb\u9501\uff08\u4e00\uff09"},{"location":"Concurrent-computing/Expert-iter_zc/CSDN-%E8%81%8A%E8%81%8A%E9%AB%98%E5%B9%B6%E5%8F%91-7-%E5%AE%9E%E7%8E%B0%E5%87%A0%E7%A7%8D%E8%87%AA%E6%97%8B%E9%94%81%E4%BA%8C/","text":"csdn \u804a\u804a\u9ad8\u5e76\u53d1\uff08\u4e03\uff09\u5b9e\u73b0\u51e0\u79cd\u81ea\u65cb\u9501\uff08\u4e8c\uff09 \u5728 \u804a\u804a\u9ad8\u5e76\u53d1\uff08\u516d\uff09\u5b9e\u73b0\u51e0\u79cd\u81ea\u65cb\u9501\uff08\u4e00\uff09 \u8fd9\u7bc7\u4e2d\u5b9e\u73b0\u4e86\u4e24\u79cd\u57fa\u672c\u7684\u81ea\u65cb\u9501\uff1aTASLock\u548cTTASLock\uff0c\u5b83\u4eec\u7684\u95ee\u9898\u662f\u4f1a\u8fdb\u884c\u9891\u7e41\u7684CAS\u64cd\u4f5c\uff0c\u5f15\u53d1\u5927\u91cf\u7684\u7f13\u5b58\u4e00\u81f4\u6027\u6d41\u91cf\uff0c\u5bfc\u81f4\u9501\u7684\u6027\u80fd\u4e0d\u597d\u3002 \u5bf9TTASLock\u7684\u4e00\u79cd\u6539\u8fdb\u662fBackoffLock\uff0c\u5b83\u4f1a\u5728\u9501\u9ad8\u4e89\u7528\u7684\u60c5\u51b5\u4e0b\u5bf9\u7ebf\u7a0b\u8fdb\u884c\u56de\u9000\uff0c\u51cf\u5c11\u7ade\u4e89\uff0c\u51cf\u5c11\u7f13\u5b58\u4e00\u81f4\u6027\u6d41\u91cf\u3002\u4f46\u662fBackoffLock\u6709\u4e09\u4e2a\u4e3b\u8981\u7684\u95ee\u9898\uff1a 1\u3001\u8fd8\u662f\u6709\u5927\u91cf\u7684\u7f13\u5b58\u4e00\u81f4\u6027\u6d41\u91cf\uff0c\u56e0\u4e3a\u6240\u6709\u7ebf\u7a0b\u5728\u540c\u4e00\u4e2a\u5171\u4eab\u53d8\u91cf\u4e0a\u65cb\u8f6c\uff0c\u6bcf\u4e00\u6b21\u6210\u529f\u7684\u83b7\u53d6\u9501\u90fd\u4f1a\u4ea7\u751f\u7f13\u5b58\u4e00\u81f4\u6027\u6d41\u91cf 2\u3001\u56e0\u4e3a\u56de\u9000\u7684\u5b58\u5728\uff0c\u4e0d\u80fd\u53ca\u65f6\u83b7\u53d6\u9501\u91ca\u653e\u7684\u4fe1\u606f\uff0c\u5b58\u5728\u4e00\u4e2a\u65f6\u95f4\u5dee\uff0c\u5bfc\u81f4\u83b7\u53d6\u9501\u7684\u65f6\u95f4\u53d8\u957f 3\u3001\u4e0d\u80fd\u4fdd\u8bc1\u65e0\u9965\u997f\uff0c\u6709\u7684\u7ebf\u7a0b\u53ef\u80fd\u4e00\u76f4\u65e0\u6cd5\u83b7\u53d6\u9501 \u8fd9\u7bc7\u4f1a\u5b9e\u73b02\u79cd\u57fa\u4e8e\u961f\u5217\u7684\u9501\uff0c\u6765\u89e3\u51b3\u4e0a\u9762\u63d0\u5230\u7684\u4e09\u4e2a\u95ee\u9898\u3002\u4e3b\u8981\u7684\u601d\u8def\u662f\u5c06\u7ebf\u7a0b\u7ec4\u7ec7\u6210\u4e00\u4e2a\u961f\u5217\uff0c\u67094\u4e2a\u4f18\u70b9\uff1a 1\u3001\u6bcf\u4e2a\u7ebf\u7a0b\u53ea\u9700\u8981\u68c0\u67e5\u5b83\u7684**\u524d\u9a71\u7ebf\u7a0b**\u7684\u72b6\u6001\uff0c\u5c06\u81ea\u65cb\u7684\u53d8\u91cf\u4ece\u4e00\u4e2a\u5206\u6563\u5230\u591a\u4e2a\uff0c\u51cf\u5c11\u7f13\u5b58\u4e00\u81f4\u6027\u6d41\u91cf 2\u3001\u53ef\u4ee5\u5373\u4f7f\u83b7\u53d6\u9501\u91ca\u653e\u7684\u901a\u77e5 3\u3001\u961f\u5217\u63d0\u4f9b\u4e86\u5148\u6765\u5148\u670d\u52a1\u7684\u516c\u5e73\u6027 4\u3001\u65e0\u9965\u997f\uff0c\u961f\u5217\u4e2d\u7684\u6bcf\u4e2a\u7ebf\u7a0b\u90fd\u80fd\u4fdd\u8bc1\u88ab\u6267\u884c\u5230 \u961f\u5217\u9501\u5206\u4e3a\u4e24\u7c7b\uff0c\u4e00\u7c7b\u662f\u57fa\u4e8e\u6709\u754c\u961f\u5217\uff0c\u4e00\u7c7b\u662f\u57fa\u4e8e\u65e0\u754c\u961f\u5217\u3002 \u57fa\u4e8e\u6709\u754c\u961f\u5217\u7684\u961f\u5217\u9501 \u5148\u770b\u4e00\u4e0b\u57fa\u4e8e\u6709\u754c\u961f\u5217\u7684\u961f\u5217\u9501\u3002 ArrayLock \u67093\u4e2a\u7279\u70b9\uff1a \\1. \u57fa\u4e8e\u4e00\u4e2avolatile\u6570\u7ec4\u6765\u7ec4\u7ec7\u7ebf\u7a0b \\2. \u901a\u8fc7\u4e00\u4e2a\u539f\u5b50\u53d8\u91cftail\u6765\u8868\u793a\u5bf9\u5c3e\u7ebf\u7a0b \\3. \u901a\u8fc7\u4e00\u4e2a ThreadLocal \u53d8\u91cf\u7ed9\u6bcf\u4e2a\u7ebf\u7a0b\u4e00\u4e2a\u7d22\u5f15\u53f7\uff0c\u8868\u793a\u5b83\u4f4d\u4e8e\u961f\u5217\u7684\u54ea\u4e2a\u4f4d\u7f6e\u3002","title":"Introduction"},{"location":"Concurrent-computing/Expert-iter_zc/CSDN-%E8%81%8A%E8%81%8A%E9%AB%98%E5%B9%B6%E5%8F%91-7-%E5%AE%9E%E7%8E%B0%E5%87%A0%E7%A7%8D%E8%87%AA%E6%97%8B%E9%94%81%E4%BA%8C/#csdn","text":"\u5728 \u804a\u804a\u9ad8\u5e76\u53d1\uff08\u516d\uff09\u5b9e\u73b0\u51e0\u79cd\u81ea\u65cb\u9501\uff08\u4e00\uff09 \u8fd9\u7bc7\u4e2d\u5b9e\u73b0\u4e86\u4e24\u79cd\u57fa\u672c\u7684\u81ea\u65cb\u9501\uff1aTASLock\u548cTTASLock\uff0c\u5b83\u4eec\u7684\u95ee\u9898\u662f\u4f1a\u8fdb\u884c\u9891\u7e41\u7684CAS\u64cd\u4f5c\uff0c\u5f15\u53d1\u5927\u91cf\u7684\u7f13\u5b58\u4e00\u81f4\u6027\u6d41\u91cf\uff0c\u5bfc\u81f4\u9501\u7684\u6027\u80fd\u4e0d\u597d\u3002 \u5bf9TTASLock\u7684\u4e00\u79cd\u6539\u8fdb\u662fBackoffLock\uff0c\u5b83\u4f1a\u5728\u9501\u9ad8\u4e89\u7528\u7684\u60c5\u51b5\u4e0b\u5bf9\u7ebf\u7a0b\u8fdb\u884c\u56de\u9000\uff0c\u51cf\u5c11\u7ade\u4e89\uff0c\u51cf\u5c11\u7f13\u5b58\u4e00\u81f4\u6027\u6d41\u91cf\u3002\u4f46\u662fBackoffLock\u6709\u4e09\u4e2a\u4e3b\u8981\u7684\u95ee\u9898\uff1a 1\u3001\u8fd8\u662f\u6709\u5927\u91cf\u7684\u7f13\u5b58\u4e00\u81f4\u6027\u6d41\u91cf\uff0c\u56e0\u4e3a\u6240\u6709\u7ebf\u7a0b\u5728\u540c\u4e00\u4e2a\u5171\u4eab\u53d8\u91cf\u4e0a\u65cb\u8f6c\uff0c\u6bcf\u4e00\u6b21\u6210\u529f\u7684\u83b7\u53d6\u9501\u90fd\u4f1a\u4ea7\u751f\u7f13\u5b58\u4e00\u81f4\u6027\u6d41\u91cf 2\u3001\u56e0\u4e3a\u56de\u9000\u7684\u5b58\u5728\uff0c\u4e0d\u80fd\u53ca\u65f6\u83b7\u53d6\u9501\u91ca\u653e\u7684\u4fe1\u606f\uff0c\u5b58\u5728\u4e00\u4e2a\u65f6\u95f4\u5dee\uff0c\u5bfc\u81f4\u83b7\u53d6\u9501\u7684\u65f6\u95f4\u53d8\u957f 3\u3001\u4e0d\u80fd\u4fdd\u8bc1\u65e0\u9965\u997f\uff0c\u6709\u7684\u7ebf\u7a0b\u53ef\u80fd\u4e00\u76f4\u65e0\u6cd5\u83b7\u53d6\u9501 \u8fd9\u7bc7\u4f1a\u5b9e\u73b02\u79cd\u57fa\u4e8e\u961f\u5217\u7684\u9501\uff0c\u6765\u89e3\u51b3\u4e0a\u9762\u63d0\u5230\u7684\u4e09\u4e2a\u95ee\u9898\u3002\u4e3b\u8981\u7684\u601d\u8def\u662f\u5c06\u7ebf\u7a0b\u7ec4\u7ec7\u6210\u4e00\u4e2a\u961f\u5217\uff0c\u67094\u4e2a\u4f18\u70b9\uff1a 1\u3001\u6bcf\u4e2a\u7ebf\u7a0b\u53ea\u9700\u8981\u68c0\u67e5\u5b83\u7684**\u524d\u9a71\u7ebf\u7a0b**\u7684\u72b6\u6001\uff0c\u5c06\u81ea\u65cb\u7684\u53d8\u91cf\u4ece\u4e00\u4e2a\u5206\u6563\u5230\u591a\u4e2a\uff0c\u51cf\u5c11\u7f13\u5b58\u4e00\u81f4\u6027\u6d41\u91cf 2\u3001\u53ef\u4ee5\u5373\u4f7f\u83b7\u53d6\u9501\u91ca\u653e\u7684\u901a\u77e5 3\u3001\u961f\u5217\u63d0\u4f9b\u4e86\u5148\u6765\u5148\u670d\u52a1\u7684\u516c\u5e73\u6027 4\u3001\u65e0\u9965\u997f\uff0c\u961f\u5217\u4e2d\u7684\u6bcf\u4e2a\u7ebf\u7a0b\u90fd\u80fd\u4fdd\u8bc1\u88ab\u6267\u884c\u5230 \u961f\u5217\u9501\u5206\u4e3a\u4e24\u7c7b\uff0c\u4e00\u7c7b\u662f\u57fa\u4e8e\u6709\u754c\u961f\u5217\uff0c\u4e00\u7c7b\u662f\u57fa\u4e8e\u65e0\u754c\u961f\u5217\u3002","title":"csdn \u804a\u804a\u9ad8\u5e76\u53d1\uff08\u4e03\uff09\u5b9e\u73b0\u51e0\u79cd\u81ea\u65cb\u9501\uff08\u4e8c\uff09"},{"location":"Concurrent-computing/Expert-iter_zc/CSDN-%E8%81%8A%E8%81%8A%E9%AB%98%E5%B9%B6%E5%8F%91-7-%E5%AE%9E%E7%8E%B0%E5%87%A0%E7%A7%8D%E8%87%AA%E6%97%8B%E9%94%81%E4%BA%8C/#_1","text":"\u5148\u770b\u4e00\u4e0b\u57fa\u4e8e\u6709\u754c\u961f\u5217\u7684\u961f\u5217\u9501\u3002 ArrayLock \u67093\u4e2a\u7279\u70b9\uff1a \\1. \u57fa\u4e8e\u4e00\u4e2avolatile\u6570\u7ec4\u6765\u7ec4\u7ec7\u7ebf\u7a0b \\2. \u901a\u8fc7\u4e00\u4e2a\u539f\u5b50\u53d8\u91cftail\u6765\u8868\u793a\u5bf9\u5c3e\u7ebf\u7a0b \\3. \u901a\u8fc7\u4e00\u4e2a ThreadLocal \u53d8\u91cf\u7ed9\u6bcf\u4e2a\u7ebf\u7a0b\u4e00\u4e2a\u7d22\u5f15\u53f7\uff0c\u8868\u793a\u5b83\u4f4d\u4e8e\u961f\u5217\u7684\u54ea\u4e2a\u4f4d\u7f6e\u3002","title":"\u57fa\u4e8e\u6709\u754c\u961f\u5217\u7684\u961f\u5217\u9501"},{"location":"Concurrent-computing/%E7%A7%92%E6%9D%80-%E6%B5%81%E9%87%8F%E5%89%8A%E5%B3%B0/","text":"\u79d2\u6740 \u4e00\u3001\u5728\u9605\u8bfb csdn \u7528\u592a\u6781\u62f3\u8bb2\u5206\u5e03\u5f0f\u7406\u8bba\uff0c\u771f\u8212\u670d\uff01 \u65f6\uff0c\u5176\u4e2d\u63d0\u53ca\u4e86 1\u3001 \u6d41\u91cf\u524a\u5cf0 \uff1a\u6bd4\u5982\u591a\u4e2a\u79d2\u6740\u573a\u6b21\uff0c\u67d0\u4e1c\u7684 8 \u70b9\u79d2\u6740\u573a\uff0c12 \u70b9\u7684\u79d2\u6740\u573a\u3002 \u4e8c\u3001\u5206\u5c42\u8fc7\u6ee4\u5c31\u662f\u62e6\u622a\u8bf7\u6c42 \u4e09\u3001\u53c2\u8003\u6587\u7ae0 1\u3001csdn \u5f7b\u5e95\u641e\u61c2\u7f13\u5b58\u7a7f\u900f\uff0c\u7f13\u5b58\u51fb\u7a7f\uff0c\u7f13\u5b58\u96ea\u5d29 2\u3001zhihu \u79d2\u6740\u4e4b\u6d41\u91cf\u524a\u5cf0 3\u3001jianshu \u9ad8\u5e76\u53d1\u67b6\u6784\u7cfb\u5217\uff1a\u4ec0\u4e48\u662f\u6d41\u91cf\u524a\u5cf0\uff1f\u5982\u4f55\u89e3\u51b3\u79d2\u6740\u4e1a\u52a1\u7684\u524a\u5cf0\u573a\u666f zhihu \u79d2\u6740\u4e4b\u6d41\u91cf\u524a\u5cf0 \u6d41\u91cf\u524a\u5cf0\u7684\u4e00\u4e9b\u5e38\u7528\u64cd\u4f5c\u601d\u8def jianshu \u9ad8\u5e76\u53d1\u67b6\u6784\u7cfb\u5217\uff1a\u4ec0\u4e48\u662f\u6d41\u91cf\u524a\u5cf0\uff1f\u5982\u4f55\u89e3\u51b3\u79d2\u6740\u4e1a\u52a1\u7684\u524a\u5cf0\u573a\u666f \u6d41\u91cf\u524a\u5cf0\u7684\u7531\u6765 \u600e\u6837\u6765\u5b9e\u73b0\u6d41\u91cf\u524a\u5cf0\u65b9\u6848 \u524a\u5cf0\u4ece\u672c\u8d28\u4e0a\u6765\u8bf4\u5c31\u662f\u66f4\u591a\u5730\u5ef6\u7f13\u7528\u6237\u8bf7\u6c42\uff0c\u4ee5\u53ca\u5c42\u5c42\u8fc7\u6ee4\u7528\u6237\u7684\u8bbf\u95ee\u9700\u6c42\uff0c\u9075\u4ece\u201c\u6700\u540e\u843d\u5730\u5230\u6570\u636e\u5e93\u7684\u8bf7\u6c42\u6570\u8981\u5c3d\u91cf\u5c11\u201d\u7684\u539f\u5219\u3002 1.\u6d88\u606f\u961f\u5217\u89e3\u51b3\u524a\u5cf0 \u8981\u5bf9\u6d41\u91cf\u8fdb\u884c\u524a\u5cf0\uff0c\u6700\u5bb9\u6613\u60f3\u5230\u7684\u89e3\u51b3\u65b9\u6848\u5c31\u662f\u7528\u6d88\u606f\u961f\u5217\u6765\u7f13\u51b2\u77ac\u65f6\u6d41\u91cf\uff0c\u628a\u540c\u6b65\u7684\u76f4\u63a5\u8c03\u7528\u8f6c\u6362\u6210\u5f02\u6b65\u7684\u95f4\u63a5\u63a8\u9001\uff0c\u4e2d\u95f4\u901a\u8fc7\u4e00\u4e2a\u961f\u5217\u5728\u4e00\u7aef\u627f\u63a5\u77ac\u65f6\u7684\u6d41\u91cf\u6d2a\u5cf0\uff0c\u5728\u53e6\u4e00\u7aef\u5e73\u6ed1\u5730\u5c06\u6d88\u606f\u63a8\u9001\u51fa\u53bb\u3002 \u6d88\u606f\u961f\u5217\u4e2d\u95f4\u4ef6\u4e3b\u8981\u89e3\u51b3\u5e94\u7528\u8026\u5408\uff0c\u5f02\u6b65\u6d88\u606f\uff0c \u6d41\u91cf\u524a\u950b\u7b49\u95ee\u9898\u3002\u5e38\u7528\u6d88\u606f\u961f\u5217\u7cfb\u7edf\uff1a\u76ee\u524d\u5728\u751f\u4ea7\u73af\u5883\uff0c\u4f7f\u7528\u8f83\u591a\u7684\u6d88\u606f\u961f\u5217\u6709 ActiveMQ\u3001RabbitMQ\u3001 ZeroMQ\u3001Kafka\u3001MetaMQ\u3001RocketMQ \u7b49\u3002 \u5728\u8fd9\u91cc\uff0c\u6d88\u606f\u961f\u5217\u5c31\u50cf\u201c\u6c34\u5e93\u201d\u4e00\u6837\uff0c\u62e6\u84c4\u4e0a\u6e38\u7684\u6d2a\u6c34\uff0c\u524a\u51cf\u8fdb\u5165\u4e0b\u6e38\u6cb3\u9053\u7684\u6d2a\u5cf0\u6d41\u91cf\uff0c\u4ece\u800c\u8fbe\u5230\u51cf\u514d\u6d2a\u6c34\u707e\u5bb3\u7684\u76ee\u7684\u3002 2.\u6d41\u91cf\u524a\u5cf0\u6f0f\u6597\uff1a\u5c42\u5c42\u524a\u5cf0 \u9488\u5bf9\u79d2\u6740\u573a\u666f\u8fd8\u6709\u4e00\u79cd\u65b9\u6cd5\uff0c\u5c31\u662f\u5bf9\u8bf7\u6c42\u8fdb\u884c\u5206\u5c42\u8fc7\u6ee4\uff0c\u4ece\u800c\u8fc7\u6ee4\u6389\u4e00\u4e9b\u65e0\u6548\u7684\u8bf7\u6c42\u3002 \u5206\u5c42\u8fc7\u6ee4\u5176\u5b9e\u5c31\u662f\u91c7\u7528\u201c\u6f0f\u6597\u201d\u5f0f\u8bbe\u8ba1\u6765\u5904\u7406\u8bf7\u6c42\u7684\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u8fd9\u6837\u5c31\u50cf\u6f0f\u6597\u4e00\u6837\uff0c\u5c3d\u91cf\u628a\u6570\u636e\u91cf\u548c\u8bf7\u6c42\u91cf\u4e00\u5c42\u4e00\u5c42\u5730\u8fc7\u6ee4\u548c\u51cf\u5c11\u4e86\u3002","title":"Introduction"},{"location":"Concurrent-computing/%E7%A7%92%E6%9D%80-%E6%B5%81%E9%87%8F%E5%89%8A%E5%B3%B0/#_1","text":"\u4e00\u3001\u5728\u9605\u8bfb csdn \u7528\u592a\u6781\u62f3\u8bb2\u5206\u5e03\u5f0f\u7406\u8bba\uff0c\u771f\u8212\u670d\uff01 \u65f6\uff0c\u5176\u4e2d\u63d0\u53ca\u4e86 1\u3001 \u6d41\u91cf\u524a\u5cf0 \uff1a\u6bd4\u5982\u591a\u4e2a\u79d2\u6740\u573a\u6b21\uff0c\u67d0\u4e1c\u7684 8 \u70b9\u79d2\u6740\u573a\uff0c12 \u70b9\u7684\u79d2\u6740\u573a\u3002 \u4e8c\u3001\u5206\u5c42\u8fc7\u6ee4\u5c31\u662f\u62e6\u622a\u8bf7\u6c42 \u4e09\u3001\u53c2\u8003\u6587\u7ae0 1\u3001csdn \u5f7b\u5e95\u641e\u61c2\u7f13\u5b58\u7a7f\u900f\uff0c\u7f13\u5b58\u51fb\u7a7f\uff0c\u7f13\u5b58\u96ea\u5d29 2\u3001zhihu \u79d2\u6740\u4e4b\u6d41\u91cf\u524a\u5cf0 3\u3001jianshu \u9ad8\u5e76\u53d1\u67b6\u6784\u7cfb\u5217\uff1a\u4ec0\u4e48\u662f\u6d41\u91cf\u524a\u5cf0\uff1f\u5982\u4f55\u89e3\u51b3\u79d2\u6740\u4e1a\u52a1\u7684\u524a\u5cf0\u573a\u666f","title":"\u79d2\u6740"},{"location":"Concurrent-computing/%E7%A7%92%E6%9D%80-%E6%B5%81%E9%87%8F%E5%89%8A%E5%B3%B0/#zhihu","text":"\u6d41\u91cf\u524a\u5cf0\u7684\u4e00\u4e9b\u5e38\u7528\u64cd\u4f5c\u601d\u8def","title":"zhihu \u79d2\u6740\u4e4b\u6d41\u91cf\u524a\u5cf0"},{"location":"Concurrent-computing/%E7%A7%92%E6%9D%80-%E6%B5%81%E9%87%8F%E5%89%8A%E5%B3%B0/#jianshu","text":"","title":"jianshu \u9ad8\u5e76\u53d1\u67b6\u6784\u7cfb\u5217\uff1a\u4ec0\u4e48\u662f\u6d41\u91cf\u524a\u5cf0\uff1f\u5982\u4f55\u89e3\u51b3\u79d2\u6740\u4e1a\u52a1\u7684\u524a\u5cf0\u573a\u666f"},{"location":"Concurrent-computing/%E7%A7%92%E6%9D%80-%E6%B5%81%E9%87%8F%E5%89%8A%E5%B3%B0/#_2","text":"","title":"\u6d41\u91cf\u524a\u5cf0\u7684\u7531\u6765"},{"location":"Concurrent-computing/%E7%A7%92%E6%9D%80-%E6%B5%81%E9%87%8F%E5%89%8A%E5%B3%B0/#_3","text":"\u524a\u5cf0\u4ece\u672c\u8d28\u4e0a\u6765\u8bf4\u5c31\u662f\u66f4\u591a\u5730\u5ef6\u7f13\u7528\u6237\u8bf7\u6c42\uff0c\u4ee5\u53ca\u5c42\u5c42\u8fc7\u6ee4\u7528\u6237\u7684\u8bbf\u95ee\u9700\u6c42\uff0c\u9075\u4ece\u201c\u6700\u540e\u843d\u5730\u5230\u6570\u636e\u5e93\u7684\u8bf7\u6c42\u6570\u8981\u5c3d\u91cf\u5c11\u201d\u7684\u539f\u5219\u3002 1.\u6d88\u606f\u961f\u5217\u89e3\u51b3\u524a\u5cf0 \u8981\u5bf9\u6d41\u91cf\u8fdb\u884c\u524a\u5cf0\uff0c\u6700\u5bb9\u6613\u60f3\u5230\u7684\u89e3\u51b3\u65b9\u6848\u5c31\u662f\u7528\u6d88\u606f\u961f\u5217\u6765\u7f13\u51b2\u77ac\u65f6\u6d41\u91cf\uff0c\u628a\u540c\u6b65\u7684\u76f4\u63a5\u8c03\u7528\u8f6c\u6362\u6210\u5f02\u6b65\u7684\u95f4\u63a5\u63a8\u9001\uff0c\u4e2d\u95f4\u901a\u8fc7\u4e00\u4e2a\u961f\u5217\u5728\u4e00\u7aef\u627f\u63a5\u77ac\u65f6\u7684\u6d41\u91cf\u6d2a\u5cf0\uff0c\u5728\u53e6\u4e00\u7aef\u5e73\u6ed1\u5730\u5c06\u6d88\u606f\u63a8\u9001\u51fa\u53bb\u3002 \u6d88\u606f\u961f\u5217\u4e2d\u95f4\u4ef6\u4e3b\u8981\u89e3\u51b3\u5e94\u7528\u8026\u5408\uff0c\u5f02\u6b65\u6d88\u606f\uff0c \u6d41\u91cf\u524a\u950b\u7b49\u95ee\u9898\u3002\u5e38\u7528\u6d88\u606f\u961f\u5217\u7cfb\u7edf\uff1a\u76ee\u524d\u5728\u751f\u4ea7\u73af\u5883\uff0c\u4f7f\u7528\u8f83\u591a\u7684\u6d88\u606f\u961f\u5217\u6709 ActiveMQ\u3001RabbitMQ\u3001 ZeroMQ\u3001Kafka\u3001MetaMQ\u3001RocketMQ \u7b49\u3002 \u5728\u8fd9\u91cc\uff0c\u6d88\u606f\u961f\u5217\u5c31\u50cf\u201c\u6c34\u5e93\u201d\u4e00\u6837\uff0c\u62e6\u84c4\u4e0a\u6e38\u7684\u6d2a\u6c34\uff0c\u524a\u51cf\u8fdb\u5165\u4e0b\u6e38\u6cb3\u9053\u7684\u6d2a\u5cf0\u6d41\u91cf\uff0c\u4ece\u800c\u8fbe\u5230\u51cf\u514d\u6d2a\u6c34\u707e\u5bb3\u7684\u76ee\u7684\u3002 2.\u6d41\u91cf\u524a\u5cf0\u6f0f\u6597\uff1a\u5c42\u5c42\u524a\u5cf0 \u9488\u5bf9\u79d2\u6740\u573a\u666f\u8fd8\u6709\u4e00\u79cd\u65b9\u6cd5\uff0c\u5c31\u662f\u5bf9\u8bf7\u6c42\u8fdb\u884c\u5206\u5c42\u8fc7\u6ee4\uff0c\u4ece\u800c\u8fc7\u6ee4\u6389\u4e00\u4e9b\u65e0\u6548\u7684\u8bf7\u6c42\u3002 \u5206\u5c42\u8fc7\u6ee4\u5176\u5b9e\u5c31\u662f\u91c7\u7528\u201c\u6f0f\u6597\u201d\u5f0f\u8bbe\u8ba1\u6765\u5904\u7406\u8bf7\u6c42\u7684\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u8fd9\u6837\u5c31\u50cf\u6f0f\u6597\u4e00\u6837\uff0c\u5c3d\u91cf\u628a\u6570\u636e\u91cf\u548c\u8bf7\u6c42\u91cf\u4e00\u5c42\u4e00\u5c42\u5730\u8fc7\u6ee4\u548c\u51cf\u5c11\u4e86\u3002","title":"\u600e\u6837\u6765\u5b9e\u73b0\u6d41\u91cf\u524a\u5cf0\u65b9\u6848"},{"location":"Distributed-computing/","text":"Distributed computing \u65f6\u4ee3\u80cc\u666f\u3001\u53d1\u5c55\u8d8b\u52bf \u5728 Parallel-computing \u4e2d\u4ecb\u7ecd\u4e86background\uff1b \u5728 Distributed-computing\\Theory\\Book-Designing-Data-Intensive-Applications \u4e2d\u4e5f\u4ecb\u7ecd\u4e86\u65f6\u4ee3\u80cc\u666f\u3002 Distributed computing and \u5206\u800c\u6cbb\u4e4b\u601d\u60f3 Distributed computing \u5728\u67d0\u79cd\u7a0b\u5ea6\u4e0a\u662f\u9075\u5faa \u5206\u800c\u6cbb\u4e4b\u601d\u60f3 \u7684\u3002 wikipedia Distributed computing Distributed computing is a field of computer science that studies distributed systems. A distributed system is a system whose components are located on different networked computers , which communicate and coordinate their actions by passing messages to one another. The components interact with one another in order to achieve a common goal. NOTE: \u4e00\u3001\u901a\u8fc7\u540e\u9762\u7684\u5185\u5bb9\u6211\u4eec\u5c06\u4f1a\u77e5\u9053: distributed computing\u4e2d\u7684\u5f88\u591a\u95ee\u9898\u90fd\u662f\u57fa\u4e8epassing message\u624d\u80fd\u591f\u89e3\u51b3\u7684\uff0c\u56e0\u6b64\u5728distributed computing\u4e2d\uff0c\u8bbe\u8ba1\u9ad8\u6548\u7684\u3001\u53ef\u9760\u7684protocol\u662f\u975e\u5e38\u5173\u952e\u7684\u5185\u5bb9\uff0c\u5728 Protocol \u7ae0\u8282\u4e2d\uff0c\u5bf9\u5404\u79cdprotocol\u8fdb\u884c\u4e86\u603b\u7ed3\u3002 \u4e8c\u3001\u901a\u8fc7 Byzantine fault \u80fd\u591f\u4fdd\u8bc1\u6211\u4eec\u5feb\u901f\u7406\u89e3\u8fd9\u6bb5\u8bdd\u7684\u5185\u6db5 Three significant characteristics of distributed systems are: 1\u3001concurrency of components 2\u3001 lack of a global clock NOTE: \u8fd9\u662f\u5206\u5e03\u5f0f\u8ba1\u7b97\u7684\u6311\u6218 3\u3001independent failure of components. NOTE: 1\u3001\u7b80\u800c\u8a00\u4e4b\uff0c\u53bb\u4e2d\u5fc3\u5316\u7684 Examples of distributed systems vary from SOA-based systems to massively multiplayer online games to peer-to-peer applications . A computer program that runs within a distributed system is called a distributed program (and distributed programming is the process of writing such programs). There are many different types of implementations for the message passing mechanism, including pure HTTP, RPC-like connectors and message queues . Distributed computing also refers to the use of distributed systems to solve computational problems. In distributed computing , a problem is divided into many tasks, each of which is solved by one or more computers, which communicate with each other via message passing. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u7684\u89e3\u91ca\u975e\u5e38\u7c7b\u4f3c\u4e8e parallel computing \u3002 Parallel and distributed computing Distributed systems are groups of networked computers which share a common goal for their work. The terms \" concurrent computing \", \" parallel computing \", and \"distributed computing\" have a lot of overlap, and no clear distinction exists between them. The same system may be characterized both as \"parallel\" and \"distributed\"; the processors in a typical distributed system run concurrently in parallel. Parallel computing may be seen as a particular tightly coupled form of distributed computing , and distributed computing may be seen as a loosely coupled form of parallel computing . Nevertheless, it is possible to roughly classify concurrent systems as \"parallel\" or \"distributed\" using the following criteria: 1\u3001In parallel computing, all processors may have access to a shared memory to exchange information between processors. 2\u3001In distributed computing, each processor has its own private memory ( distributed memory ). Information is exchanged by passing messages between the processors. (a), (b): a distributed system. \u00a9: a parallel system. The figure on the right illustrates the difference between distributed and parallel systems. Figure (a) is a schematic view of a typical distributed system; the system is represented as a network topology in which each node is a computer and each line connecting the nodes is a communication link. Figure (b) shows the same distributed system in more detail: each computer has its own local memory, and information can be exchanged only by passing messages from one node to another by using the available communication links. Figure \u00a9 shows a parallel system in which each processor has a direct access to a shared memory. The situation is further complicated by the traditional uses of the terms parallel and distributed algorithm that do not quite match the above definitions of parallel and distributed systems (see below for more detailed discussion). Nevertheless, as a rule of thumb, high-performance parallel computation in a shared-memory multiprocessor uses parallel algorithms while the coordination of a large-scale distributed system uses distributed algorithms. Architectures Various hardware and software architectures are used for distributed computing. At a lower level, it is necessary to interconnect multiple CPUs with some sort of network, regardless of whether that network is printed onto a circuit board or made up of loosely coupled devices and cables. At a higher level, it is necessary to interconnect processes running on those CPUs with some sort of communication system . Distributed programming typically falls into one of several basic architectures: client\u2013server , three-tier , n -tier , or peer-to-peer ; or categories: loose coupling , or tight coupling . NOTE: \u53c2\u89c1 Architecture \u7ae0\u8282","title":"Introduction"},{"location":"Distributed-computing/#distributed#computing","text":"","title":"Distributed computing"},{"location":"Distributed-computing/#_1","text":"\u5728 Parallel-computing \u4e2d\u4ecb\u7ecd\u4e86background\uff1b \u5728 Distributed-computing\\Theory\\Book-Designing-Data-Intensive-Applications \u4e2d\u4e5f\u4ecb\u7ecd\u4e86\u65f6\u4ee3\u80cc\u666f\u3002","title":"\u65f6\u4ee3\u80cc\u666f\u3001\u53d1\u5c55\u8d8b\u52bf"},{"location":"Distributed-computing/#distributed#computing#and","text":"Distributed computing \u5728\u67d0\u79cd\u7a0b\u5ea6\u4e0a\u662f\u9075\u5faa \u5206\u800c\u6cbb\u4e4b\u601d\u60f3 \u7684\u3002","title":"Distributed computing and \u5206\u800c\u6cbb\u4e4b\u601d\u60f3"},{"location":"Distributed-computing/#wikipedia#distributed#computing","text":"Distributed computing is a field of computer science that studies distributed systems. A distributed system is a system whose components are located on different networked computers , which communicate and coordinate their actions by passing messages to one another. The components interact with one another in order to achieve a common goal. NOTE: \u4e00\u3001\u901a\u8fc7\u540e\u9762\u7684\u5185\u5bb9\u6211\u4eec\u5c06\u4f1a\u77e5\u9053: distributed computing\u4e2d\u7684\u5f88\u591a\u95ee\u9898\u90fd\u662f\u57fa\u4e8epassing message\u624d\u80fd\u591f\u89e3\u51b3\u7684\uff0c\u56e0\u6b64\u5728distributed computing\u4e2d\uff0c\u8bbe\u8ba1\u9ad8\u6548\u7684\u3001\u53ef\u9760\u7684protocol\u662f\u975e\u5e38\u5173\u952e\u7684\u5185\u5bb9\uff0c\u5728 Protocol \u7ae0\u8282\u4e2d\uff0c\u5bf9\u5404\u79cdprotocol\u8fdb\u884c\u4e86\u603b\u7ed3\u3002 \u4e8c\u3001\u901a\u8fc7 Byzantine fault \u80fd\u591f\u4fdd\u8bc1\u6211\u4eec\u5feb\u901f\u7406\u89e3\u8fd9\u6bb5\u8bdd\u7684\u5185\u6db5 Three significant characteristics of distributed systems are: 1\u3001concurrency of components 2\u3001 lack of a global clock NOTE: \u8fd9\u662f\u5206\u5e03\u5f0f\u8ba1\u7b97\u7684\u6311\u6218 3\u3001independent failure of components. NOTE: 1\u3001\u7b80\u800c\u8a00\u4e4b\uff0c\u53bb\u4e2d\u5fc3\u5316\u7684 Examples of distributed systems vary from SOA-based systems to massively multiplayer online games to peer-to-peer applications . A computer program that runs within a distributed system is called a distributed program (and distributed programming is the process of writing such programs). There are many different types of implementations for the message passing mechanism, including pure HTTP, RPC-like connectors and message queues . Distributed computing also refers to the use of distributed systems to solve computational problems. In distributed computing , a problem is divided into many tasks, each of which is solved by one or more computers, which communicate with each other via message passing. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u7684\u89e3\u91ca\u975e\u5e38\u7c7b\u4f3c\u4e8e parallel computing \u3002","title":"wikipedia Distributed computing"},{"location":"Distributed-computing/#parallel#and#distributed#computing","text":"Distributed systems are groups of networked computers which share a common goal for their work. The terms \" concurrent computing \", \" parallel computing \", and \"distributed computing\" have a lot of overlap, and no clear distinction exists between them. The same system may be characterized both as \"parallel\" and \"distributed\"; the processors in a typical distributed system run concurrently in parallel. Parallel computing may be seen as a particular tightly coupled form of distributed computing , and distributed computing may be seen as a loosely coupled form of parallel computing . Nevertheless, it is possible to roughly classify concurrent systems as \"parallel\" or \"distributed\" using the following criteria: 1\u3001In parallel computing, all processors may have access to a shared memory to exchange information between processors. 2\u3001In distributed computing, each processor has its own private memory ( distributed memory ). Information is exchanged by passing messages between the processors. (a), (b): a distributed system. \u00a9: a parallel system. The figure on the right illustrates the difference between distributed and parallel systems. Figure (a) is a schematic view of a typical distributed system; the system is represented as a network topology in which each node is a computer and each line connecting the nodes is a communication link. Figure (b) shows the same distributed system in more detail: each computer has its own local memory, and information can be exchanged only by passing messages from one node to another by using the available communication links. Figure \u00a9 shows a parallel system in which each processor has a direct access to a shared memory. The situation is further complicated by the traditional uses of the terms parallel and distributed algorithm that do not quite match the above definitions of parallel and distributed systems (see below for more detailed discussion). Nevertheless, as a rule of thumb, high-performance parallel computation in a shared-memory multiprocessor uses parallel algorithms while the coordination of a large-scale distributed system uses distributed algorithms.","title":"Parallel and distributed computing"},{"location":"Distributed-computing/#architectures","text":"Various hardware and software architectures are used for distributed computing. At a lower level, it is necessary to interconnect multiple CPUs with some sort of network, regardless of whether that network is printed onto a circuit board or made up of loosely coupled devices and cables. At a higher level, it is necessary to interconnect processes running on those CPUs with some sort of communication system . Distributed programming typically falls into one of several basic architectures: client\u2013server , three-tier , n -tier , or peer-to-peer ; or categories: loose coupling , or tight coupling . NOTE: \u53c2\u89c1 Architecture \u7ae0\u8282","title":"Architectures"},{"location":"Distributed-computing/Architecture/","text":"Architecture of distributed computing distributed computing\u662f\u4e00\u4e2a\u975e\u5e38\u5bbd\u6cdb\u7684\u6982\u5ff5\uff0c\u975e\u5e38\u591a\u7684\u67b6\u6784\u90fd\u53ef\u4ee5\u5f52\u5165\u5230distributed computing\u7684\u8303\u8f74\u4e2d\u3002\u672c\u7ae0\u68b3\u7406\u8fd9\u4e9b\u67b6\u6784\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u67b6\u6784\u4e4b\u95f4\u6ca1\u6709\u4f18\u52a3\u4e4b\u5206\uff0c\u53ea\u6709\u8c01\u6bd4\u8c01\u66f4\u52a0\u9002\u5408\u4e8e\u5177\u4f53\u7684\u95ee\u9898\u3002 Client-server VS peer-to-peer Peer-to-peer \u548c Client\u2013server \u8fd9\u4e24\u79cdArchitectures\u662f Distributed computing application \u6700\u6700\u5e38\u89c1\u7684\u4e24\u79cdarchitecture\uff0c\u6709\u5fc5\u8981\u5bf9\u8fd9\u4e24\u79cdarchitecture\u8fdb\u884c\u5bf9\u6bd4\u3002 \u5728 wikipedia Peer-to-peer \u4e2d\uff0c\u4e00\u76f4\u62ff\u4ed6\u548c Client\u2013server \u8fdb\u884c\u5bf9\u6bd4\uff0c\u540c\u6837 Client\u2013server \u4e5f\u4e13\u95e8\u603b\u7ed3\u4e86\u5b83\u548c Peer-to-peer \u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u4e0b\u9762\u68b3\u7406\u4e00\u4e0b\u8bbe\u8ba1\u5230\u4e24\u8005\u5bf9\u6bd4\u7684\u5185\u5bb9\u7684\u7ae0\u8282\uff1a 1\u3001 Peer-to-peer \u5728\" Peer-to-peer # Resilient and scalable computer networks \"\u5c0f\u8282\u4e2d\u4ece single point of failure \u4e2d\u5bf9\u6bd4\u4e86\u4e24\u8005\u3002 2\u3001 Client\u2013server model \u5728 Client\u2013server model # Comparison with peer-to-peer architecture \u4e2d\u5bf9\u4e24\u8005\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002\u8fd9\u4e00\u6bb5\u7684\u5185\u5bb9\u7ed9\u6211\u7684\u4e00\u4e2a\u91cd\u8981\u63d0\u793a\u662f\uff1a Peer-to-peer \u662f decentralized system \uff0c\u4f46\u662f Client\u2013server architecture \u4e0d\u662fcentralized system\u3002 \u9664\u6b64\u4e4b\u5916\uff0c\u5728\u4e00\u4e9b\u5e94\u7528\u4e2d\uff0c\u4e5f\u80fd\u591f\u770b\u5230\u5bf9\u8fd9\u4e24\u79cd\u67b6\u6784\u7684\u6bd4\u8f83\uff0c\u6bd4\u5982\uff1a 1\u3001git \u5728Progit book\u7684 1.1 Getting Started - About Version Control \u4e2d\u5c31\u5bf9\u6bd4\u4e86 Distributed version control \u548c Centralized version control \u3002 TODO P2P VS shared nothing architecture \u6211\u89c9\u5f97\u4e24\u79cd\u7684\u4e00\u4e2a\u5171\u6027\u662f: decentralized\u3002 Centralized VS Decentralized \u641c\u7f57\u4e86\u4e00\u4e0b\uff0c\u7ef4\u57fa\u767e\u79d1\u4e2d\u5173\u4e8e\u8fd9\u4e24\u8005\u7684\u6587\u7ae0\u6709\u5982\u4e0b\uff1a wikipedia Decentralized computing wikipedia Centralized computing wikipedia Decentralised system \u4e0a\u8ff0\u6587\u7ae0\u4e2d\uff0c Decentralised system \u662f\u6700\u6700\u901a\u4fd7\u6613\u61c2\u7684\uff0c\u524d\u4e24\u7bc7\u8d28\u91cf\u4e00\u822c\u3002","title":"Introduction"},{"location":"Distributed-computing/Architecture/#architecture#of#distributed#computing","text":"distributed computing\u662f\u4e00\u4e2a\u975e\u5e38\u5bbd\u6cdb\u7684\u6982\u5ff5\uff0c\u975e\u5e38\u591a\u7684\u67b6\u6784\u90fd\u53ef\u4ee5\u5f52\u5165\u5230distributed computing\u7684\u8303\u8f74\u4e2d\u3002\u672c\u7ae0\u68b3\u7406\u8fd9\u4e9b\u67b6\u6784\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u67b6\u6784\u4e4b\u95f4\u6ca1\u6709\u4f18\u52a3\u4e4b\u5206\uff0c\u53ea\u6709\u8c01\u6bd4\u8c01\u66f4\u52a0\u9002\u5408\u4e8e\u5177\u4f53\u7684\u95ee\u9898\u3002","title":"Architecture of distributed computing"},{"location":"Distributed-computing/Architecture/#client-server#vs#peer-to-peer","text":"Peer-to-peer \u548c Client\u2013server \u8fd9\u4e24\u79cdArchitectures\u662f Distributed computing application \u6700\u6700\u5e38\u89c1\u7684\u4e24\u79cdarchitecture\uff0c\u6709\u5fc5\u8981\u5bf9\u8fd9\u4e24\u79cdarchitecture\u8fdb\u884c\u5bf9\u6bd4\u3002 \u5728 wikipedia Peer-to-peer \u4e2d\uff0c\u4e00\u76f4\u62ff\u4ed6\u548c Client\u2013server \u8fdb\u884c\u5bf9\u6bd4\uff0c\u540c\u6837 Client\u2013server \u4e5f\u4e13\u95e8\u603b\u7ed3\u4e86\u5b83\u548c Peer-to-peer \u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u4e0b\u9762\u68b3\u7406\u4e00\u4e0b\u8bbe\u8ba1\u5230\u4e24\u8005\u5bf9\u6bd4\u7684\u5185\u5bb9\u7684\u7ae0\u8282\uff1a 1\u3001 Peer-to-peer \u5728\" Peer-to-peer # Resilient and scalable computer networks \"\u5c0f\u8282\u4e2d\u4ece single point of failure \u4e2d\u5bf9\u6bd4\u4e86\u4e24\u8005\u3002 2\u3001 Client\u2013server model \u5728 Client\u2013server model # Comparison with peer-to-peer architecture \u4e2d\u5bf9\u4e24\u8005\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002\u8fd9\u4e00\u6bb5\u7684\u5185\u5bb9\u7ed9\u6211\u7684\u4e00\u4e2a\u91cd\u8981\u63d0\u793a\u662f\uff1a Peer-to-peer \u662f decentralized system \uff0c\u4f46\u662f Client\u2013server architecture \u4e0d\u662fcentralized system\u3002 \u9664\u6b64\u4e4b\u5916\uff0c\u5728\u4e00\u4e9b\u5e94\u7528\u4e2d\uff0c\u4e5f\u80fd\u591f\u770b\u5230\u5bf9\u8fd9\u4e24\u79cd\u67b6\u6784\u7684\u6bd4\u8f83\uff0c\u6bd4\u5982\uff1a 1\u3001git \u5728Progit book\u7684 1.1 Getting Started - About Version Control \u4e2d\u5c31\u5bf9\u6bd4\u4e86 Distributed version control \u548c Centralized version control \u3002","title":"Client-server VS peer-to-peer"},{"location":"Distributed-computing/Architecture/#todo#p2p#vs#shared#nothing#architecture","text":"\u6211\u89c9\u5f97\u4e24\u79cd\u7684\u4e00\u4e2a\u5171\u6027\u662f: decentralized\u3002","title":"TODO P2P  VS shared nothing architecture"},{"location":"Distributed-computing/Architecture/#centralized#vs#decentralized","text":"\u641c\u7f57\u4e86\u4e00\u4e0b\uff0c\u7ef4\u57fa\u767e\u79d1\u4e2d\u5173\u4e8e\u8fd9\u4e24\u8005\u7684\u6587\u7ae0\u6709\u5982\u4e0b\uff1a wikipedia Decentralized computing wikipedia Centralized computing wikipedia Decentralised system \u4e0a\u8ff0\u6587\u7ae0\u4e2d\uff0c Decentralised system \u662f\u6700\u6700\u901a\u4fd7\u6613\u61c2\u7684\uff0c\u524d\u4e24\u7bc7\u8d28\u91cf\u4e00\u822c\u3002","title":"Centralized VS Decentralized"},{"location":"Distributed-computing/Architecture/Client%E2%80%93server-architecture/","text":"Client\u2013server model wikipedia Client\u2013server model Client\u2013server model is a distributed application structure that partitions tasks or workloads between the providers of a resource or service, called servers , and service requesters, called clients .","title":"Introduction"},{"location":"Distributed-computing/Architecture/Client%E2%80%93server-architecture/#clientserver#model","text":"","title":"Client\u2013server model"},{"location":"Distributed-computing/Architecture/Client%E2%80%93server-architecture/#wikipedia#clientserver#model","text":"Client\u2013server model is a distributed application structure that partitions tasks or workloads between the providers of a resource or service, called servers , and service requesters, called clients .","title":"wikipedia Client\u2013server model"},{"location":"Distributed-computing/Architecture/Peer-to-peer-architecture/","text":"wikipedia Peer-to-peer Peer-to-peer ( P2P ) computing or networking is a distributed application architecture that partitions tasks or workloads between peers. Peers are equally privileged, equipotent\uff08\u5747\u7b49\u7684\uff09 participants in the application. They are said to form a peer-to-peer network of nodes. NOTE: \u601d\u8003\uff1a\u5982\u4f55partition\uff1f\u57fa\u4e8e\u4ec0\u4e48\u8fdb\u884cpartition\uff1f Peers make a portion of their resources, such as processing power, disk storage or network bandwidth, directly available to other network participants, without the need for central coordination by servers or stable hosts. Peers are both suppliers and consumers of resources, in contrast to the traditional client-server model in which the consumption and supply of resources is divided. NOTE: \u8fd9\u6bb5\u8bdd\u89e3\u91ca\u4e86\u4e0a\u4e00\u6bb5\u4e2d\u6240\u8c13\u7684equally privileged\u3001equipotent\u7684\u542b\u4e49\uff0c\u7b80\u5355\u8bf4\u6765\u5c31\u662fpeer-to-peer network\u4e2d\uff0c\u6bcf\u4e2a\u8282\u70b9\u7684\u540c\u65f6\u517c\u5177supplier \u548c consumer \u89d2\u8272\uff08\u6216 client \u548c server \u89d2\u8272\uff09\u3002\u5728\u4e0b\u9762\u7684 Architecture \u7ae0\u8282\u4e5f\u5bf9\u6b64\u8fdb\u884c\u4e86\u8bf4\u660e\u3002 Emerging collaborative P2P systems are going beyond the era of peers doing similar things while sharing resources, and are looking for diverse peers that can bring in unique resources and capabilities to a virtual community thereby empowering it to engage in greater tasks beyond those that can be accomplished by individual peers, yet that are beneficial to all the peers. NOTE: \u8fd9\u6bb5\u8bdd\u7684\u7ed3\u6784\u662f\"are going\"\u548c\"are looking for \"\u662f\u5e76\u5217\u7684\uff08\u7531\"are looking for \"\u524d\u9762\u7684\"and\"\u8fde\u63a5\uff09\u3002 While P2P systems had previously been used in many application domains, the architecture was popularized by the file sharing system Napster , originally released in 1999. The concept has inspired new structures and philosophies in many areas of human interaction. In such social contexts, peer-to-peer as a meme refers to the egalitarian social networking that has emerged throughout society, enabled by Internet technologies in general. Historical development The peer-to-peer movement allowed millions of Internet users to connect \"directly, forming groups and collaborating to become user-created search engines, virtual supercomputers, and filesystems.\" NOTE: \u8fd9\u53e5\u8bdd\u603b\u7ed3\u7684\u975e\u5e38\u597d Architecture A peer-to-peer network is designed around the notion of equal peer nodes simultaneously functioning as both \"clients\" and \"servers\" to the other nodes on the network. This model of network arrangement differs from the client\u2013server model where communication is usually to and from a central server. A typical example of a file transfer that uses the client-server model is the File Transfer Protocol (FTP) service in which the client and server programs are distinct: the clients initiate the transfer, and the servers satisfy these requests. Routing and resource discovery Peer-to-peer networks generally implement some form of virtual overlay network on top of the physical network topology, where the nodes in the overlay form a subset of the nodes in the physical network. Data is still exchanged directly over the underlying TCP/IP network, but at the application layer peers are able to communicate with each other directly, via the logical overlay links (each of which corresponds to a path through the underlying physical network). Overlays are used for indexing and peer discovery, and make the P2P system independent from the physical network topology. Based on how the nodes are linked to each other within the overlay network, and how resources are indexed and located, we can classify networks as unstructured or structured (or as a hybrid between the two). NOTE: \u8bfb\u4e86\u4e0b\u9762\u7684Structured networks\uff0c\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u5185\u5bb9\u5c31\u597d\u7406\u89e3\u4e86\u3002 Structured networks In structured peer-to-peer networks the overlay is organized into a specific topology, and the protocol ensures that any node can efficiently search the network for a file/resource, even if the resource is extremely rare. The most common type of structured P2P networks implement a distributed hash table (DHT), in which a variant of consistent hashing is used to assign ownership of each file to a particular peer. This enables peers to search for resources on the network using a hash table : that is, ( key , value ) pairs are stored in the DHT, and any participating node can efficiently retrieve the value associated with a given key. However, in order to route traffic efficiently through the network, nodes in a structured overlay must maintain lists of neighbors that satisfy specific criteria. This makes them less robust in networks with a high rate of churn (i.e. with large numbers of nodes frequently joining and leaving the network). More recent evaluation of P2P resource discovery solutions under real workloads have pointed out several issues in DHT-based solutions such as high cost of advertising/discovering resources and static and dynamic load imbalance. Notable distributed networks that use DHTs include Tixati , an alternative to BitTorrent's distributed tracker, the Kad network , the Storm botnet , YaCy , and the Coral Content Distribution Network . Some prominent research projects include the Chord project , Kademlia , PAST storage utility , P-Grid , a self-organized and emerging overlay network, and CoopNet content distribution system . DHT-based networks have also been widely utilized for accomplishing efficient resource discovery for grid computing systems, as it aids in resource management and scheduling of applications. NOTE: Redis cluster \u5c31\u662f\u91c7\u7528\u7684\u8fd9\u79cd\u7ed3\u6784\u3002 Unstructured networks NOTE: \u76ee\u524d\u8fd8\u6ca1\u6709\u63a5\u89e6\u8fc7\u8fd9\u79cd\u7ed3\u6784\u7684 Hybrid models Hybrid models are a combination of peer-to-peer and client-server models. A common hybrid model is to have a central server that helps peers find each other. NOTE: \u76ee\u524d\u8fd8\u6ca1\u6709\u63a5\u89e6\u8fc7\u8fd9\u79cd\u7ed3\u6784 Security and trust Resilient and scalable computer networks See also: Wireless mesh network and Distributed computing The decentralized nature of P2P networks increases robustness because it removes the single point of failure that can be inherent in a client-server based system. As nodes arrive and demand on the system increases, the total capacity of the system also increases, and the likelihood of failure decreases. If one peer on the network fails to function properly, the whole network is not compromised or damaged. In contrast, in a typical client\u2013server architecture, clients share only their demands with the system, but not their resources. In this case, as more clients join the system, fewer resources are available to serve each client, and if the central server fails, the entire network is taken down. NOTE: \u8fd9\u6bb5\u6240\u9610\u8ff0\u7684\u5176\u5b9e\u7684P2P architecture\u76f8\u5bf9\u4e8eclient\u2013server architecture\u7684\u4f18\u52bf\u6240\u5728\u3002 Distributed storage and search Applications Content delivery File-sharing networks Other P2P applications Bitcoin and alternatives such as Ether , Nxt and Peercoin are peer-to-peer-based digital cryptocurrencies . \u5982\u4e0b\u662f\u8865\u5145\u7684 1\u3001 Distributed version control 2\u3001 Redis Cluster TODO digitalcitizen What are P2P (peer-to-peer) networks and what are they used for?","title":"Introduction"},{"location":"Distributed-computing/Architecture/Peer-to-peer-architecture/#wikipedia#peer-to-peer","text":"Peer-to-peer ( P2P ) computing or networking is a distributed application architecture that partitions tasks or workloads between peers. Peers are equally privileged, equipotent\uff08\u5747\u7b49\u7684\uff09 participants in the application. They are said to form a peer-to-peer network of nodes. NOTE: \u601d\u8003\uff1a\u5982\u4f55partition\uff1f\u57fa\u4e8e\u4ec0\u4e48\u8fdb\u884cpartition\uff1f Peers make a portion of their resources, such as processing power, disk storage or network bandwidth, directly available to other network participants, without the need for central coordination by servers or stable hosts. Peers are both suppliers and consumers of resources, in contrast to the traditional client-server model in which the consumption and supply of resources is divided. NOTE: \u8fd9\u6bb5\u8bdd\u89e3\u91ca\u4e86\u4e0a\u4e00\u6bb5\u4e2d\u6240\u8c13\u7684equally privileged\u3001equipotent\u7684\u542b\u4e49\uff0c\u7b80\u5355\u8bf4\u6765\u5c31\u662fpeer-to-peer network\u4e2d\uff0c\u6bcf\u4e2a\u8282\u70b9\u7684\u540c\u65f6\u517c\u5177supplier \u548c consumer \u89d2\u8272\uff08\u6216 client \u548c server \u89d2\u8272\uff09\u3002\u5728\u4e0b\u9762\u7684 Architecture \u7ae0\u8282\u4e5f\u5bf9\u6b64\u8fdb\u884c\u4e86\u8bf4\u660e\u3002 Emerging collaborative P2P systems are going beyond the era of peers doing similar things while sharing resources, and are looking for diverse peers that can bring in unique resources and capabilities to a virtual community thereby empowering it to engage in greater tasks beyond those that can be accomplished by individual peers, yet that are beneficial to all the peers. NOTE: \u8fd9\u6bb5\u8bdd\u7684\u7ed3\u6784\u662f\"are going\"\u548c\"are looking for \"\u662f\u5e76\u5217\u7684\uff08\u7531\"are looking for \"\u524d\u9762\u7684\"and\"\u8fde\u63a5\uff09\u3002 While P2P systems had previously been used in many application domains, the architecture was popularized by the file sharing system Napster , originally released in 1999. The concept has inspired new structures and philosophies in many areas of human interaction. In such social contexts, peer-to-peer as a meme refers to the egalitarian social networking that has emerged throughout society, enabled by Internet technologies in general.","title":"wikipedia Peer-to-peer"},{"location":"Distributed-computing/Architecture/Peer-to-peer-architecture/#historical#development","text":"The peer-to-peer movement allowed millions of Internet users to connect \"directly, forming groups and collaborating to become user-created search engines, virtual supercomputers, and filesystems.\" NOTE: \u8fd9\u53e5\u8bdd\u603b\u7ed3\u7684\u975e\u5e38\u597d","title":"Historical development"},{"location":"Distributed-computing/Architecture/Peer-to-peer-architecture/#architecture","text":"A peer-to-peer network is designed around the notion of equal peer nodes simultaneously functioning as both \"clients\" and \"servers\" to the other nodes on the network. This model of network arrangement differs from the client\u2013server model where communication is usually to and from a central server. A typical example of a file transfer that uses the client-server model is the File Transfer Protocol (FTP) service in which the client and server programs are distinct: the clients initiate the transfer, and the servers satisfy these requests.","title":"Architecture"},{"location":"Distributed-computing/Architecture/Peer-to-peer-architecture/#routing#and#resource#discovery","text":"Peer-to-peer networks generally implement some form of virtual overlay network on top of the physical network topology, where the nodes in the overlay form a subset of the nodes in the physical network. Data is still exchanged directly over the underlying TCP/IP network, but at the application layer peers are able to communicate with each other directly, via the logical overlay links (each of which corresponds to a path through the underlying physical network). Overlays are used for indexing and peer discovery, and make the P2P system independent from the physical network topology. Based on how the nodes are linked to each other within the overlay network, and how resources are indexed and located, we can classify networks as unstructured or structured (or as a hybrid between the two). NOTE: \u8bfb\u4e86\u4e0b\u9762\u7684Structured networks\uff0c\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u5185\u5bb9\u5c31\u597d\u7406\u89e3\u4e86\u3002","title":"Routing and resource discovery"},{"location":"Distributed-computing/Architecture/Peer-to-peer-architecture/#structured#networks","text":"In structured peer-to-peer networks the overlay is organized into a specific topology, and the protocol ensures that any node can efficiently search the network for a file/resource, even if the resource is extremely rare. The most common type of structured P2P networks implement a distributed hash table (DHT), in which a variant of consistent hashing is used to assign ownership of each file to a particular peer. This enables peers to search for resources on the network using a hash table : that is, ( key , value ) pairs are stored in the DHT, and any participating node can efficiently retrieve the value associated with a given key. However, in order to route traffic efficiently through the network, nodes in a structured overlay must maintain lists of neighbors that satisfy specific criteria. This makes them less robust in networks with a high rate of churn (i.e. with large numbers of nodes frequently joining and leaving the network). More recent evaluation of P2P resource discovery solutions under real workloads have pointed out several issues in DHT-based solutions such as high cost of advertising/discovering resources and static and dynamic load imbalance. Notable distributed networks that use DHTs include Tixati , an alternative to BitTorrent's distributed tracker, the Kad network , the Storm botnet , YaCy , and the Coral Content Distribution Network . Some prominent research projects include the Chord project , Kademlia , PAST storage utility , P-Grid , a self-organized and emerging overlay network, and CoopNet content distribution system . DHT-based networks have also been widely utilized for accomplishing efficient resource discovery for grid computing systems, as it aids in resource management and scheduling of applications. NOTE: Redis cluster \u5c31\u662f\u91c7\u7528\u7684\u8fd9\u79cd\u7ed3\u6784\u3002","title":"Structured networks"},{"location":"Distributed-computing/Architecture/Peer-to-peer-architecture/#unstructured#networks","text":"NOTE: \u76ee\u524d\u8fd8\u6ca1\u6709\u63a5\u89e6\u8fc7\u8fd9\u79cd\u7ed3\u6784\u7684","title":"Unstructured networks"},{"location":"Distributed-computing/Architecture/Peer-to-peer-architecture/#hybrid#models","text":"Hybrid models are a combination of peer-to-peer and client-server models. A common hybrid model is to have a central server that helps peers find each other. NOTE: \u76ee\u524d\u8fd8\u6ca1\u6709\u63a5\u89e6\u8fc7\u8fd9\u79cd\u7ed3\u6784","title":"Hybrid models"},{"location":"Distributed-computing/Architecture/Peer-to-peer-architecture/#security#and#trust","text":"","title":"Security and trust"},{"location":"Distributed-computing/Architecture/Peer-to-peer-architecture/#resilient#and#scalable#computer#networks","text":"See also: Wireless mesh network and Distributed computing The decentralized nature of P2P networks increases robustness because it removes the single point of failure that can be inherent in a client-server based system. As nodes arrive and demand on the system increases, the total capacity of the system also increases, and the likelihood of failure decreases. If one peer on the network fails to function properly, the whole network is not compromised or damaged. In contrast, in a typical client\u2013server architecture, clients share only their demands with the system, but not their resources. In this case, as more clients join the system, fewer resources are available to serve each client, and if the central server fails, the entire network is taken down. NOTE: \u8fd9\u6bb5\u6240\u9610\u8ff0\u7684\u5176\u5b9e\u7684P2P architecture\u76f8\u5bf9\u4e8eclient\u2013server architecture\u7684\u4f18\u52bf\u6240\u5728\u3002","title":"Resilient and scalable computer networks"},{"location":"Distributed-computing/Architecture/Peer-to-peer-architecture/#distributed#storage#and#search","text":"","title":"Distributed storage and search"},{"location":"Distributed-computing/Architecture/Peer-to-peer-architecture/#applications","text":"","title":"Applications"},{"location":"Distributed-computing/Architecture/Peer-to-peer-architecture/#content#delivery","text":"","title":"Content delivery"},{"location":"Distributed-computing/Architecture/Peer-to-peer-architecture/#file-sharing#networks","text":"","title":"File-sharing networks"},{"location":"Distributed-computing/Architecture/Peer-to-peer-architecture/#other#p2p#applications","text":"Bitcoin and alternatives such as Ether , Nxt and Peercoin are peer-to-peer-based digital cryptocurrencies .","title":"Other P2P applications"},{"location":"Distributed-computing/Architecture/Peer-to-peer-architecture/#_1","text":"1\u3001 Distributed version control 2\u3001 Redis Cluster","title":"\u5982\u4e0b\u662f\u8865\u5145\u7684"},{"location":"Distributed-computing/Architecture/Peer-to-peer-architecture/#todo","text":"digitalcitizen What are P2P (peer-to-peer) networks and what are they used for?","title":"TODO"},{"location":"Distributed-computing/Architecture/Shared-nothing-architecture/","text":"Shared-nothing architecture wanweibaike Shared-nothing architecture A shared-nothing architecture ( SN ) is a distributed computing architecture in which each update request is satisfied by a single node (processor/memory/storage unit). The intent is to eliminate contention among nodes. Nodes do not share (independently access) the same memory or storage. One alternative architecture is shared everything, in which requests are satisfied by arbitrary combinations of nodes. This may introduce contention, as multiple nodes may seek to update the same data at the same time. NOTE: \u4e24\u79cd\u5b8c\u5168\u7684\u6781\u7aef: 1\u3001shared-nothing architecture no contention 2\u3001shared-everything architecture SN eliminates single points of failure , allowing the overall system to continue operating despite failures in individual nodes and allowing individual nodes to upgrade without a system-wide shutdown. NOTE: \u4e00\u3001SN architecture\u7684\u4f18\u52bf: 1\u3001eliminates single points of failure 2\u3001\u80fd\u591f\u72ec\u7acb\u5347\u7ea7 3\u3001scalability \u8fd9\u662f\u4e0b\u9762\u8fd9\u4e00\u6bb5\u63d0\u51fa\u7684 A SN system can scale simply by adding nodes, since no central resource bottlenecks the system.[ 2] In databases, another term for SN is sharding . A SN system typically partitions its data among many nodes. A refinement is to replicate commonly used but infrequently modified data across many nodes, allowing more requests to be resolved on a single node. NOTE: \u5173\u4e8e sharding \uff0c\u53c2\u89c1 Distributed-data-store\\Data-Sharding-Strategy \u7ae0\u8282 TODO colinbreck Shared-Nothing Architectures for Server Replication and Synchronization","title":"Introduction"},{"location":"Distributed-computing/Architecture/Shared-nothing-architecture/#shared-nothing#architecture","text":"","title":"Shared-nothing architecture"},{"location":"Distributed-computing/Architecture/Shared-nothing-architecture/#wanweibaike#shared-nothing#architecture","text":"A shared-nothing architecture ( SN ) is a distributed computing architecture in which each update request is satisfied by a single node (processor/memory/storage unit). The intent is to eliminate contention among nodes. Nodes do not share (independently access) the same memory or storage. One alternative architecture is shared everything, in which requests are satisfied by arbitrary combinations of nodes. This may introduce contention, as multiple nodes may seek to update the same data at the same time. NOTE: \u4e24\u79cd\u5b8c\u5168\u7684\u6781\u7aef: 1\u3001shared-nothing architecture no contention 2\u3001shared-everything architecture SN eliminates single points of failure , allowing the overall system to continue operating despite failures in individual nodes and allowing individual nodes to upgrade without a system-wide shutdown. NOTE: \u4e00\u3001SN architecture\u7684\u4f18\u52bf: 1\u3001eliminates single points of failure 2\u3001\u80fd\u591f\u72ec\u7acb\u5347\u7ea7 3\u3001scalability \u8fd9\u662f\u4e0b\u9762\u8fd9\u4e00\u6bb5\u63d0\u51fa\u7684 A SN system can scale simply by adding nodes, since no central resource bottlenecks the system.[ 2] In databases, another term for SN is sharding . A SN system typically partitions its data among many nodes. A refinement is to replicate commonly used but infrequently modified data across many nodes, allowing more requests to be resolved on a single node. NOTE: \u5173\u4e8e sharding \uff0c\u53c2\u89c1 Distributed-data-store\\Data-Sharding-Strategy \u7ae0\u8282","title":"wanweibaike Shared-nothing architecture"},{"location":"Distributed-computing/Architecture/Shared-nothing-architecture/#todo","text":"colinbreck Shared-Nothing Architectures for Server Replication and Synchronization","title":"TODO"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u662f\u9605\u8bfb Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems \u7684\u7b14\u8bb0\u3002\u201cdata-intensive\u201d\u7684\u542b\u4e49\u662f\u201c\u6570\u636e\u5bc6\u96c6\u7684\u201d\u3002 \u5199\u4f5c\u601d\u8def \u4f5c\u8005\u5199\u4f5c\u672c\u4e66\u7684\u601d\u8def\u53ef\u4ee5\u6982\u62ec\u5982\u4e0b: \u6211\u4eec\u7684\u76ee\u6807 \u5728 CHAPTER 1 Reliable, Scalable, and Maintainable Applications \u4e2d\uff0c\u4f5c\u8005\u9996\u5148\u9610\u8ff0\u4e86\u76ee\u6807\uff0c\u540e\u9762\u7684\u5f88\u591a\u5185\u5bb9\u90fd\u662f\u56f4\u7ed5\u8fd9\u4e9b\u76ee\u6807\u800c\u5c55\u5f00\u7684\u3002 \u4e3a\u4ec0\u4e48\u9700\u8981distributed computing\uff1f \u8fd9\u4e2a\u95ee\u9898\u5728Preface\u7ae0\u8282\u4e2d\u7ed9\u51fa\u4e86\u56de\u7b54\uff0c\u4e0b\u9762\u662f\u6211\u8ba4\u4e3a\u7684\u975e\u5e38\u91cd\u8981\u7684\u539f\u56e0: 1) Parallel computing\u7684\u5174\u8d77 2) Networks are getting faster 3) HA \u663e\u7136distributed computing\u7684\u5174\u8d77\uff0c\u80cc\u540e\u662f\u7531\u4e8e\u5f53\u4eca\u8ba1\u7b97\u673a\u79d1\u5b66\u7684\u73b0\u72b6\u6240\u51b3\u5b9a\u7684\u3002 distributed computing\u7684\u6311\u6218\u6709\u54ea\u4e9b\uff1f \u8fd9\u4e2a\u95ee\u9898\u5728 PART II Distributed Data#CHAPTER 8 The Trouble with Distributed Systems \u8fdb\u884c\u4e86\u8ba8\u8bba\uff0c\u4e3b\u8981\u539f\u56e0\u5982\u4e0b: 1) Faults and Partial Failures 2) Unreliable Networks 3) Unreliable Clocks 4) Knowledge, Truth, and Lies \u6211\u89c9\u5f97\u4e0a\u8ff0\u8fd9\u4e9b\u6311\u6218\u7684\u6839\u672c\u539f\u56e0\u5728\u4e8e: distributed computing\u4e2d\uff0c\u7f3a\u4e4f\u4e2d\u5fc3\u89d2\u8272\uff0c\u5373\u5b83\u662f\u53bb\u4e2d\u5fc3\u5316\u7684\u3002 \u5982\u4f55\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff1f \u5728 CHAPTER 9 Consistency and Consensus \u4e2d\uff0c\u4f5c\u8005\u7ed9\u51fa\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u7684\u4e00\u822c\u601d\u8def: \u521b\u9020\u5408\u9002\u7684\u62bd\u8c61\u3002","title":"Introduction"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/#_1","text":"\u672c\u7ae0\u662f\u9605\u8bfb Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems \u7684\u7b14\u8bb0\u3002\u201cdata-intensive\u201d\u7684\u542b\u4e49\u662f\u201c\u6570\u636e\u5bc6\u96c6\u7684\u201d\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/#_2","text":"\u4f5c\u8005\u5199\u4f5c\u672c\u4e66\u7684\u601d\u8def\u53ef\u4ee5\u6982\u62ec\u5982\u4e0b:","title":"\u5199\u4f5c\u601d\u8def"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/#_3","text":"\u5728 CHAPTER 1 Reliable, Scalable, and Maintainable Applications \u4e2d\uff0c\u4f5c\u8005\u9996\u5148\u9610\u8ff0\u4e86\u76ee\u6807\uff0c\u540e\u9762\u7684\u5f88\u591a\u5185\u5bb9\u90fd\u662f\u56f4\u7ed5\u8fd9\u4e9b\u76ee\u6807\u800c\u5c55\u5f00\u7684\u3002","title":"\u6211\u4eec\u7684\u76ee\u6807"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/#distributed#computing","text":"\u8fd9\u4e2a\u95ee\u9898\u5728Preface\u7ae0\u8282\u4e2d\u7ed9\u51fa\u4e86\u56de\u7b54\uff0c\u4e0b\u9762\u662f\u6211\u8ba4\u4e3a\u7684\u975e\u5e38\u91cd\u8981\u7684\u539f\u56e0: 1) Parallel computing\u7684\u5174\u8d77 2) Networks are getting faster 3) HA \u663e\u7136distributed computing\u7684\u5174\u8d77\uff0c\u80cc\u540e\u662f\u7531\u4e8e\u5f53\u4eca\u8ba1\u7b97\u673a\u79d1\u5b66\u7684\u73b0\u72b6\u6240\u51b3\u5b9a\u7684\u3002","title":"\u4e3a\u4ec0\u4e48\u9700\u8981distributed computing\uff1f"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/#distributed#computing_1","text":"\u8fd9\u4e2a\u95ee\u9898\u5728 PART II Distributed Data#CHAPTER 8 The Trouble with Distributed Systems \u8fdb\u884c\u4e86\u8ba8\u8bba\uff0c\u4e3b\u8981\u539f\u56e0\u5982\u4e0b: 1) Faults and Partial Failures 2) Unreliable Networks 3) Unreliable Clocks 4) Knowledge, Truth, and Lies \u6211\u89c9\u5f97\u4e0a\u8ff0\u8fd9\u4e9b\u6311\u6218\u7684\u6839\u672c\u539f\u56e0\u5728\u4e8e: distributed computing\u4e2d\uff0c\u7f3a\u4e4f\u4e2d\u5fc3\u89d2\u8272\uff0c\u5373\u5b83\u662f\u53bb\u4e2d\u5fc3\u5316\u7684\u3002","title":"distributed computing\u7684\u6311\u6218\u6709\u54ea\u4e9b\uff1f"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/#_4","text":"\u5728 CHAPTER 9 Consistency and Consensus \u4e2d\uff0c\u4f5c\u8005\u7ed9\u51fa\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u7684\u4e00\u822c\u601d\u8def: \u521b\u9020\u5408\u9002\u7684\u62bd\u8c61\u3002","title":"\u5982\u4f55\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff1f"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/Outline/","text":"Outline of This Book \u8bfb\u4e00\u672c\u4e66\uff0c\u9996\u5148\u770b\u770b\u4f5c\u8005\u662f\u5982\u4f55\u6765\u7ec4\u7ec7\u5185\u5bb9\u7684\uff0c\u4e5f\u5c31\u662f\u770b\u770b\u4e66\u7684\u76ee\u5f55\uff0c\u5728\u539f\u4e66\u7684\u300aOutline of This Book\u300b\u5bf9\u6b64\u8fdb\u884c\u4e86\u8bf4\u660e\u3002 \u8fd9\u672c\u4e66\u5206\u4e3athree parts\uff1a \u5728Part I\uff0c\u4f5c\u8005\u9996\u5148\u63cf\u8ff0\u7684\u662f\u201cfundamental ideas that underpin\uff08\u652f\u6301\uff09 the design of data-intensive applications\u201d\uff0c\u5373\u8bbe\u8ba1\u4e00\u4e2adata-intensive application\u7684\u6307\u5bfc\u601d\u60f3\u3001\u7406\u5ff5\uff0c\u8fd9\u4e9b\u201cideas\u201d\u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u5b83\u6307\u5bfc\u7740\u8bbe\u8ba1\u8005\u8fdb\u884c\u8bbe\u8ba1\uff0c\u5b83\u4eec\u662f\u8bbe\u8ba1\u8005\u8ffd\u6c42\u7684\u76ee\u6807\uff0c\u5728\u6587\u7ae0 \u60f3\u6cd5 \u7684\u201c\u610f\u56fe\u51b3\u5b9a\u6700\u7ec8\u7684\u7ed3\u679c\u201d\u4e2d\uff0c\u5bf9\u6b64\u6709\u63cf\u8ff0\u3002 \u63a5\u7740\u4f5c\u8005\u5f00\u59cb\u63cf\u8ff0\u201cdata stored on one machine\u201d\u76f8\u5173\u7684\u5185\u5bb9\u3002 \u5728Part II, \u4f5c\u8005\u5f00\u59cb\u63cf\u8ff0\u201c data that is distributed across multiple machines\u201d In Part I of this book, we discussed aspects of data systems that apply when data is stored on a single machine. Now, in Part II, we move up a level and ask: what happens if multiple machines are involved in storage and retrieval of data? \u4ecesingle machine\u5230multiple machine\uff0c\u300aredis\u8bbe\u8ba1\u4e0e\u5b9e\u73b0(\u7b2c\u4e8c\u7248)\u300b\u4e5f\u662f\u6309\u7167\u8fd9\u79cd\u601d\u8def\u6765\u7ec4\u7ec7\u7684\u3002 \u5728Part III\uff0c\u4f5c\u8005\u8ba8\u8bba\u7684\u662fintegrate several different data system\u7684\u95ee\u9898\u3002 \u603b\u7684\u6765\u8bf4\uff0c\u4f5c\u8005\u7684\u7f16\u5199\u601d\u8def\u662f\u5faa\u5e8f\u6e10\u8fdb\uff0c\u7531\u6613\u5230\u96be\u3002","title":"Introduction"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/Outline/#outline#of#this#book","text":"\u8bfb\u4e00\u672c\u4e66\uff0c\u9996\u5148\u770b\u770b\u4f5c\u8005\u662f\u5982\u4f55\u6765\u7ec4\u7ec7\u5185\u5bb9\u7684\uff0c\u4e5f\u5c31\u662f\u770b\u770b\u4e66\u7684\u76ee\u5f55\uff0c\u5728\u539f\u4e66\u7684\u300aOutline of This Book\u300b\u5bf9\u6b64\u8fdb\u884c\u4e86\u8bf4\u660e\u3002 \u8fd9\u672c\u4e66\u5206\u4e3athree parts\uff1a \u5728Part I\uff0c\u4f5c\u8005\u9996\u5148\u63cf\u8ff0\u7684\u662f\u201cfundamental ideas that underpin\uff08\u652f\u6301\uff09 the design of data-intensive applications\u201d\uff0c\u5373\u8bbe\u8ba1\u4e00\u4e2adata-intensive application\u7684\u6307\u5bfc\u601d\u60f3\u3001\u7406\u5ff5\uff0c\u8fd9\u4e9b\u201cideas\u201d\u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u5b83\u6307\u5bfc\u7740\u8bbe\u8ba1\u8005\u8fdb\u884c\u8bbe\u8ba1\uff0c\u5b83\u4eec\u662f\u8bbe\u8ba1\u8005\u8ffd\u6c42\u7684\u76ee\u6807\uff0c\u5728\u6587\u7ae0 \u60f3\u6cd5 \u7684\u201c\u610f\u56fe\u51b3\u5b9a\u6700\u7ec8\u7684\u7ed3\u679c\u201d\u4e2d\uff0c\u5bf9\u6b64\u6709\u63cf\u8ff0\u3002 \u63a5\u7740\u4f5c\u8005\u5f00\u59cb\u63cf\u8ff0\u201cdata stored on one machine\u201d\u76f8\u5173\u7684\u5185\u5bb9\u3002 \u5728Part II, \u4f5c\u8005\u5f00\u59cb\u63cf\u8ff0\u201c data that is distributed across multiple machines\u201d In Part I of this book, we discussed aspects of data systems that apply when data is stored on a single machine. Now, in Part II, we move up a level and ask: what happens if multiple machines are involved in storage and retrieval of data? \u4ecesingle machine\u5230multiple machine\uff0c\u300aredis\u8bbe\u8ba1\u4e0e\u5b9e\u73b0(\u7b2c\u4e8c\u7248)\u300b\u4e5f\u662f\u6309\u7167\u8fd9\u79cd\u601d\u8def\u6765\u7ec4\u7ec7\u7684\u3002 \u5728Part III\uff0c\u4f5c\u8005\u8ba8\u8bba\u7684\u662fintegrate several different data system\u7684\u95ee\u9898\u3002 \u603b\u7684\u6765\u8bf4\uff0c\u4f5c\u8005\u7684\u7f16\u5199\u601d\u8def\u662f\u5faa\u5e8f\u6e10\u8fdb\uff0c\u7531\u6613\u5230\u96be\u3002","title":"Outline of This Book"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/PART-I/1-Reliable-Scalable-Maintainable-Applications/","text":"CHAPTER 1 Reliable, Scalable, and Maintainable Applications Many applications today are data-intensive , as opposed to compute-intensive . Raw CPU power is rarely a limiting factor for these applications\u2014bigger problems are usually the amount of data, the complexity of data, and the speed at which it is changing. NOTE: *data-intensive*\u5373\u201c\u6570\u636e\u5bc6\u96c6\u578b\u201d\uff0c\u5b83\u662f I/O bound \uff1b *compute-intensive*\u5373\u201c\u8ba1\u7b97\u5bc6\u96c6\u578b\u201d\uff0c\u5b83\u662f CPU-bound \u3002\u5173\u4e8e\u8fd9\u4e24\u4e2a\u8bcd\u8bed\uff0c\u5728Preface\u4e2d\u5bf9\u5b83\u4eec\u8fdb\u884c\u4e86\u5b9a\u4e49\u3002 A data-intensive application is typically built from standard building blocks that provide commonly needed functionality. For example, many applications need to: Store data so that they, or another application, can find it again later (databases) Remember the result of an expensive operation, to speed up reads (caches) Allow users to search data by keyword or filter it in various ways (search indexes) Send a message to another process, to be handled asynchronously (stream processing) NOTE: message queue\u662fstream processing\uff1f Periodically crunch a large amount of accumulated data (batch processing) Thinking About Data Systems NOTE: data system\u3001tool\u3001application \u539f\u6587\u7684\u8fd9\u4e00\u7ae0\u4e2d\uff0c\u4e0a\u8ff0\u4e09\u4e2a\u6982\u5ff5\u662f\u9891\u7e41\u51fa\u73b0\u7684\uff0c\u4f5c\u8005\u5e76\u6ca1\u6709\u7ed9\u51fa\u5b83\u4eec\u7684\u51c6\u786e\u5b9a\u4e49\u3002\u5176\u4e2dtool\u662f\u6700\u597d\u7406\u89e3\u7684\uff0c\u5b83\u7684\u542b\u4e49\u5c31\u662f\u5b83\u7684\u5b57\u9762\u610f\u601d\uff0credis\u3001Kafka\u7b49\uff1bapplication\u4e5f\u662f\u6bd4\u8f83\u597d\u7406\u89e3\u7684\uff0c\u5b83\u7684\u542b\u4e49\u4e5f\u662f\u5b83\u7684\u5b57\u9762\u610f\u601d\u3002\u90a3\u5982\u4f55\u6765\u7406\u89e3data system\u5462\uff1fdata system\u662f\u4f5c\u8005\u521b\u9020\u7684\u4e00\u4e2a\u6982\u5ff5\uff0c\u5b83\u662f\u4e00\u4e2a\u62bd\u8c61\uff0c\u5b83\u662f\u672c\u4e66\u63cf\u8ff0\u7684\u5bf9\u8c61\uff0credis\u3001kafka\u3001\u4ee5\u53ca\u4f7f\u7528\u8fd9\u4e9btool\u6784\u5efa\u8d77\u6765\u7684application\u90fd\u53ef\u4ee5\u770b\u505a\u662f\u4e00\u79cddata system\uff0c\u90fd\u53ef\u4ee5\u4f7f\u7528\u672c\u4e66\u4e2d\u7684\u7406\u8bba\u6765\u63cf\u8ff0\u5b83\u4eec\u3002 So why should we lump them all together under an umbrella term like data systems? NOTE: \u4f7f\u7528\u62bd\u8c61\u7684\u601d\u60f3\u6765\u7406\u89e3\u8fd9\u6bb5\u8bdd\u662f\u975e\u5e38\u5bb9\u6613\u7406\u89e3\u7684\uff0c\u4e0a\u9762\u7684note\u5df2\u7ecf\u5bf9\u6b64\u8fdb\u884c\u4e86\u63cf\u8ff0\u3002 NOTE: \u539f\u4e66\u7684\u7d27\u63a5\u7740\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u51e0\u6bb5\u4f5c\u8005\u6240\u8981\u8868\u8fbe\u7684\u601d\u60f3\u662f\uff1a\u73b0\u5b9e\u7684\u95ee\u9898\u5f80\u5f80\u66f4\u52a0\u590d\u6742\uff0c\u65e0\u6cd5\u4f7f\u7528\u4e00\u4e2a\u5355\u4e00\u7684model\u8fdb\u884c\u63cf\u8ff0\u3002 If you are designing a data system or service, a lot of tricky questions arise. How do you ensure that the data remains correct and complete, even when things go wrong internally? How do you provide consistently good performance to clients, even when parts of your system are degraded? How do you scale to handle an increase in load? What does a good API for the service look like? In this book, we focus on three concerns that are important in most software systems: Reliability The system should continue to work correctly (performing the correct function at the desired level of performance) even in the face of adversity (hardware or software faults, and even human error). See \u201cReliability\u201d on page 6. Scalability As the system grows (in data volume, traffic volume, or complexity), there should be reasonable ways of dealing with that growth. See \u201cScalability\u201d on page 10. Maintainability Over time, many different people will work on the system (engineering and operations, both maintaining current behavior and adapting the system to new use cases), and they should all be able to work on it productively. See \u201cMaintainability\u201d on page 18.","title":"Introduction"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/PART-I/1-Reliable-Scalable-Maintainable-Applications/#chapter#1#reliable#scalable#and#maintainable#applications","text":"Many applications today are data-intensive , as opposed to compute-intensive . Raw CPU power is rarely a limiting factor for these applications\u2014bigger problems are usually the amount of data, the complexity of data, and the speed at which it is changing. NOTE: *data-intensive*\u5373\u201c\u6570\u636e\u5bc6\u96c6\u578b\u201d\uff0c\u5b83\u662f I/O bound \uff1b *compute-intensive*\u5373\u201c\u8ba1\u7b97\u5bc6\u96c6\u578b\u201d\uff0c\u5b83\u662f CPU-bound \u3002\u5173\u4e8e\u8fd9\u4e24\u4e2a\u8bcd\u8bed\uff0c\u5728Preface\u4e2d\u5bf9\u5b83\u4eec\u8fdb\u884c\u4e86\u5b9a\u4e49\u3002 A data-intensive application is typically built from standard building blocks that provide commonly needed functionality. For example, many applications need to: Store data so that they, or another application, can find it again later (databases) Remember the result of an expensive operation, to speed up reads (caches) Allow users to search data by keyword or filter it in various ways (search indexes) Send a message to another process, to be handled asynchronously (stream processing) NOTE: message queue\u662fstream processing\uff1f Periodically crunch a large amount of accumulated data (batch processing)","title":"CHAPTER 1 Reliable, Scalable, and Maintainable Applications"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/PART-I/1-Reliable-Scalable-Maintainable-Applications/#thinking#about#data#systems","text":"NOTE:","title":"Thinking About Data Systems"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/PART-I/1-Reliable-Scalable-Maintainable-Applications/#data#systemtoolapplication","text":"\u539f\u6587\u7684\u8fd9\u4e00\u7ae0\u4e2d\uff0c\u4e0a\u8ff0\u4e09\u4e2a\u6982\u5ff5\u662f\u9891\u7e41\u51fa\u73b0\u7684\uff0c\u4f5c\u8005\u5e76\u6ca1\u6709\u7ed9\u51fa\u5b83\u4eec\u7684\u51c6\u786e\u5b9a\u4e49\u3002\u5176\u4e2dtool\u662f\u6700\u597d\u7406\u89e3\u7684\uff0c\u5b83\u7684\u542b\u4e49\u5c31\u662f\u5b83\u7684\u5b57\u9762\u610f\u601d\uff0credis\u3001Kafka\u7b49\uff1bapplication\u4e5f\u662f\u6bd4\u8f83\u597d\u7406\u89e3\u7684\uff0c\u5b83\u7684\u542b\u4e49\u4e5f\u662f\u5b83\u7684\u5b57\u9762\u610f\u601d\u3002\u90a3\u5982\u4f55\u6765\u7406\u89e3data system\u5462\uff1fdata system\u662f\u4f5c\u8005\u521b\u9020\u7684\u4e00\u4e2a\u6982\u5ff5\uff0c\u5b83\u662f\u4e00\u4e2a\u62bd\u8c61\uff0c\u5b83\u662f\u672c\u4e66\u63cf\u8ff0\u7684\u5bf9\u8c61\uff0credis\u3001kafka\u3001\u4ee5\u53ca\u4f7f\u7528\u8fd9\u4e9btool\u6784\u5efa\u8d77\u6765\u7684application\u90fd\u53ef\u4ee5\u770b\u505a\u662f\u4e00\u79cddata system\uff0c\u90fd\u53ef\u4ee5\u4f7f\u7528\u672c\u4e66\u4e2d\u7684\u7406\u8bba\u6765\u63cf\u8ff0\u5b83\u4eec\u3002 So why should we lump them all together under an umbrella term like data systems? NOTE: \u4f7f\u7528\u62bd\u8c61\u7684\u601d\u60f3\u6765\u7406\u89e3\u8fd9\u6bb5\u8bdd\u662f\u975e\u5e38\u5bb9\u6613\u7406\u89e3\u7684\uff0c\u4e0a\u9762\u7684note\u5df2\u7ecf\u5bf9\u6b64\u8fdb\u884c\u4e86\u63cf\u8ff0\u3002 NOTE: \u539f\u4e66\u7684\u7d27\u63a5\u7740\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u51e0\u6bb5\u4f5c\u8005\u6240\u8981\u8868\u8fbe\u7684\u601d\u60f3\u662f\uff1a\u73b0\u5b9e\u7684\u95ee\u9898\u5f80\u5f80\u66f4\u52a0\u590d\u6742\uff0c\u65e0\u6cd5\u4f7f\u7528\u4e00\u4e2a\u5355\u4e00\u7684model\u8fdb\u884c\u63cf\u8ff0\u3002 If you are designing a data system or service, a lot of tricky questions arise. How do you ensure that the data remains correct and complete, even when things go wrong internally? How do you provide consistently good performance to clients, even when parts of your system are degraded? How do you scale to handle an increase in load? What does a good API for the service look like? In this book, we focus on three concerns that are important in most software systems: Reliability The system should continue to work correctly (performing the correct function at the desired level of performance) even in the face of adversity (hardware or software faults, and even human error). See \u201cReliability\u201d on page 6. Scalability As the system grows (in data volume, traffic volume, or complexity), there should be reasonable ways of dealing with that growth. See \u201cScalability\u201d on page 10. Maintainability Over time, many different people will work on the system (engineering and operations, both maintaining current behavior and adapting the system to new use cases), and they should all be able to work on it productively. See \u201cMaintainability\u201d on page 18.","title":"data system\u3001tool\u3001application"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/PART-II/","text":"PART II Distributed Data In Part I of this book, we discussed aspects of data systems that apply when data is stored on a single machine. Now, in Part II, we move up a level and ask: what happens if multiple machines are involved in storage and retrieval of data? NOTE: \u53c2\u89c1\u6587\u7ae0 One-to-multiple . There are various reasons why you might want to distribute a database across multiple machines: Scalability If your data volume, read load, or write load grows bigger than a single machine can handle, you can potentially spread the load across multiple machines. Fault tolerance/high availability If your application needs to continue working even if one machine (or several machines, or the network, or an entire datacenter) goes down, you can use multiple machines to give you redundancy . When one fails, another one can take over. NOTE: \u53c2\u89c1\uff1a \u7ef4\u57fa\u767e\u79d1 High availability \u7ef4\u57fa\u767e\u79d1 Single point of failure \u7ef4\u57fa\u767e\u79d1 Redundancy (engineering) Latency If you have users around the world, you might want to have servers at various locations worldwide so that each user can be served from a datacenter that is geographically close to them. That avoids the users having to wait for network packets to travel halfway around the world. Scaling to Higher Load NOTE: \u5982\u4f55\u8fdb\u884cscale\uff1f shared-memory architecture\uff08vertical scaling or scaling up\uff09 shared-disk architecture shared-nothing architectures\uff08horizontal scaling or scaling out\uff09 Shared-Nothing Architectures NOTE: \u8fd9\u662f\u672c\u4e66\u91cd\u70b9\u5173\u6ce8\u7684\u3002\u8fd9\u603barchitecture\u662fdistributed system\u3002 Replication Versus Partitioning There are two common ways data is distributed across multiple nodes: Replication Keeping a copy of the same data on several different nodes, potentially in different locations. Replication provides redundancy: if some nodes are unavailable, the data can still be served from the remaining nodes. Replication can also help improve performance. We discuss replication in Chapter 5. Partitioning Splitting a big database into smaller subsets called partitions so that different partitions can be assigned to different nodes (also known as sharding ). We discuss partitioning in Chapter 6. These are separate mechanisms, but they often go hand in hand, as illustrated in Figure II-1.","title":"PART-II-Distributed-Data"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/PART-II/#part#ii#distributed#data","text":"In Part I of this book, we discussed aspects of data systems that apply when data is stored on a single machine. Now, in Part II, we move up a level and ask: what happens if multiple machines are involved in storage and retrieval of data? NOTE: \u53c2\u89c1\u6587\u7ae0 One-to-multiple . There are various reasons why you might want to distribute a database across multiple machines: Scalability If your data volume, read load, or write load grows bigger than a single machine can handle, you can potentially spread the load across multiple machines. Fault tolerance/high availability If your application needs to continue working even if one machine (or several machines, or the network, or an entire datacenter) goes down, you can use multiple machines to give you redundancy . When one fails, another one can take over. NOTE: \u53c2\u89c1\uff1a \u7ef4\u57fa\u767e\u79d1 High availability \u7ef4\u57fa\u767e\u79d1 Single point of failure \u7ef4\u57fa\u767e\u79d1 Redundancy (engineering) Latency If you have users around the world, you might want to have servers at various locations worldwide so that each user can be served from a datacenter that is geographically close to them. That avoids the users having to wait for network packets to travel halfway around the world.","title":"PART II Distributed Data"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/PART-II/#scaling#to#higher#load","text":"NOTE: \u5982\u4f55\u8fdb\u884cscale\uff1f shared-memory architecture\uff08vertical scaling or scaling up\uff09 shared-disk architecture shared-nothing architectures\uff08horizontal scaling or scaling out\uff09","title":"Scaling to Higher Load"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/PART-II/#shared-nothing#architectures","text":"NOTE: \u8fd9\u662f\u672c\u4e66\u91cd\u70b9\u5173\u6ce8\u7684\u3002\u8fd9\u603barchitecture\u662fdistributed system\u3002","title":"Shared-Nothing Architectures"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/PART-II/#replication#versus#partitioning","text":"There are two common ways data is distributed across multiple nodes:","title":"Replication Versus Partitioning"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/PART-II/#replication","text":"Keeping a copy of the same data on several different nodes, potentially in different locations. Replication provides redundancy: if some nodes are unavailable, the data can still be served from the remaining nodes. Replication can also help improve performance. We discuss replication in Chapter 5.","title":"Replication"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/PART-II/#partitioning","text":"Splitting a big database into smaller subsets called partitions so that different partitions can be assigned to different nodes (also known as sharding ). We discuss partitioning in Chapter 6. These are separate mechanisms, but they often go hand in hand, as illustrated in Figure II-1.","title":"Partitioning"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/PART-II/5-Replication/","text":"CHAPTER 5 Replication There are several reasons why you might want to replicate data: 1\u3001To keep data geographically close to your users (and thus reduce latency) 2\u3001To allow the system to continue working even if some of its parts have failed (and thus increase availability) 3\u3001To scale out the number of machines that can serve read queries (and thus increase read throughput) NOTE: \u8bfb\u5199\u5206\u79bb All of the difficulty in replication lies in handling changes to replicated data, and that\u2019s what this chapter is about. We will discuss three popular algorithms for replicating changes between nodes: single-leader , multi-leader , and leaderless replication . Almost all distributed databases use one of these three approaches. They all have various pros and cons, which we will examine in detail. There are many trade-offs to consider with replication: for example, whether to use synchronous or asynchronous replication, and how to handle failed replicas. Those are often configuration options in databases, and although the details vary by database, the general principles are similar across many different implementations. In \u201cProblems with Replication Lag\u201d on page 161 we will get more precise about eventual consistency and discuss things like the read-your-writes and monotonic reads guarantees.","title":"Introduction"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/PART-II/5-Replication/#chapter#5#replication","text":"There are several reasons why you might want to replicate data: 1\u3001To keep data geographically close to your users (and thus reduce latency) 2\u3001To allow the system to continue working even if some of its parts have failed (and thus increase availability) 3\u3001To scale out the number of machines that can serve read queries (and thus increase read throughput) NOTE: \u8bfb\u5199\u5206\u79bb All of the difficulty in replication lies in handling changes to replicated data, and that\u2019s what this chapter is about. We will discuss three popular algorithms for replicating changes between nodes: single-leader , multi-leader , and leaderless replication . Almost all distributed databases use one of these three approaches. They all have various pros and cons, which we will examine in detail. There are many trade-offs to consider with replication: for example, whether to use synchronous or asynchronous replication, and how to handle failed replicas. Those are often configuration options in databases, and although the details vary by database, the general principles are similar across many different implementations. In \u201cProblems with Replication Lag\u201d on page 161 we will get more precise about eventual consistency and discuss things like the read-your-writes and monotonic reads guarantees.","title":"CHAPTER 5 Replication"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/PART-II/8-The-Trouble-with-Distributed-Systems/","text":"CHAPTER 8 The Trouble with Distributed Systems NOTE: \u672c\u7ae0\u6240\u603b\u7ed3\u7684\u662f\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u6311\u6218\uff0c\u5b83\u662f\u7406\u89e3\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u5404\u79cd\u95ee\u9898\u7684\u5173\u952e\u6240\u5728 A recurring theme in the last few chapters has been how systems handle things going wrong. For example, we discussed replica failover (\u201cHandling Node Outages\u201d on page 156), replication lag (\u201cProblems with Replication Lag\u201d on page 161), and concurrency control for transactions (\u201cWeak Isolation Levels\u201d on page 233). As we come to understand various edge cases(\u6781\u7aef\u60c5\u51b5) that can occur in real systems, we get better at handling them. However, even though we have talked a lot about faults, the last few chapters have still been too optimistic. The reality is even darker. Working with distributed systems is fundamentally different from writing software on a single computer\u2014and the main difference is that there are lots of new and exciting ways for things to go wrong. In this chapter, we will get a taste of the problems that arise in practice, and an understanding of the things we can and cannot rely on. NOTE: single computer VS distributed system\u3002 In the end, our task as engineers is to build systems that do their job (i.e., meet the guarantees that users are expecting), in spite of everything going wrong. In Chapter 9, we will look at some examples of algorithms that can provide such guarantees in a distributed system. But first, in this chapter, we must understand what challenges we are up against. This chapter is a thoroughly pessimistic and depressing overview of things that may go wrong in a distributed system. We will look into problems with networks (\u201cUnreliable Networks\u201d on page 277); clocks and timing issues (\u201cUnreliable Clocks\u201d on page 287); and we\u2019ll discuss to what degree they are avoidable. The consequences of all these issues are disorienting, so we\u2019ll explore how to think about the state of a distributed system and how to reason about things that have happened (\u201cKnowledge, Truth, and Lies\u201d on page 300). Faults and Partial Failures NOTE: \u539f\u6587\u9996\u5148\u63cf\u8ff0\u4e86program on a single computer\u7684**deterministic**\uff0c\u4e0e\u6b64\u76f8\u5bf9\u6bd4\u7684\u662f\uff0cprogram on distributed system\u7684**nondeterministic**\u3002 In distributed systems, we are no longer operating in an idealized system model\u2014we have no choice but to confront the messy reality of the physical world. And in the physical world, a remarkably wide range of things can go wrong, as illustrated by this anecdote In my limited experience I\u2019ve dealt with long-lived network partitions in a single data center (DC), PDU [power distribution unit] failures, switch failures, accidental power cycles of whole racks, whole-DC backbone failures, whole-DC power failures, and a hypoglycemic driver smashing his Ford pickup truck into a DC\u2019s HVAC [heating, ventilation, and air conditioning] system. And I\u2019m not even an ops guy. \u2014Coda Hale In a distributed system, there may well be some parts of the system that are broken in some unpredictable way, even though other parts of the system are working fine. This is known as a partial failure . The difficulty is that partial failures are nondeterministic : if you try to do anything involving multiple nodes and the network, it may sometimes work and sometimes unpredictably fail. As we shall see, you may not even know whether something succeeded or not, as the time it takes for a message to travel across a network is also nondeterministic ! NOTE: \u539f\u6587\u4e2d\u5c06***know***\u7279\u5730\u7ed9\u6807\u6ce8\u51fa\u6765\u4e86\uff0c\u6211\u89c9\u5f97\u5b83\u5e94\u8be5\u662f\u5bf9\u5e94\u7684\u540e\u9762\u7684\u201cKnowledge, Truth, and Lies\"\u7ae0\u8282\u3002 If we want to make distributed systems work, we must accept the possibility of partial failure and build fault-tolerance mechanisms into the software. In other words, we need to build a reliable system from unreliable components. (As discussed in \u201cReliability\u201d on page 6, there is no such thing as perfect reliability, so we\u2019ll need to understand the limits of what we can realistically promise.) Even in smaller systems consisting of only a few nodes, it\u2019s important to think about partial failure . In a small system, it\u2019s quite likely that most of the components are working correctly most of the time. However, sooner or later, some part of the system will become faulty, and the software will have to somehow handle it. The fault handling must be part of the software design, and you (as operator of the software) need to know what behavior to expect from the software in the case of a fault. It would be unwise to assume that faults are rare and simply hope for the best. It is important to consider a wide range of possible faults\u2014even fairly unlikely ones\u2014and to artificially create such situations in your testing environment to see what happens. In distributed systems, suspicion, pessimism, and paranoia pay off. NOTE: \u4fdd\u6301\u6000\u7591\u7cbe\u795e Unreliable Networks Shared-nothing is not the only way of building systems, but it has become the dominant approach for building internet services, for several reasons: it\u2019s comparatively cheap because it requires no special hardware, it can make use of commoditized(\u5546\u4e1a\u5316\u7684) cloud computing services, and it can achieve high reliability through redundancy across multiple geographically distributed datacenters. NOTE: \u4e00\u3001\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u6240\u603b\u7ed3\u7684\u662f\"shared-nothing architecture\"\u7684\u4f18\u52bf \u4e8c\u3001\"it can achieve high reliability through redundancy across multiple geographically distributed datacenters\"\u5176\u5b9e\u5c31\u662fHA replication The internet and most internal networks in datacenters (often Ethernet) are asynchronous packet networks. In this kind of network, one node can send a message (a packet) to another node, but the network gives no guarantees as to when it will arrive, or whether it will arrive at all. If you send a request and expect a response, many things could go wrong (some of which are illustrated in Figure 8-1): 1\u3001 Your request may have been lost (perhaps someone unplugged a network cable). 2\u3001 Your request may be waiting in a queue and will be delivered later (perhaps the network or the recipient is overloaded). 3\u3001 The remote node may have failed (perhaps it crashed or it was powered down). 4\u3001 The remote node may have temporarily stopped responding (perhaps it is experiencing a long garbage collection pause; see \u201cProcess Pauses\u201d on page 295), but it will start responding again later. 5\u3001 The remote node may have processed your request, but the response has been lost on the network (perhaps a network switch has been misconfigured). 6\u3001 The remote node may have processed your request, but the response has been delayed and will be delivered later (perhaps the network or your own machine is overloaded). NOTE: \u770b\u4e86\u4e0a\u9762\u7684\u5185\u5bb9\uff0c\u4e0d\u8981\u89c9\u5f97\u7cfb\u7edf\u592a\u8106\u5f31\u4e86\uff1b\u56e0\u4e3a\u4e0a\u9762\u63cf\u8ff0\u7684\u5404\u79cd\u4e8b\u4ef6\uff0c\u6709\u7684\u53d1\u751f\u7684\u53ef\u80fd\u6027\u662f\u975e\u5e38\u4f4e\u7684 The sender can\u2019t even tell whether the packet was delivered: the only option is for the recipient to send a response message, which may in turn be lost or delayed. These issues are indistinguishable in an asynchronous network: the only information you have is that you haven\u2019t received a response yet. If you send a request to another node and don\u2019t receive a response, it is impossible to tell why. The usual way of handling this issue is a timeout : after some time you give up waiting and assume that the response is not going to arrive. However, when a timeout occurs, you still don\u2019t know whether the remote node got your request or not (and if the request is still queued somewhere, it may still be delivered to the recipient, even if the sender has given up on it). NOTE: redis\u6240\u91c7\u7528\u7684\u5c31\u662ftimeout\u7684\u65b9\u5f0f\u3002 Network Faults in Practice We have been building computer networks for decades\u2014one might hope that by now we would have figured out how to make them reliable. However, it seems that we have not yet succeeded. Network partitions When one part of the network is cut off from the rest due to a network fault, that is sometimes called a network partition or netsplit . In this book we\u2019ll generally stick with the more general term network fault , to avoid confusion with partitions (shards) of a storage system, as discussed in Chapter 6. Detecting Faults Many systems need to automatically detect faulty nodes. For example: A load balancer needs to stop sending requests to a node that is dead (i.e., take it out of rotation). In a distributed database with single-leader replication, if the leader fails, one of the followers needs to be promoted to be the new leader (see \u201cHandling Node Outages\u201d on page 156). Unfortunately, the uncertainty about the network makes it difficult to tell whether a node is working or not. In some specific circumstances you might get some feedback to explicitly tell you that something is not working: If you can reach the machine on which the node should be running, but no process is listening on the destination port (e.g., because the process crashed), the operating system will helpfully close or refuse TCP connections by sending a RST or FIN packet in reply. However, if the node crashed while it was handling your request, you have no way of knowing how much data was actually processed by the remote node [22]. If a node process crashed (or was killed by an administrator) but the node\u2019s operating system is still running, a script can notify other nodes about the crash so that another node can take over quickly without having to wait for a timeout to expire. For example, HBase does this [23]. If you have access to the management interface of the network switches in your datacenter, you can query them to detect link failures at a hardware level (e.g., if the remote machine is powered down). This option is ruled out if you\u2019re connecting via the internet, or if you\u2019re in a shared datacenter with no access to the switches themselves, or if you can\u2019t reach the management interface due to a network problem. If a router is sure that the IP address you\u2019re trying to connect to is unreachable, it may reply to you with an ICMP Destination Unreachable packet. However, the router doesn\u2019t have a magic failure detection capability either\u2014it is subject to the same limitations as other participants of the network. Conversely, if something has gone wrong, you may get an error response at some level of the stack, but in general you have to assume that you will get no response at all. You can retry a few times (TCP retries transparently, but you may also retry at the application level), wait for a timeout to elapse, and eventually declare the node dead if you don\u2019t hear back within the timeout. NOTE: timeout\u662f\u4e00\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u3001\u51c6\u786e\u7684\u65b9\u6cd5 Timeouts and Unbounded Delays If a timeout is the only sure way of detecting a fault, then how long should the timeout be? There is unfortunately no simple answer. A long timeout means a long wait until a node is declared dead (and during this time, users may have to wait or see error messages). A short timeout detects faults faster, but carries a higher risk of incorrectly declaring a node dead when in fact it has only suffered a temporary slowdown (e.g., due to a load spike on the node or the network). Prematurely\uff08\u8fc7\u65e9\u5730\uff09 declaring a node dead is problematic: if the node is actually alive and in the middle of performing some action (for example, sending an email), and another node takes over, the action may end up being performed twice. We will discuss this issue in more detail in \u201cKnowledge, Truth, and Lies\u201d on page 300, and in Chapters 9 and 11. Network congestion and queueing When driving a car, travel times on road networks often vary most due to traffic congestion. Similarly, the variability of packet delays on computer networks is most often due to queueing Unreliable Clocks Clocks and time are important. Applications depend on clocks in various ways to answer questions like the following: 1. Has this request timed out yet? 2. What\u2019s the 99 th percentile response time of this service? 3. How many queries per second did this service handle on average in the last five minutes? 4. How long did the user spend on our site? 5. When was this article published? 6. At what date and time should the reminder email be sent? 7. When does this cache entry expire? 8. What is the timestamp on this error message in the log file? Knowledge, Truth, and Lies So far in this chapter we have explored the ways in which distributed systems are different from programs running on a single computer: there is no shared memory, only message passing via an unreliable network with variable delays, and the systems may suffer from partial failures, unreliable clocks, and processing pauses. NOTE: \u8fd9\u6bb5\u8bdd\u603b\u7ed3\u7684\u975e\u5e38\u597d\u3002 The consequences of these issues are profoundly disorienting if you\u2019re not used to distributed systems. A node in the network cannot know anything for sure\u2014it can only make guesses based on the messages it receives (or doesn\u2019t receive) via the network. A node can only find out what state another node is in (what data it has stored, whether it is correctly functioning, etc.) by exchanging messages with it. If a remote node doesn\u2019t respond, there is no way of knowing what state it is in, because problems in the network cannot reliably be distinguished from problems at a node. Discussions of these systems border(\u4e0e...\u63a5\u58e4) on the philosophical: What do we know to be true or false in our system? How sure can we be of that knowledge, if the mechanisms for perception and measurement are unreliable? Should software systems obey the laws that we expect of the physical world, such as cause and effect? NOTE: \u5bf9\u8fd9\u4e9b\u7cfb\u7edf\u7684\u8ba8\u8bba\u8fd1\u4e4e\u4e8e\u54f2\u5b66\u3002\u8f6f\u4ef6\u7cfb\u7edf\u662f\u5426\u5e94\u8be5\u9075\u5faa\u6211\u4eec\u6240\u671f\u671b\u7684\u7269\u7406\u4e16\u754c\u7684\u6cd5\u5219\uff0c\u6bd4\u5982\u56e0\u679c\u5173\u7cfb? Fortunately, we don\u2019t need to go as far as figuring out the meaning of life. In a distributed system, we can state the assumptions we are making about the behavior (the system model ) and design the actual system in such a way that it meets those assumptions. Algorithms can be proved to function correctly within a certain system model . This means that reliable behavior is achievable, even if the underlying system model provides very few guarantees. NOTE: \u5efa\u7acbsystem model\uff0c\u6bcf\u4e2amodel\u90fd\u662f\u6709\u4e00\u5b9a\u7684assumption\uff0c\u5373\u5047\u8bbe\u3001\u524d\u63d0\u6761\u4ef6 However, although it is possible to make software well behaved in an unreliable system model , it is not straightforward to do so. In the rest of this chapter we will further explore the notions of knowledge and truth in distributed systems, which will help us think about the kinds of assumptions we can make and the guarantees we may want to provide. In Chapter 9 we will proceed to look at some examples of distributed systems, algorithms that provide particular guarantees under particular assumptions. The Truth Is Defined by the Majority The moral\uff08\u5bd3\u610f\uff09 of these stories is that a node cannot necessarily trust its own judgment of a situation. A distributed system cannot exclusively rely on a single node, because a node may fail at any time, potentially leaving the system stuck and unable to recover. Instead, many distributed algorithms rely on a quorum , that is, voting among the nodes (see \u201cQuorums for reading and writing\u201d on page 179): decisions require some minimum number of votes from several nodes in order to reduce the dependence on any one particular node. That includes decisions about declaring nodes dead. If a quorum of nodes declares another node dead, then it must be considered dead, even if that node still very much feels alive. The individual node must abide by(\u9075\u5b88) the quorum decision and step down.","title":"CHAPTER-8-The-Trouble-with-Distributed-Systems"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/PART-II/8-The-Trouble-with-Distributed-Systems/#chapter#8#the#trouble#with#distributed#systems","text":"NOTE: \u672c\u7ae0\u6240\u603b\u7ed3\u7684\u662f\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u6311\u6218\uff0c\u5b83\u662f\u7406\u89e3\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u5404\u79cd\u95ee\u9898\u7684\u5173\u952e\u6240\u5728 A recurring theme in the last few chapters has been how systems handle things going wrong. For example, we discussed replica failover (\u201cHandling Node Outages\u201d on page 156), replication lag (\u201cProblems with Replication Lag\u201d on page 161), and concurrency control for transactions (\u201cWeak Isolation Levels\u201d on page 233). As we come to understand various edge cases(\u6781\u7aef\u60c5\u51b5) that can occur in real systems, we get better at handling them. However, even though we have talked a lot about faults, the last few chapters have still been too optimistic. The reality is even darker. Working with distributed systems is fundamentally different from writing software on a single computer\u2014and the main difference is that there are lots of new and exciting ways for things to go wrong. In this chapter, we will get a taste of the problems that arise in practice, and an understanding of the things we can and cannot rely on. NOTE: single computer VS distributed system\u3002 In the end, our task as engineers is to build systems that do their job (i.e., meet the guarantees that users are expecting), in spite of everything going wrong. In Chapter 9, we will look at some examples of algorithms that can provide such guarantees in a distributed system. But first, in this chapter, we must understand what challenges we are up against. This chapter is a thoroughly pessimistic and depressing overview of things that may go wrong in a distributed system. We will look into problems with networks (\u201cUnreliable Networks\u201d on page 277); clocks and timing issues (\u201cUnreliable Clocks\u201d on page 287); and we\u2019ll discuss to what degree they are avoidable. The consequences of all these issues are disorienting, so we\u2019ll explore how to think about the state of a distributed system and how to reason about things that have happened (\u201cKnowledge, Truth, and Lies\u201d on page 300).","title":"CHAPTER 8 The Trouble with Distributed Systems"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/PART-II/8-The-Trouble-with-Distributed-Systems/#faults#and#partial#failures","text":"NOTE: \u539f\u6587\u9996\u5148\u63cf\u8ff0\u4e86program on a single computer\u7684**deterministic**\uff0c\u4e0e\u6b64\u76f8\u5bf9\u6bd4\u7684\u662f\uff0cprogram on distributed system\u7684**nondeterministic**\u3002 In distributed systems, we are no longer operating in an idealized system model\u2014we have no choice but to confront the messy reality of the physical world. And in the physical world, a remarkably wide range of things can go wrong, as illustrated by this anecdote In my limited experience I\u2019ve dealt with long-lived network partitions in a single data center (DC), PDU [power distribution unit] failures, switch failures, accidental power cycles of whole racks, whole-DC backbone failures, whole-DC power failures, and a hypoglycemic driver smashing his Ford pickup truck into a DC\u2019s HVAC [heating, ventilation, and air conditioning] system. And I\u2019m not even an ops guy. \u2014Coda Hale In a distributed system, there may well be some parts of the system that are broken in some unpredictable way, even though other parts of the system are working fine. This is known as a partial failure . The difficulty is that partial failures are nondeterministic : if you try to do anything involving multiple nodes and the network, it may sometimes work and sometimes unpredictably fail. As we shall see, you may not even know whether something succeeded or not, as the time it takes for a message to travel across a network is also nondeterministic ! NOTE: \u539f\u6587\u4e2d\u5c06***know***\u7279\u5730\u7ed9\u6807\u6ce8\u51fa\u6765\u4e86\uff0c\u6211\u89c9\u5f97\u5b83\u5e94\u8be5\u662f\u5bf9\u5e94\u7684\u540e\u9762\u7684\u201cKnowledge, Truth, and Lies\"\u7ae0\u8282\u3002 If we want to make distributed systems work, we must accept the possibility of partial failure and build fault-tolerance mechanisms into the software. In other words, we need to build a reliable system from unreliable components. (As discussed in \u201cReliability\u201d on page 6, there is no such thing as perfect reliability, so we\u2019ll need to understand the limits of what we can realistically promise.) Even in smaller systems consisting of only a few nodes, it\u2019s important to think about partial failure . In a small system, it\u2019s quite likely that most of the components are working correctly most of the time. However, sooner or later, some part of the system will become faulty, and the software will have to somehow handle it. The fault handling must be part of the software design, and you (as operator of the software) need to know what behavior to expect from the software in the case of a fault. It would be unwise to assume that faults are rare and simply hope for the best. It is important to consider a wide range of possible faults\u2014even fairly unlikely ones\u2014and to artificially create such situations in your testing environment to see what happens. In distributed systems, suspicion, pessimism, and paranoia pay off. NOTE: \u4fdd\u6301\u6000\u7591\u7cbe\u795e","title":"Faults and Partial Failures"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/PART-II/8-The-Trouble-with-Distributed-Systems/#unreliable#networks","text":"Shared-nothing is not the only way of building systems, but it has become the dominant approach for building internet services, for several reasons: it\u2019s comparatively cheap because it requires no special hardware, it can make use of commoditized(\u5546\u4e1a\u5316\u7684) cloud computing services, and it can achieve high reliability through redundancy across multiple geographically distributed datacenters. NOTE: \u4e00\u3001\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u6240\u603b\u7ed3\u7684\u662f\"shared-nothing architecture\"\u7684\u4f18\u52bf \u4e8c\u3001\"it can achieve high reliability through redundancy across multiple geographically distributed datacenters\"\u5176\u5b9e\u5c31\u662fHA replication The internet and most internal networks in datacenters (often Ethernet) are asynchronous packet networks. In this kind of network, one node can send a message (a packet) to another node, but the network gives no guarantees as to when it will arrive, or whether it will arrive at all. If you send a request and expect a response, many things could go wrong (some of which are illustrated in Figure 8-1): 1\u3001 Your request may have been lost (perhaps someone unplugged a network cable). 2\u3001 Your request may be waiting in a queue and will be delivered later (perhaps the network or the recipient is overloaded). 3\u3001 The remote node may have failed (perhaps it crashed or it was powered down). 4\u3001 The remote node may have temporarily stopped responding (perhaps it is experiencing a long garbage collection pause; see \u201cProcess Pauses\u201d on page 295), but it will start responding again later. 5\u3001 The remote node may have processed your request, but the response has been lost on the network (perhaps a network switch has been misconfigured). 6\u3001 The remote node may have processed your request, but the response has been delayed and will be delivered later (perhaps the network or your own machine is overloaded). NOTE: \u770b\u4e86\u4e0a\u9762\u7684\u5185\u5bb9\uff0c\u4e0d\u8981\u89c9\u5f97\u7cfb\u7edf\u592a\u8106\u5f31\u4e86\uff1b\u56e0\u4e3a\u4e0a\u9762\u63cf\u8ff0\u7684\u5404\u79cd\u4e8b\u4ef6\uff0c\u6709\u7684\u53d1\u751f\u7684\u53ef\u80fd\u6027\u662f\u975e\u5e38\u4f4e\u7684 The sender can\u2019t even tell whether the packet was delivered: the only option is for the recipient to send a response message, which may in turn be lost or delayed. These issues are indistinguishable in an asynchronous network: the only information you have is that you haven\u2019t received a response yet. If you send a request to another node and don\u2019t receive a response, it is impossible to tell why. The usual way of handling this issue is a timeout : after some time you give up waiting and assume that the response is not going to arrive. However, when a timeout occurs, you still don\u2019t know whether the remote node got your request or not (and if the request is still queued somewhere, it may still be delivered to the recipient, even if the sender has given up on it). NOTE: redis\u6240\u91c7\u7528\u7684\u5c31\u662ftimeout\u7684\u65b9\u5f0f\u3002","title":"Unreliable Networks"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/PART-II/8-The-Trouble-with-Distributed-Systems/#network#faults#in#practice","text":"We have been building computer networks for decades\u2014one might hope that by now we would have figured out how to make them reliable. However, it seems that we have not yet succeeded. Network partitions When one part of the network is cut off from the rest due to a network fault, that is sometimes called a network partition or netsplit . In this book we\u2019ll generally stick with the more general term network fault , to avoid confusion with partitions (shards) of a storage system, as discussed in Chapter 6.","title":"Network Faults in Practice"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/PART-II/8-The-Trouble-with-Distributed-Systems/#detecting#faults","text":"Many systems need to automatically detect faulty nodes. For example: A load balancer needs to stop sending requests to a node that is dead (i.e., take it out of rotation). In a distributed database with single-leader replication, if the leader fails, one of the followers needs to be promoted to be the new leader (see \u201cHandling Node Outages\u201d on page 156). Unfortunately, the uncertainty about the network makes it difficult to tell whether a node is working or not. In some specific circumstances you might get some feedback to explicitly tell you that something is not working: If you can reach the machine on which the node should be running, but no process is listening on the destination port (e.g., because the process crashed), the operating system will helpfully close or refuse TCP connections by sending a RST or FIN packet in reply. However, if the node crashed while it was handling your request, you have no way of knowing how much data was actually processed by the remote node [22]. If a node process crashed (or was killed by an administrator) but the node\u2019s operating system is still running, a script can notify other nodes about the crash so that another node can take over quickly without having to wait for a timeout to expire. For example, HBase does this [23]. If you have access to the management interface of the network switches in your datacenter, you can query them to detect link failures at a hardware level (e.g., if the remote machine is powered down). This option is ruled out if you\u2019re connecting via the internet, or if you\u2019re in a shared datacenter with no access to the switches themselves, or if you can\u2019t reach the management interface due to a network problem. If a router is sure that the IP address you\u2019re trying to connect to is unreachable, it may reply to you with an ICMP Destination Unreachable packet. However, the router doesn\u2019t have a magic failure detection capability either\u2014it is subject to the same limitations as other participants of the network. Conversely, if something has gone wrong, you may get an error response at some level of the stack, but in general you have to assume that you will get no response at all. You can retry a few times (TCP retries transparently, but you may also retry at the application level), wait for a timeout to elapse, and eventually declare the node dead if you don\u2019t hear back within the timeout. NOTE: timeout\u662f\u4e00\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u3001\u51c6\u786e\u7684\u65b9\u6cd5","title":"Detecting Faults"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/PART-II/8-The-Trouble-with-Distributed-Systems/#timeouts#and#unbounded#delays","text":"If a timeout is the only sure way of detecting a fault, then how long should the timeout be? There is unfortunately no simple answer. A long timeout means a long wait until a node is declared dead (and during this time, users may have to wait or see error messages). A short timeout detects faults faster, but carries a higher risk of incorrectly declaring a node dead when in fact it has only suffered a temporary slowdown (e.g., due to a load spike on the node or the network). Prematurely\uff08\u8fc7\u65e9\u5730\uff09 declaring a node dead is problematic: if the node is actually alive and in the middle of performing some action (for example, sending an email), and another node takes over, the action may end up being performed twice. We will discuss this issue in more detail in \u201cKnowledge, Truth, and Lies\u201d on page 300, and in Chapters 9 and 11.","title":"Timeouts and Unbounded Delays"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/PART-II/8-The-Trouble-with-Distributed-Systems/#network#congestion#and#queueing","text":"When driving a car, travel times on road networks often vary most due to traffic congestion. Similarly, the variability of packet delays on computer networks is most often due to queueing","title":"Network congestion and queueing"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/PART-II/8-The-Trouble-with-Distributed-Systems/#unreliable#clocks","text":"Clocks and time are important. Applications depend on clocks in various ways to answer questions like the following: 1. Has this request timed out yet? 2. What\u2019s the 99 th percentile response time of this service? 3. How many queries per second did this service handle on average in the last five minutes? 4. How long did the user spend on our site? 5. When was this article published? 6. At what date and time should the reminder email be sent? 7. When does this cache entry expire? 8. What is the timestamp on this error message in the log file?","title":"Unreliable Clocks"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/PART-II/8-The-Trouble-with-Distributed-Systems/#knowledge#truth#and#lies","text":"So far in this chapter we have explored the ways in which distributed systems are different from programs running on a single computer: there is no shared memory, only message passing via an unreliable network with variable delays, and the systems may suffer from partial failures, unreliable clocks, and processing pauses. NOTE: \u8fd9\u6bb5\u8bdd\u603b\u7ed3\u7684\u975e\u5e38\u597d\u3002 The consequences of these issues are profoundly disorienting if you\u2019re not used to distributed systems. A node in the network cannot know anything for sure\u2014it can only make guesses based on the messages it receives (or doesn\u2019t receive) via the network. A node can only find out what state another node is in (what data it has stored, whether it is correctly functioning, etc.) by exchanging messages with it. If a remote node doesn\u2019t respond, there is no way of knowing what state it is in, because problems in the network cannot reliably be distinguished from problems at a node. Discussions of these systems border(\u4e0e...\u63a5\u58e4) on the philosophical: What do we know to be true or false in our system? How sure can we be of that knowledge, if the mechanisms for perception and measurement are unreliable? Should software systems obey the laws that we expect of the physical world, such as cause and effect? NOTE: \u5bf9\u8fd9\u4e9b\u7cfb\u7edf\u7684\u8ba8\u8bba\u8fd1\u4e4e\u4e8e\u54f2\u5b66\u3002\u8f6f\u4ef6\u7cfb\u7edf\u662f\u5426\u5e94\u8be5\u9075\u5faa\u6211\u4eec\u6240\u671f\u671b\u7684\u7269\u7406\u4e16\u754c\u7684\u6cd5\u5219\uff0c\u6bd4\u5982\u56e0\u679c\u5173\u7cfb? Fortunately, we don\u2019t need to go as far as figuring out the meaning of life. In a distributed system, we can state the assumptions we are making about the behavior (the system model ) and design the actual system in such a way that it meets those assumptions. Algorithms can be proved to function correctly within a certain system model . This means that reliable behavior is achievable, even if the underlying system model provides very few guarantees. NOTE: \u5efa\u7acbsystem model\uff0c\u6bcf\u4e2amodel\u90fd\u662f\u6709\u4e00\u5b9a\u7684assumption\uff0c\u5373\u5047\u8bbe\u3001\u524d\u63d0\u6761\u4ef6 However, although it is possible to make software well behaved in an unreliable system model , it is not straightforward to do so. In the rest of this chapter we will further explore the notions of knowledge and truth in distributed systems, which will help us think about the kinds of assumptions we can make and the guarantees we may want to provide. In Chapter 9 we will proceed to look at some examples of distributed systems, algorithms that provide particular guarantees under particular assumptions.","title":"Knowledge, Truth, and Lies"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/PART-II/8-The-Trouble-with-Distributed-Systems/#the#truth#is#defined#by#the#majority","text":"The moral\uff08\u5bd3\u610f\uff09 of these stories is that a node cannot necessarily trust its own judgment of a situation. A distributed system cannot exclusively rely on a single node, because a node may fail at any time, potentially leaving the system stuck and unable to recover. Instead, many distributed algorithms rely on a quorum , that is, voting among the nodes (see \u201cQuorums for reading and writing\u201d on page 179): decisions require some minimum number of votes from several nodes in order to reduce the dependence on any one particular node. That includes decisions about declaring nodes dead. If a quorum of nodes declares another node dead, then it must be considered dead, even if that node still very much feels alive. The individual node must abide by(\u9075\u5b88) the quorum decision and step down.","title":"The Truth Is Defined by the Majority"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/Preface/","text":"Preface NOTE: \u539f\u4e66\u7684preface\u5199\u7684\u975e\u5e38\u68d2\uff0c\u4f5c\u8005\u53ef\u4ee5\u8bf4\u662f\u7ad9\u5728\u4e86\u975e\u5e38\u9ad8\u7684\u89d2\u5ea6\u6765\u603b\u7ed3\u8ba1\u7b97\u673a/\u4e92\u8054\u7f51\u6280\u672f\u53d1\u5c55\u7684\u8d8b\u52bf\u3002 In the last decade we have seen many interesting developments in databases, in distributed systems, and in the ways we build applications on top of them. There are various driving forces for these developments: 1\u3001Internet companies such as Google, Yahoo!, Amazon, Facebook, LinkedIn, Microsoft, and Twitter are handling huge volumes of data and traffic, forcing them to create new tools that enable them to efficiently handle such scale. NOTE: \u6570\u636e\u7206\u70b8 2\u3001Businesses need to be agile, test hypotheses cheaply, and respond quickly to new market insights by keeping development cycles short and data models flexible. NOTE: \u4e1a\u52a1\u9700\u8981\u7075\u6d3b\uff0c\u6d4b\u8bd5\u5047\u8bbe\u6210\u672c\u4f4e\uff0c\u901a\u8fc7\u4fdd\u6301\u5f00\u53d1\u5468\u671f\u77ed\u548c\u6570\u636e\u6a21\u578b\u7075\u6d3b\uff0c\u5bf9\u65b0\u7684\u5e02\u573a\u6d1e\u5bdf\u505a\u51fa\u5feb\u901f\u54cd\u5e94\u3002 3\u3001Free and open source software has become very successful and is now preferred to commercial or bespoke in-house software in many environments. 4\u3001CPU clock speeds are barely increasing, but multi-core processors are standard, and networks are getting faster. This means parallelism is only going to increase. NOTE: CPU clock speed\u51e0\u4e4e\u4e0d\u589e\u52a0\uff0cmulti-core processors\u88ab\u5e7f\u6cdb\u91c7\u7528\uff0c\u7f51\u7edc\u8d8a\u6765\u8d8a\u5feb\uff0c\u8fd9\u4e9b\u90fd\u610f\u5473\u7740\u201cparallelism\u201d\u5c06\u4f1a\u589e\u52a0\u3002 5\u3001Even if you work on a small team, you can now build systems that are distributed across many machines and even multiple geographic regions, thanks to infrastructure as a service (IaaS) such as Amazon Web Services. 6\u3001Many services are now expected to be highly available; extended downtime due to outages or maintenance is becoming increasingly unacceptable. NOTE: HA Data-intensive applications are pushing the boundaries of what is possible by making use of these technological developments. We call an application data-intensive if data is its primary challenge\u2014the quantity of data, the complexity of data, or the speed at which it is changing\u2014as opposed to compute-intensive , where CPU cycles are the bottleneck. NOTE: \u4e00\u3001\u4f5c\u8005\u7ed9\u51fa\u4e86data-intensive\u548ccompute-insensitive\u7684\u5b9a\u4e49: compute-intensive \u5373\u8ba1\u7b97\u5bc6\u96c6\u578b\uff0c\u5b83\u662fCPU-bound data-intensive \u5373\u6570\u636e\u5bc6\u96c6\u578b\uff0c\u5b83\u662fdata-bound Fortunately, behind the rapid changes in technology, there are enduring principles that remain true, no matter which version of a particular tool you are using. If you understand those principles , you\u2019re in a position to see where each tool fits in, how to make good use of it, and how to avoid its pitfalls. NOTE: \u53d8**\u4e0e**\u4e0d\u53d8 We will dig into the internals of those systems, tease apart their key algorithms, discuss their principles and the trade-offs they have to make. On this journey, we will try to find useful ways of thinking about data systems \u2014not just how they work, but also why they work that way, and what questions we need to ask. NOTE: \u5176\u5b9e\u5c31\u662f \u7ad9\u5728\u8bbe\u8ba1\u8005\u7684\u89d2\u5ea6\u6765\u601d\u8003 \u3002","title":"Introduction"},{"location":"Distributed-computing/Book-Designing-Data-Intensive-Applications/Preface/#preface","text":"NOTE: \u539f\u4e66\u7684preface\u5199\u7684\u975e\u5e38\u68d2\uff0c\u4f5c\u8005\u53ef\u4ee5\u8bf4\u662f\u7ad9\u5728\u4e86\u975e\u5e38\u9ad8\u7684\u89d2\u5ea6\u6765\u603b\u7ed3\u8ba1\u7b97\u673a/\u4e92\u8054\u7f51\u6280\u672f\u53d1\u5c55\u7684\u8d8b\u52bf\u3002 In the last decade we have seen many interesting developments in databases, in distributed systems, and in the ways we build applications on top of them. There are various driving forces for these developments: 1\u3001Internet companies such as Google, Yahoo!, Amazon, Facebook, LinkedIn, Microsoft, and Twitter are handling huge volumes of data and traffic, forcing them to create new tools that enable them to efficiently handle such scale. NOTE: \u6570\u636e\u7206\u70b8 2\u3001Businesses need to be agile, test hypotheses cheaply, and respond quickly to new market insights by keeping development cycles short and data models flexible. NOTE: \u4e1a\u52a1\u9700\u8981\u7075\u6d3b\uff0c\u6d4b\u8bd5\u5047\u8bbe\u6210\u672c\u4f4e\uff0c\u901a\u8fc7\u4fdd\u6301\u5f00\u53d1\u5468\u671f\u77ed\u548c\u6570\u636e\u6a21\u578b\u7075\u6d3b\uff0c\u5bf9\u65b0\u7684\u5e02\u573a\u6d1e\u5bdf\u505a\u51fa\u5feb\u901f\u54cd\u5e94\u3002 3\u3001Free and open source software has become very successful and is now preferred to commercial or bespoke in-house software in many environments. 4\u3001CPU clock speeds are barely increasing, but multi-core processors are standard, and networks are getting faster. This means parallelism is only going to increase. NOTE: CPU clock speed\u51e0\u4e4e\u4e0d\u589e\u52a0\uff0cmulti-core processors\u88ab\u5e7f\u6cdb\u91c7\u7528\uff0c\u7f51\u7edc\u8d8a\u6765\u8d8a\u5feb\uff0c\u8fd9\u4e9b\u90fd\u610f\u5473\u7740\u201cparallelism\u201d\u5c06\u4f1a\u589e\u52a0\u3002 5\u3001Even if you work on a small team, you can now build systems that are distributed across many machines and even multiple geographic regions, thanks to infrastructure as a service (IaaS) such as Amazon Web Services. 6\u3001Many services are now expected to be highly available; extended downtime due to outages or maintenance is becoming increasingly unacceptable. NOTE: HA Data-intensive applications are pushing the boundaries of what is possible by making use of these technological developments. We call an application data-intensive if data is its primary challenge\u2014the quantity of data, the complexity of data, or the speed at which it is changing\u2014as opposed to compute-intensive , where CPU cycles are the bottleneck. NOTE: \u4e00\u3001\u4f5c\u8005\u7ed9\u51fa\u4e86data-intensive\u548ccompute-insensitive\u7684\u5b9a\u4e49: compute-intensive \u5373\u8ba1\u7b97\u5bc6\u96c6\u578b\uff0c\u5b83\u662fCPU-bound data-intensive \u5373\u6570\u636e\u5bc6\u96c6\u578b\uff0c\u5b83\u662fdata-bound Fortunately, behind the rapid changes in technology, there are enduring principles that remain true, no matter which version of a particular tool you are using. If you understand those principles , you\u2019re in a position to see where each tool fits in, how to make good use of it, and how to avoid its pitfalls. NOTE: \u53d8**\u4e0e**\u4e0d\u53d8 We will dig into the internals of those systems, tease apart their key algorithms, discuss their principles and the trade-offs they have to make. On this journey, we will try to find useful ways of thinking about data systems \u2014not just how they work, but also why they work that way, and what questions we need to ask. NOTE: \u5176\u5b9e\u5c31\u662f \u7ad9\u5728\u8bbe\u8ba1\u8005\u7684\u89d2\u5ea6\u6765\u601d\u8003 \u3002","title":"Preface"},{"location":"Distributed-computing/Book-Distributed-systems-for-fun-and-profit/","text":"mixu Distributed systems","title":"Introduction"},{"location":"Distributed-computing/Book-Distributed-systems-for-fun-and-profit/#mixu#distributed#systems","text":"","title":"mixu Distributed systems"},{"location":"Distributed-computing/Book-Distributed-systems-for-fun-and-profit/4-Replication/","text":"4. Replication NOTE: \u4e00\u3001\u5728zhihu \u5206\u5e03\u5f0f\u7cfb\u7edf\u57fa\u7840\u7406\u8bba\uff08\u516d\uff09 \u4e2d\uff0c\u53c2\u8003\u4e86\u8fd9\u7bc7\u6587\u7ae0 \u4e8c\u3001nakivo Synchronous vs. Asynchronous Replication Strategy \u4e09\u3001evidian \u540c\u6b65\u590d\u5236vs\u5f02\u6b65\u590d\u5236 Synchronous replication Asynchronous replication","title":"Introduction"},{"location":"Distributed-computing/Book-Distributed-systems-for-fun-and-profit/4-Replication/#4#replication","text":"NOTE: \u4e00\u3001\u5728zhihu \u5206\u5e03\u5f0f\u7cfb\u7edf\u57fa\u7840\u7406\u8bba\uff08\u516d\uff09 \u4e2d\uff0c\u53c2\u8003\u4e86\u8fd9\u7bc7\u6587\u7ae0 \u4e8c\u3001nakivo Synchronous vs. Asynchronous Replication Strategy \u4e09\u3001evidian \u540c\u6b65\u590d\u5236vs\u5f02\u6b65\u590d\u5236","title":"4. Replication"},{"location":"Distributed-computing/Book-Distributed-systems-for-fun-and-profit/4-Replication/#synchronous#replication","text":"","title":"Synchronous replication"},{"location":"Distributed-computing/Book-Distributed-systems-for-fun-and-profit/4-Replication/#asynchronous#replication","text":"","title":"Asynchronous replication"},{"location":"Distributed-computing/Course/Course-Northeastern-University-CS7680/","text":"Northeastern University CS 7680 : Programming Models for Distributed Computing \u5176\u4e2d\u63a8\u8350\u4e86 Distributed Systems for Fun and Profit \uff0c\u770b\u4e86\u4e00\u4e0b\uff0c\u8fd9\u672c\u4e66\u662f\u6bd4\u8f83\u597d\u7684\uff0c\u503c\u5f97\u5b66\u4e60\u3002","title":"Introduction"},{"location":"Distributed-computing/Course/Course-Northeastern-University-CS7680/#northeastern#university#cs#7680#programming#models#for#distributed#computing","text":"\u5176\u4e2d\u63a8\u8350\u4e86 Distributed Systems for Fun and Profit \uff0c\u770b\u4e86\u4e00\u4e0b\uff0c\u8fd9\u672c\u4e66\u662f\u6bd4\u8f83\u597d\u7684\uff0c\u503c\u5f97\u5b66\u4e60\u3002","title":"Northeastern University CS 7680:  Programming Models for Distributed Computing"},{"location":"Distributed-computing/Course/Course-Rutgers-Distributed-Systems/","text":"rutgers CS 417: Distributed Systems https://www.cs.rutgers.edu/~pxk/417/notes/paxos.html","title":"Introduction"},{"location":"Distributed-computing/Course/Course-Rutgers-Distributed-Systems/#rutgers#cs#417#distributed#systems","text":"https://www.cs.rutgers.edu/~pxk/417/notes/paxos.html","title":"rutgers CS 417: Distributed Systems"},{"location":"Distributed-computing/Course/Course-Rutgers-Distributed-Systems/Understanding-Paxos/","text":"rutgers Understanding Paxos","title":"Introduction"},{"location":"Distributed-computing/Course/Course-Rutgers-Distributed-Systems/Understanding-Paxos/#rutgers#understanding#paxos","text":"","title":"rutgers Understanding Paxos"},{"location":"Distributed-computing/Course/Course-disco.ethz-Distributed-Systems/","text":"disco.ethz.ch Distributed Systems Part 2 (HS 2015) NOTE: 1\u3001\u662f\u5728Google \"spinning lock with timeout\"\u7684\u65f6\u5019\uff0cGoogle\u5230\u4e86\u5b83\u7684 Locking Part 2, Chapter 11 \uff0c\u4ece\u800c\u53d1\u73b0\u7684\u8fd9\u4e2a\u8bfe\u7a0b\u3002 2\u3001\u5176\u4e2d\u5bf9distributed computing\u7684\u8bb2\u8ff0\uff0c \u6db5\u76d6\u9762\u633a\u5e7f\u6cdb\u7684","title":"Introduction"},{"location":"Distributed-computing/Course/Course-disco.ethz-Distributed-Systems/#discoethzch#distributed#systems#part#2#hs#2015","text":"NOTE: 1\u3001\u662f\u5728Google \"spinning lock with timeout\"\u7684\u65f6\u5019\uff0cGoogle\u5230\u4e86\u5b83\u7684 Locking Part 2, Chapter 11 \uff0c\u4ece\u800c\u53d1\u73b0\u7684\u8fd9\u4e2a\u8bfe\u7a0b\u3002 2\u3001\u5176\u4e2d\u5bf9distributed computing\u7684\u8bb2\u8ff0\uff0c \u6db5\u76d6\u9762\u633a\u5e7f\u6cdb\u7684","title":"disco.ethz.ch Distributed Systems Part 2 (HS 2015)"},{"location":"Distributed-computing/Expert-Dr-Werner-Vogels/","text":"allthingsdistributed Dr. Werner Vogels NOTE: \u5728\u9605\u8bfb zhihu \u5173\u4e8e\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684NWR quorum consistency \u65f6\uff0c\u5176\u4e2d\u7ed9\u51fa\u4e86 allthingsdistributed Eventually Consistent - Revisited \u8fd9\u7bc7\u6587\u7ae0\u7684\u94fe\u63a5\uff0c\u4ece\u800c\u53d1\u73b0\u4e86 Dr. Werner Vogels is Chief Technology Officer at Amazon.com where he is responsible for driving the company\u2019s customer-centric technology vision. NOTE: \u4ed6\u662f\u4e9a\u9a6c\u900a\u7684\u9996\u5e2d\u6280\u672f\u5b98\uff0c\u6b64\u4eba\u662fdistributed computing\u9886\u57df\u7684\u9876\u7ea7\u4e13\u5bb6 \u770b\u4e86\u4e00\u4e0b\uff0c\u4ed6\u5199\u7684\u6587\u7ae0\u8fd8\u4e0d\u9519","title":"Introduction"},{"location":"Distributed-computing/Expert-Dr-Werner-Vogels/#allthingsdistributed#dr#werner#vogels","text":"NOTE: \u5728\u9605\u8bfb zhihu \u5173\u4e8e\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684NWR quorum consistency \u65f6\uff0c\u5176\u4e2d\u7ed9\u51fa\u4e86 allthingsdistributed Eventually Consistent - Revisited \u8fd9\u7bc7\u6587\u7ae0\u7684\u94fe\u63a5\uff0c\u4ece\u800c\u53d1\u73b0\u4e86 Dr. Werner Vogels is Chief Technology Officer at Amazon.com where he is responsible for driving the company\u2019s customer-centric technology vision. NOTE: \u4ed6\u662f\u4e9a\u9a6c\u900a\u7684\u9996\u5e2d\u6280\u672f\u5b98\uff0c\u6b64\u4eba\u662fdistributed computing\u9886\u57df\u7684\u9876\u7ea7\u4e13\u5bb6 \u770b\u4e86\u4e00\u4e0b\uff0c\u4ed6\u5199\u7684\u6587\u7ae0\u8fd8\u4e0d\u9519","title":"allthingsdistributed Dr. Werner Vogels"},{"location":"Distributed-computing/Expert-Dr-Werner-Vogels/Eventually-Consistent-Revisited/","text":"allthingsdistributed Eventually Consistent - Revisited NOTE: \u5728zhihu \u5173\u4e8e\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684NWR quorum consistency \u4e2d\uff0c\u7ed9\u51fa\u4e86\u8fd9\u7bc7\u6587\u7ae0\u7684\u94fe\u63a5 Eventually Consistent - Building reliable distributed systems at a worldwide scale demands trade-offs between consistency and availability.","title":"Introduction"},{"location":"Distributed-computing/Expert-Dr-Werner-Vogels/Eventually-Consistent-Revisited/#allthingsdistributed#eventually#consistent#-#revisited","text":"NOTE: \u5728zhihu \u5173\u4e8e\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684NWR quorum consistency \u4e2d\uff0c\u7ed9\u51fa\u4e86\u8fd9\u7bc7\u6587\u7ae0\u7684\u94fe\u63a5 Eventually Consistent - Building reliable distributed systems at a worldwide scale demands trade-offs between consistency and availability.","title":"allthingsdistributed Eventually Consistent - Revisited"},{"location":"Distributed-computing/Expert-Leslie-Lamport/","text":"Leslie Lamport \u4e00\u3001\u5206\u5e03\u5f0f\u8ba1\u7b97\u9886\u57df\u7684\u5f00\u62d3\u8005\uff0c\u51ed\u501f\u4e00\u5df1\u4e4b\u529b\uff0c\u6784\u7b51\u4e86\u5206\u5e03\u5f0f\u8ba1\u7b97\u5927\u53a6\u7684\u5e95\u5ea7\uff0c\u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\uff0c\u5bf9\u4ed6\u8fdb\u884c\u4e86\u4ecb\u7ecd: 1\u3001\u8303\u658c zhihu \u6f2b\u8bdd\u5206\u5e03\u5f0f\u7cfb\u7edf\u5171\u8bc6\u534f\u8bae: Paxos\u7bc7 \u5728\u6211\u770b\u6765, Paxos\u7b97\u6cd5(\u8fde\u540cLamport\u7684\u5176\u4ed6\u5982BFT, Vector Clock\u7b49\u6210\u5c31)\u662f\u4e0a\u4e2a\u4e16\u7eaa\u516b\u5341/\u4e5d\u5341\u5e74\u4ee3\u7684\u7ecf\u5178\u5206\u5e03\u5f0f\u7cfb\u7edf\u7814\u7a76\u4e2d\u6700\u7eaf\u7cb9\u6700\u4f18\u7f8e, \u4e5f\u662f\u6574\u680b\u5927\u53a6\u5e95\u5ea7\u6700\u575a\u5b9e\u7684\u90a3\u4e00\u90e8\u5206. \u4e8c\u3001cornell Distributed Systems: Ordering and Consistency wikipedia Leslie Lamport Leslie Lamport was the winner of the 2013 Turing Award [ 4] for imposing clear, well-defined coherence on the seemingly chaotic(\u6df7\u4e71\u7684) behavior of distributed computing systems, in which several autonomous computers communicate with each other by passing messages. NOTE: \"make it computational\" Career and research Distributed Systems NOTE: \u9700\u8981\u5bf9\u5b83\u5728Distributed Systems\u4e2d\u7684\u7406\u8bba\u4e86\u89e3\u4e00\u4e0b Lamport's research contributions have laid the foundations of the theory of distributed systems. Among his most notable papers are 1\u3001\"Time, Clocks, and the Ordering of Events in a Distributed System\",[ 6] which received the PODC Influential Paper Award in 2000,[ 12] NOTE: lamport timestamp 2\u3001\"How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs\",[ 13] which defined the notion of sequential consistency , 3\u3001\" The Byzantine Generals' Problem \",[ 14] 4\u3001\"Distributed Snapshots: Determining Global States of a Distributed System\"[ 15] and 5\u3001\"The Part-Time Parliament\".[ 16] These papers relate to such concepts as logical clocks (and the happened-before relationship) and Byzantine failures . They are among the most cited papers in the field of computer science,[ 17] and describe algorithms to solve many fundamental problems in distributed systems, including: NOTE: \u4e0a\u8ff0 happened-before\u8ba9\u6211\u60f3\u8d77\u6765C++ memory model\u4e2d\u7684 std::memory_order \u4e2d\u7684*happens-before* 1\u3001the Paxos algorithm for consensus , 2\u3001the bakery algorithm for mutual exclusion of multiple threads in a computer system that require the same resources at the same time, 3\u3001the Chandy-Lamport algorithm for the determination of consistent global states (snapshot), and 4\u3001the Lamport signature , one of the prototypes of the digital signature. LESLIE LAMPORT'S HOME PAGE","title":"Introduction"},{"location":"Distributed-computing/Expert-Leslie-Lamport/#leslie#lamport","text":"\u4e00\u3001\u5206\u5e03\u5f0f\u8ba1\u7b97\u9886\u57df\u7684\u5f00\u62d3\u8005\uff0c\u51ed\u501f\u4e00\u5df1\u4e4b\u529b\uff0c\u6784\u7b51\u4e86\u5206\u5e03\u5f0f\u8ba1\u7b97\u5927\u53a6\u7684\u5e95\u5ea7\uff0c\u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\uff0c\u5bf9\u4ed6\u8fdb\u884c\u4e86\u4ecb\u7ecd: 1\u3001\u8303\u658c zhihu \u6f2b\u8bdd\u5206\u5e03\u5f0f\u7cfb\u7edf\u5171\u8bc6\u534f\u8bae: Paxos\u7bc7 \u5728\u6211\u770b\u6765, Paxos\u7b97\u6cd5(\u8fde\u540cLamport\u7684\u5176\u4ed6\u5982BFT, Vector Clock\u7b49\u6210\u5c31)\u662f\u4e0a\u4e2a\u4e16\u7eaa\u516b\u5341/\u4e5d\u5341\u5e74\u4ee3\u7684\u7ecf\u5178\u5206\u5e03\u5f0f\u7cfb\u7edf\u7814\u7a76\u4e2d\u6700\u7eaf\u7cb9\u6700\u4f18\u7f8e, \u4e5f\u662f\u6574\u680b\u5927\u53a6\u5e95\u5ea7\u6700\u575a\u5b9e\u7684\u90a3\u4e00\u90e8\u5206. \u4e8c\u3001cornell Distributed Systems: Ordering and Consistency","title":"Leslie Lamport"},{"location":"Distributed-computing/Expert-Leslie-Lamport/#wikipedia#leslie#lamport","text":"Leslie Lamport was the winner of the 2013 Turing Award [ 4] for imposing clear, well-defined coherence on the seemingly chaotic(\u6df7\u4e71\u7684) behavior of distributed computing systems, in which several autonomous computers communicate with each other by passing messages. NOTE: \"make it computational\"","title":"wikipedia Leslie Lamport"},{"location":"Distributed-computing/Expert-Leslie-Lamport/#career#and#research","text":"","title":"Career and research"},{"location":"Distributed-computing/Expert-Leslie-Lamport/#distributed#systems","text":"NOTE: \u9700\u8981\u5bf9\u5b83\u5728Distributed Systems\u4e2d\u7684\u7406\u8bba\u4e86\u89e3\u4e00\u4e0b Lamport's research contributions have laid the foundations of the theory of distributed systems. Among his most notable papers are 1\u3001\"Time, Clocks, and the Ordering of Events in a Distributed System\",[ 6] which received the PODC Influential Paper Award in 2000,[ 12] NOTE: lamport timestamp 2\u3001\"How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs\",[ 13] which defined the notion of sequential consistency , 3\u3001\" The Byzantine Generals' Problem \",[ 14] 4\u3001\"Distributed Snapshots: Determining Global States of a Distributed System\"[ 15] and 5\u3001\"The Part-Time Parliament\".[ 16] These papers relate to such concepts as logical clocks (and the happened-before relationship) and Byzantine failures . They are among the most cited papers in the field of computer science,[ 17] and describe algorithms to solve many fundamental problems in distributed systems, including: NOTE: \u4e0a\u8ff0 happened-before\u8ba9\u6211\u60f3\u8d77\u6765C++ memory model\u4e2d\u7684 std::memory_order \u4e2d\u7684*happens-before* 1\u3001the Paxos algorithm for consensus , 2\u3001the bakery algorithm for mutual exclusion of multiple threads in a computer system that require the same resources at the same time, 3\u3001the Chandy-Lamport algorithm for the determination of consistent global states (snapshot), and 4\u3001the Lamport signature , one of the prototypes of the digital signature.","title":"Distributed Systems"},{"location":"Distributed-computing/Expert-Leslie-Lamport/#leslie#lamports#home#page","text":"","title":"LESLIE  LAMPORT'S  HOME  PAGE"},{"location":"Distributed-computing/Guide/","text":"Guide to distributed computing \u5206\u5e03\u5f0f\u8ba1\u7b97\u6d89\u53ca\u5230\u7684\u5185\u5bb9\u662f\u975e\u5e38\u591a\u7684\uff0c\u672c\u7ae0\u9996\u5148\u4ece\u8f83\u9ad8\u5730\u5c42\u6b21\u6765\u63a2\u8ba8\u5206\u5e03\u5f0f\u8ba1\u7b97: 1\u3001The challenge of Distributed Systems \u53ea\u6709\u628a\u63e1\u4e86\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u5404\u79cd\u95ee\u9898\u3001\u6311\u6218\uff0c\u624d\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u5404\u79cd\u6280\u672f\uff0c\u8fd9\u5728 Distributed-system-challenge \u7ae0\u8282\u8fdb\u884c\u8ba8\u8bba\uff1b 2\u3001Abstraction \u5bf9\u4e8e\u5206\u5e03\u5f0f\u7cfb\u7edf\u8fd9\u6837\u7684\u590d\u6742\u5de5\u7a0b\uff0c\u521b\u5efaabstraction\u662f\u89e3\u51b3\u5b83\u7684\u5fc5\u7531\u4e4b\u8def\uff0c\u8fd9\u5728 Abstraction \u7ae0\u8282\u8fdb\u884c\u4e86\u8ba8\u8bba\u3002 \u5206\u5e03\u5f0f\u8ba1\u7b97\u7684\u7406\u8bba\u3001\u6280\u672f\u6982\u8ff0 \u6536\u5f55\u4e86\u4e00\u4e9b\u6bd4\u8f83\u597d\u7684\u6587\u7ae0\uff0c\u4f5c\u4e3aguide: 1\u3001CSDN \u609f\u7a7a\u804a\u67b6\u6784\u7684\u5206\u5e03\u5f0f\u4e13\u680f 2\u3001martinfowler Patterns of Distributed Systems","title":"Introduction"},{"location":"Distributed-computing/Guide/#guide#to#distributed#computing","text":"\u5206\u5e03\u5f0f\u8ba1\u7b97\u6d89\u53ca\u5230\u7684\u5185\u5bb9\u662f\u975e\u5e38\u591a\u7684\uff0c\u672c\u7ae0\u9996\u5148\u4ece\u8f83\u9ad8\u5730\u5c42\u6b21\u6765\u63a2\u8ba8\u5206\u5e03\u5f0f\u8ba1\u7b97: 1\u3001The challenge of Distributed Systems \u53ea\u6709\u628a\u63e1\u4e86\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u5404\u79cd\u95ee\u9898\u3001\u6311\u6218\uff0c\u624d\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u5404\u79cd\u6280\u672f\uff0c\u8fd9\u5728 Distributed-system-challenge \u7ae0\u8282\u8fdb\u884c\u8ba8\u8bba\uff1b 2\u3001Abstraction \u5bf9\u4e8e\u5206\u5e03\u5f0f\u7cfb\u7edf\u8fd9\u6837\u7684\u590d\u6742\u5de5\u7a0b\uff0c\u521b\u5efaabstraction\u662f\u89e3\u51b3\u5b83\u7684\u5fc5\u7531\u4e4b\u8def\uff0c\u8fd9\u5728 Abstraction \u7ae0\u8282\u8fdb\u884c\u4e86\u8ba8\u8bba\u3002","title":"Guide to distributed computing"},{"location":"Distributed-computing/Guide/#_1","text":"\u6536\u5f55\u4e86\u4e00\u4e9b\u6bd4\u8f83\u597d\u7684\u6587\u7ae0\uff0c\u4f5c\u4e3aguide: 1\u3001CSDN \u609f\u7a7a\u804a\u67b6\u6784\u7684\u5206\u5e03\u5f0f\u4e13\u680f 2\u3001martinfowler Patterns of Distributed Systems","title":"\u5206\u5e03\u5f0f\u8ba1\u7b97\u7684\u7406\u8bba\u3001\u6280\u672f\u6982\u8ff0"},{"location":"Distributed-computing/Guide/Abstraction/","text":"Abstraction in distributed computing \u5728distributed computing\u4e2d\uff0c\u521b\u5efa\u5408\u9002\u7684**abstraction**\u662f\u89e3\u51b3\u5f88\u591a\u95ee\u9898\u7684\u5173\u952e\uff0c\u8fd9\u4e2a\u601d\u60f3\u5728Book-Designing-Data-Intensive-Applications\u7684 CHAPTER 9 Consistency and Consensus \u4e2d\u8fdb\u884c\u4e86\u9610\u8ff0\uff0c\u628a\u63e1\u8fd9\u4e2a\u601d\u60f3\uff0c\u662f\u638c\u63e1distributed computing\u4e2d\u5404\u79cdalgorithm\u7684\u5173\u952e\u6240\u5728\u3002\u5728Book-Designing-Data-Intensive-Applications\u4e2d\uff0c\u4f5c\u8005\u5df2\u7ecf\u8868\u8fbe\u4e86\u8fd9\u6837\u7684\u89c2\u70b9\uff0c\u4e0b\u9762\u662f\u5c06\u539f\u4e66\u4e2d\u5173\u4e8eabstraction\u7684\u4e00\u4e9b\u7ae0\u8282\u7684\u6458\u5f55\uff0c\u4ece\u8fd9\u4e9b\u5185\u5bb9\u57fa\u672c\u4e0a\u53ef\u4ee5\u4e86\u89e3\u4f5c\u8005\u7684\u601d\u60f3: 1) Chapter1. Reliable, Scalable, and Maintainable Applications#Simplicity: Managing Complexity : NOTE: \u539f\u6587\u8fd9\u6bb5\u7684\u6838\u5fc3\u601d\u60f3\u662f: creat abstraction to remove accidental complexity. Making a system simpler does not necessarily mean reducing its functionality; it can also mean removing accidental complexity . Moseley and Marks [32] define complexity as accidental if it is not inherent in the problem that the software solves (as seen by the users) but arises only from the implementation. NOTE: **accidental complexity**\u662f\u6307\u4e0d\u662f\u7531\u95ee\u9898\u672c\u8eab\u5f15\u5165\u7684complexity\uff0c\u800c\u662f\u7531implementation\u5f15\u5165\u7684complexity\u3002 One of the best tools we have for removing accidental complexity is abstraction . A good abstraction can hide a great deal of implementation detail behind a clean, simple-to-understand fa\u00e7ade. A good abstraction can also be used for a wide range of different applications. Not only is this reuse more efficient than reimplementing a similar thing multiple times, but it also leads to higher-quality software, as quality improvements in the abstracted component benefit all applications that use it. For example, high-level programming languages are abstractions that hide machine code, CPU registers, and syscalls. SQL is an abstraction that hides complex on-disk and in-memory data structures, concurrent requests from other clients, and inconsistencies after crashes. Of course, when programming in a high-level language, we are still using machine code; we are just not using it directly, because the programming language abstraction saves us from having to think about it. However, finding good abstractions is very hard. In the field of distributed systems , although there are many good algorithms, it is much less clear how we should be packaging them into abstractions that help us keep the complexity of the system at a manageable level. NOTE: \u627e\u5230\u4e00\u4e2agood abstraction\u662f\u975e\u5e38\u56f0\u96be\u7684 Throughout this book, we will keep our eyes open for good abstractions that allow us to extract parts of a large system into well-defined, reusable components. 2) Chapter1. Reliable, Scalable, and Maintainable Applications#Summary : NOTE: \u7d27\u968f\u4e0a\u4e00\u6bb5\u800c\u6765 Maintainability has many facets(\u9762), but in essence it\u2019s about making life better for the engineering and operations teams who need to work with the system. Good abstractions can help reduce complexity and make the system easier to modify and adapt for new use cases. 3) CHAPTER 2 Data Models and Query Languages : NOTE: level model\uff0c\u53c2\u89c1\u5de5\u7a0b software-engineering \u7684 Software-design\\Principle\\Abstraction \u3002 In a complex application there may be more intermediary(\u4e2d\u95f4) levels, such as APIs built upon APIs, but the basic idea is still the same: each layer hides the complexity of the layers below it by providing a clean data model. These abstractions allow different groups of people\u2014for example, the engineers at the database vendor and the application developers using their database\u2014to work together effectively. 4) CHAPTER 7 Transactions#Summary NOTE: \u6838\u5fc3\u601d\u60f3\u662f: Transactions are an abstraction layer . Transactions are an abstraction layer that allows an application to pretend that certain concurrency problems and certain kinds of hardware and software faults don\u2019t exist. A large class of errors is reduced down to a simple transaction abort, and the application just needs to try again. 5) CHAPTER 8 The Trouble with Distributed Systems#Knowledge, Truth, and Lies#System Model and Reality NOTE: \u6838\u5fc3\u601d\u60f3\u662f: system models are abstractions Many algorithms have been designed to solve distributed systems problems\u2014for example, we will examine solutions for the consensus problem in Chapter 9. In order to be useful, these algorithms need to tolerate the various faults of distributed systems that we discussed in this chapter. Algorithms need to be written in a way that does not depend too heavily on the details of the hardware and software configuration on which they are run. This in turn requires that we somehow formalize the kinds of faults that we expect to happen in a system. We do this by defining a system model , which is an abstraction that describes what things an algorithm may assume. 6) CHAPTER 8 The Trouble with Distributed Systems#Knowledge, Truth, and Lies#System Model and Reality#Mapping system models to the real world NOTE: \u6838\u5fc3\u601d\u60f3\u662f: abstraction is simplification of reality. However, when implementing an algorithm in practice, the messy facts of reality come back to bite you again, and it becomes clear that the system model is a simplified abstraction of reality. 7) CHAPTER 9 Consistency and Consensus NOTE: \u8fd9\u6bb5\u6240\u603b\u7ed3\u7684\u662f\u6838\u5fc3\u601d\u60f3: \"The best way of building fault-tolerant systems is to find some general-purpose abstractions with useful guarantees, implement them once, and then let applications rely on those guarantees.\" The best way of building fault-tolerant systems is to find some general-purpose abstractions with useful guarantees, implement them once, and then let applications rely on those guarantees. This is the same approach as we used with transactions in Chapter 7: by using a transaction , the application can pretend that there are no crashes ( atomicity ), that nobody else is concurrently accessing the database ( isolation ), and that storage devices are perfectly reliable ( durability ). Even though crashes, race conditions, and disk failures do occur, the transaction abstraction hides those problems so that the application doesn\u2019t need to worry about them. We will now continue along the same lines, and seek abstractions that can allow an application to ignore some of the problems with distributed systems . For example, one of the most important abstractions for distributed systems is consensus : that is, getting all of the nodes to agree on something. But first we first need to explore the range of guarantees and abstractions that can be provided in a distributed system.","title":"Introduction"},{"location":"Distributed-computing/Guide/Abstraction/#abstraction#in#distributed#computing","text":"\u5728distributed computing\u4e2d\uff0c\u521b\u5efa\u5408\u9002\u7684**abstraction**\u662f\u89e3\u51b3\u5f88\u591a\u95ee\u9898\u7684\u5173\u952e\uff0c\u8fd9\u4e2a\u601d\u60f3\u5728Book-Designing-Data-Intensive-Applications\u7684 CHAPTER 9 Consistency and Consensus \u4e2d\u8fdb\u884c\u4e86\u9610\u8ff0\uff0c\u628a\u63e1\u8fd9\u4e2a\u601d\u60f3\uff0c\u662f\u638c\u63e1distributed computing\u4e2d\u5404\u79cdalgorithm\u7684\u5173\u952e\u6240\u5728\u3002\u5728Book-Designing-Data-Intensive-Applications\u4e2d\uff0c\u4f5c\u8005\u5df2\u7ecf\u8868\u8fbe\u4e86\u8fd9\u6837\u7684\u89c2\u70b9\uff0c\u4e0b\u9762\u662f\u5c06\u539f\u4e66\u4e2d\u5173\u4e8eabstraction\u7684\u4e00\u4e9b\u7ae0\u8282\u7684\u6458\u5f55\uff0c\u4ece\u8fd9\u4e9b\u5185\u5bb9\u57fa\u672c\u4e0a\u53ef\u4ee5\u4e86\u89e3\u4f5c\u8005\u7684\u601d\u60f3: 1) Chapter1. Reliable, Scalable, and Maintainable Applications#Simplicity: Managing Complexity : NOTE: \u539f\u6587\u8fd9\u6bb5\u7684\u6838\u5fc3\u601d\u60f3\u662f: creat abstraction to remove accidental complexity. Making a system simpler does not necessarily mean reducing its functionality; it can also mean removing accidental complexity . Moseley and Marks [32] define complexity as accidental if it is not inherent in the problem that the software solves (as seen by the users) but arises only from the implementation. NOTE: **accidental complexity**\u662f\u6307\u4e0d\u662f\u7531\u95ee\u9898\u672c\u8eab\u5f15\u5165\u7684complexity\uff0c\u800c\u662f\u7531implementation\u5f15\u5165\u7684complexity\u3002 One of the best tools we have for removing accidental complexity is abstraction . A good abstraction can hide a great deal of implementation detail behind a clean, simple-to-understand fa\u00e7ade. A good abstraction can also be used for a wide range of different applications. Not only is this reuse more efficient than reimplementing a similar thing multiple times, but it also leads to higher-quality software, as quality improvements in the abstracted component benefit all applications that use it. For example, high-level programming languages are abstractions that hide machine code, CPU registers, and syscalls. SQL is an abstraction that hides complex on-disk and in-memory data structures, concurrent requests from other clients, and inconsistencies after crashes. Of course, when programming in a high-level language, we are still using machine code; we are just not using it directly, because the programming language abstraction saves us from having to think about it. However, finding good abstractions is very hard. In the field of distributed systems , although there are many good algorithms, it is much less clear how we should be packaging them into abstractions that help us keep the complexity of the system at a manageable level. NOTE: \u627e\u5230\u4e00\u4e2agood abstraction\u662f\u975e\u5e38\u56f0\u96be\u7684 Throughout this book, we will keep our eyes open for good abstractions that allow us to extract parts of a large system into well-defined, reusable components. 2) Chapter1. Reliable, Scalable, and Maintainable Applications#Summary : NOTE: \u7d27\u968f\u4e0a\u4e00\u6bb5\u800c\u6765 Maintainability has many facets(\u9762), but in essence it\u2019s about making life better for the engineering and operations teams who need to work with the system. Good abstractions can help reduce complexity and make the system easier to modify and adapt for new use cases. 3) CHAPTER 2 Data Models and Query Languages : NOTE: level model\uff0c\u53c2\u89c1\u5de5\u7a0b software-engineering \u7684 Software-design\\Principle\\Abstraction \u3002 In a complex application there may be more intermediary(\u4e2d\u95f4) levels, such as APIs built upon APIs, but the basic idea is still the same: each layer hides the complexity of the layers below it by providing a clean data model. These abstractions allow different groups of people\u2014for example, the engineers at the database vendor and the application developers using their database\u2014to work together effectively. 4) CHAPTER 7 Transactions#Summary NOTE: \u6838\u5fc3\u601d\u60f3\u662f: Transactions are an abstraction layer . Transactions are an abstraction layer that allows an application to pretend that certain concurrency problems and certain kinds of hardware and software faults don\u2019t exist. A large class of errors is reduced down to a simple transaction abort, and the application just needs to try again. 5) CHAPTER 8 The Trouble with Distributed Systems#Knowledge, Truth, and Lies#System Model and Reality NOTE: \u6838\u5fc3\u601d\u60f3\u662f: system models are abstractions Many algorithms have been designed to solve distributed systems problems\u2014for example, we will examine solutions for the consensus problem in Chapter 9. In order to be useful, these algorithms need to tolerate the various faults of distributed systems that we discussed in this chapter. Algorithms need to be written in a way that does not depend too heavily on the details of the hardware and software configuration on which they are run. This in turn requires that we somehow formalize the kinds of faults that we expect to happen in a system. We do this by defining a system model , which is an abstraction that describes what things an algorithm may assume. 6) CHAPTER 8 The Trouble with Distributed Systems#Knowledge, Truth, and Lies#System Model and Reality#Mapping system models to the real world NOTE: \u6838\u5fc3\u601d\u60f3\u662f: abstraction is simplification of reality. However, when implementing an algorithm in practice, the messy facts of reality come back to bite you again, and it becomes clear that the system model is a simplified abstraction of reality. 7) CHAPTER 9 Consistency and Consensus NOTE: \u8fd9\u6bb5\u6240\u603b\u7ed3\u7684\u662f\u6838\u5fc3\u601d\u60f3: \"The best way of building fault-tolerant systems is to find some general-purpose abstractions with useful guarantees, implement them once, and then let applications rely on those guarantees.\" The best way of building fault-tolerant systems is to find some general-purpose abstractions with useful guarantees, implement them once, and then let applications rely on those guarantees. This is the same approach as we used with transactions in Chapter 7: by using a transaction , the application can pretend that there are no crashes ( atomicity ), that nobody else is concurrently accessing the database ( isolation ), and that storage devices are perfectly reliable ( durability ). Even though crashes, race conditions, and disk failures do occur, the transaction abstraction hides those problems so that the application doesn\u2019t need to worry about them. We will now continue along the same lines, and seek abstractions that can allow an application to ignore some of the problems with distributed systems . For example, one of the most important abstractions for distributed systems is consensus : that is, getting all of the nodes to agree on something. But first we first need to explore the range of guarantees and abstractions that can be provided in a distributed system.","title":"Abstraction in distributed computing"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/","text":"CSDN \u609f\u7a7a\u804a\u67b6\u6784\u7684\u5206\u5e03\u5f0f\u4e13\u680f \u5728\u4f7f\u7528bing\u641c\u7d22\"NWR quorum consistency\"\u65f6\uff0c\u68c0\u7d22\u5230\u4e86\u8fd9\u7bc7\u6587\u7ae0: chowdera Distributed quorum NWR of taishanglaojun's Alchemy furnace Distributed series \uff1a 1. Talk about distributed algorithms in Three Kingdoms , Comfortable, right \uff1f 2. Use Taijiquan to teach distributed theory , It is really comfortable \uff01 3. Zhugeliang VS Pang Tong , Take down Paxos Consensus algorithm 4. Use dynamic diagrams to explain distributed Raft 5. Han Xin's big move \uff1a Consistent Hashing 6. Virus invasion \uff1a It's all distributed Gossip agreement 7. It's been a terrible three years , Expose ten pits 8. The distribution of taishanglaojun's Alchemy furnace Quorum NWR \u770b\u4e86\u4e00\u4e9b\u4e0a\u9762\u7684\u8fd9\u4e9b\u6587\u7ae0\uff0c\u90fd\u662fCSDN\u4e0a\u7684\uff0c\u5bf9\u5e94\u7684\u662f 1.\u7528\u4e09\u56fd\u6740\u8bb2\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u8212\u9002\u4e86\u5427\uff1f 2.\u7528\u592a\u6781\u62f3\u8bb2\u5206\u5e03\u5f0f\u7406\u8bba\uff0c\u771f\u8212\u670d\uff01 3.\u8bf8\u845b\u4eae VS \u5e9e\u7edf\uff0c\u62ff\u4e0b Paxos \u5171\u8bc6\u7b97\u6cd5 4.\u7528\u52a8\u56fe\u8bb2\u89e3\u5206\u5e03\u5f0f Raft 5.\u97e9\u4fe1\u5927\u62db\uff1a\u4e00\u81f4\u6027\u54c8\u5e0c 6.\u75c5\u6bd2\u5165\u4fb5\uff1a\u5168\u9760\u5206\u5e03\u5f0f Gossip \u534f\u8bae 7.\u8fd9\u4e09\u5e74\u88ab\u5206\u5e03\u5f0f\u5751\u60e8\u4e86\uff0c\u66dd\u5149\u5341\u5927\u5751 8.\u592a\u4e0a\u8001\u541b\u7684\u70bc\u4e39\u7089\u4e4b\u5206\u5e03\u5f0f Quorum NWR \u770b\u4e86\u4e00\u4e0b\u5176\u4e2d\u7684\u5185\u5bb9\uff0c\u4f5c\u8005\u5199\u5f97\u8fd8\u4e0d\u9519\uff0c\u6bd4\u8f83\u5bb9\u6613\u7406\u89e3\uff0c\u53ef\u4ee5\u4f5c\u4e3aguide\u3002","title":"Introduction"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/#csdn","text":"\u5728\u4f7f\u7528bing\u641c\u7d22\"NWR quorum consistency\"\u65f6\uff0c\u68c0\u7d22\u5230\u4e86\u8fd9\u7bc7\u6587\u7ae0: chowdera Distributed quorum NWR of taishanglaojun's Alchemy furnace Distributed series \uff1a 1. Talk about distributed algorithms in Three Kingdoms , Comfortable, right \uff1f 2. Use Taijiquan to teach distributed theory , It is really comfortable \uff01 3. Zhugeliang VS Pang Tong , Take down Paxos Consensus algorithm 4. Use dynamic diagrams to explain distributed Raft 5. Han Xin's big move \uff1a Consistent Hashing 6. Virus invasion \uff1a It's all distributed Gossip agreement 7. It's been a terrible three years , Expose ten pits 8. The distribution of taishanglaojun's Alchemy furnace Quorum NWR \u770b\u4e86\u4e00\u4e9b\u4e0a\u9762\u7684\u8fd9\u4e9b\u6587\u7ae0\uff0c\u90fd\u662fCSDN\u4e0a\u7684\uff0c\u5bf9\u5e94\u7684\u662f 1.\u7528\u4e09\u56fd\u6740\u8bb2\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u8212\u9002\u4e86\u5427\uff1f 2.\u7528\u592a\u6781\u62f3\u8bb2\u5206\u5e03\u5f0f\u7406\u8bba\uff0c\u771f\u8212\u670d\uff01 3.\u8bf8\u845b\u4eae VS \u5e9e\u7edf\uff0c\u62ff\u4e0b Paxos \u5171\u8bc6\u7b97\u6cd5 4.\u7528\u52a8\u56fe\u8bb2\u89e3\u5206\u5e03\u5f0f Raft 5.\u97e9\u4fe1\u5927\u62db\uff1a\u4e00\u81f4\u6027\u54c8\u5e0c 6.\u75c5\u6bd2\u5165\u4fb5\uff1a\u5168\u9760\u5206\u5e03\u5f0f Gossip \u534f\u8bae 7.\u8fd9\u4e09\u5e74\u88ab\u5206\u5e03\u5f0f\u5751\u60e8\u4e86\uff0c\u66dd\u5149\u5341\u5927\u5751 8.\u592a\u4e0a\u8001\u541b\u7684\u70bc\u4e39\u7089\u4e4b\u5206\u5e03\u5f0f Quorum NWR \u770b\u4e86\u4e00\u4e0b\u5176\u4e2d\u7684\u5185\u5bb9\uff0c\u4f5c\u8005\u5199\u5f97\u8fd8\u4e0d\u9519\uff0c\u6bd4\u8f83\u5bb9\u6613\u7406\u89e3\uff0c\u53ef\u4ee5\u4f5c\u4e3aguide\u3002","title":"CSDN \u609f\u7a7a\u804a\u67b6\u6784\u7684\u5206\u5e03\u5f0f\u4e13\u680f"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/01-%E7%94%A8%E4%B8%89%E5%9B%BD%E6%9D%80%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/","text":"csdn \u7528\u4e09\u56fd\u6740\u8bb2\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u8212\u9002\u4e86\u5427\uff1f NOTE: \u4e00\u3001\u8fd9\u7bc7\u6587\u7ae0\u5bf9\"\u62dc\u5360\u5ead\u5c06\u519b\u95ee\u9898\"\u7684\u8bb2\u89e3\u662f\u975e\u5e38\u597d\u7684 \u4e8c\u3001\u4f5c\u8005\u9996\u5148\u7ed9\u51fa\u7684\u5b66\u4e60\u8def\u7ebf\uff0c\u57fa\u672c\u4e0a\u6db5\u76d6\u5206\u5e03\u5f0f\u7684\u5927\u4f53\u5185\u5bb9 \u5b66\u4e60\u8def\u7ebf \u5b66\u4e60**\u5206\u5e03\u5f0f\u534f\u8bae**\u548c**\u7b97\u6cd5**\u7684\u8def\u7ebf\u53ef\u4ee5\u662f\u5148\u5b66\u4e60\u56db\u5927\u57fa\u7840\u7406\u8bba\uff0c\u4f5c\u4e3a\u5730\u57fa\uff0c\u518d\u5b66\u4e60\u5206\u5e03\u5f0f\u534f\u8bae\u548c\u7b97\u6cd5\uff0c\u5c31\u50cf\u662f\u5728\u5730\u57fa\u4e0a\u5efa\u623f\u5b50\u3002\u5730\u57fa\u6253\u597d\u4e86\uff0c\u624d\u80fd\u5efa\u66f4\u7a33\u56fa\u7684\u9ad8\u697c\u5927\u53a6\u3002 \u56db\u5927\u57fa\u7840\u7406\u8bba 1\u3001\u62dc\u5360\u5ead\u5c06\u519b\u95ee\u9898 2\u3001CAP \u7406\u8bba 3\u3001ACID \u7406\u8bba 4\u3001BASE \u7406\u8bba \u516b\u5927\u5206\u5e03\u5f0f\u534f\u8bae\u548c\u7b97\u6cd5 1\u3001Paxos \u7b97\u6cd5 2\u3001Raft \u7b97\u6cd5 3\u3001\u4e00\u81f4\u6027 Hash \u7b97\u6cd5 4\u3001Gossip \u534f\u8bae\u7b97\u6cd5 5\u3001Quorum NWR \u7b97\u6cd5 6\u3001FBFT \u7b97\u6cd5 7\u3001POW \u7b97\u6cd5 8\u3001ZAB \u534f\u8bae \u56e0\u7bc7\u5e45\u539f\u56e0\uff0c\u672c\u7bc7\u53ea\u6d89\u53ca\u62dc\u5360\u5ead\u5c06\u519b\u95ee\u9898\u3002 \u62dc\u5360\u5ead\u5c06\u519b\u95ee\u9898 \u5927\u5bb6\u53ef\u80fd\u542c\u8fc7\u62dc\u5360\u5ead\u5c06\u519b\u95ee\u9898\u3002\u5b83\u662f\u7531\u83b1\u65af\u5229\u00b7\u5170\u4f2f\u7279\u63d0\u51fa\u7684\u70b9\u5bf9\u70b9\u901a\u4fe1\u4e2d\u7684\u57fa\u672c\u95ee\u9898\u3002 NOTE: \u662f\u7531 LESLIE LAMPORT \u63d0\u51fa\u7684\uff0c\u4ed6\u5df2\u7ecf\u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u4e86\u975e\u5e38\u8be6\u7ec6\u7684\u5206\u6790\u4e86 \u62dc\u5360\u5ead\u4f4d\u4e8e\u5982\u4eca\u7684\u571f\u8033\u5176\u7684\u4f0a\u65af\u5766\u5e03\u5c14\uff0c\u662f\u4e1c\u7f57\u9a6c\u5e1d\u56fd\u7684\u9996\u90fd\u3002\u7531\u4e8e\u5f53\u65f6\u62dc\u5360\u5ead\u7f57\u9a6c\u5e1d\u56fd\u56fd\u571f\u8fbd\u9614\uff0c\u4e3a\u4e86\u8fbe\u5230\u9632\u5fa1\u76ee\u7684\uff0c\u6bcf\u4e2a\u519b\u961f\u90fd\u5206\u9694\u5f88\u8fdc\uff0c\u5c06\u519b\u4e0e\u5c06\u519b\u4e4b\u95f4\u53ea\u80fd\u9760\u4fe1\u5dee\u4f20\u6d88\u606f\u3002\u5728\u6218\u4e89\u7684\u65f6\u5019\uff0c\u62dc\u5360\u5ead\u519b\u961f\u5185\u6240\u6709\u5c06\u519b\u548c\u526f\u5b98\u5fc5\u987b\u8fbe\u6210\u4e00\u81f4\u7684**\u5171\u8bc6**\uff0c\u51b3\u5b9a\u662f\u5426\u6709\u8d62\u7684\u673a\u4f1a\u624d\u53bb\u653b\u6253\u654c\u4eba\u7684\u9635\u8425\u3002\u4f46\u662f\uff0c\u5728\u519b\u961f\u5185\u6709\u53ef\u80fd\u5b58\u6709\u53db\u5f92\u548c\u654c\u519b\u7684\u95f4\u8c0d\uff0c\u8fd9\u4e2a\u5c31\u662f\u62dc\u5360\u5ead\u5bb9\u9519\u95ee\u9898\u3002 \u5b9e\u9645\u4e0a\u62dc\u5360\u5ead\u95ee\u9898\u662f\u5206\u5e03\u5f0f\u9886\u57df\u6700\u590d\u6742\u7684\u4e00\u4e2a\u5bb9\u9519\u6a21\u578b\uff0c\u4e00\u65e6\u7406\u89e3\u5b83\uff0c\u5c31\u80fd\u638c\u63e1\u5206\u5e03\u5f0f\u5171\u8bc6\u95ee\u9898\u7684\u89e3\u51b3\u601d\u8def\uff0c\u8fd8\u80fd\u5e2e\u52a9\u5927\u5bb6\u7406\u89e3\u5e38\u7528\u7684**\u5171\u8bc6\u7b97\u6cd5**\uff0c\u4e5f\u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u5728\u5de5\u4f5c\u4e2d\u9009\u62e9\u5408\u9002\u7684\u7b97\u6cd5\uff0c\u6216\u8005\u8bbe\u8ba1\u5408\u9002\u7684\u7b97\u6cd5\u3002 NOTE: \u4e00\u3001\u5171\u8bc6\u5373\"consensus\" \u4e8c\u3001BFT Byzantine-Fault-Tolerance \u4e3a\u4ec0\u4e48\u7b2c\u4e00\u4e2a\u57fa\u7840\u7406\u8bba\u662f\u62dc\u5360\u5ead\u5c06\u519b\u95ee\u9898\uff1f \u56e0\u4e3a\u5b83\u5f88\u597d\u5730\u62bd\u8c61\u51fa\u4e86\u5206\u5e03\u5f0f\u7cfb\u7edf\u9762\u4e34\u7684\u5171\u8bc6\u95ee\u9898\u3002 \u4e0a\u9762\u63d0\u5230\u7684 8 \u79cd\u5206\u5e03\u5f0f\u7b97\u6cd5\u4e2d\u6709 5 \u79cd\u8ddf\u62dc\u5360\u5ead\u95ee\u9898\u76f8\u5173\uff0c\u53ef\u4ee5\u8bf4\u5f04\u61c2\u62dc\u5360\u5ead\u95ee\u9898\u5bf9\u540e\u9762\u5b66\u4e60\u5176\u4ed6\u7b97\u6cd5\u5c31\u4f1a\u5bb9\u6613\u5f88\u591a\u3002 NOTE: \u4e00\u3001\"\u62dc\u5360\u5ead\u5c06\u519b\u95ee\u9898\"\u53ef\u4ee5\u7b97\u4f5c\u662f\u4e00\u79cd\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u6311\u6218 \u4e0b\u9762\u6211\u7528\u4e09\u56fd\u6740\u6e38\u620f\u4e2d\u7684\u8eab\u4efd\u724c\u6765\u8bb2\u89e3\u62dc\u5360\u5ead\u5c06\u519b\u95ee\u9898\u3002 \u4e09\u56fd\u6740\u8eab\u4efd\u724c \u4e09\u56fd\u6740\u4e2d\u4e3b\u8981\u6709\u56db\u79cd\u8eab\u4efd\uff1a\u4e3b\u516c\u3001\u5fe0\u81e3\u3001\u53cd\u8d3c\u3001\u5185\u5978\u3002\u6bcf\u4e2a\u6e38\u620f\u73a9\u5bb6\u90fd\u4f1a\u83b7\u5f97\u4e00\u4e2a\u8eab\u4efd\u724c\u3002\u4e3b\u516c\u53ea\u6709 1 \u4e2a\u3002\u5fe0\u81e3 \u6700\u591a 2 \u4e2a\uff0c\u53cd\u8d3c\u6700\u591a 4\u4e2a\uff0c\u5185\u5978\u6700\u591a\u4e00\u4e2a\u3002 \u8fd8\u539f\u62dc\u5360\u5ead\u95ee\u9898 NOTE: \u4e00\u3001\u975e\u5e38\u597d\u7684\u7c7b\u6bd4\uff0c\u5f88\u597d\u7684\u8fd8\u539f\u4e86\"Byzantine general problem\" \u4e8c\u3001\u4e09\u4e2a\u4eba\u90fd\u6295\u7968\uff0c\u5e76\u5c06\u6295\u7968\u7ed3\u679c\u901a\u8fc7\u7f51\u7edc\u544a\u8bc9\u5bf9\u65b9 \u4e1c\u6c49\u672b\u5e74\uff0c\u8881\u7ecd\u4f5c\u4e3a\u76df\u4e3b\uff0c\u6c47\u5408\u4e86\u5341\u516b\u8def\u8bf8\u4faf\u4e00\u8d77\u653b\u6253\u8463\u5353\u3002\u628a\u8463\u5353\u5b9a\u4e3a\u53cd\u8d3c\uff0c\u8881\u7ecd\u5b9a\u4e3a\u4e3b\u516c\uff0c\u53e6\u5916\u6709\u4e24\u4e2a\u5fe0\u8bda\u548c\u4e00\u4e2a\u5185\u5978\uff0c\u5c31\u9009\u8fd9\u4e09\u4e2a\u98ce\u4e91\u4eba\u7269\uff1a\u66f9\u64cd\uff0c\u5218\u5907\uff0c\u5b59\u575a\uff08\u5b59\u6743\u7684\u7238\u6bd4\uff09\uff0c\u5185\u5978\u626e\u6f14\u7684\u89d2\u8272\u662f\u5fe0\u81e3\uff0c\u4e3b\u516c\u548c\u4e24\u4e2a\u5fe0\u81e3\u4e0d\u77e5\u9053\u5185\u5978\u7684\u8eab\u4efd\uff0c\u90fd\u5f53\u4f5c\u5fe0\u81e3\u5bf9\u5f85\u4e86\u3002 ![\u6218\u5c40 3 vs 2] NOTE: 2\u6307\u7684\u662f\u4e00\u4e2a\u5185\u5978\u3001\u4e00\u4e2a\u53cd\u8d3c \u8981\u60f3\u5e72\u6389\u8463\u5353\uff0c\u8881\u7ecd\u5fc5\u987b\u7edf\u4e00\u5fe0\u81e3\u7684\u4f5c\u6218\u8ba1\u5212\uff0c\u4e09\u4f4d\u5fe0\u81e3\u8fd8\u4e0d\u77e5\u9053\u6709\u4ec0\u4e48\u5176\u4ed6\u82b1\u82b1\u80a0\u5b50\uff0c\u6709\u4e00\u4e2a\u8fd8\u662f\u5185\u5978\u3002\u5982\u679c\u5185\u5978\u6697\u901a\u53cd\u8d3c\u8463\u5353\uff0c\u7ed9\u5fe0\u81e3\u53d1\u9001\u8bef\u5bfc\u6027\u7684\u4f5c\u6218\u4fe1\u606f\uff0c\u8be5\u600e\u4e48\u529e\uff1f\u53e6\u5916\u5047\u5b9a\u8fd9\u51e0\u4e2a\u5fe0\u81e3\u90fd\u662f\u901a\u8fc7\u4e66\u4fe1\u4ea4\u6d41\u4f5c\u6218\u4fe1\u606f\uff0c\u5982\u679c\u4e66\u4fe1\u88ab\u62e6\u622a\u4e86\u6216\u4e66\u4fe1\u91cc\u9762\u7684\u4fe1\u606f\u88ab\u66ff\u6362\u4e86\u548b\u529e\uff1f\u8fd9\u4e9b\u573a\u666f\u90fd\u53ef\u80fd\u6270\u4e71\u4f5c\u6218\u8ba1\u5212\uff0c\u6700\u540e\u51fa\u73b0\u6709\u7684\u5fe0\u81e3\u5728\u8fdb\u653b\uff0c\u6709\u7684\u5fe0\u81e3\u64a4\u9000\u4e86\u3002\u90a3\u4e48\u53cd\u8d3c\u5c31\u53ef\u4ee5\u4e58\u6b64\u673a\u4f1a\u53d1\u8d77\u8fdb\u653b\uff0c\u9010\u4e00\u653b\u7834\u3002 \u8881\u7ecd\u672c\u6765\u5c31\u6ca1\u6709\u66f9\u64cd\u7684\u673a\u667a\uff0c\u90a3\u4ed6**\u5982\u4f55\u8ba9\u5fe0\u81e3\u4eec\u8fbe\u6210\u5171\u8bc6\uff0c\u5236\u5b9a\u7edf\u4e00\u7684\u4f5c\u6218\u8ba1\u5212\u5462\uff1f** \u4e0a\u9762\u7684\u6620\u5c04\u5173\u7cfb\u5c31\u662f\u4e00\u4e2a\u62dc\u5360\u5ead\u5c06\u519b\u95ee\u9898\u7684\u4e00\u4e2a\u7b80\u5316\u8868\u8ff0\uff0c\u8881\u7ecd\u73b0\u5728\u9762\u4e34\u7684\u5c31\u662f\u5178\u578b\u7684**\u5171\u8bc6\u95ee\u9898**\u3002\u4e5f\u5c31\u662f\u5728\u53ef\u80fd\u6709\u8bef\u5bfc\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u91c7\u7528\u5408\u9002\u7684\u901a\u8baf\u673a\u5236\uff0c\u8ba9\u591a\u4e2a\u5c06\u519b**\u8fbe\u6210\u5171\u8bc6**\uff0c\u5236\u5b9a\u4e00\u81f4\u6027\u7684\u4f5c\u6218\u8ba1\u5212\u3002 \u4e00\u65b9\u9009\u62e9\u64a4\u9000 NOTE: \u66f9\u64cd\u9009\u62e9\u64a4\u9000 \u5218\u5907\u3001\u66f9\u64cd\u3001\u5b59\u575a\u901a\u8fc7**\u4fe1\u4f7f**\u4f20\u9012\u8fdb\u653b\u6216\u64a4\u9000\u7684\u4fe1\u606f\uff0c\u7136\u540e\u8fdb\u884c\u534f\u5546\uff0c\u5230\u5e95\u662f\u8fdb\u653b\u8fd8\u662f\u64a4\u9000\u3002\u9075\u5faa\u5c11\u6570\u670d\u4ece\u591a\u6570\uff0c\u4e0d\u5141\u8bb8\u5f03\u6743\u3002 \u66f9\u64cd\u7591\u5fc3\u6bd4\u8f83\u91cd\uff0c\u4fa6\u67e5\u4e86\u53cd\u8d3c\u7684\u5730\u5f62\u540e\uff0c\u51b3\u5b9a\u64a4\u9000\u3002\u800c\u5218\u5907\u548c\u5b59\u575a\u51b3\u5b9a\u8fdb\u653b\u3002 1\u3001\u5218\u5907\u51b3\u5b9a**\u8fdb\u653b**\uff0c\u901a\u8fc7\u4fe1\u4f7f\u544a\u8bc9\u66f9\u64cd\u548c\u5b59\u575a**\u8fdb\u653b**\u3002 2\u3001\u66f9\u64cd\u51b3\u5b9a**\u64a4\u9000**\uff0c\u901a\u8fc7\u4fe1\u4f7f\u544a\u8bc9\u5218\u5907\u548c\u5b59\u575a**\u64a4\u9000**\u3002 3\u3001\u5b59\u575a\u51b3\u5b9a**\u8fdb\u653b**\uff0c\u901a\u8fc7\u4fe1\u4f7f\u544a\u8bc9\u66f9\u64cd\u548c\u5218\u5907**\u8fdb\u653b**\u3002 \u66f9\u64cd\u6536\u5230\u7684\u4fe1\u606f\uff1a\u8fdb\u653b 2 \u7968\uff0c\u81ea\u5df1\u7684\u4e00\u5f20\u64a4\u9000\u7968\uff0c\u7968\u6570\u4e00\u6bd4\uff0c\u8fdb\u653b\u7968\uff1a\u64a4\u9000\u7968 = 2 : 1\uff0c\u6309\u7167\u4e0a\u9762\u7684\u5c11\u6570\u670d\u4ece\u591a\u6570\u539f\u5219\u8fdb\u884c\u6295\u7968\u8868\u51b3\uff0c\u66f9\u64cd\u8fd8\u662f\u4f1a\u8fdb\u653b\u3002\u90a3\u4e48\u4e09\u65b9\u7684\u4f5c\u6218\u65b9\u6848\u90fd\u662f\u8fdb\u653b\uff0c\u6240\u4ee5\u662f\u4e00\u4e2a**\u4e00\u81f4\u6027**\u7684\u4f5c\u6218\u65b9\u6848\u3002\u6700\u540e\u6218\u80dc\u4e86\u8463\u5353\u3002 \u5185\u5978\u767b\u573a-\u64a4\u9000 \u56e0\u4e3a\u6211\u4eec\u524d\u671f\u7684\u8bbe\u5b9a\uff0c\u5b59\u575a\u4f5c\u4e3a\u5185\u5978\uff0c\u65e9\u5df2\u4e0e\u53cd\u8d3c\u8463\u5353\u79c1\u4e0b\u6c9f\u901a\u597d\u4e86\uff0c\u4e0d\u653b\u6253\u8463\u5353\u3002 1\u3001\u5218\u5907\u51b3\u5b9a**\u8fdb\u653b**\uff0c\u901a\u8fc7\u4fe1\u4f7f\u544a\u8bc9\u66f9\u64cd\u548c\u5b59\u575a**\u8fdb\u653b**\u3002 2\u3001\u66f9\u64cd\u51b3\u5b9a**\u64a4\u9000**\uff0c\u901a\u8fc7\u4fe1\u4f7f\u544a\u8bc9\u66f9\u64cd\u548c\u5b59\u575a**\u64a4\u9000**\u3002 3\u3001\u5b59\u575a\u51b3\u5b9a**\u64a4\u9000**\uff0c\u901a\u8fc7\u4fe1\u4f7f\u544a\u8bc9\u66f9\u64cd\u548c\u5218\u5907**\u64a4\u9000**\u3002 \u5218\u5907\u6536\u5230\u8fdb\u653b\u548c\u64a4\u9000\u5404\u4e00\u7968\uff0c\u800c\u81ea\u5df1\u53c8\u9009\u62e9\u64a4\u9000\uff0c\u6240\u4ee5\u5218\u5907\u5f97\u5230\u7684\u7968\u6570\u662f\uff1a\u8fdb\u653b : \u64a4\u9000 = 1 : 2\uff0c\u9075\u4ece\u5c11\u6570\u670d\u4ece\u591a\u6570\u7684\u539f\u5219\uff0c\u5218\u5907\u9009\u62e9\u6700\u540e\u9009\u62e9\u64a4\u9000\uff0c\u90a3\u4e48\u4e09\u65b9\u7684\u4f5c\u6218\u65b9\u6848\u90fd\u662f\u64a4\u9000\uff0c\u6240\u4ee5\u4e5f\u662f\u4e00\u4e2a**\u4e00\u81f4\u6027**\u7684\u4f5c\u6218\u65b9\u6848\u3002 \u5185\u5978\u4f7f\u8bc8-\u4e00\u8fdb\u4e00\u9000 NOTE: \u76ee\u7684\u662f\u8fbe\u4e0d\u6210\u5171\u8bc6 \u5185\u5978\u770b\u4e86\u4e0a\u8ff0\u8ba1\u5212\uff0c\u53d1\u73b0\u5fe0\u81e3\u90fd\u64a4\u9000\u4e86\uff0c\u5e76\u6ca1\u6709\u88ab\u6d88\u706d\uff0c\u5c31\u60f3\u901a\u8fc7\u4f7f\u8bc8\u7684\u65b9\u5f0f\u6765\u6d88\u706d\u5176\u4e2d\u4e00\u4e2a\u5fe0\u81e3\u3002 1\u3001\u5218\u5907\u51b3\u5b9a\u8fdb\u653b\uff0c\u901a\u8fc7\u4fe1\u4f7f\u544a\u8bc9\u66f9\u64cd\u548c\u5b59\u575a**\u8fdb\u653b**\u3002 2\u3001\u66f9\u64cd\u51b3\u5b9a\u64a4\u9000\uff0c\u901a\u8fc7\u4fe1\u4f7f\u544a\u8bc9\u66f9\u64cd\u548c\u5b59\u575a**\u64a4\u9000**\u3002 3\u3001\u5b59\u575a\u4f5c\u4e3a\u5185\u5978\u4f7f\u8bc8\uff0c\u901a\u8fc7\u4fe1\u4f7f\u544a\u8bc9\u5218\u5907**\u8fdb\u653b**\uff0c\u544a\u8bc9\u66f9\u64cd**\u64a4\u9000**\u3002 \u90a3\u4e48\u7ed3\u679c\u662f\u4ec0\u4e48\u5462\uff1f \u5218\u5907\u7684\u7968\u6570\u4e3a\u8fdb\u653b 2 \u7968\uff0c\u64a4\u9000 1 \u7968\uff0c\u66f9\u64cd\u7684\u7968\u6570\u4e3a\u8fdb\u653b 1 \u7968\uff0c\u64a4\u9000 2 \u7968\u3002\u6309\u7167\u5c11\u6570\u670d\u4ece\u591a\u6570\u7684\u539f\u5219\uff0c\u5218\u5907\u6700\u540e\u4f1a\u9009\u62e9\u8fdb\u653b\uff0c\u800c\u66f9\u64cd\u4f1a\u9009\u62e9\u64a4\u9000\uff0c\u5b59\u575a\u4f5c\u4e3a\u5185\u5978\u80af\u5b9a\u4e0d\u4f1a\u8fdb\u653b\uff0c\u5218\u5907\u5355\u72ec\u8fdb\u653b\u53cd\u8d3c\u8463\u5353\uff0c\u52bf\u5355\u529b\u8584\uff0c\u88ab\u8463\u5353\u5e72\u6389\u4e86\u3002 \u4ece\u8fd9\u4e2a\u573a\u666f\u4e2d\uff0c\u6211\u4eec\u770b\u5230\u5185\u5978\u5b59\u575a\u901a\u8fc7\u53d1\u9001\u8bef\u5bfc\u4fe1\u606f\uff0c\u975e\u5e38\u5bb9\u6613\u5730\u5c31\u5e72\u6270\u4e86\u5218\u5907\u548c\u66f9\u64cd\u7684\u4f5c\u6218\u8ba1\u5212\uff0c\u5bfc\u81f4\u4e24\u4f4d\u5fe0\u81e3\u88ab\u9010\u4e00\u51fb\u7834\u3002\u8fd9\u4e2a\u73b0\u8c61\u5c31\u662f**\u4e8c\u5fe0\u4e00\u5224**\u96be\u9898\u3002\u90a3\u4e48\u4e3b\u516c\u8881\u7ecd\u8be5\u600e\u4e48\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff1f NOTE: \" \u4e8c\u5fe0\u4e00\u5224 \"\u5199\u9519\u8bef\u4e86\uff0c\u5e94\u8be5\u662f \"\u4e8c\u5fe0\u4e00\u53db\" \u62dc\u5360\u5ead\u95ee\u9898\u89e3\u6cd5 \u89e3\u6cd5\u539f\u7406 \u5c31\u662f\u8bb2\u8881\u7ecd\u4e5f\u53c2\u4e0e\u8fdb\u6765\u8fdb\u884c\u6295\u7968\uff0c\u8fd9\u6837\u5c31\u589e\u52a0\u4e86\u4e00\u4f4d\u5fe0\u81e3\u7684\u6570\u91cf\u3002\u4e09\u4e2a\u5fe0\u81e3\u4e00\u4e2a\u53db\u8d3c\u3002\u7136\u540e 4 \u4f4d\u5c06\u519b\u505a\u4e86\u4e00\u4e2a\u7ea6\u5b9a\uff0c\u5982\u679c\u6ca1\u6709\u6536\u5230\u547d\u4ee4\uff0c\u5219\u6267\u884c\u9ed8\u8ba4\u547d\u4ee4\uff0c\u6bd4\u5982\u64a4\u9000\u3002\u53e6\u5916\u7ea6\u5b9a\u6d41\u7a0b\u6765\u53d1\u9001\u4f5c\u6218\u4fe1\u606f\u548c\u5982\u4f55\u6267\u884c\u4f5c\u6218\u6307\u4ee4\u3002\u8fd9\u4e2a\u89e3\u6cd5\u7684\u5173\u952e\u70b9\u5c31\u662f\u6267\u884c**\u4e24\u8f6e**\u4f5c\u6218\u4fe1\u606f\u534f\u5546\u3002 NOTE: \"\u6267\u884c**\u4e24\u8f6e**\u4f5c\u6218\u4fe1\u606f\u534f\u5546\" \u5176\u5b9e\u5c31\u662f\u6267\u884c\u4e24\u8f6e\u6295\u7968 \u6211\u4eec\u6765\u770b\u4e0b\u7b2c\u4e00\u8f6e\u662f\u600e\u4e48\u505a\u7684\u3002 \u7b2c\u4e00\u6b65\uff1a\u5148\u53d1\u9001\u4f5c\u6218\u4fe1\u606f\u7684\u5c06\u519b\u6211\u4eec\u628a\u4ed6\u79f0\u4e3a**\u6307\u6325\u5b98**\uff08\u8881\u7ecd\uff09\uff0c\u53e6\u5916\u7684\u5c06\u519b\u6211\u4eec\u79f0\u4f5c**\u526f\u5b98**\uff08\u5218\u5907\uff0c\u66f9\u64cd\uff0c\u5b59\u575a\uff09\u3002 \u7b2c\u4e8c\u6b65\uff1a\u6307\u6325\u5b98\u5c06\u4ed6\u7684\u4f5c\u6218\u4fe1\u606f\u53d1\u9001\u7ed9\u6240\u6709\u7684**\u526f\u5b98**\u3002 \u7b2c\u4e09\u6b65\uff1a\u6bcf\u4e00\u4f4d**\u526f\u5b98**\u5c06\u4ece**\u6307\u6325\u5b98**\u5904\u6536\u5230\u7684\u4f5c\u6218\u4fe1\u606f\uff0c\u4f5c\u4e3a\u81ea\u5df1\u7684\u4f5c\u6218\u6307\u4ee4\uff1b\u5047\u5982\u6ca1\u6709\u6536\u5230**\u6307\u6325\u5b98**\u7684\u4f5c\u6218\u4fe1\u606f\uff0c\u5c06\u628a\u9ed8\u8ba4\u7684\u64a4\u9000\u4f5c\u4e3a\u4f5c\u6218\u6307\u4ee4 \u6211\u4eec\u7528\u56fe\u6765\u6f14\u793a\uff1a \u8881\u7ecd**\u4f5c\u4e3a\u4e3b\u516c\u5148\u53d1\u9001\u4f5c\u6218\u4fe1\u606f\uff0c\u4f5c\u6218\u6307\u4ee4\u4e3a**\u8fdb\u653b \u3002\u7136\u540e\u66f9\u64cd\u3001\u5218\u5907\u3001\u5b59\u575a\u6536\u5230**\u8fdb\u653b**\u7684\u4f5c\u6218\u6307\u4ee4\u3002 \u518d\u6765\u770b\u4e0b**\u7b2c\u4e8c\u8f6e**\u662f\u600e\u4e48\u505a\u7684\u3002 1\u3001\u7b2c\u4e00\u8f6e**\u6307\u6325\u5b98**\uff08\u8881\u7ecd\uff09\u5df2\u7ecf\u53d1\u9001\u6307\u4ee4\u4e86\uff0c\u73b0\u5728\u5c31\u9700\u8981\u5218\u5907\u3001\u66f9\u64cd\u3001\u5b59\u575a\u4f9d\u6b21\u4f5c\u4e3a**\u6307\u6325\u5b98**\u7ed9\u5176\u4ed6\u4e24\u4f4d**\u526f\u5c06**\u53d1\u9001\u4f5c\u6218\u4fe1\u606f\u3002 2\u3001\u7136\u540e\u8fd9\u4e09\u4f4d\u526f\u5c06\u6309\u7167\u5c11\u6570\u670d\u4ece\u591a\u6570\u7684\u539f\u5219\uff0c\u6267\u884c\u6536\u5230\u7684\u4f5c\u6218\u6307\u4ee4\u3002 \u5b59\u575a\u4f7f\u8bc8 - \u4e24\u64a4\u9000 \u5982\u679c\u5b59\u575a\u4f7f\u8bc8\uff0c\u6bd4\u5982\u7ed9\u66f9\u64cd\u548c\u5218\u5907\u90fd\u53d1\u9001\u64a4\u9000\u4fe1\u606f\uff0c\u5982\u4e0b\u56fe\u6240\u793a\u3002\u90a3\u4e48\u5218\u5907\u548c\u66f9\u64cd\u6536\u5230\u7684\u4f5c\u6218\u4fe1\u606f\u4e3a \u8fdb\u653b 2\u7968\uff0c\u64a4\u9000 1 \u7968\uff0c\u6309\u7167\u5c11\u6570\u670d\u4ece\u591a\u6570\u7684\u539f\u5219\uff0c\u6700\u540e\u5218\u5907\u548c\u66f9\u64cd\u6267\u884c\u8fdb\u653b\uff0c\u5b9e\u73b0\u4e86\u4f5c\u6218\u8ba1\u5212\u7684\u4e00\u81f4\u6027\uff0c\u66f9\u64cd\u548c\u5218\u5907\u8054\u5408\u4f5c\u6218\u51fb\u8d25\u4e86\u53cd\u8d3c\u8463\u5353\uff08\u5373\u4f7f\u5b59\u575a\u6ca1\u6709\u53c2\u52a0\u4f5c\u6218\u3002\uff09 \u5b59\u575a\u4f7f\u8bc8 - \u4e00\u8fdb\u4e00\u9000 \u5047\u5982\u5b59\u575a\u4f7f\u8bc8\uff0c\u7ed9\u66f9\u64cd\u53d1\u9001\u64a4\u9000\u6307\u4ee4\uff0c\u7ed9\u5218\u5907\u53d1\u9001\u8fdb\u653b\u6307\u4ee4\uff0c\u90a3\u4e48\u5218\u5907\u6536\u5230\u7684\u4f5c\u6218\u4fe1\u606f\u662f\u8fdb\u653b 3\u7968\uff0c\u80af\u5b9a\u4f1a\u53d1\u8d77\u8fdb\u653b\u4e86\uff0c\u800c\u66f9\u64cd\u6536\u5230\u7684\u4f5c\u6218\u4fe1\u606f\u662f\u8fdb\u653b 2 \u7968\uff0c\u64a4\u9000 1 \u7968\uff0c\u6700\u540e\u66f9\u64cd\u8fd8\u662f\u4f1a\u8fdb\u653b\uff0c\u6240\u4ee5\u5218\u5907\u548c\u66f9\u64cd\u8fd8\u662f\u8054\u5408\u4f5c\u6218\u51fb\u8d25\u4e86\u53cd\u8d3c\u8463\u5353\u3002 \u5982\u6b64\u770b\u6765\uff0c\u5f15\u5165\u4e86\u4e00\u4f4d\u6307\u6325\u5b98\u540e\uff0c\u786e\u5b9e\u53ef\u4ee5\u907f\u514d\u5b59\u575a\u4f7f\u8bc8\uff0c\u4f46\u5982\u679c\u662f\u5b59\u575a\u5728\u7b2c\u4e00\u8f6e\u4f5c\u4e3a\u6307\u6325\u5b98\uff0c\u5176\u4ed6\u4eba\u4f5c\u4e3a\u526f\u5b98\u5462\uff1f \u5b59\u575a\u4f5c\u4e3a\u6307\u6325\u5b98 \u7b2c\u4e00\u8f6e\u5b59\u575a\u5411\u5176\u4e2d\u4e00\u4e2a\u526f\u5b98\u8881\u7ecd\u53d1\u9001**\u64a4\u9000**\u6307\u4ee4\uff0c\u5411\u53e6\u5916\u4e24\u4e2a\u526f\u5b98\u66f9\u64cd\u3001\u5218\u5907\u53d1\u9001**\u8fdb\u653b**\u6307\u4ee4\u3002\u90a3\u4e48\u7b2c\u4e00\u8f6e\u7684\u7ed3\u679c\u5982\u4e0b\u56fe\uff1a \u7b2c\u4e8c\u8f6e\u5b59\u575a\u4f11\u606f\uff0c\u5176\u4ed6\u526f\u5b98\u6309\u7167\u5b59\u575a\u53d1\u9001\u7684\u6307\u4ee4\u5f00\u59cb\u5411\u53e6\u5916\u7684\u526f\u5b98\u53d1\u9001\u6307\u4ee4\u3002 \u66f9\u64cd\u5411\u5218\u5907\u548c\u8881\u7ecd\u53d1\u9001**\u8fdb\u653b**\u6307\u4ee4\u3002 \u5218\u5907\u5411\u66f9\u64cd\u548c\u8881\u7ecd\u53d1\u9001**\u8fdb\u653b**\u6307\u4ee4\u3002 \u8881\u7ecd\u5411\u66f9\u64cd\u548c\u5218\u5907\u53d1\u9001**\u64a4\u9000**\u6307\u4ee4\u3002 \u5982\u4e0b\u56fe\u6240\u793a\uff0c\u6700\u540e\u66f9\u64cd\u3001\u5218\u5907\u3001\u8881\u7ecd\u6536\u5230\u7684\u6307\u4ee4\u4e3a\u8fdb\u653b 2 \u7968\uff0c\u64a4\u9000 1 \u7968\uff0c\u6309\u7167\u5c11\u6570\u670d\u4ece\u591a\u6570\u539f\u5219\uff0c\u4e09\u4e2a\u4eba\u90fd\u662f\u53d1\u8d77\u8fdb\u653b\u3002\u6267\u884c\u4e86\u4e00\u81f4\u7684\u4f5c\u6218\u8ba1\u5212\uff0c\u4fdd\u8bc1\u4f5c\u6218\u7684\u80dc\u5229\u3002 \u5c0f\u7ed3 \u901a\u8fc7\u4e0a\u9762\u7684\u6f14\u793a\uff0c\u6211\u4eec\u77e5\u9053\u4e86\u5982\u4f55\u89e3\u51b3\u62dc\u5360\u5ead\u5c06\u519b\u95ee\u9898\u3002\u5176\u5b9e\u5170\u4f2f\u7279\u5728\u4ed6\u7684\u8bba\u6587\u4e2d\u4e5f\u63d0\u5230\u8fc7\u5982\u4f55\u89e3\u51b3\u3002 \u5982\u679c\u53db\u5c06\u4eba\u6570\u4e3a m\uff0c\u5c06\u519b\u6570 n >= 3m + 1\uff0c\u90a3\u4e48\u5c31\u53ef\u4ee5\u89e3\u51b3\u62dc\u5360\u5ead\u5c06\u519b\u95ee\u9898\u3002 \u524d\u63d0\u6761\u4ef6\uff1a\u53db\u5c06\u6570 m \u4e00\u81f4\uff0c\u9700\u8981\u8fdb\u884c m + 1 \u8f6e\u7684\u4f5c\u6218\u534f\u5546\u3002 \u8fd9\u4e2a\u516c\u5f0f\uff0c\u5927\u5bb6\u53ea\u9700\u8981\u8bb0\u4f4f\u5c31\u53ef\u4ee5\u4e86\uff0c\u63a8\u5230\u8fc7\u7a0b\u53ef\u4ee5\u53c2\u8003\u8bba\u6587\u3002 \u6bd4\u5982\u4e0a\u8ff0\u7684\u653b\u6253\u8463\u5353\u95ee\u9898\uff0c\u66f9\u64cd\u3001\u5218\u5907\u3001\u5b59\u575a\u4e09\u4e2a\u4eba\u5f53\u4e2d\uff0c\u5b59\u575a\u662f\u53db\u5c06\uff0c\u5b83\u53ef\u4ee5\u4f7f\u8bc8\uff0c\u4f7f\u4f5c\u6218\u8ba1\u5212\u4e0d\u7edf\u4e00\u3002\u5fc5\u987b\u589e\u52a0\u4e00\u4f4d\u5fe0\u81e3\u8881\u7ecd\u6765\u534f\u5546\u5171\u8bc6\uff0c\u624d\u80fd\u8fbe\u6210\u4e00\u81f4\u6027\u4f5c\u6218\u8ba1\u5212\u3002 \u62dc\u5360\u5ead\u89e3\u6cd5\u4e8c-\u7b7e\u540d \u90a3\u53ef\u4ee5\u5728\u4e0d\u589e\u52a0\u5fe0\u81e3\u7684\u60c5\u51b5\u4e0b\uff0c\u89e3\u51b3\u62dc\u5360\u5ead\u7684\u4e8c\u5fe0\u4e00\u5224\u95ee\u9898\u5462\uff1f \u89e3\u6cd5\u4e8c\u5c31\u662f\u901a\u8fc7\u7b7e\u540d\u6d88\u606f\u3002\u6bd4\u5982\u5c06\u519b\u4e4b\u95f4\u901a\u8fc7\u5370\u7ae0\u3001\u864e\u7b26\u7b49\u4fe1\u7269\u8fdb\u884c\u901a\u4fe1\u3002\u6765\u4fdd\u8bc1\u8fd9\u51e0\u4e2a\u7279\u5f81\uff1a \u7b7e\u540d\u65e0\u6cd5\u4f2a\u9020\uff0c\u5bf9\u7b7e\u540d\u6d88\u606f\u7684\u5185\u5bb9\u8fdb\u884c\u4efb\u4f55\u66f4\u6539\u90fd\u4f1a\u88ab\u53d1\u73b0\u3002 \u4efb\u4f55\u4eba\u90fd\u80fd\u9a8c\u8bc1\u5c06\u519b\u7b7e\u540d\u7684\u771f\u4f2a\u3002 \u603b\u7ed3 \u901a\u8fc7\u4e09\u56fd\u6740\u89d2\u8272\u6765\u8bb2\u89e3\u5206\u5e03\u5f0f\u4e2d\u5171\u8bc6\u573a\u666f\u3002\u90a3\u4ed6\u4eec\u548c\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u6620\u5c04\u5173\u7cfb\u662f\u600e\u4e48\u6837\u7684\u5462\uff1f \u5c06\u519b\u5bf9\u5e94\u8ba1\u7b97\u673a\u8282\u70b9\u3002 \u5fe0\u81e3\u7684\u5c06\u519b\u5bf9\u5e94\u6b63\u5e38\u8fd0\u884c\u7684\u8ba1\u7b97\u673a\u8282\u70b9\u3002 \u53db\u53d8\u7684\u5c06\u519b\u5bf9\u5e94\u51fa\u73b0\u6545\u969c\u5e76\u4f1a\u53d1\u9001\u8bef\u5bfc\u4fe1\u606f\u7684\u8ba1\u7b97\u673a\u8282\u70b9\u3002 \u4fe1\u4f7f\u88ab\u6740\u5bf9\u5e94\u901a\u8baf\u6545\u969c\u3001\u4fe1\u606f\u4e22\u5931\u3002 \u4fe1\u4f7f\u88ab\u95f4\u8c0d\u66ff\u6362\u5bf9\u5e94\u4e3a\u901a\u8baf\u88ab\u6076\u610f\u653b\u51fb\u3001\u4f2a\u9020\u4fe1\u606f\u6216\u52ab\u6301\u901a\u8baf\u3002 NOTE: \u975e\u5e38\u597d\u7684\u7c7b\u6bd4 \u62dc\u5360\u5ead\u5bb9\u9519\u7b97\u6cd5 \u53ef\u4e0d\u8981\u5c0f\u77a7\u62dc\u5360\u5ead\u95ee\u9898\uff0c\u5b83\u53ef\u662f\u5206\u5e03\u5f0f\u573a\u666f\u6700\u590d\u6742\u7684\u7684\u6545\u969c\u573a\u666f\u3002\u6bd4\u5982\u5728\u6570\u5b57\u8d27\u5e01\u7684\u533a\u5757\u94fe\u6280\u672f\u4e2d\u5c31\u6709\u7528\u5230\u8fd9\u4e9b\u77e5\u8bc6\u70b9\u3002\u800c\u4e14\u5fc5\u987b\u4f7f\u7528**\u62dc\u5360\u5ead\u5bb9\u9519**\u7b97\u6cd5\uff08\u4e5f\u5c31\u662f Byzantine Fault Tolerance\uff0c BFT \uff09\u3002 \u62dc\u5360\u5ead\u5bb9\u9519\u7b97\u6cd5\u8fd8\u6709 FBFT \u7b97\u6cd5\uff0c PoW \u7b97\u6cd5\uff0c\u5f53\u7136\u4e0d\u4f1a\u5728\u8fd9\u7bc7\u4e2d\u53bb\u8bb2\u8fd9\u4e9b\u7b97\u6cd5\uff0c\u540e\u7eed\u518d\u8bb2\u89e3\u3002 \u975e\u62dc\u5360\u5ead\u5bb9\u9519\u7b97\u6cd5 \u6709\u4e86**\u62dc\u5360\u5ead\u5bb9\u9519\u7b97\u6cd5**\uff0c\u80af\u5b9a\u6709**\u975e\u62dc\u5360\u5ead\u5bb9\u9519\u7b97\u6cd5**\uff0c\u987e\u540d\u601d\u4e49\uff0c\u5c31\u662f\u6ca1\u6709\u53d1\u9001\u8bef\u5bfc\u4fe1\u606f\u7684\u8282\u70b9\u3002CFT \u7b97\u6cd5\u5c31\u662f\u89e3\u51b3\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u5b58\u5728\u6545\u969c\uff0c\u4f46\u4e0d\u5b58\u5728\u6076\u610f\u8282\u70b9\u7684\u573a\u666f\u4e0b\u7684\u5171\u8bc6\u95ee\u9898\u3002\u7b80\u5355\u6765\u8bf4\u5c31\u662f\u53ef\u80fd\u56e0\u7cfb\u7edf\u6545\u969c\u9020\u6210\u4e22\u5931\u6d88\u606f\u6216\u6d88\u606f\u91cd\u590d\uff0c\u4f46\u4e0d\u5b58\u5728\u9519\u8bef\u6d88\u606f\u3001\u4f2a\u9020\u6d88\u606f\u3002\u5bf9\u5e94\u7684\u7b97\u6cd5\u6709 Paxos \u7b97\u6cd5\u3001Raft \u7b97\u6cd5\u3001ZAB \u534f\u8bae \u4e0a\u9762\u63d0\u5230\u4e86 5 \u79cd\u7b97\u6cd5\uff0c\u5c45\u7136\u90fd\u662f\u8ddf\u62dc\u5360\u5ead\u95ee\u9898\u6709\u5173\uff0c\u4f60\u8bf4\u4eca\u5929\u8bb2\u7684\u62dc\u5360\u5ead\u95ee\u9898\u91cd\u8981\u4e0d\u91cd\u8981\uff1f \u8fd9\u4e48\u591a\u7b97\u6cd5\u8be5\u5982\u4f55\u9009\u62e9\uff1f \u8282\u70b9\u53ef\u4fe1\uff0c\u9009\u975e\u62dc\u5360\u5ead\u5bb9\u9519\u7b97\u6cd5\u3002\u5426\u5219\u5c31\u7528\u62dc\u5360\u5ead\u5bb9\u9519\u7b97\u6cd5\uff0c\u5982\u533a\u5757\u94fe\u4e2d\u7528\u5230\u7684 PoW \u7b97\u6cd5\u3002","title":"Introduction"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/01-%E7%94%A8%E4%B8%89%E5%9B%BD%E6%9D%80%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/#csdn","text":"NOTE: \u4e00\u3001\u8fd9\u7bc7\u6587\u7ae0\u5bf9\"\u62dc\u5360\u5ead\u5c06\u519b\u95ee\u9898\"\u7684\u8bb2\u89e3\u662f\u975e\u5e38\u597d\u7684 \u4e8c\u3001\u4f5c\u8005\u9996\u5148\u7ed9\u51fa\u7684\u5b66\u4e60\u8def\u7ebf\uff0c\u57fa\u672c\u4e0a\u6db5\u76d6\u5206\u5e03\u5f0f\u7684\u5927\u4f53\u5185\u5bb9","title":"csdn \u7528\u4e09\u56fd\u6740\u8bb2\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u8212\u9002\u4e86\u5427\uff1f"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/01-%E7%94%A8%E4%B8%89%E5%9B%BD%E6%9D%80%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/#_1","text":"\u5b66\u4e60**\u5206\u5e03\u5f0f\u534f\u8bae**\u548c**\u7b97\u6cd5**\u7684\u8def\u7ebf\u53ef\u4ee5\u662f\u5148\u5b66\u4e60\u56db\u5927\u57fa\u7840\u7406\u8bba\uff0c\u4f5c\u4e3a\u5730\u57fa\uff0c\u518d\u5b66\u4e60\u5206\u5e03\u5f0f\u534f\u8bae\u548c\u7b97\u6cd5\uff0c\u5c31\u50cf\u662f\u5728\u5730\u57fa\u4e0a\u5efa\u623f\u5b50\u3002\u5730\u57fa\u6253\u597d\u4e86\uff0c\u624d\u80fd\u5efa\u66f4\u7a33\u56fa\u7684\u9ad8\u697c\u5927\u53a6\u3002","title":"\u5b66\u4e60\u8def\u7ebf"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/01-%E7%94%A8%E4%B8%89%E5%9B%BD%E6%9D%80%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/#_2","text":"1\u3001\u62dc\u5360\u5ead\u5c06\u519b\u95ee\u9898 2\u3001CAP \u7406\u8bba 3\u3001ACID \u7406\u8bba 4\u3001BASE \u7406\u8bba","title":"\u56db\u5927\u57fa\u7840\u7406\u8bba"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/01-%E7%94%A8%E4%B8%89%E5%9B%BD%E6%9D%80%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/#_3","text":"1\u3001Paxos \u7b97\u6cd5 2\u3001Raft \u7b97\u6cd5 3\u3001\u4e00\u81f4\u6027 Hash \u7b97\u6cd5 4\u3001Gossip \u534f\u8bae\u7b97\u6cd5 5\u3001Quorum NWR \u7b97\u6cd5 6\u3001FBFT \u7b97\u6cd5 7\u3001POW \u7b97\u6cd5 8\u3001ZAB \u534f\u8bae \u56e0\u7bc7\u5e45\u539f\u56e0\uff0c\u672c\u7bc7\u53ea\u6d89\u53ca\u62dc\u5360\u5ead\u5c06\u519b\u95ee\u9898\u3002","title":"\u516b\u5927\u5206\u5e03\u5f0f\u534f\u8bae\u548c\u7b97\u6cd5"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/01-%E7%94%A8%E4%B8%89%E5%9B%BD%E6%9D%80%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/#_4","text":"\u5927\u5bb6\u53ef\u80fd\u542c\u8fc7\u62dc\u5360\u5ead\u5c06\u519b\u95ee\u9898\u3002\u5b83\u662f\u7531\u83b1\u65af\u5229\u00b7\u5170\u4f2f\u7279\u63d0\u51fa\u7684\u70b9\u5bf9\u70b9\u901a\u4fe1\u4e2d\u7684\u57fa\u672c\u95ee\u9898\u3002 NOTE: \u662f\u7531 LESLIE LAMPORT \u63d0\u51fa\u7684\uff0c\u4ed6\u5df2\u7ecf\u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u4e86\u975e\u5e38\u8be6\u7ec6\u7684\u5206\u6790\u4e86 \u62dc\u5360\u5ead\u4f4d\u4e8e\u5982\u4eca\u7684\u571f\u8033\u5176\u7684\u4f0a\u65af\u5766\u5e03\u5c14\uff0c\u662f\u4e1c\u7f57\u9a6c\u5e1d\u56fd\u7684\u9996\u90fd\u3002\u7531\u4e8e\u5f53\u65f6\u62dc\u5360\u5ead\u7f57\u9a6c\u5e1d\u56fd\u56fd\u571f\u8fbd\u9614\uff0c\u4e3a\u4e86\u8fbe\u5230\u9632\u5fa1\u76ee\u7684\uff0c\u6bcf\u4e2a\u519b\u961f\u90fd\u5206\u9694\u5f88\u8fdc\uff0c\u5c06\u519b\u4e0e\u5c06\u519b\u4e4b\u95f4\u53ea\u80fd\u9760\u4fe1\u5dee\u4f20\u6d88\u606f\u3002\u5728\u6218\u4e89\u7684\u65f6\u5019\uff0c\u62dc\u5360\u5ead\u519b\u961f\u5185\u6240\u6709\u5c06\u519b\u548c\u526f\u5b98\u5fc5\u987b\u8fbe\u6210\u4e00\u81f4\u7684**\u5171\u8bc6**\uff0c\u51b3\u5b9a\u662f\u5426\u6709\u8d62\u7684\u673a\u4f1a\u624d\u53bb\u653b\u6253\u654c\u4eba\u7684\u9635\u8425\u3002\u4f46\u662f\uff0c\u5728\u519b\u961f\u5185\u6709\u53ef\u80fd\u5b58\u6709\u53db\u5f92\u548c\u654c\u519b\u7684\u95f4\u8c0d\uff0c\u8fd9\u4e2a\u5c31\u662f\u62dc\u5360\u5ead\u5bb9\u9519\u95ee\u9898\u3002 \u5b9e\u9645\u4e0a\u62dc\u5360\u5ead\u95ee\u9898\u662f\u5206\u5e03\u5f0f\u9886\u57df\u6700\u590d\u6742\u7684\u4e00\u4e2a\u5bb9\u9519\u6a21\u578b\uff0c\u4e00\u65e6\u7406\u89e3\u5b83\uff0c\u5c31\u80fd\u638c\u63e1\u5206\u5e03\u5f0f\u5171\u8bc6\u95ee\u9898\u7684\u89e3\u51b3\u601d\u8def\uff0c\u8fd8\u80fd\u5e2e\u52a9\u5927\u5bb6\u7406\u89e3\u5e38\u7528\u7684**\u5171\u8bc6\u7b97\u6cd5**\uff0c\u4e5f\u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u5728\u5de5\u4f5c\u4e2d\u9009\u62e9\u5408\u9002\u7684\u7b97\u6cd5\uff0c\u6216\u8005\u8bbe\u8ba1\u5408\u9002\u7684\u7b97\u6cd5\u3002 NOTE: \u4e00\u3001\u5171\u8bc6\u5373\"consensus\" \u4e8c\u3001BFT Byzantine-Fault-Tolerance \u4e3a\u4ec0\u4e48\u7b2c\u4e00\u4e2a\u57fa\u7840\u7406\u8bba\u662f\u62dc\u5360\u5ead\u5c06\u519b\u95ee\u9898\uff1f \u56e0\u4e3a\u5b83\u5f88\u597d\u5730\u62bd\u8c61\u51fa\u4e86\u5206\u5e03\u5f0f\u7cfb\u7edf\u9762\u4e34\u7684\u5171\u8bc6\u95ee\u9898\u3002 \u4e0a\u9762\u63d0\u5230\u7684 8 \u79cd\u5206\u5e03\u5f0f\u7b97\u6cd5\u4e2d\u6709 5 \u79cd\u8ddf\u62dc\u5360\u5ead\u95ee\u9898\u76f8\u5173\uff0c\u53ef\u4ee5\u8bf4\u5f04\u61c2\u62dc\u5360\u5ead\u95ee\u9898\u5bf9\u540e\u9762\u5b66\u4e60\u5176\u4ed6\u7b97\u6cd5\u5c31\u4f1a\u5bb9\u6613\u5f88\u591a\u3002 NOTE: \u4e00\u3001\"\u62dc\u5360\u5ead\u5c06\u519b\u95ee\u9898\"\u53ef\u4ee5\u7b97\u4f5c\u662f\u4e00\u79cd\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u6311\u6218 \u4e0b\u9762\u6211\u7528\u4e09\u56fd\u6740\u6e38\u620f\u4e2d\u7684\u8eab\u4efd\u724c\u6765\u8bb2\u89e3\u62dc\u5360\u5ead\u5c06\u519b\u95ee\u9898\u3002","title":"\u62dc\u5360\u5ead\u5c06\u519b\u95ee\u9898"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/01-%E7%94%A8%E4%B8%89%E5%9B%BD%E6%9D%80%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/#_5","text":"\u4e09\u56fd\u6740\u4e2d\u4e3b\u8981\u6709\u56db\u79cd\u8eab\u4efd\uff1a\u4e3b\u516c\u3001\u5fe0\u81e3\u3001\u53cd\u8d3c\u3001\u5185\u5978\u3002\u6bcf\u4e2a\u6e38\u620f\u73a9\u5bb6\u90fd\u4f1a\u83b7\u5f97\u4e00\u4e2a\u8eab\u4efd\u724c\u3002\u4e3b\u516c\u53ea\u6709 1 \u4e2a\u3002\u5fe0\u81e3 \u6700\u591a 2 \u4e2a\uff0c\u53cd\u8d3c\u6700\u591a 4\u4e2a\uff0c\u5185\u5978\u6700\u591a\u4e00\u4e2a\u3002","title":"\u4e09\u56fd\u6740\u8eab\u4efd\u724c"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/01-%E7%94%A8%E4%B8%89%E5%9B%BD%E6%9D%80%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/#_6","text":"NOTE: \u4e00\u3001\u975e\u5e38\u597d\u7684\u7c7b\u6bd4\uff0c\u5f88\u597d\u7684\u8fd8\u539f\u4e86\"Byzantine general problem\" \u4e8c\u3001\u4e09\u4e2a\u4eba\u90fd\u6295\u7968\uff0c\u5e76\u5c06\u6295\u7968\u7ed3\u679c\u901a\u8fc7\u7f51\u7edc\u544a\u8bc9\u5bf9\u65b9 \u4e1c\u6c49\u672b\u5e74\uff0c\u8881\u7ecd\u4f5c\u4e3a\u76df\u4e3b\uff0c\u6c47\u5408\u4e86\u5341\u516b\u8def\u8bf8\u4faf\u4e00\u8d77\u653b\u6253\u8463\u5353\u3002\u628a\u8463\u5353\u5b9a\u4e3a\u53cd\u8d3c\uff0c\u8881\u7ecd\u5b9a\u4e3a\u4e3b\u516c\uff0c\u53e6\u5916\u6709\u4e24\u4e2a\u5fe0\u8bda\u548c\u4e00\u4e2a\u5185\u5978\uff0c\u5c31\u9009\u8fd9\u4e09\u4e2a\u98ce\u4e91\u4eba\u7269\uff1a\u66f9\u64cd\uff0c\u5218\u5907\uff0c\u5b59\u575a\uff08\u5b59\u6743\u7684\u7238\u6bd4\uff09\uff0c\u5185\u5978\u626e\u6f14\u7684\u89d2\u8272\u662f\u5fe0\u81e3\uff0c\u4e3b\u516c\u548c\u4e24\u4e2a\u5fe0\u81e3\u4e0d\u77e5\u9053\u5185\u5978\u7684\u8eab\u4efd\uff0c\u90fd\u5f53\u4f5c\u5fe0\u81e3\u5bf9\u5f85\u4e86\u3002","title":"\u8fd8\u539f\u62dc\u5360\u5ead\u95ee\u9898"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/01-%E7%94%A8%E4%B8%89%E5%9B%BD%E6%9D%80%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/#3#vs#2","text":"NOTE: 2\u6307\u7684\u662f\u4e00\u4e2a\u5185\u5978\u3001\u4e00\u4e2a\u53cd\u8d3c \u8981\u60f3\u5e72\u6389\u8463\u5353\uff0c\u8881\u7ecd\u5fc5\u987b\u7edf\u4e00\u5fe0\u81e3\u7684\u4f5c\u6218\u8ba1\u5212\uff0c\u4e09\u4f4d\u5fe0\u81e3\u8fd8\u4e0d\u77e5\u9053\u6709\u4ec0\u4e48\u5176\u4ed6\u82b1\u82b1\u80a0\u5b50\uff0c\u6709\u4e00\u4e2a\u8fd8\u662f\u5185\u5978\u3002\u5982\u679c\u5185\u5978\u6697\u901a\u53cd\u8d3c\u8463\u5353\uff0c\u7ed9\u5fe0\u81e3\u53d1\u9001\u8bef\u5bfc\u6027\u7684\u4f5c\u6218\u4fe1\u606f\uff0c\u8be5\u600e\u4e48\u529e\uff1f\u53e6\u5916\u5047\u5b9a\u8fd9\u51e0\u4e2a\u5fe0\u81e3\u90fd\u662f\u901a\u8fc7\u4e66\u4fe1\u4ea4\u6d41\u4f5c\u6218\u4fe1\u606f\uff0c\u5982\u679c\u4e66\u4fe1\u88ab\u62e6\u622a\u4e86\u6216\u4e66\u4fe1\u91cc\u9762\u7684\u4fe1\u606f\u88ab\u66ff\u6362\u4e86\u548b\u529e\uff1f\u8fd9\u4e9b\u573a\u666f\u90fd\u53ef\u80fd\u6270\u4e71\u4f5c\u6218\u8ba1\u5212\uff0c\u6700\u540e\u51fa\u73b0\u6709\u7684\u5fe0\u81e3\u5728\u8fdb\u653b\uff0c\u6709\u7684\u5fe0\u81e3\u64a4\u9000\u4e86\u3002\u90a3\u4e48\u53cd\u8d3c\u5c31\u53ef\u4ee5\u4e58\u6b64\u673a\u4f1a\u53d1\u8d77\u8fdb\u653b\uff0c\u9010\u4e00\u653b\u7834\u3002 \u8881\u7ecd\u672c\u6765\u5c31\u6ca1\u6709\u66f9\u64cd\u7684\u673a\u667a\uff0c\u90a3\u4ed6**\u5982\u4f55\u8ba9\u5fe0\u81e3\u4eec\u8fbe\u6210\u5171\u8bc6\uff0c\u5236\u5b9a\u7edf\u4e00\u7684\u4f5c\u6218\u8ba1\u5212\u5462\uff1f** \u4e0a\u9762\u7684\u6620\u5c04\u5173\u7cfb\u5c31\u662f\u4e00\u4e2a\u62dc\u5360\u5ead\u5c06\u519b\u95ee\u9898\u7684\u4e00\u4e2a\u7b80\u5316\u8868\u8ff0\uff0c\u8881\u7ecd\u73b0\u5728\u9762\u4e34\u7684\u5c31\u662f\u5178\u578b\u7684**\u5171\u8bc6\u95ee\u9898**\u3002\u4e5f\u5c31\u662f\u5728\u53ef\u80fd\u6709\u8bef\u5bfc\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u91c7\u7528\u5408\u9002\u7684\u901a\u8baf\u673a\u5236\uff0c\u8ba9\u591a\u4e2a\u5c06\u519b**\u8fbe\u6210\u5171\u8bc6**\uff0c\u5236\u5b9a\u4e00\u81f4\u6027\u7684\u4f5c\u6218\u8ba1\u5212\u3002","title":"![\u6218\u5c40 3 vs 2]"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/01-%E7%94%A8%E4%B8%89%E5%9B%BD%E6%9D%80%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/#_7","text":"NOTE: \u66f9\u64cd\u9009\u62e9\u64a4\u9000 \u5218\u5907\u3001\u66f9\u64cd\u3001\u5b59\u575a\u901a\u8fc7**\u4fe1\u4f7f**\u4f20\u9012\u8fdb\u653b\u6216\u64a4\u9000\u7684\u4fe1\u606f\uff0c\u7136\u540e\u8fdb\u884c\u534f\u5546\uff0c\u5230\u5e95\u662f\u8fdb\u653b\u8fd8\u662f\u64a4\u9000\u3002\u9075\u5faa\u5c11\u6570\u670d\u4ece\u591a\u6570\uff0c\u4e0d\u5141\u8bb8\u5f03\u6743\u3002 \u66f9\u64cd\u7591\u5fc3\u6bd4\u8f83\u91cd\uff0c\u4fa6\u67e5\u4e86\u53cd\u8d3c\u7684\u5730\u5f62\u540e\uff0c\u51b3\u5b9a\u64a4\u9000\u3002\u800c\u5218\u5907\u548c\u5b59\u575a\u51b3\u5b9a\u8fdb\u653b\u3002 1\u3001\u5218\u5907\u51b3\u5b9a**\u8fdb\u653b**\uff0c\u901a\u8fc7\u4fe1\u4f7f\u544a\u8bc9\u66f9\u64cd\u548c\u5b59\u575a**\u8fdb\u653b**\u3002 2\u3001\u66f9\u64cd\u51b3\u5b9a**\u64a4\u9000**\uff0c\u901a\u8fc7\u4fe1\u4f7f\u544a\u8bc9\u5218\u5907\u548c\u5b59\u575a**\u64a4\u9000**\u3002 3\u3001\u5b59\u575a\u51b3\u5b9a**\u8fdb\u653b**\uff0c\u901a\u8fc7\u4fe1\u4f7f\u544a\u8bc9\u66f9\u64cd\u548c\u5218\u5907**\u8fdb\u653b**\u3002 \u66f9\u64cd\u6536\u5230\u7684\u4fe1\u606f\uff1a\u8fdb\u653b 2 \u7968\uff0c\u81ea\u5df1\u7684\u4e00\u5f20\u64a4\u9000\u7968\uff0c\u7968\u6570\u4e00\u6bd4\uff0c\u8fdb\u653b\u7968\uff1a\u64a4\u9000\u7968 = 2 : 1\uff0c\u6309\u7167\u4e0a\u9762\u7684\u5c11\u6570\u670d\u4ece\u591a\u6570\u539f\u5219\u8fdb\u884c\u6295\u7968\u8868\u51b3\uff0c\u66f9\u64cd\u8fd8\u662f\u4f1a\u8fdb\u653b\u3002\u90a3\u4e48\u4e09\u65b9\u7684\u4f5c\u6218\u65b9\u6848\u90fd\u662f\u8fdb\u653b\uff0c\u6240\u4ee5\u662f\u4e00\u4e2a**\u4e00\u81f4\u6027**\u7684\u4f5c\u6218\u65b9\u6848\u3002\u6700\u540e\u6218\u80dc\u4e86\u8463\u5353\u3002","title":"\u4e00\u65b9\u9009\u62e9\u64a4\u9000"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/01-%E7%94%A8%E4%B8%89%E5%9B%BD%E6%9D%80%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/#-","text":"\u56e0\u4e3a\u6211\u4eec\u524d\u671f\u7684\u8bbe\u5b9a\uff0c\u5b59\u575a\u4f5c\u4e3a\u5185\u5978\uff0c\u65e9\u5df2\u4e0e\u53cd\u8d3c\u8463\u5353\u79c1\u4e0b\u6c9f\u901a\u597d\u4e86\uff0c\u4e0d\u653b\u6253\u8463\u5353\u3002 1\u3001\u5218\u5907\u51b3\u5b9a**\u8fdb\u653b**\uff0c\u901a\u8fc7\u4fe1\u4f7f\u544a\u8bc9\u66f9\u64cd\u548c\u5b59\u575a**\u8fdb\u653b**\u3002 2\u3001\u66f9\u64cd\u51b3\u5b9a**\u64a4\u9000**\uff0c\u901a\u8fc7\u4fe1\u4f7f\u544a\u8bc9\u66f9\u64cd\u548c\u5b59\u575a**\u64a4\u9000**\u3002 3\u3001\u5b59\u575a\u51b3\u5b9a**\u64a4\u9000**\uff0c\u901a\u8fc7\u4fe1\u4f7f\u544a\u8bc9\u66f9\u64cd\u548c\u5218\u5907**\u64a4\u9000**\u3002 \u5218\u5907\u6536\u5230\u8fdb\u653b\u548c\u64a4\u9000\u5404\u4e00\u7968\uff0c\u800c\u81ea\u5df1\u53c8\u9009\u62e9\u64a4\u9000\uff0c\u6240\u4ee5\u5218\u5907\u5f97\u5230\u7684\u7968\u6570\u662f\uff1a\u8fdb\u653b : \u64a4\u9000 = 1 : 2\uff0c\u9075\u4ece\u5c11\u6570\u670d\u4ece\u591a\u6570\u7684\u539f\u5219\uff0c\u5218\u5907\u9009\u62e9\u6700\u540e\u9009\u62e9\u64a4\u9000\uff0c\u90a3\u4e48\u4e09\u65b9\u7684\u4f5c\u6218\u65b9\u6848\u90fd\u662f\u64a4\u9000\uff0c\u6240\u4ee5\u4e5f\u662f\u4e00\u4e2a**\u4e00\u81f4\u6027**\u7684\u4f5c\u6218\u65b9\u6848\u3002","title":"\u5185\u5978\u767b\u573a-\u64a4\u9000"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/01-%E7%94%A8%E4%B8%89%E5%9B%BD%E6%9D%80%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/#-_1","text":"NOTE: \u76ee\u7684\u662f\u8fbe\u4e0d\u6210\u5171\u8bc6 \u5185\u5978\u770b\u4e86\u4e0a\u8ff0\u8ba1\u5212\uff0c\u53d1\u73b0\u5fe0\u81e3\u90fd\u64a4\u9000\u4e86\uff0c\u5e76\u6ca1\u6709\u88ab\u6d88\u706d\uff0c\u5c31\u60f3\u901a\u8fc7\u4f7f\u8bc8\u7684\u65b9\u5f0f\u6765\u6d88\u706d\u5176\u4e2d\u4e00\u4e2a\u5fe0\u81e3\u3002 1\u3001\u5218\u5907\u51b3\u5b9a\u8fdb\u653b\uff0c\u901a\u8fc7\u4fe1\u4f7f\u544a\u8bc9\u66f9\u64cd\u548c\u5b59\u575a**\u8fdb\u653b**\u3002 2\u3001\u66f9\u64cd\u51b3\u5b9a\u64a4\u9000\uff0c\u901a\u8fc7\u4fe1\u4f7f\u544a\u8bc9\u66f9\u64cd\u548c\u5b59\u575a**\u64a4\u9000**\u3002 3\u3001\u5b59\u575a\u4f5c\u4e3a\u5185\u5978\u4f7f\u8bc8\uff0c\u901a\u8fc7\u4fe1\u4f7f\u544a\u8bc9\u5218\u5907**\u8fdb\u653b**\uff0c\u544a\u8bc9\u66f9\u64cd**\u64a4\u9000**\u3002 \u90a3\u4e48\u7ed3\u679c\u662f\u4ec0\u4e48\u5462\uff1f \u5218\u5907\u7684\u7968\u6570\u4e3a\u8fdb\u653b 2 \u7968\uff0c\u64a4\u9000 1 \u7968\uff0c\u66f9\u64cd\u7684\u7968\u6570\u4e3a\u8fdb\u653b 1 \u7968\uff0c\u64a4\u9000 2 \u7968\u3002\u6309\u7167\u5c11\u6570\u670d\u4ece\u591a\u6570\u7684\u539f\u5219\uff0c\u5218\u5907\u6700\u540e\u4f1a\u9009\u62e9\u8fdb\u653b\uff0c\u800c\u66f9\u64cd\u4f1a\u9009\u62e9\u64a4\u9000\uff0c\u5b59\u575a\u4f5c\u4e3a\u5185\u5978\u80af\u5b9a\u4e0d\u4f1a\u8fdb\u653b\uff0c\u5218\u5907\u5355\u72ec\u8fdb\u653b\u53cd\u8d3c\u8463\u5353\uff0c\u52bf\u5355\u529b\u8584\uff0c\u88ab\u8463\u5353\u5e72\u6389\u4e86\u3002 \u4ece\u8fd9\u4e2a\u573a\u666f\u4e2d\uff0c\u6211\u4eec\u770b\u5230\u5185\u5978\u5b59\u575a\u901a\u8fc7\u53d1\u9001\u8bef\u5bfc\u4fe1\u606f\uff0c\u975e\u5e38\u5bb9\u6613\u5730\u5c31\u5e72\u6270\u4e86\u5218\u5907\u548c\u66f9\u64cd\u7684\u4f5c\u6218\u8ba1\u5212\uff0c\u5bfc\u81f4\u4e24\u4f4d\u5fe0\u81e3\u88ab\u9010\u4e00\u51fb\u7834\u3002\u8fd9\u4e2a\u73b0\u8c61\u5c31\u662f**\u4e8c\u5fe0\u4e00\u5224**\u96be\u9898\u3002\u90a3\u4e48\u4e3b\u516c\u8881\u7ecd\u8be5\u600e\u4e48\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff1f NOTE: \" \u4e8c\u5fe0\u4e00\u5224 \"\u5199\u9519\u8bef\u4e86\uff0c\u5e94\u8be5\u662f \"\u4e8c\u5fe0\u4e00\u53db\"","title":"\u5185\u5978\u4f7f\u8bc8-\u4e00\u8fdb\u4e00\u9000"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/01-%E7%94%A8%E4%B8%89%E5%9B%BD%E6%9D%80%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/#_8","text":"","title":"\u62dc\u5360\u5ead\u95ee\u9898\u89e3\u6cd5"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/01-%E7%94%A8%E4%B8%89%E5%9B%BD%E6%9D%80%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/#_9","text":"\u5c31\u662f\u8bb2\u8881\u7ecd\u4e5f\u53c2\u4e0e\u8fdb\u6765\u8fdb\u884c\u6295\u7968\uff0c\u8fd9\u6837\u5c31\u589e\u52a0\u4e86\u4e00\u4f4d\u5fe0\u81e3\u7684\u6570\u91cf\u3002\u4e09\u4e2a\u5fe0\u81e3\u4e00\u4e2a\u53db\u8d3c\u3002\u7136\u540e 4 \u4f4d\u5c06\u519b\u505a\u4e86\u4e00\u4e2a\u7ea6\u5b9a\uff0c\u5982\u679c\u6ca1\u6709\u6536\u5230\u547d\u4ee4\uff0c\u5219\u6267\u884c\u9ed8\u8ba4\u547d\u4ee4\uff0c\u6bd4\u5982\u64a4\u9000\u3002\u53e6\u5916\u7ea6\u5b9a\u6d41\u7a0b\u6765\u53d1\u9001\u4f5c\u6218\u4fe1\u606f\u548c\u5982\u4f55\u6267\u884c\u4f5c\u6218\u6307\u4ee4\u3002\u8fd9\u4e2a\u89e3\u6cd5\u7684\u5173\u952e\u70b9\u5c31\u662f\u6267\u884c**\u4e24\u8f6e**\u4f5c\u6218\u4fe1\u606f\u534f\u5546\u3002 NOTE: \"\u6267\u884c**\u4e24\u8f6e**\u4f5c\u6218\u4fe1\u606f\u534f\u5546\" \u5176\u5b9e\u5c31\u662f\u6267\u884c\u4e24\u8f6e\u6295\u7968 \u6211\u4eec\u6765\u770b\u4e0b\u7b2c\u4e00\u8f6e\u662f\u600e\u4e48\u505a\u7684\u3002 \u7b2c\u4e00\u6b65\uff1a\u5148\u53d1\u9001\u4f5c\u6218\u4fe1\u606f\u7684\u5c06\u519b\u6211\u4eec\u628a\u4ed6\u79f0\u4e3a**\u6307\u6325\u5b98**\uff08\u8881\u7ecd\uff09\uff0c\u53e6\u5916\u7684\u5c06\u519b\u6211\u4eec\u79f0\u4f5c**\u526f\u5b98**\uff08\u5218\u5907\uff0c\u66f9\u64cd\uff0c\u5b59\u575a\uff09\u3002 \u7b2c\u4e8c\u6b65\uff1a\u6307\u6325\u5b98\u5c06\u4ed6\u7684\u4f5c\u6218\u4fe1\u606f\u53d1\u9001\u7ed9\u6240\u6709\u7684**\u526f\u5b98**\u3002 \u7b2c\u4e09\u6b65\uff1a\u6bcf\u4e00\u4f4d**\u526f\u5b98**\u5c06\u4ece**\u6307\u6325\u5b98**\u5904\u6536\u5230\u7684\u4f5c\u6218\u4fe1\u606f\uff0c\u4f5c\u4e3a\u81ea\u5df1\u7684\u4f5c\u6218\u6307\u4ee4\uff1b\u5047\u5982\u6ca1\u6709\u6536\u5230**\u6307\u6325\u5b98**\u7684\u4f5c\u6218\u4fe1\u606f\uff0c\u5c06\u628a\u9ed8\u8ba4\u7684\u64a4\u9000\u4f5c\u4e3a\u4f5c\u6218\u6307\u4ee4 \u6211\u4eec\u7528\u56fe\u6765\u6f14\u793a\uff1a \u8881\u7ecd**\u4f5c\u4e3a\u4e3b\u516c\u5148\u53d1\u9001\u4f5c\u6218\u4fe1\u606f\uff0c\u4f5c\u6218\u6307\u4ee4\u4e3a**\u8fdb\u653b \u3002\u7136\u540e\u66f9\u64cd\u3001\u5218\u5907\u3001\u5b59\u575a\u6536\u5230**\u8fdb\u653b**\u7684\u4f5c\u6218\u6307\u4ee4\u3002 \u518d\u6765\u770b\u4e0b**\u7b2c\u4e8c\u8f6e**\u662f\u600e\u4e48\u505a\u7684\u3002 1\u3001\u7b2c\u4e00\u8f6e**\u6307\u6325\u5b98**\uff08\u8881\u7ecd\uff09\u5df2\u7ecf\u53d1\u9001\u6307\u4ee4\u4e86\uff0c\u73b0\u5728\u5c31\u9700\u8981\u5218\u5907\u3001\u66f9\u64cd\u3001\u5b59\u575a\u4f9d\u6b21\u4f5c\u4e3a**\u6307\u6325\u5b98**\u7ed9\u5176\u4ed6\u4e24\u4f4d**\u526f\u5c06**\u53d1\u9001\u4f5c\u6218\u4fe1\u606f\u3002 2\u3001\u7136\u540e\u8fd9\u4e09\u4f4d\u526f\u5c06\u6309\u7167\u5c11\u6570\u670d\u4ece\u591a\u6570\u7684\u539f\u5219\uff0c\u6267\u884c\u6536\u5230\u7684\u4f5c\u6218\u6307\u4ee4\u3002","title":"\u89e3\u6cd5\u539f\u7406"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/01-%E7%94%A8%E4%B8%89%E5%9B%BD%E6%9D%80%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/#-_2","text":"\u5982\u679c\u5b59\u575a\u4f7f\u8bc8\uff0c\u6bd4\u5982\u7ed9\u66f9\u64cd\u548c\u5218\u5907\u90fd\u53d1\u9001\u64a4\u9000\u4fe1\u606f\uff0c\u5982\u4e0b\u56fe\u6240\u793a\u3002\u90a3\u4e48\u5218\u5907\u548c\u66f9\u64cd\u6536\u5230\u7684\u4f5c\u6218\u4fe1\u606f\u4e3a \u8fdb\u653b 2\u7968\uff0c\u64a4\u9000 1 \u7968\uff0c\u6309\u7167\u5c11\u6570\u670d\u4ece\u591a\u6570\u7684\u539f\u5219\uff0c\u6700\u540e\u5218\u5907\u548c\u66f9\u64cd\u6267\u884c\u8fdb\u653b\uff0c\u5b9e\u73b0\u4e86\u4f5c\u6218\u8ba1\u5212\u7684\u4e00\u81f4\u6027\uff0c\u66f9\u64cd\u548c\u5218\u5907\u8054\u5408\u4f5c\u6218\u51fb\u8d25\u4e86\u53cd\u8d3c\u8463\u5353\uff08\u5373\u4f7f\u5b59\u575a\u6ca1\u6709\u53c2\u52a0\u4f5c\u6218\u3002\uff09","title":"\u5b59\u575a\u4f7f\u8bc8 - \u4e24\u64a4\u9000"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/01-%E7%94%A8%E4%B8%89%E5%9B%BD%E6%9D%80%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/#-_3","text":"\u5047\u5982\u5b59\u575a\u4f7f\u8bc8\uff0c\u7ed9\u66f9\u64cd\u53d1\u9001\u64a4\u9000\u6307\u4ee4\uff0c\u7ed9\u5218\u5907\u53d1\u9001\u8fdb\u653b\u6307\u4ee4\uff0c\u90a3\u4e48\u5218\u5907\u6536\u5230\u7684\u4f5c\u6218\u4fe1\u606f\u662f\u8fdb\u653b 3\u7968\uff0c\u80af\u5b9a\u4f1a\u53d1\u8d77\u8fdb\u653b\u4e86\uff0c\u800c\u66f9\u64cd\u6536\u5230\u7684\u4f5c\u6218\u4fe1\u606f\u662f\u8fdb\u653b 2 \u7968\uff0c\u64a4\u9000 1 \u7968\uff0c\u6700\u540e\u66f9\u64cd\u8fd8\u662f\u4f1a\u8fdb\u653b\uff0c\u6240\u4ee5\u5218\u5907\u548c\u66f9\u64cd\u8fd8\u662f\u8054\u5408\u4f5c\u6218\u51fb\u8d25\u4e86\u53cd\u8d3c\u8463\u5353\u3002 \u5982\u6b64\u770b\u6765\uff0c\u5f15\u5165\u4e86\u4e00\u4f4d\u6307\u6325\u5b98\u540e\uff0c\u786e\u5b9e\u53ef\u4ee5\u907f\u514d\u5b59\u575a\u4f7f\u8bc8\uff0c\u4f46\u5982\u679c\u662f\u5b59\u575a\u5728\u7b2c\u4e00\u8f6e\u4f5c\u4e3a\u6307\u6325\u5b98\uff0c\u5176\u4ed6\u4eba\u4f5c\u4e3a\u526f\u5b98\u5462\uff1f","title":"\u5b59\u575a\u4f7f\u8bc8 - \u4e00\u8fdb\u4e00\u9000"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/01-%E7%94%A8%E4%B8%89%E5%9B%BD%E6%9D%80%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/#_10","text":"\u7b2c\u4e00\u8f6e\u5b59\u575a\u5411\u5176\u4e2d\u4e00\u4e2a\u526f\u5b98\u8881\u7ecd\u53d1\u9001**\u64a4\u9000**\u6307\u4ee4\uff0c\u5411\u53e6\u5916\u4e24\u4e2a\u526f\u5b98\u66f9\u64cd\u3001\u5218\u5907\u53d1\u9001**\u8fdb\u653b**\u6307\u4ee4\u3002\u90a3\u4e48\u7b2c\u4e00\u8f6e\u7684\u7ed3\u679c\u5982\u4e0b\u56fe\uff1a \u7b2c\u4e8c\u8f6e\u5b59\u575a\u4f11\u606f\uff0c\u5176\u4ed6\u526f\u5b98\u6309\u7167\u5b59\u575a\u53d1\u9001\u7684\u6307\u4ee4\u5f00\u59cb\u5411\u53e6\u5916\u7684\u526f\u5b98\u53d1\u9001\u6307\u4ee4\u3002 \u66f9\u64cd\u5411\u5218\u5907\u548c\u8881\u7ecd\u53d1\u9001**\u8fdb\u653b**\u6307\u4ee4\u3002 \u5218\u5907\u5411\u66f9\u64cd\u548c\u8881\u7ecd\u53d1\u9001**\u8fdb\u653b**\u6307\u4ee4\u3002 \u8881\u7ecd\u5411\u66f9\u64cd\u548c\u5218\u5907\u53d1\u9001**\u64a4\u9000**\u6307\u4ee4\u3002 \u5982\u4e0b\u56fe\u6240\u793a\uff0c\u6700\u540e\u66f9\u64cd\u3001\u5218\u5907\u3001\u8881\u7ecd\u6536\u5230\u7684\u6307\u4ee4\u4e3a\u8fdb\u653b 2 \u7968\uff0c\u64a4\u9000 1 \u7968\uff0c\u6309\u7167\u5c11\u6570\u670d\u4ece\u591a\u6570\u539f\u5219\uff0c\u4e09\u4e2a\u4eba\u90fd\u662f\u53d1\u8d77\u8fdb\u653b\u3002\u6267\u884c\u4e86\u4e00\u81f4\u7684\u4f5c\u6218\u8ba1\u5212\uff0c\u4fdd\u8bc1\u4f5c\u6218\u7684\u80dc\u5229\u3002","title":"\u5b59\u575a\u4f5c\u4e3a\u6307\u6325\u5b98"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/01-%E7%94%A8%E4%B8%89%E5%9B%BD%E6%9D%80%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/#_11","text":"\u901a\u8fc7\u4e0a\u9762\u7684\u6f14\u793a\uff0c\u6211\u4eec\u77e5\u9053\u4e86\u5982\u4f55\u89e3\u51b3\u62dc\u5360\u5ead\u5c06\u519b\u95ee\u9898\u3002\u5176\u5b9e\u5170\u4f2f\u7279\u5728\u4ed6\u7684\u8bba\u6587\u4e2d\u4e5f\u63d0\u5230\u8fc7\u5982\u4f55\u89e3\u51b3\u3002 \u5982\u679c\u53db\u5c06\u4eba\u6570\u4e3a m\uff0c\u5c06\u519b\u6570 n >= 3m + 1\uff0c\u90a3\u4e48\u5c31\u53ef\u4ee5\u89e3\u51b3\u62dc\u5360\u5ead\u5c06\u519b\u95ee\u9898\u3002 \u524d\u63d0\u6761\u4ef6\uff1a\u53db\u5c06\u6570 m \u4e00\u81f4\uff0c\u9700\u8981\u8fdb\u884c m + 1 \u8f6e\u7684\u4f5c\u6218\u534f\u5546\u3002 \u8fd9\u4e2a\u516c\u5f0f\uff0c\u5927\u5bb6\u53ea\u9700\u8981\u8bb0\u4f4f\u5c31\u53ef\u4ee5\u4e86\uff0c\u63a8\u5230\u8fc7\u7a0b\u53ef\u4ee5\u53c2\u8003\u8bba\u6587\u3002 \u6bd4\u5982\u4e0a\u8ff0\u7684\u653b\u6253\u8463\u5353\u95ee\u9898\uff0c\u66f9\u64cd\u3001\u5218\u5907\u3001\u5b59\u575a\u4e09\u4e2a\u4eba\u5f53\u4e2d\uff0c\u5b59\u575a\u662f\u53db\u5c06\uff0c\u5b83\u53ef\u4ee5\u4f7f\u8bc8\uff0c\u4f7f\u4f5c\u6218\u8ba1\u5212\u4e0d\u7edf\u4e00\u3002\u5fc5\u987b\u589e\u52a0\u4e00\u4f4d\u5fe0\u81e3\u8881\u7ecd\u6765\u534f\u5546\u5171\u8bc6\uff0c\u624d\u80fd\u8fbe\u6210\u4e00\u81f4\u6027\u4f5c\u6218\u8ba1\u5212\u3002","title":"\u5c0f\u7ed3"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/01-%E7%94%A8%E4%B8%89%E5%9B%BD%E6%9D%80%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/#-_4","text":"\u90a3\u53ef\u4ee5\u5728\u4e0d\u589e\u52a0\u5fe0\u81e3\u7684\u60c5\u51b5\u4e0b\uff0c\u89e3\u51b3\u62dc\u5360\u5ead\u7684\u4e8c\u5fe0\u4e00\u5224\u95ee\u9898\u5462\uff1f \u89e3\u6cd5\u4e8c\u5c31\u662f\u901a\u8fc7\u7b7e\u540d\u6d88\u606f\u3002\u6bd4\u5982\u5c06\u519b\u4e4b\u95f4\u901a\u8fc7\u5370\u7ae0\u3001\u864e\u7b26\u7b49\u4fe1\u7269\u8fdb\u884c\u901a\u4fe1\u3002\u6765\u4fdd\u8bc1\u8fd9\u51e0\u4e2a\u7279\u5f81\uff1a \u7b7e\u540d\u65e0\u6cd5\u4f2a\u9020\uff0c\u5bf9\u7b7e\u540d\u6d88\u606f\u7684\u5185\u5bb9\u8fdb\u884c\u4efb\u4f55\u66f4\u6539\u90fd\u4f1a\u88ab\u53d1\u73b0\u3002 \u4efb\u4f55\u4eba\u90fd\u80fd\u9a8c\u8bc1\u5c06\u519b\u7b7e\u540d\u7684\u771f\u4f2a\u3002","title":"\u62dc\u5360\u5ead\u89e3\u6cd5\u4e8c-\u7b7e\u540d"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/01-%E7%94%A8%E4%B8%89%E5%9B%BD%E6%9D%80%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/#_12","text":"\u901a\u8fc7\u4e09\u56fd\u6740\u89d2\u8272\u6765\u8bb2\u89e3\u5206\u5e03\u5f0f\u4e2d\u5171\u8bc6\u573a\u666f\u3002\u90a3\u4ed6\u4eec\u548c\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u6620\u5c04\u5173\u7cfb\u662f\u600e\u4e48\u6837\u7684\u5462\uff1f \u5c06\u519b\u5bf9\u5e94\u8ba1\u7b97\u673a\u8282\u70b9\u3002 \u5fe0\u81e3\u7684\u5c06\u519b\u5bf9\u5e94\u6b63\u5e38\u8fd0\u884c\u7684\u8ba1\u7b97\u673a\u8282\u70b9\u3002 \u53db\u53d8\u7684\u5c06\u519b\u5bf9\u5e94\u51fa\u73b0\u6545\u969c\u5e76\u4f1a\u53d1\u9001\u8bef\u5bfc\u4fe1\u606f\u7684\u8ba1\u7b97\u673a\u8282\u70b9\u3002 \u4fe1\u4f7f\u88ab\u6740\u5bf9\u5e94\u901a\u8baf\u6545\u969c\u3001\u4fe1\u606f\u4e22\u5931\u3002 \u4fe1\u4f7f\u88ab\u95f4\u8c0d\u66ff\u6362\u5bf9\u5e94\u4e3a\u901a\u8baf\u88ab\u6076\u610f\u653b\u51fb\u3001\u4f2a\u9020\u4fe1\u606f\u6216\u52ab\u6301\u901a\u8baf\u3002 NOTE: \u975e\u5e38\u597d\u7684\u7c7b\u6bd4","title":"\u603b\u7ed3"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/01-%E7%94%A8%E4%B8%89%E5%9B%BD%E6%9D%80%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/#_13","text":"\u53ef\u4e0d\u8981\u5c0f\u77a7\u62dc\u5360\u5ead\u95ee\u9898\uff0c\u5b83\u53ef\u662f\u5206\u5e03\u5f0f\u573a\u666f\u6700\u590d\u6742\u7684\u7684\u6545\u969c\u573a\u666f\u3002\u6bd4\u5982\u5728\u6570\u5b57\u8d27\u5e01\u7684\u533a\u5757\u94fe\u6280\u672f\u4e2d\u5c31\u6709\u7528\u5230\u8fd9\u4e9b\u77e5\u8bc6\u70b9\u3002\u800c\u4e14\u5fc5\u987b\u4f7f\u7528**\u62dc\u5360\u5ead\u5bb9\u9519**\u7b97\u6cd5\uff08\u4e5f\u5c31\u662f Byzantine Fault Tolerance\uff0c BFT \uff09\u3002 \u62dc\u5360\u5ead\u5bb9\u9519\u7b97\u6cd5\u8fd8\u6709 FBFT \u7b97\u6cd5\uff0c PoW \u7b97\u6cd5\uff0c\u5f53\u7136\u4e0d\u4f1a\u5728\u8fd9\u7bc7\u4e2d\u53bb\u8bb2\u8fd9\u4e9b\u7b97\u6cd5\uff0c\u540e\u7eed\u518d\u8bb2\u89e3\u3002","title":"\u62dc\u5360\u5ead\u5bb9\u9519\u7b97\u6cd5"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/01-%E7%94%A8%E4%B8%89%E5%9B%BD%E6%9D%80%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/#_14","text":"\u6709\u4e86**\u62dc\u5360\u5ead\u5bb9\u9519\u7b97\u6cd5**\uff0c\u80af\u5b9a\u6709**\u975e\u62dc\u5360\u5ead\u5bb9\u9519\u7b97\u6cd5**\uff0c\u987e\u540d\u601d\u4e49\uff0c\u5c31\u662f\u6ca1\u6709\u53d1\u9001\u8bef\u5bfc\u4fe1\u606f\u7684\u8282\u70b9\u3002CFT \u7b97\u6cd5\u5c31\u662f\u89e3\u51b3\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u5b58\u5728\u6545\u969c\uff0c\u4f46\u4e0d\u5b58\u5728\u6076\u610f\u8282\u70b9\u7684\u573a\u666f\u4e0b\u7684\u5171\u8bc6\u95ee\u9898\u3002\u7b80\u5355\u6765\u8bf4\u5c31\u662f\u53ef\u80fd\u56e0\u7cfb\u7edf\u6545\u969c\u9020\u6210\u4e22\u5931\u6d88\u606f\u6216\u6d88\u606f\u91cd\u590d\uff0c\u4f46\u4e0d\u5b58\u5728\u9519\u8bef\u6d88\u606f\u3001\u4f2a\u9020\u6d88\u606f\u3002\u5bf9\u5e94\u7684\u7b97\u6cd5\u6709 Paxos \u7b97\u6cd5\u3001Raft \u7b97\u6cd5\u3001ZAB \u534f\u8bae \u4e0a\u9762\u63d0\u5230\u4e86 5 \u79cd\u7b97\u6cd5\uff0c\u5c45\u7136\u90fd\u662f\u8ddf\u62dc\u5360\u5ead\u95ee\u9898\u6709\u5173\uff0c\u4f60\u8bf4\u4eca\u5929\u8bb2\u7684\u62dc\u5360\u5ead\u95ee\u9898\u91cd\u8981\u4e0d\u91cd\u8981\uff1f","title":"\u975e\u62dc\u5360\u5ead\u5bb9\u9519\u7b97\u6cd5"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/01-%E7%94%A8%E4%B8%89%E5%9B%BD%E6%9D%80%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/#_15","text":"\u8282\u70b9\u53ef\u4fe1\uff0c\u9009\u975e\u62dc\u5360\u5ead\u5bb9\u9519\u7b97\u6cd5\u3002\u5426\u5219\u5c31\u7528\u62dc\u5360\u5ead\u5bb9\u9519\u7b97\u6cd5\uff0c\u5982\u533a\u5757\u94fe\u4e2d\u7528\u5230\u7684 PoW \u7b97\u6cd5\u3002","title":"\u8fd9\u4e48\u591a\u7b97\u6cd5\u8be5\u5982\u4f55\u9009\u62e9\uff1f"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/02-%E7%94%A8%E5%A4%AA%E6%9E%81%E6%8B%B3%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/","text":"csdn \u7528\u592a\u6781\u62f3\u8bb2\u5206\u5e03\u5f0f\u7406\u8bba\uff0c\u771f\u8212\u670d\uff01 NOTE: \u4e00\u3001\u867d\u7136\u539f\u6587\u4f7f\u7528\u4e86\u5f88\u591a\u7c7b\u6bd4\uff0c\u5176\u5b9e\u6838\u5fc3\u601d\u60f3\u662f\u975e\u5e38\u7b80\u5355\u7684: CP \u521a AP \u67d4 \u4e8c\u3001tradeoff \u4e09\u3001\u539f\u6587\u8fd8\u4ecb\u7ecd\u4e862PC\u30013PC \u4e00\u3001\u592a\u6781\u7684\u4e24\u9762 NOTE: \u7b80\u800c\u8a00\u4e4b: CAP 1\u3001CP \u521a 2\u3001AP \u67d4 CAP \u7406\u8bba\u662f\u5bf9\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u7279\u6027\u505a\u4e86\u4e00\u4e2a\u9ad8\u5ea6\u7684\u62bd\u8c61\uff0c\u53d8\u6210\u4e86\u4e09\u5927\u6307\u6807\uff1a 1\u3001\u4e00\u81f4\u6027\uff08Consistency\uff09 2\u3001\u53ef\u7528\u6027\uff08Availability\uff09 3\u3001\u5206\u533a\u5bb9\u9519\u6027\uff08Partition Tolerance\uff09 \u4e00\u81f4\u6027 \u5206\u5e03\u5f0f\u4e2d\u7684\u4e00\u81f4\u6027\uff0c\u6211\u4eec\u53ef\u4ee5\u7406\u89e3\u4e3a\u5ba2\u6237\u7aef\u7684\u6bcf\u6b21 \u8bfb\u64cd\u4f5c \uff0c\u4e0d\u7ba1\u8bbf\u95ee\u7684\u662f\u54ea\u4e2a\u51e0\u70b9\uff0c\u8981\u4e48\u8bfb\u5230\u7684\u90fd\u662f\u540c\u4e00\u4efd\u6700\u65b0\u5199\u5165\u7684\u6570\u636e\uff0c\u8981\u4e48\u8bfb\u53d6\u5931\u8d25\u3002\u8fd9\u5c31\u5f88\u521a\u4e86\uff0c\u4e0d\u80fd\u8bf4\u8fd9\u79cd \u521a \u4e0d\u597d\uff0c\u5728\u5f88\u591a\u573a\u666f\u4e2d\uff0c\u4e5f\u786e\u5b9e\u9700\u8981\u4fdd\u8bc1\u9ad8\u5ea6\u7684\u4e00\u81f4\u6027\u3002 \u4e3a\u4e86\u5e2e\u52a9\u5927\u5bb6\u7406\u89e3\u4e00\u81f4\u6027\uff0c\u6211\u4e3e\u4e2a \u501a\u5929\u5c60\u9f99\u8bb0 \u7684\u6545\u4e8b\uff1a \u516d\u5927\u6d3e\u56f4\u653b\u5149\u660e\u9876 \u3002 1.1 \u7406\u89e3\u5206\u5e03\u5f0f\u4e2d\u7684 CAP \u4e00\u81f4\u6027 \u521a \u4e00\u81f4\u6027 \u5f3a\u8c03\u7684\u662f\u6570\u636e\u6b63\u786e\uff0c\u6bcf\u6b21\u8bfb\u53d6\u8282\u70b9\u4e2d\u7684\u6570\u636e\u90fd\u662f\u6700\u65b0\u5199\u5165\u7684\u6570\u636e\u3002\u8fd9\u4e2a\u6211\u79f0\u4f5c \u521a \u3002 \u4f46\u662f\u6211\u4eec\u751f\u4ea7\u7684\u96c6\u7fa4\u73af\u5883\u4e0b\u5982\u679c\u53d1\u751f\u5206\u533a\u6545\u969c\u65f6\uff08\u8282\u70b9\u5931\u8054\uff0c\u8282\u70b9\u65e0\u6cd5\u54cd\u5e94\uff0c\u8282\u70b9\u65e0\u6cd5\u5199\u5165\u6570\u636e\uff09\uff0c\u5ba2\u6237\u7aef\u67e5\u8be2\u8282\u70b9\u65f6\uff0c\u6211\u4eec\u4e0d\u80fd\u8fd4\u56de\u9519\u8bef\u4fe1\u606f\u7ed9\u5ba2\u6237\u7aef\u3002\u6bd4\u5982\u8bf4\u4e1a\u52a1\u96c6\u7fa4\u4e2d\u7684\u4e00\u4e9b\u5173\u952e\u7cfb\u7edf\uff0c\u5982\u6ce8\u518c\u4e2d\u5fc3\uff0c\u4e0d\u80fd\u56e0\u4e3a\u67d0\u4e2a\u8282\u70b9\u5931\u8054\u4e86\uff0c\u5c31\u4e0d\u54cd\u5e94\u6700\u65b0\u7684\u6570\u636e\u3002\u90a3\u4e48\u76f8\u5173\u7684\u4e1a\u52a1\u4e5f\u83b7\u53d6\u4e0d\u5230\u6b63\u786e\u7684\u6ce8\u518c\u4fe1\u606f\u800c\u5bfc\u81f4\u7cfb\u7edf\u762b\u75ea\u3002 NOTE: \u5982\u679c\u6309\u7167 CP\uff0c\u5219\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u662f\u4f1a\" \u8fd4\u56de\u9519\u8bef\u4fe1\u606f\u7ed9\u5ba2\u6237\u7aef \"\u7684 \u53ef\u7528\u6027 \u67d4 \u53ef\u7528\u6027 \u5c31\u6d3e\u4e0a\u7528\u573a\u4e86\uff0c\u727a\u7272\u6570\u636e\u51c6\u786e\u6027\uff0c\u6bcf\u4e2a\u8282\u70b9\u4f7f\u7528\u672c\u5730\u6570\u636e\u6765\u54cd\u5e94\u5ba2\u6237\u7aef\u7684\u8bf7\u6c42\u3002\u53e6\u5916\u5f53\u8282\u70b9\u4e0d\u53ef\u7528\u65f6\uff0c\u53ef\u4ee5\u4f7f\u7528**\u5feb\u901f\u5931\u8d25\u7b56\u7565**\uff0c\u81f3\u5c11\u4e0d\u80fd\u8ba9\u670d\u52a1\u957f\u65f6\u95f4\u4e0d\u80fd\u54cd\u5e94\u53ef\u7528\u6027\u5f3a\u8c03\u7684\u662f\u670d\u52a1\u53ef\u7528\uff0c\u4e0d\u4fdd\u8bc1\u6570\u636e\u6b63\u786e\u3002\u8fd9\u4e2a\u6211\u79f0\u4f5c \u67d4 \u3002 \u5206\u533a\u5bb9\u9519\u6027 \u5206\u533a\u5bb9\u9519\u6027 \u7684\u542b\u4e49\u5c31\u662f\u8282\u70b9\u95f4\u51fa\u73b0\u4efb\u610f\u6570\u91cf\u7684\u6d88\u606f\u4e22\u5931\u6216\u9ad8\u5ef6\u8fdf\u7684\u65f6\u5019\uff0c\u7cfb\u7edf\u4ecd\u7136\u5728\u7ee7\u7eed\u5de5\u4f5c\u3002\u5206\u5e03\u5f0f\u7cfb\u7edf\u544a\u8bc9\u5ba2\u6237\u7aef\uff0c\u6211\u7684\u5185\u90e8\u4e0d\u8bba\u51fa\u73b0\u4ec0\u4e48\u6837\u7684\u6570\u636e\u540c\u6b65\u95ee\u9898\uff0c\u6211\u4f1a\u4e00\u76f4\u8fd0\u884c\u3002\u5f3a\u8c03\u7684\u662f\u96c6\u7fa4\u5806\u5206\u533a\u6545\u969c\u7684\u5bb9\u9519\u80fd\u529b\u3002 1.2 CAP \u4e09\u89d2 \u90a3\u4e48\u8fd9\u4e09\u4e2a\u6307\u6807\u53c8\u6709\u4ec0\u4e48\u5173\u7cfb\u5462\uff1f\u8fd9\u4e2a\u5c31\u662f\u6211\u4eec\u7ecf\u5e38\u542c\u5230\u7684 CAP \u7406\u8bba\u3002 C \u4ee3\u8868\u4e00\u81f4\u6027\uff08Consistency\uff09\uff0c A \u4ee3\u8868\u53ef\u7528\u6027\uff08Availability\uff09\u3001 P \u4ee3\u8868\u5206\u533a\u5bb9\u9519\u6027\uff08Partition Tolerance\uff09\u3002 \u5bf9\u4e8e\u5206\u5e03\u5f0f\u7cfb\u7edf\uff0cCAP \u4e09\u4e2a\u6307\u6807\u53ea\u80fd\u9009\u62e9\u5176\u4e2d\u4e24\u4e2a\u3002 CA NOTE: \u5176\u5b9e\u8fd9\u79cd\u60c5\u51b5\u538b\u6839\u5c31\u4e0d\u662f\u5206\u5e03\u5f0f \u4fdd\u8bc1\u4e00\u81f4\u6027\u548c\u53ef\u7528\u6027\u3002\u5f53\u5206\u5e03\u5f0f\u7cfb\u7edf\u6b63\u5e38\u8fd0\u884c\u65f6\uff08\u5927\u90e8\u5206\u65f6\u5019\u6240\u5904\u7684\u72b6\u6001\uff09\uff0c\u8fd9\u4e2a\u65f6\u5019\u4e0d\u9700\u8981 P\uff0c\u90a3\u4e48 C \u548c A \u80fd\u591f\u540c\u65f6\u4fdd\u8bc1\u3002\u53ea\u6709\u5728\u53d1\u751f\u5206\u533a\u6545\u969c\u65f6\uff0c\u624d\u9700\u8981 P\uff0c\u8fd9\u4e2a\u65f6\u5019\u5c31\u53ea\u80fd\u5728 C \u548c A \u4e4b\u95f4\u505a\u51fa\u9009\u62e9\u3002 \u5178\u578b\u5e94\u7528 \uff1a\u5355\u673a\u7248\u90e8\u7f72\u7684 MySQL\u3002 CP \u4fdd\u8bc1\u6570\u636e\u7684\u4e00\u81f4\u6027\u548c\u5206\u533a\u5bb9\u9519\u6027\uff0c\u6bd4\u5982\u914d\u7f6e\u4fe1\u606f\uff0c\u5fc5\u987b\u4fdd\u8bc1\u6bcf\u4e2a\u8282\u70b9\u5b58\u7684\u90fd\u662f\u6700\u65b0\u7684\uff0c\u6b63\u786e\u7684\u6570\u636e\u3002\u6bd4\u5982 Raft \u7684\u5f3a\u4e00\u81f4\u6027\u7cfb\u7edf\uff0c\u4f1a\u5bfc\u81f4\u65e0\u6cd5\u6267\u884c\u8bfb\u64cd\u4f5c\u548c\u5199\u64cd\u4f5c\u3002 \u5178\u578b\u5e94\u7528 \uff1aEtcd\u3001Consul\u3001Hbase\u3002 AP \u4fdd\u8bc1\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u53ef\u7528\u6027\u548c\u5206\u533a\u5bb9\u9519\u6027\u3002\u7528\u6237\u8bbf\u95ee\u7cfb\u7edf\uff0c\u90fd\u80fd\u5f97\u5230\u76f8\u5e94\u6570\u636e\uff0c\u4e0d\u4f1a\u51fa\u73b0\u54cd\u5e94\u9519\u8bef\uff0c\u4f46\u662f\u53ef\u80fd\u4f1a\u8bfb\u5230\u65e7\u7684\u6570\u636e\u3002 \u5178\u578b\u5e94\u7528 \uff1aCassandra \u548c DynamoDB\u3002 \u4e8c\u3001\u592a\u6781\u7684\u521a NOTE: \u9009\u62e9consistency 2.1 ACID \u7684\u521a NOTE: \u4e00\u3001\u6240\u8c13\"ACID\u7684\u521a\"\uff0c\u6307\u7684\u662f\u5b83\u5f3a\u8c03consistency \u4e8c\u3001\u8bfb\u8005\u80af\u5b9a\u4f1a\u601d\u8003CAP\u3001ACID\u7684\u5173\u7cfb\uff0c\u5728\"\u4e94\u3001\u603b\u7ed3\"\u4e2d\uff0c\u4f5c\u8005\u7ed9\u51fa\u4e86\u975e\u5e38\u597d\u7684\u603b\u7ed3: \"ACID \u662f\u4f20\u7edf\u6570\u636e\u5e93\u7684\u8bbe\u8ba1\u7406\u5ff5\uff0c\u8ffd\u6c42\u5f3a\u4e00\u81f4\u6027\u3002\u56db\u4e2a\u6307\u6807\uff1a\u539f\u5b50\u6027\u3001\u4e00\u81f4\u6027\u3001\u9694\u79bb\u6027\u3001\u6301\u4e45\u6027\u3002\u662f CAP \u4e2d CP \u7684\u5ef6\u4f38\u3002\" \u6700\u5f00\u59cb\u77e5\u9053 ACID \u662f\u7814\u7a76 SQL \u6570\u636e\u5e93\u7684\u65f6\u5019\uff0c \u539f\u5b50\u6027 \uff08Atomicity\uff09\u3001 \u4e00\u81f4\u6027 \uff08Consistency\uff09\u3001 \u9694\u79bb\u6027 \uff08Isolation\uff09\u3001 \u6301\u4e45\u6027 \uff08Durability\uff09\u3002 \u8fd9\u56db\u4e2a\u5c5e\u6027\u662f\u9488\u5bf9 \u4e8b\u52a1 \u800c\u8a00\u7684\uff0c\u800c\u4e8b\u52a1\u5c31\u662f\u4e3a\u5355\u4e2a\u5de5\u4f5c\u5355\u5143\u800c\u6267\u884c\u7684\u4e00\u7cfb\u5217\u64cd\u4f5c\u3002\u5982\u67e5\u8be2\u3001\u4fee\u6539\u6570\u636e\u3001\u4fee\u6539\u6570\u636e\u5b9a\u4e49\u3002 \u4e8b\u52a1\u4e0d\u4ec5\u4ec5\u53ea\u7528\u5728\u6570\u636e\u5e93\u4e0a\uff0c\u8fd8\u53ef\u4ee5\u7528\u5728\u4e1a\u52a1\u7cfb\u7edf\u4e2d\uff0c\u6bd4\u5982\u53d1\u5238\u540e\u6263\u51cf\u5e93\u5b58\uff0c\u8fd9\u79cd\u4e1a\u52a1\u573a\u666f\u53ef\u4ee5\u5b9a\u4e49\u4e3a\u4e00\u4e2a\u4e8b\u52a1\u3002\u5355\u673a\u573a\u666f\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u52a0\u9501\u3001\u65f6\u95f4\u5e8f\u5217\u7b49\u673a\u5236\u6765\u4fdd\u8bc1\u5355\u4e2a\u8282\u70b9\u4e0a\u7684 ACID \u7279\u6027\uff0c\u4f46\u65e0\u6cd5\u4fdd\u8bc1\u8282\u70b9\u95f4\u64cd\u4f5c\u7684 ACID \u7279\u6027\u3002 \u90a3\u4e48\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e0b\u8be5\u5982\u4f55\u89e3\u51b3\u4e8b\u52a1\u95ee\u9898\u5462\uff1f\u8fd9\u4e5f\u662f\u9762\u8bd5\u4e2d\u7ecf\u5e38\u9047\u5230\u7684\u9898\u3002\u5206\u5e03\u5f0f\u4e8b\u52a1\u534f\u8bae\u5927\u5bb6\u4e00\u5b9a\u542c\u8fc7\uff0c\u6bd4\u5982 \u4e8c\u9636\u6bb5\u63d0\u4ea4\u534f\u8bae \u548c TCC \u534f\u8bae \uff0c\u4e0b\u9762\u6211\u8fd8\u662f\u7528 \u516d\u5927\u6d3e\u56f4\u653b\u5149\u660e\u9876 \u6545\u4e8b\u6765\u8bb2\u89e3\u4e8c\u9636\u6bb5\u534f\u8bae\u3002 NOTE: \"TCC\u534f\u8bae\" \u662f\u63073PC\uff0c\u5373three-phase commit protocol 2.2 \u56f4\u653b\u5149\u660e\u9876 NOTE: \u7ed3\u5408\u5177\u4f53\u4f8b\u5b50\u6765\u8bf4\u660e2PL \u5ce8\u7709\u6d3e \u60f3\u6c47\u96c6 \u5c11\u6797\u6d3e \u3001 \u6b66\u5f53\u6d3e \u3001 \u6606\u4ed1\u6d3e \u660e\u5929\u4e00\u8d77\u8fdb\u653b \u5149\u660e\u9876 \u3002\u5982\u679c\u6709\u4e00\u65b9\u4e0d\u540c\u610f\u8fdb\u653b\uff0c\u6216\u8005\u8fdb\u653b\u65f6\u673a\u4e0d\u4e00\u81f4\uff0c\u5219\u9700\u8981\u53d6\u6d88\u6574\u4e2a\u884c\u52a8\u8ba1\u5212\u3002\u5c11\u6797\u6d3e\u3001\u6b66\u5f53\u6d3e\u3001\u6606\u4ed1\u6d3e\u8fdb\u653b\u5149\u660e\u9876\u8fd9\u4e00\u7ec4\u884c\u52a8\u53ef\u4ee5\u770b\u6210\u662f \u4e00\u4e2a\u5206\u5e03\u5f0f\u4e8b\u52a1 \uff0c \u8981\u4e48\u5168\u90e8\u6267\u884c\u3001\u8981\u4e48\u5168\u90e8\u4e0d\u6267\u884c \u3002 NOTE: \u4e00\u3001\u9700\u8981\u6240\u6709\u8282\u70b9\u90fd\u540c\u610f\uff0c\u800c\u4e0d\u662f\u5c11\u6570\u670d\u4ece\u591a\u6570\uff0c\u56e0\u6b64\u9700\u8981\u6709\u4e00\u4e2a\"Propose\"\u9636\u6bb5\uff0c\u5373\u63d0\u8bae\u9636\u6bb5\uff0c\u5728\u63d0\u8bae\u9636\u6bb5\uff0c\u53ef\u4ee5\u770b\u51fa\u662f\u5426\u6240\u6709\u7684\u8282\u70b9\u90fd\u540c\u610f\uff0c\u5982\u679c\u4e0d\u662f\uff0c\u5219\u901a\u77e5\u6240\u6709\u7684\u8282\u70b9\u53d6\u6d88\uff1b\u5982\u679c\u662f\uff0c\u5219\u901a\u77e5\u6240\u6709\u7684\u8282\u70b9commit\uff1b \u4e8c\u3001\u5b83\u7684\u5f3a\u4e00\u81f4\u4f53\u73b0\u5728: \u6240\u6709\u7684node\u90fd\u9700\u8981\u4fdd\u6301\u4e00\u81f4: \"\u5982\u679c\u6709\u4e00\u65b9\u4e0d\u540c\u610f\u8fdb\u653b\uff0c\u6216\u8005\u8fdb\u653b\u65f6\u673a\u4e0d\u4e00\u81f4\uff0c\u5219\u9700\u8981\u53d6\u6d88\u6574\u4e2a\u884c\u52a8\u8ba1\u5212\" \u5982\u4e0b\u56fe\u6240\u793a\uff1a 2.3 \u4e8c\u9636\u6bb5\u63d0\u4ea4\u534f\u8bae \u5728\u4e8c\u9636\u6bb5\u63d0\u4ea4\u534f\u8bae\u4e2d\uff0c\u706d\u7edd\u5e08\u592a\u5148\u7ed9\u5c11\u6797\u6d3e\u53d1\u9001\u8fdb\u653b\u7684\u6d88\u606f\uff0c\u5c11\u6797\u6d3e\u4f5c\u4e3a**\u534f\u8c03\u8005**\u7684\u8eab\u4efd\uff0c\u7531\u5c11\u6797\u6d3e\u8054\u7cfb\u6b66\u5f53\u6d3e\u548c\u6606\u4ed1\u6d3e\u662f\u8fdb\u653b\u8fd8\u662f\u64a4\u9000\u3002 NOTE: \u4e00\u3001\u6709\"\u534f\u8c03\u8005\"\uff0c\u663e\u7136\u5c31\u5b58\u5728single point failure\u7684\u98ce\u9669 \u4e8c\u3001\"\u534f\u8c03\u8005\"\u5373\"coordinator\" \u4e09\u3001\u8fd9\u4e00\u6bb5\u5173\u4e8e2PL\u7684\u4ecb\u7ecd\uff0c\u6ca1\u6709\u8303\u658c zhihu \u6f2b\u8bdd\u5206\u5e03\u5f0f\u7cfb\u7edf\u5171\u8bc6\u534f\u8bae: 2PC/3PC\u7bc7 \u7684\u597d \u4e8c\u9636\u6bb5\u5c31\u662f\u8bf4\u6709\u4e24\u4e2a\u9636\u6bb5\uff0c 1.\u63d0\u4ea4\u8bf7\u6c42\u9636\u6bb5 \uff08\u6295\u7968\u9636\u6bb5\uff09\uff0c 2.\u63d0\u4ea4\u6267\u884c\u9636\u6bb5\uff08\u5b8c\u6210\u9636\u6bb5\uff09 \u3002 \u9636\u6bb5\u4e00\uff1a\u63d0\u4ea4\u8bf7\u6c42\u9636\u6bb5(Propose)\uff1a 1\u3001 \u7b2c\u4e00\u6b65 \uff1a\u5c11\u6797\u6d3e\u4f5c\u4e3a\u534f\u8c03\u8005\u5206\u522b\u7ed9\u6b66\u5f53\u6d3e\u548c\u6606\u4ed1\u6d3e\u53d1\u9001\u6d88\u606f\uff1a \u201c\u660e\u5929\u8fdb\u653b\u5149\u660e\u9876\uff0c\u53ef\u884c\uff1f\u201d 2\u3001 \u7b2c\u4e8c\u6b65 \uff1a\u5c11\u6797\u6d3e\u3001\u6b66\u5f53\u6d3e\u3001\u6606\u4ed1\u6d3e\u5206\u522b\u8bc4\u4f30\u660e\u5929\u662f\u5426\u80fd\u8fdb\u653b\u5149\u660e\u9876\uff0c\u5982\u679c\u80fd\uff0c\u5c31\u9884\u7559\u65f6\u95f4\u5e76\u9501\u5b9a\uff0c\u4e0d\u518d\u5b89\u6392\u5176\u4ed6\u7684\u8fdb\u653b\u4e8b\u9879\u3002 NOTE: \u6b64\u5904\u4f1a\u9501\u5b9a\u8d44\u6e90 3\u3001 \u7b2c\u4e09\u6b65 \uff1a\u5c11\u6797\u6d3e\u5f97\u5230\u5168\u90e8\u7684\u56de\u590d\u7ed3\u679c\uff0c\u5305\u62ec\u5c11\u6797\u6d3e\u81ea\u5df1\u7684\u8bc4\u4f30\u7ed3\u679c\u3002\u6700\u540e\u4e09\u65b9\u7684\u7ed3\u679c\u90fd\u662f \u53ef\u884c \u3002 \u5982\u4e0b\u56fe\u6240\u793a\uff1a NOTE: \u4e0a\u8ff0\u56fe\u4e2d\u5c55\u793a\u7684\u6267\u884c\u987a\u5e8f: \u7b2c\u4e00\u6b65: 1.1\u30011.2 \u7b2c\u4e8c\u6b65: 2.1\u30012.2\u30012.3 \u7b2c\u4e09\u6b65: 3.2\u30013.3 \u53ef\u4ee5\u770b\u51fa\uff0c\u4f5c\u8005\u7684\u753b\u56fe\u8fd8\u662f\u975e\u5e38\u8ba4\u771f\u7684\uff0c\u548c\u524d\u9762\u7684\u6b65\u9aa4\u662f\u5bf9\u5e94\u7684 \u9636\u6bb5\u4e8c\uff1a\u63d0\u4ea4\u6267\u884c\u9636\u6bb5(Commit)\uff1a 1\u3001 \u7b2c\u4e00\u6b65 \uff1a\u5c11\u6797\u6d3e\u7edf\u8ba1\u81ea\u5df1\u3001\u6606\u4ed1\u6d3e\u548c\u6b66\u5f53\u6d3e\u7684\u6d88\u606f\uff0c\u90fd\u662f \u53ef\u4ee5\u8fdb\u653b \uff0c\u6240\u4ee5\u53ef\u4ee5\u6267\u884c\u5206\u5e03\u5f0f\u4e8b\u52a1\uff0c\u8fdb\u653b\u5149\u660e\u9876\u3002 NOTE: \u9700\u8981\u6ce8\u610f\u7684\u662f: \u53ef\u80fd\u4e0d\u662f\u90fd\u540c\u610f\uff0c\u672c\u8282\u6240\u8ba8\u8bba\u7684\u662f\u6267\u884c\u7684\u60c5\u51b5\uff1b 2\u3001 \u7b2c\u4e8c\u6b65 \uff1a\u5c11\u6797\u6d3e\u901a\u77e5\u6606\u4ed1\u6d3e\u548c\u6b66\u5f53\u6d3e\u8fdb\u653b\u5149\u660e\u9876\u3002 3\u3001 \u7b2c\u4e09\u6b65 \uff1a\u5c11\u6797\u6d3e\u3001\u6606\u4ed1\u6d3e\u3001\u6b66\u5f53\u6d3e\u53ec\u96c6\u624b\u4e0b\u5f1f\u5b50\uff0c\u8fdb\u653b\u5149\u660e\u9876\uff08\u6267\u884c\u4e8b\u52a1\uff09\u3002 4\u3001 \u7b2c\u56db\u6b65 \uff1a\u6606\u4ed1\u6d3e\u3001\u6b66\u5f53\u6d3e\u5c06 \u662f\u5426\u5df2\u53d1\u8d77\u8fdb\u653b \u544a\u8bc9\u5c11\u6797\u6d3e\u3002 5\u3001 \u7b2c\u4e94\u6b65 \uff1a\u5c11\u6797\u6d3e\u6c47\u603b\u81ea\u5df1\u3001\u6606\u4ed1\u6d3e\u3001\u6b66\u5f53\u6d3e\u7684\u8fdb\u653b\u7ed3\u679c\u7ed9\u706d\u7edd\u5e08\u592a\u3002\u8fd9\u6837\u706d\u7edd\u5e08\u592a\u770b\u5230\u7684\u5c31\u662f\u7edf\u4e00\u7684\u4f5c\u6218\u8ba1\u5212\u3002 NOTE: \u7b2c\u4e00\u6b65: 1 \u7b2c\u4e8c\u90e8: 2.1\u30012.2 \u7b2c\u4e09\u6b65: 3.1\u30013.2\u30013.3 \u7b2c\u56db\u6b65: 4.1\u30014.2 \u7b2c\u4e94\u6b65: 5 \u6ce8\u610f\uff1a 1\u3001\u53ef\u4ee5\u5c06\u706d\u7edd\u5e08\u592a\u5f53\u505a\u5ba2\u6237\u7aef\u3002\u5c11\u6797\u6d3e\u3001\u6b66\u5f53\u6d3e\u3001\u6606\u4ed1\u6d3e\u5f53\u505a\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u4e09\u4e2a\u8282\u70b9\u3002\u5c11\u6797\u6d3e\u4f5c\u4e3a\u534f\u8c03\u8005\u3002 2\u3001\u5c06\u8bc4\u4f30\u662f\u5426\u80fd\u8fdb\u653b\u5149\u660e\u9876\u4ee5\u53ca\u9884\u7559\u65f6\u95f4\u53ef\u4ee5\u7406\u89e3\u4e3a\u9700\u8981\u64cd\u4f5c\u7684\u5bf9\u8c61\u548c\u5bf9\u8c61\u72b6\u6001\uff0c\u662f\u5426\u5df2\u7ecf\u51c6\u5907\u597d\u4e86\uff0c\u80fd\u5426\u63d0\u4ea4\u65b0\u7684\u64cd\u4f5c\u3002 3\u3001\u53d1\u9001\u6d88\u606f\u3001\u98de\u9e3d\u4f20\u4e66\u53ef\u4ee5\u7406\u89e3\u4e3a\u7f51\u7edc\u6d88\u606f\u3002 4\u3001\u7b2c\u4e00\u4e2a\u9636\u6bb5\u4e2d\uff0c\u6bcf\u4e2a\u53c2\u4e0e\u8005\u6295\u7968\u8868\u51b3\u4e8b\u52a1\u662f\u653e\u5f03\u8fd8\u662f\u63d0\u4ea4\uff0c\u4e00\u65e6\u6295\u7968\u8981\u6c42\u63d0\u4ea4\u4e8b\u52a1\uff0c\u90a3\u4e48\u5c31\u4e0d\u5141\u8bb8\u653e\u5f03\u4e8b\u52a1\u3002 5\u3001\u7b2c\u4e8c\u4e2a\u9636\u6bb5\u4e2d\uff0c\u6bcf\u4e2a\u53c2\u4e0e\u8005\u6267\u884c\u6700\u7ec8\u7edf\u4e00\u7684\u51b3\u5b9a\uff0c\u63d0\u4ea4\u4e8b\u52a1\u6216\u8005\u653e\u5f03\u4e8b\u52a1\u3002\u8fd9\u4e2a\u5c31\u662f ACID \u7684\u539f\u5b50\u6027\u3002 NOTE: \u8fd9\u5c31\u662ftransaction 6\u3001\u7b2c\u4e00\u4e2a\u9636\u6bb5\u4e2d\uff0c\u9700\u8981\u9884\u7559\u8d44\u6e90\uff0c\u9884\u7559\u671f\u95f4\uff0c\u5176\u4ed6\u4eba\u4e0d\u80fd\u64cd\u4f5c\u8fd9\u4e2a\u8d44\u6e90\u3002 2.4 \u4e8c\u9636\u6bb5\u534f\u8bae\u5e26\u6765\u7684\u95ee\u9898 ACID \u7279\u6027\u662f CAP \u4e2d\u4e00\u81f4\u6027\u7684 \u8fb9\u754c \uff0c\u53ef\u4ee5\u79f0\u4f5c\u6700\u5f3a\u7684\u4e00\u81f4\u6027\uff0c\u5982\u679c\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u4e00\u81f4\u6027\uff0c\u5fc5\u7136\u4f1a\u5f71\u54cd\u5230 \u53ef\u7528\u6027 \u3002\u5982\u679c\u4e00\u4e2a\u8282\u70b9\u5931\u8d25\uff0c\u8fd9\u4e2a\u5206\u5e03\u5f0f\u4e8b\u52a1\u7684\u6267\u884c\u90fd\u662f\u5931\u8d25\u7684\u3002 \u7edd\u5927\u6570\u573a\u666f\u4e2d\uff0c\u5bf9\u4e00\u81f4\u6027\u8981\u6c42\u6ca1\u90a3\u4e48\u9ad8\uff0c\u5e76\u4e0d\u9700\u8981\u4fdd\u8bc1\u5f3a\u4e00\u81f4\u6027\uff0c\u77ed\u6682\u7684\u4e0d\u4e00\u81f4\u4e5f\u80fd\u63a5\u6536\uff0c\u6700\u540e\u80fd\u4fdd\u8bc1\u6570\u636e\u662f\u6b63\u786e\u7684\u5c31OK\u3002\u4e5f\u5c31\u662f\u8bf4\u6211\u4eec\u53ef\u4ee5\u7528 \u6700\u7ec8\u4e00\u81f4\u6027 \u65b9\u6848\u6765\u4fdd\u8bc1\u6570\u636e\u7684\u4e00\u81f4\u6027\u3002 \u53e6\u5916\u8981\u63d0\u5230\u7684\u5c31\u662f TCC \u534f\u8bae\uff08\u4e09\u9636\u6bb5\u63d0\u4ea4\u534f\u8bae\uff09\uff0c\u4ed6\u662f\u9488\u5bf9\u4e8c\u9636\u6bb5\u63d0\u4ea4\u4e2d\u7684\uff1a\u534f\u8c03\u8005\u6545\u969c\uff0c\u53c2\u4e0e\u8005\u957f\u671f\u9501\u5b9a\u8d44\u6e90\u7684 \u75db\u70b9 \u800c\u51fa\u7684\u534f\u8bae\u3002\u5f15\u5165\u4e86\u8be2\u95ee\u9636\u6bb5\u548c\u8d85\u65f6\u673a\u5236\uff0c\u51cf\u5c11\u8d44\u6e90\u88ab\u957f\u65f6\u95f4\u9501\u5b9a\u3002\u4f46\u662f\u9700\u8981\u66f4\u591a\u7684\u6d88\u606f\u8fdb\u884c\u534f\u5546\uff0c\u589e\u52a0\u4e86\u7cfb\u7edf\u8d1f\u8f7d\u548c\u54cd\u5e94\u5ef6\u8fdf\uff0c\u6240\u4ee5\u4e09\u9636\u6bb5\u63d0\u4ea4\u534f\u8bae\u5f88\u5c11\u88ab\u4f7f\u7528\u3002 NOTE: \u76f8\u6bd4\u4e8ePaxos\u534f\u8bae\uff0c2PC\u662f\u6bd4\u8f83\u8106\u5f31\u7684 \u4e09\u3001\u592a\u6781\u7684\u67d4 3.1 BASE \u7684\u67d4 \u8bb2\u4e86\u592a\u6781\u7684\u521a\uff0c\u4e0b\u9762\u6765\u8bb2\u592a\u6781\u7684\u67d4\u3002\u8c08\u5230\u5206\u5e03\u5f0f\u4e8b\u52a1\u7684\u67d4\uff0c\u4e00\u5b9a\u4f1a\u63d0\u5230 BASE \u7406\u8bba\uff0c\u4fd7\u79f0 \u67d4\u6027\u4e8b\u52a1 \u3002 BASE \u7406\u8bba\u662f CAP \u7406\u8bba\u4e2d AP \u7684\u6269\u5c55\u3002\u5927\u90e8\u5206\u4e92\u8054\u7f51\u5206\u5e03\u5f0f\u7cfb\u7edf\u90fd\u5f3a\u8c03\u53ef\u7528\u6027\uff0c\u90fd\u4f1a\u8003\u8651\u5f15\u5165 BASE \u652f\u6301\u3002\u8fd9\u4e2a\u7406\u8bba\u975e\u5e38\u975e\u5e38\u91cd\u8981\uff0c\u6211\u8981\u544a\u8bc9\u4f60\u7684\u662f\uff0c\u638c\u63e1\u4e86\u8fd9\u4e2a\u7406\u8bba\uff0c\u8bbe\u8ba1\u51fa\u7b26\u5408\u81ea\u5df1\u4e1a\u52a1\u7684\u5206\u5e03\u5f0f\u67b6\u6784\u4e5f\u4f1a\u53d8\u5f97\u5bb9\u6613\u5f88\u591a\uff0c\u800c\u4e0d\u662f\u6478\u4e0d\u7740\u5934\u8111\u3002 BASE \u7684\u6838\u5fc3 \uff1a 1\u3001\u57fa\u672c\u53ef\u7528 BA \uff08Basically Available\uff09 NOTE: \u9700\u8981\u6ce8\u610f\uff0c\u4e0d\u662f\u767e\u5206\u4e4b\u767e\u7684\u53ef\u7528 2\u3001\u8f6f\u72b6\u6001 S \uff08Soft state\uff09 NOTE: \u5982\u4f55\u7406\u89e3\uff1f 3\u3001\u6700\u7ec8\u4e00\u81f4\u6027 E \uff08Eventually consistent\uff09 \u90a3\u4e3a\u4ec0\u4e48\u53eb\u5b83\u67d4\u6027\u4e8b\u52a1\uff1f\u5176\u5b9e\u5b83\u548c ACID \u662f\u76f8\u5bf9\u7684\uff0c\u4e0d\u9700\u8981\u4fdd\u8bc1**\u5f3a\u4e00\u81f4\u6027**\u3002 3.3 \u57fa\u672c\u53ef\u7528 NOTE: Redis cluster\u5c31\u662fBASE\u7684 \u600e\u4e48\u7406\u89e3\u57fa\u672c\u53ef\u7528\uff1f\u91cd\u70b9\u662f\u5728\u8fd9\u4e2a\u57fa\u672c\uff0c\u8fd9\u4e2a\u7406\u8bba\u5e76\u6ca1\u6709\u544a\u8bc9\u6211\u4eec\u600e\u4e48\u5b9a\u4e49\u57fa\u672c\uff0c\u8fd9\u662f\u4e00\u4e2a\u6a21\u7cca\u7684\u6982\u5ff5\u3002\u5176\u5b9e\u5c31\u662f\u8981 \u67d4 \u5230\u4ec0\u4e48\u7a0b\u5ea6\u3002 \u5728\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u628a\u57fa\u672c\u53ef\u7528\u7406\u89e3\u4e3a\u4fdd\u8bc1\u6838\u5fc3\u529f\u80fd\u53ef\u7528\uff0c\u5141\u8bb8\u635f\u5931\u90e8\u5206\u529f\u80fd\u7684\u53ef\u7528\u6027\u3002 \u57fa\u672c\u53ef\u7528 \u53ef\u4ee5\u7528\u56db\u79cd\u65b9\u6848\u6765\u5b9e\u73b0\u3002 1\u3001 \u6d41\u91cf\u524a\u5cf0 \uff1a\u6bd4\u5982\u591a\u4e2a\u79d2\u6740\u573a\u6b21\uff0c\u67d0\u4e1c\u7684 8 \u70b9\u79d2\u6740\u573a\uff0c12 \u70b9\u7684\u79d2\u6740\u573a\u3002 NOTE: \u53c2\u89c1: jianshu \u9ad8\u5e76\u53d1\u67b6\u6784\u7cfb\u5217\uff1a\u4ec0\u4e48\u662f\u6d41\u91cf\u524a\u5cf0\uff1f\u5982\u4f55\u89e3\u51b3\u79d2\u6740\u4e1a\u52a1\u7684\u524a\u5cf0\u573a\u666f zhihu \u79d2\u6740\u4e4b\u6d41\u91cf\u524a\u5cf0 2\u3001 \u5ef6\u8fdf\u54cd\u5e94 \uff1a\u6bd4\u5982\u53cc 11 \u671f\u95f4\u67d0\u5546\u57ce\u521b\u5efa\u7684\u8ba2\u5355\uff0c\u4f1a\u63d0\u793a\u5ba2\u6237\u8ba2\u5355\u6b63\u5728\u521b\u5efa\u4e2d\uff0c\u53ef\u80fd\u9700\u8981\u7b49\u4e2a\u5341\u51e0\u79d2\u3002 3\u3001 \u4f53\u9a8c\u964d\u7ea7 \uff1a\u6bd4\u5982\u67d0\u6b21\u6bd4\u8d5b\u6d3b\u52a8\uff0c\u6709\u5927\u91cf\u7528\u6237\u8fdb\u6d3b\u52a8\u9875\u67e5\u770b\u56fe\u7247\uff0c\u8fd9\u4e2a\u65f6\u5019\uff0c\u5927\u91cf\u56fe\u7247\u56e0\u4e3a\u7f51\u7edc\u8d85\u65f6\u800c\u65e0\u6cd5\u663e\u793a\uff0c\u8fd9\u4e2a\u65f6\u5019\u5c31\u53ef\u4ee5\u8003\u8651\u66ff\u6362\u539f\u6709\u56fe\u7247\uff0c\u8fd4\u56de\u6e05\u6670\u5ea6\u6ca1\u6709\u90a3\u4e48\u9ad8\u6216\u56fe\u7247\u6bd4\u8f83\u5c0f\u7684\u56fe\u7247\u3002 4\u3001 \u8fc7\u8f7d\u4fdd\u62a4 \uff1a\u6bd4\u5982\u6211\u4eec\u5e38\u7528\u7684\u6d88\u606f\u961f\u5217\u5360\u6ee1\u4e86\uff0c\u53ef\u4ee5\u8003\u8651\u4e22\u5f03\u540e\u6765\u7684\u8bf7\u6c42\uff0c\u6216\u6e05\u9664\u961f\u5217\u4e2d\u7684\u4e00\u4e9b\u8bf7\u6c42\uff0c\u4fdd\u62a4\u7cfb\u7edf\u4e0d\u8fc7\u8f7d\uff0c\u4f46\u8fd9\u90fd\u9700\u8981\u7ed3\u5408\u81ea\u8eab\u7684\u4e1a\u52a1\u573a\u666f\u6765\u8bbe\u8ba1\u3002 3.4 \u6700\u7ec8\u4e00\u81f4\u6027 \u6700\u7ec8\u4e00\u81f4\u6027 \uff1a\u7cfb\u7edf\u4e2d\u7684\u6240\u6709\u7684\u6570\u636e\u526f\u672c\u5728\u7ecf\u8fc7\u4e00\u6bb5\u65f6\u95f4\u7684\u540c\u6b65\u540e\uff0c\u6700\u7ec8\u80fd\u591f\u8fbe\u5230\u4e00\u4e2a\u4e00\u81f4\u7684\u72b6\u6001\u3002\u6700\u7ec8\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e00\u4e2a\u77ed\u6682\u7684\u5ef6\u8fdf\u3002 \u6700\u7ec8\u4e00\u81f4\u6027\u5728\u975e\u5e38\u591a\u7684\u4e92\u8054\u7f51\u4e1a\u52a1\u4e2d\u91c7\u7528\u3002\u4f46\u662f\u8ddf\u94b1\u6253\u4ea4\u9053\u6216\u91d1\u878d\u7cfb\u7edf\u4f1a\u91c7\u7528\u5f3a\u4e00\u81f4\u6027\u6216\u4e8b\u52a1\u3002 \u524d\u9762\u63d0\u5230\u4e86 ACID \u7684 \u5f3a\u4e00\u81f4\u6027 \uff0c\u800c \u6700\u7ec8\u4e00\u81f4\u6027 \u548c\u5b83\u662f\u4ec0\u4e48\u5173\u7cfb\uff1f \u5f3a\u4e00\u81f4\u6027\u5176\u5b9e\u4e5f\u662f\u6700\u7ec8\u4e00\u81f4\u6027\u7684\u4e00\u79cd\u3002\u90a3\u6700\u7ec8\u4e00\u81f4\u6027\u600e\u4e48\u7406\u89e3\uff1f\u5f3a\u4e00\u81f4\u6027\u53ef\u4ee5\u770b\u4f5c\u4e0d\u5b58\u5728\u5ef6\u8fdf\u7684\u4e00\u81f4\u6027\u3002\u5982\u679c\u65e0\u6cd5\u5bb9\u5fcd\u5ef6\u8fdf\u5c31\u7528\u5f3a\u4e00\u81f4\u6027\uff0c\u5426\u5219\u5c31\u7528\u6700\u7ec8\u4e00\u81f4\u6027\u3002 \u4e94\u3001\u603b\u7ed3 1\u3001\u592a\u6781\u62f3\u5206\u4e3a\u9634\u548c\u9633\u4e24\u65b9\u9762\uff0c\u5c31\u5982 CAP \u4e2d\u7684 C \u548c A\u3002 2\u3001ACID \u662f\u4f20\u7edf\u6570\u636e\u5e93\u7684\u8bbe\u8ba1\u7406\u5ff5\uff0c\u8ffd\u6c42\u5f3a\u4e00\u81f4\u6027\u3002\u56db\u4e2a\u6307\u6807\uff1a\u539f\u5b50\u6027\u3001\u4e00\u81f4\u6027\u3001\u9694\u79bb\u6027\u3001\u6301\u4e45\u6027\u3002\u662f CAP \u4e2d CP \u7684\u5ef6\u4f38\u3002 NOTE: \u603b\u7ed3\u5f97\u975e\u5e38\u597d 3\u3001BASE \u7406\u8bba\u662f CAP \u4e2d\u4e00\u81f4\u6027\u548c\u53ef\u7528\u6027\u6743\u8861\u7684\u7ed3\u679c\u3002\u662f CAP \u4e2d\u7684 AP \u7684\u5ef6\u4f38\u3002\u6ce8\u91cd\u53ef\u7528\u6027\u548c\u6027\u80fd\u4f18\u5148\uff0c\u6839\u636e\u4e1a\u52a1\u7684\u573a\u666f\u7279\u70b9\uff0c\u5b9e\u73b0\u5f39\u6027\u7684\u57fa\u672c\u53ef\u7528\uff0c\u7136\u540e\u5b9e\u73b0\u6570\u636e\u7684\u6700\u7ec8\u4e00\u81f4\u6027\u3002 4\u3001BASE \u7406\u8bba\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\uff0c\u89e3\u51b3\u4e86\u4e8b\u52a1\u6027\u7cfb\u7edf\u5728\u6027\u80fd\u3001\u5bb9\u9519\u3001\u53ef\u7528\u6027\u7b49\u65b9\u9762\u7684\u75db\u70b9\u3002 5\u3001BASE \u7406\u8bba\u5728 NoSQL \u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u662f NoSQL \u7cfb\u7edf\u8bbe\u8ba1\u7684\u4e8b\u5b9e\u4e0a\u7684\u7406\u8bba\u652f\u6491\u3002 NOTE: Redis\u5c31\u662f\u5178\u578b\u7684BASE","title":"Introduction"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/02-%E7%94%A8%E5%A4%AA%E6%9E%81%E6%8B%B3%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/#csdn","text":"NOTE: \u4e00\u3001\u867d\u7136\u539f\u6587\u4f7f\u7528\u4e86\u5f88\u591a\u7c7b\u6bd4\uff0c\u5176\u5b9e\u6838\u5fc3\u601d\u60f3\u662f\u975e\u5e38\u7b80\u5355\u7684: CP \u521a AP \u67d4 \u4e8c\u3001tradeoff \u4e09\u3001\u539f\u6587\u8fd8\u4ecb\u7ecd\u4e862PC\u30013PC","title":"csdn \u7528\u592a\u6781\u62f3\u8bb2\u5206\u5e03\u5f0f\u7406\u8bba\uff0c\u771f\u8212\u670d\uff01"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/02-%E7%94%A8%E5%A4%AA%E6%9E%81%E6%8B%B3%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/#_1","text":"NOTE: \u7b80\u800c\u8a00\u4e4b: CAP 1\u3001CP \u521a 2\u3001AP \u67d4 CAP \u7406\u8bba\u662f\u5bf9\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u7279\u6027\u505a\u4e86\u4e00\u4e2a\u9ad8\u5ea6\u7684\u62bd\u8c61\uff0c\u53d8\u6210\u4e86\u4e09\u5927\u6307\u6807\uff1a 1\u3001\u4e00\u81f4\u6027\uff08Consistency\uff09 2\u3001\u53ef\u7528\u6027\uff08Availability\uff09 3\u3001\u5206\u533a\u5bb9\u9519\u6027\uff08Partition Tolerance\uff09","title":"\u4e00\u3001\u592a\u6781\u7684\u4e24\u9762"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/02-%E7%94%A8%E5%A4%AA%E6%9E%81%E6%8B%B3%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/#_2","text":"\u5206\u5e03\u5f0f\u4e2d\u7684\u4e00\u81f4\u6027\uff0c\u6211\u4eec\u53ef\u4ee5\u7406\u89e3\u4e3a\u5ba2\u6237\u7aef\u7684\u6bcf\u6b21 \u8bfb\u64cd\u4f5c \uff0c\u4e0d\u7ba1\u8bbf\u95ee\u7684\u662f\u54ea\u4e2a\u51e0\u70b9\uff0c\u8981\u4e48\u8bfb\u5230\u7684\u90fd\u662f\u540c\u4e00\u4efd\u6700\u65b0\u5199\u5165\u7684\u6570\u636e\uff0c\u8981\u4e48\u8bfb\u53d6\u5931\u8d25\u3002\u8fd9\u5c31\u5f88\u521a\u4e86\uff0c\u4e0d\u80fd\u8bf4\u8fd9\u79cd \u521a \u4e0d\u597d\uff0c\u5728\u5f88\u591a\u573a\u666f\u4e2d\uff0c\u4e5f\u786e\u5b9e\u9700\u8981\u4fdd\u8bc1\u9ad8\u5ea6\u7684\u4e00\u81f4\u6027\u3002 \u4e3a\u4e86\u5e2e\u52a9\u5927\u5bb6\u7406\u89e3\u4e00\u81f4\u6027\uff0c\u6211\u4e3e\u4e2a \u501a\u5929\u5c60\u9f99\u8bb0 \u7684\u6545\u4e8b\uff1a \u516d\u5927\u6d3e\u56f4\u653b\u5149\u660e\u9876 \u3002","title":"\u4e00\u81f4\u6027"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/02-%E7%94%A8%E5%A4%AA%E6%9E%81%E6%8B%B3%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/#11#cap","text":"","title":"1.1 \u7406\u89e3\u5206\u5e03\u5f0f\u4e2d\u7684 CAP"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/02-%E7%94%A8%E5%A4%AA%E6%9E%81%E6%8B%B3%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/#_3","text":"\u4e00\u81f4\u6027 \u5f3a\u8c03\u7684\u662f\u6570\u636e\u6b63\u786e\uff0c\u6bcf\u6b21\u8bfb\u53d6\u8282\u70b9\u4e2d\u7684\u6570\u636e\u90fd\u662f\u6700\u65b0\u5199\u5165\u7684\u6570\u636e\u3002\u8fd9\u4e2a\u6211\u79f0\u4f5c \u521a \u3002 \u4f46\u662f\u6211\u4eec\u751f\u4ea7\u7684\u96c6\u7fa4\u73af\u5883\u4e0b\u5982\u679c\u53d1\u751f\u5206\u533a\u6545\u969c\u65f6\uff08\u8282\u70b9\u5931\u8054\uff0c\u8282\u70b9\u65e0\u6cd5\u54cd\u5e94\uff0c\u8282\u70b9\u65e0\u6cd5\u5199\u5165\u6570\u636e\uff09\uff0c\u5ba2\u6237\u7aef\u67e5\u8be2\u8282\u70b9\u65f6\uff0c\u6211\u4eec\u4e0d\u80fd\u8fd4\u56de\u9519\u8bef\u4fe1\u606f\u7ed9\u5ba2\u6237\u7aef\u3002\u6bd4\u5982\u8bf4\u4e1a\u52a1\u96c6\u7fa4\u4e2d\u7684\u4e00\u4e9b\u5173\u952e\u7cfb\u7edf\uff0c\u5982\u6ce8\u518c\u4e2d\u5fc3\uff0c\u4e0d\u80fd\u56e0\u4e3a\u67d0\u4e2a\u8282\u70b9\u5931\u8054\u4e86\uff0c\u5c31\u4e0d\u54cd\u5e94\u6700\u65b0\u7684\u6570\u636e\u3002\u90a3\u4e48\u76f8\u5173\u7684\u4e1a\u52a1\u4e5f\u83b7\u53d6\u4e0d\u5230\u6b63\u786e\u7684\u6ce8\u518c\u4fe1\u606f\u800c\u5bfc\u81f4\u7cfb\u7edf\u762b\u75ea\u3002 NOTE: \u5982\u679c\u6309\u7167 CP\uff0c\u5219\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u662f\u4f1a\" \u8fd4\u56de\u9519\u8bef\u4fe1\u606f\u7ed9\u5ba2\u6237\u7aef \"\u7684","title":"\u4e00\u81f4\u6027 \u521a"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/02-%E7%94%A8%E5%A4%AA%E6%9E%81%E6%8B%B3%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/#_4","text":"\u53ef\u7528\u6027 \u5c31\u6d3e\u4e0a\u7528\u573a\u4e86\uff0c\u727a\u7272\u6570\u636e\u51c6\u786e\u6027\uff0c\u6bcf\u4e2a\u8282\u70b9\u4f7f\u7528\u672c\u5730\u6570\u636e\u6765\u54cd\u5e94\u5ba2\u6237\u7aef\u7684\u8bf7\u6c42\u3002\u53e6\u5916\u5f53\u8282\u70b9\u4e0d\u53ef\u7528\u65f6\uff0c\u53ef\u4ee5\u4f7f\u7528**\u5feb\u901f\u5931\u8d25\u7b56\u7565**\uff0c\u81f3\u5c11\u4e0d\u80fd\u8ba9\u670d\u52a1\u957f\u65f6\u95f4\u4e0d\u80fd\u54cd\u5e94\u53ef\u7528\u6027\u5f3a\u8c03\u7684\u662f\u670d\u52a1\u53ef\u7528\uff0c\u4e0d\u4fdd\u8bc1\u6570\u636e\u6b63\u786e\u3002\u8fd9\u4e2a\u6211\u79f0\u4f5c \u67d4 \u3002","title":"\u53ef\u7528\u6027 \u67d4"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/02-%E7%94%A8%E5%A4%AA%E6%9E%81%E6%8B%B3%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/#_5","text":"\u5206\u533a\u5bb9\u9519\u6027 \u7684\u542b\u4e49\u5c31\u662f\u8282\u70b9\u95f4\u51fa\u73b0\u4efb\u610f\u6570\u91cf\u7684\u6d88\u606f\u4e22\u5931\u6216\u9ad8\u5ef6\u8fdf\u7684\u65f6\u5019\uff0c\u7cfb\u7edf\u4ecd\u7136\u5728\u7ee7\u7eed\u5de5\u4f5c\u3002\u5206\u5e03\u5f0f\u7cfb\u7edf\u544a\u8bc9\u5ba2\u6237\u7aef\uff0c\u6211\u7684\u5185\u90e8\u4e0d\u8bba\u51fa\u73b0\u4ec0\u4e48\u6837\u7684\u6570\u636e\u540c\u6b65\u95ee\u9898\uff0c\u6211\u4f1a\u4e00\u76f4\u8fd0\u884c\u3002\u5f3a\u8c03\u7684\u662f\u96c6\u7fa4\u5806\u5206\u533a\u6545\u969c\u7684\u5bb9\u9519\u80fd\u529b\u3002","title":"\u5206\u533a\u5bb9\u9519\u6027"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/02-%E7%94%A8%E5%A4%AA%E6%9E%81%E6%8B%B3%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/#12#cap","text":"\u90a3\u4e48\u8fd9\u4e09\u4e2a\u6307\u6807\u53c8\u6709\u4ec0\u4e48\u5173\u7cfb\u5462\uff1f\u8fd9\u4e2a\u5c31\u662f\u6211\u4eec\u7ecf\u5e38\u542c\u5230\u7684 CAP \u7406\u8bba\u3002 C \u4ee3\u8868\u4e00\u81f4\u6027\uff08Consistency\uff09\uff0c A \u4ee3\u8868\u53ef\u7528\u6027\uff08Availability\uff09\u3001 P \u4ee3\u8868\u5206\u533a\u5bb9\u9519\u6027\uff08Partition Tolerance\uff09\u3002 \u5bf9\u4e8e\u5206\u5e03\u5f0f\u7cfb\u7edf\uff0cCAP \u4e09\u4e2a\u6307\u6807\u53ea\u80fd\u9009\u62e9\u5176\u4e2d\u4e24\u4e2a\u3002","title":"1.2 CAP \u4e09\u89d2"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/02-%E7%94%A8%E5%A4%AA%E6%9E%81%E6%8B%B3%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/#ca","text":"NOTE: \u5176\u5b9e\u8fd9\u79cd\u60c5\u51b5\u538b\u6839\u5c31\u4e0d\u662f\u5206\u5e03\u5f0f \u4fdd\u8bc1\u4e00\u81f4\u6027\u548c\u53ef\u7528\u6027\u3002\u5f53\u5206\u5e03\u5f0f\u7cfb\u7edf\u6b63\u5e38\u8fd0\u884c\u65f6\uff08\u5927\u90e8\u5206\u65f6\u5019\u6240\u5904\u7684\u72b6\u6001\uff09\uff0c\u8fd9\u4e2a\u65f6\u5019\u4e0d\u9700\u8981 P\uff0c\u90a3\u4e48 C \u548c A \u80fd\u591f\u540c\u65f6\u4fdd\u8bc1\u3002\u53ea\u6709\u5728\u53d1\u751f\u5206\u533a\u6545\u969c\u65f6\uff0c\u624d\u9700\u8981 P\uff0c\u8fd9\u4e2a\u65f6\u5019\u5c31\u53ea\u80fd\u5728 C \u548c A \u4e4b\u95f4\u505a\u51fa\u9009\u62e9\u3002 \u5178\u578b\u5e94\u7528 \uff1a\u5355\u673a\u7248\u90e8\u7f72\u7684 MySQL\u3002","title":"CA"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/02-%E7%94%A8%E5%A4%AA%E6%9E%81%E6%8B%B3%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/#cp","text":"\u4fdd\u8bc1\u6570\u636e\u7684\u4e00\u81f4\u6027\u548c\u5206\u533a\u5bb9\u9519\u6027\uff0c\u6bd4\u5982\u914d\u7f6e\u4fe1\u606f\uff0c\u5fc5\u987b\u4fdd\u8bc1\u6bcf\u4e2a\u8282\u70b9\u5b58\u7684\u90fd\u662f\u6700\u65b0\u7684\uff0c\u6b63\u786e\u7684\u6570\u636e\u3002\u6bd4\u5982 Raft \u7684\u5f3a\u4e00\u81f4\u6027\u7cfb\u7edf\uff0c\u4f1a\u5bfc\u81f4\u65e0\u6cd5\u6267\u884c\u8bfb\u64cd\u4f5c\u548c\u5199\u64cd\u4f5c\u3002 \u5178\u578b\u5e94\u7528 \uff1aEtcd\u3001Consul\u3001Hbase\u3002","title":"CP"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/02-%E7%94%A8%E5%A4%AA%E6%9E%81%E6%8B%B3%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/#ap","text":"\u4fdd\u8bc1\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u53ef\u7528\u6027\u548c\u5206\u533a\u5bb9\u9519\u6027\u3002\u7528\u6237\u8bbf\u95ee\u7cfb\u7edf\uff0c\u90fd\u80fd\u5f97\u5230\u76f8\u5e94\u6570\u636e\uff0c\u4e0d\u4f1a\u51fa\u73b0\u54cd\u5e94\u9519\u8bef\uff0c\u4f46\u662f\u53ef\u80fd\u4f1a\u8bfb\u5230\u65e7\u7684\u6570\u636e\u3002 \u5178\u578b\u5e94\u7528 \uff1aCassandra \u548c DynamoDB\u3002","title":"AP"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/02-%E7%94%A8%E5%A4%AA%E6%9E%81%E6%8B%B3%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/#_6","text":"NOTE: \u9009\u62e9consistency","title":"\u4e8c\u3001\u592a\u6781\u7684\u521a"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/02-%E7%94%A8%E5%A4%AA%E6%9E%81%E6%8B%B3%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/#21#acid","text":"NOTE: \u4e00\u3001\u6240\u8c13\"ACID\u7684\u521a\"\uff0c\u6307\u7684\u662f\u5b83\u5f3a\u8c03consistency \u4e8c\u3001\u8bfb\u8005\u80af\u5b9a\u4f1a\u601d\u8003CAP\u3001ACID\u7684\u5173\u7cfb\uff0c\u5728\"\u4e94\u3001\u603b\u7ed3\"\u4e2d\uff0c\u4f5c\u8005\u7ed9\u51fa\u4e86\u975e\u5e38\u597d\u7684\u603b\u7ed3: \"ACID \u662f\u4f20\u7edf\u6570\u636e\u5e93\u7684\u8bbe\u8ba1\u7406\u5ff5\uff0c\u8ffd\u6c42\u5f3a\u4e00\u81f4\u6027\u3002\u56db\u4e2a\u6307\u6807\uff1a\u539f\u5b50\u6027\u3001\u4e00\u81f4\u6027\u3001\u9694\u79bb\u6027\u3001\u6301\u4e45\u6027\u3002\u662f CAP \u4e2d CP \u7684\u5ef6\u4f38\u3002\" \u6700\u5f00\u59cb\u77e5\u9053 ACID \u662f\u7814\u7a76 SQL \u6570\u636e\u5e93\u7684\u65f6\u5019\uff0c \u539f\u5b50\u6027 \uff08Atomicity\uff09\u3001 \u4e00\u81f4\u6027 \uff08Consistency\uff09\u3001 \u9694\u79bb\u6027 \uff08Isolation\uff09\u3001 \u6301\u4e45\u6027 \uff08Durability\uff09\u3002 \u8fd9\u56db\u4e2a\u5c5e\u6027\u662f\u9488\u5bf9 \u4e8b\u52a1 \u800c\u8a00\u7684\uff0c\u800c\u4e8b\u52a1\u5c31\u662f\u4e3a\u5355\u4e2a\u5de5\u4f5c\u5355\u5143\u800c\u6267\u884c\u7684\u4e00\u7cfb\u5217\u64cd\u4f5c\u3002\u5982\u67e5\u8be2\u3001\u4fee\u6539\u6570\u636e\u3001\u4fee\u6539\u6570\u636e\u5b9a\u4e49\u3002 \u4e8b\u52a1\u4e0d\u4ec5\u4ec5\u53ea\u7528\u5728\u6570\u636e\u5e93\u4e0a\uff0c\u8fd8\u53ef\u4ee5\u7528\u5728\u4e1a\u52a1\u7cfb\u7edf\u4e2d\uff0c\u6bd4\u5982\u53d1\u5238\u540e\u6263\u51cf\u5e93\u5b58\uff0c\u8fd9\u79cd\u4e1a\u52a1\u573a\u666f\u53ef\u4ee5\u5b9a\u4e49\u4e3a\u4e00\u4e2a\u4e8b\u52a1\u3002\u5355\u673a\u573a\u666f\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u52a0\u9501\u3001\u65f6\u95f4\u5e8f\u5217\u7b49\u673a\u5236\u6765\u4fdd\u8bc1\u5355\u4e2a\u8282\u70b9\u4e0a\u7684 ACID \u7279\u6027\uff0c\u4f46\u65e0\u6cd5\u4fdd\u8bc1\u8282\u70b9\u95f4\u64cd\u4f5c\u7684 ACID \u7279\u6027\u3002 \u90a3\u4e48\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e0b\u8be5\u5982\u4f55\u89e3\u51b3\u4e8b\u52a1\u95ee\u9898\u5462\uff1f\u8fd9\u4e5f\u662f\u9762\u8bd5\u4e2d\u7ecf\u5e38\u9047\u5230\u7684\u9898\u3002\u5206\u5e03\u5f0f\u4e8b\u52a1\u534f\u8bae\u5927\u5bb6\u4e00\u5b9a\u542c\u8fc7\uff0c\u6bd4\u5982 \u4e8c\u9636\u6bb5\u63d0\u4ea4\u534f\u8bae \u548c TCC \u534f\u8bae \uff0c\u4e0b\u9762\u6211\u8fd8\u662f\u7528 \u516d\u5927\u6d3e\u56f4\u653b\u5149\u660e\u9876 \u6545\u4e8b\u6765\u8bb2\u89e3\u4e8c\u9636\u6bb5\u534f\u8bae\u3002 NOTE: \"TCC\u534f\u8bae\" \u662f\u63073PC\uff0c\u5373three-phase commit protocol","title":"2.1 ACID \u7684\u521a"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/02-%E7%94%A8%E5%A4%AA%E6%9E%81%E6%8B%B3%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/#22","text":"NOTE: \u7ed3\u5408\u5177\u4f53\u4f8b\u5b50\u6765\u8bf4\u660e2PL \u5ce8\u7709\u6d3e \u60f3\u6c47\u96c6 \u5c11\u6797\u6d3e \u3001 \u6b66\u5f53\u6d3e \u3001 \u6606\u4ed1\u6d3e \u660e\u5929\u4e00\u8d77\u8fdb\u653b \u5149\u660e\u9876 \u3002\u5982\u679c\u6709\u4e00\u65b9\u4e0d\u540c\u610f\u8fdb\u653b\uff0c\u6216\u8005\u8fdb\u653b\u65f6\u673a\u4e0d\u4e00\u81f4\uff0c\u5219\u9700\u8981\u53d6\u6d88\u6574\u4e2a\u884c\u52a8\u8ba1\u5212\u3002\u5c11\u6797\u6d3e\u3001\u6b66\u5f53\u6d3e\u3001\u6606\u4ed1\u6d3e\u8fdb\u653b\u5149\u660e\u9876\u8fd9\u4e00\u7ec4\u884c\u52a8\u53ef\u4ee5\u770b\u6210\u662f \u4e00\u4e2a\u5206\u5e03\u5f0f\u4e8b\u52a1 \uff0c \u8981\u4e48\u5168\u90e8\u6267\u884c\u3001\u8981\u4e48\u5168\u90e8\u4e0d\u6267\u884c \u3002 NOTE: \u4e00\u3001\u9700\u8981\u6240\u6709\u8282\u70b9\u90fd\u540c\u610f\uff0c\u800c\u4e0d\u662f\u5c11\u6570\u670d\u4ece\u591a\u6570\uff0c\u56e0\u6b64\u9700\u8981\u6709\u4e00\u4e2a\"Propose\"\u9636\u6bb5\uff0c\u5373\u63d0\u8bae\u9636\u6bb5\uff0c\u5728\u63d0\u8bae\u9636\u6bb5\uff0c\u53ef\u4ee5\u770b\u51fa\u662f\u5426\u6240\u6709\u7684\u8282\u70b9\u90fd\u540c\u610f\uff0c\u5982\u679c\u4e0d\u662f\uff0c\u5219\u901a\u77e5\u6240\u6709\u7684\u8282\u70b9\u53d6\u6d88\uff1b\u5982\u679c\u662f\uff0c\u5219\u901a\u77e5\u6240\u6709\u7684\u8282\u70b9commit\uff1b \u4e8c\u3001\u5b83\u7684\u5f3a\u4e00\u81f4\u4f53\u73b0\u5728: \u6240\u6709\u7684node\u90fd\u9700\u8981\u4fdd\u6301\u4e00\u81f4: \"\u5982\u679c\u6709\u4e00\u65b9\u4e0d\u540c\u610f\u8fdb\u653b\uff0c\u6216\u8005\u8fdb\u653b\u65f6\u673a\u4e0d\u4e00\u81f4\uff0c\u5219\u9700\u8981\u53d6\u6d88\u6574\u4e2a\u884c\u52a8\u8ba1\u5212\" \u5982\u4e0b\u56fe\u6240\u793a\uff1a","title":"2.2 \u56f4\u653b\u5149\u660e\u9876"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/02-%E7%94%A8%E5%A4%AA%E6%9E%81%E6%8B%B3%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/#23","text":"\u5728\u4e8c\u9636\u6bb5\u63d0\u4ea4\u534f\u8bae\u4e2d\uff0c\u706d\u7edd\u5e08\u592a\u5148\u7ed9\u5c11\u6797\u6d3e\u53d1\u9001\u8fdb\u653b\u7684\u6d88\u606f\uff0c\u5c11\u6797\u6d3e\u4f5c\u4e3a**\u534f\u8c03\u8005**\u7684\u8eab\u4efd\uff0c\u7531\u5c11\u6797\u6d3e\u8054\u7cfb\u6b66\u5f53\u6d3e\u548c\u6606\u4ed1\u6d3e\u662f\u8fdb\u653b\u8fd8\u662f\u64a4\u9000\u3002 NOTE: \u4e00\u3001\u6709\"\u534f\u8c03\u8005\"\uff0c\u663e\u7136\u5c31\u5b58\u5728single point failure\u7684\u98ce\u9669 \u4e8c\u3001\"\u534f\u8c03\u8005\"\u5373\"coordinator\" \u4e09\u3001\u8fd9\u4e00\u6bb5\u5173\u4e8e2PL\u7684\u4ecb\u7ecd\uff0c\u6ca1\u6709\u8303\u658c zhihu \u6f2b\u8bdd\u5206\u5e03\u5f0f\u7cfb\u7edf\u5171\u8bc6\u534f\u8bae: 2PC/3PC\u7bc7 \u7684\u597d \u4e8c\u9636\u6bb5\u5c31\u662f\u8bf4\u6709\u4e24\u4e2a\u9636\u6bb5\uff0c 1.\u63d0\u4ea4\u8bf7\u6c42\u9636\u6bb5 \uff08\u6295\u7968\u9636\u6bb5\uff09\uff0c 2.\u63d0\u4ea4\u6267\u884c\u9636\u6bb5\uff08\u5b8c\u6210\u9636\u6bb5\uff09 \u3002","title":"2.3 \u4e8c\u9636\u6bb5\u63d0\u4ea4\u534f\u8bae"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/02-%E7%94%A8%E5%A4%AA%E6%9E%81%E6%8B%B3%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/#propose","text":"1\u3001 \u7b2c\u4e00\u6b65 \uff1a\u5c11\u6797\u6d3e\u4f5c\u4e3a\u534f\u8c03\u8005\u5206\u522b\u7ed9\u6b66\u5f53\u6d3e\u548c\u6606\u4ed1\u6d3e\u53d1\u9001\u6d88\u606f\uff1a \u201c\u660e\u5929\u8fdb\u653b\u5149\u660e\u9876\uff0c\u53ef\u884c\uff1f\u201d 2\u3001 \u7b2c\u4e8c\u6b65 \uff1a\u5c11\u6797\u6d3e\u3001\u6b66\u5f53\u6d3e\u3001\u6606\u4ed1\u6d3e\u5206\u522b\u8bc4\u4f30\u660e\u5929\u662f\u5426\u80fd\u8fdb\u653b\u5149\u660e\u9876\uff0c\u5982\u679c\u80fd\uff0c\u5c31\u9884\u7559\u65f6\u95f4\u5e76\u9501\u5b9a\uff0c\u4e0d\u518d\u5b89\u6392\u5176\u4ed6\u7684\u8fdb\u653b\u4e8b\u9879\u3002 NOTE: \u6b64\u5904\u4f1a\u9501\u5b9a\u8d44\u6e90 3\u3001 \u7b2c\u4e09\u6b65 \uff1a\u5c11\u6797\u6d3e\u5f97\u5230\u5168\u90e8\u7684\u56de\u590d\u7ed3\u679c\uff0c\u5305\u62ec\u5c11\u6797\u6d3e\u81ea\u5df1\u7684\u8bc4\u4f30\u7ed3\u679c\u3002\u6700\u540e\u4e09\u65b9\u7684\u7ed3\u679c\u90fd\u662f \u53ef\u884c \u3002 \u5982\u4e0b\u56fe\u6240\u793a\uff1a NOTE: \u4e0a\u8ff0\u56fe\u4e2d\u5c55\u793a\u7684\u6267\u884c\u987a\u5e8f: \u7b2c\u4e00\u6b65: 1.1\u30011.2 \u7b2c\u4e8c\u6b65: 2.1\u30012.2\u30012.3 \u7b2c\u4e09\u6b65: 3.2\u30013.3 \u53ef\u4ee5\u770b\u51fa\uff0c\u4f5c\u8005\u7684\u753b\u56fe\u8fd8\u662f\u975e\u5e38\u8ba4\u771f\u7684\uff0c\u548c\u524d\u9762\u7684\u6b65\u9aa4\u662f\u5bf9\u5e94\u7684","title":"\u9636\u6bb5\u4e00\uff1a\u63d0\u4ea4\u8bf7\u6c42\u9636\u6bb5(Propose)\uff1a"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/02-%E7%94%A8%E5%A4%AA%E6%9E%81%E6%8B%B3%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/#commit","text":"1\u3001 \u7b2c\u4e00\u6b65 \uff1a\u5c11\u6797\u6d3e\u7edf\u8ba1\u81ea\u5df1\u3001\u6606\u4ed1\u6d3e\u548c\u6b66\u5f53\u6d3e\u7684\u6d88\u606f\uff0c\u90fd\u662f \u53ef\u4ee5\u8fdb\u653b \uff0c\u6240\u4ee5\u53ef\u4ee5\u6267\u884c\u5206\u5e03\u5f0f\u4e8b\u52a1\uff0c\u8fdb\u653b\u5149\u660e\u9876\u3002 NOTE: \u9700\u8981\u6ce8\u610f\u7684\u662f: \u53ef\u80fd\u4e0d\u662f\u90fd\u540c\u610f\uff0c\u672c\u8282\u6240\u8ba8\u8bba\u7684\u662f\u6267\u884c\u7684\u60c5\u51b5\uff1b 2\u3001 \u7b2c\u4e8c\u6b65 \uff1a\u5c11\u6797\u6d3e\u901a\u77e5\u6606\u4ed1\u6d3e\u548c\u6b66\u5f53\u6d3e\u8fdb\u653b\u5149\u660e\u9876\u3002 3\u3001 \u7b2c\u4e09\u6b65 \uff1a\u5c11\u6797\u6d3e\u3001\u6606\u4ed1\u6d3e\u3001\u6b66\u5f53\u6d3e\u53ec\u96c6\u624b\u4e0b\u5f1f\u5b50\uff0c\u8fdb\u653b\u5149\u660e\u9876\uff08\u6267\u884c\u4e8b\u52a1\uff09\u3002 4\u3001 \u7b2c\u56db\u6b65 \uff1a\u6606\u4ed1\u6d3e\u3001\u6b66\u5f53\u6d3e\u5c06 \u662f\u5426\u5df2\u53d1\u8d77\u8fdb\u653b \u544a\u8bc9\u5c11\u6797\u6d3e\u3002 5\u3001 \u7b2c\u4e94\u6b65 \uff1a\u5c11\u6797\u6d3e\u6c47\u603b\u81ea\u5df1\u3001\u6606\u4ed1\u6d3e\u3001\u6b66\u5f53\u6d3e\u7684\u8fdb\u653b\u7ed3\u679c\u7ed9\u706d\u7edd\u5e08\u592a\u3002\u8fd9\u6837\u706d\u7edd\u5e08\u592a\u770b\u5230\u7684\u5c31\u662f\u7edf\u4e00\u7684\u4f5c\u6218\u8ba1\u5212\u3002 NOTE: \u7b2c\u4e00\u6b65: 1 \u7b2c\u4e8c\u90e8: 2.1\u30012.2 \u7b2c\u4e09\u6b65: 3.1\u30013.2\u30013.3 \u7b2c\u56db\u6b65: 4.1\u30014.2 \u7b2c\u4e94\u6b65: 5","title":"\u9636\u6bb5\u4e8c\uff1a\u63d0\u4ea4\u6267\u884c\u9636\u6bb5(Commit)\uff1a"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/02-%E7%94%A8%E5%A4%AA%E6%9E%81%E6%8B%B3%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/#_7","text":"1\u3001\u53ef\u4ee5\u5c06\u706d\u7edd\u5e08\u592a\u5f53\u505a\u5ba2\u6237\u7aef\u3002\u5c11\u6797\u6d3e\u3001\u6b66\u5f53\u6d3e\u3001\u6606\u4ed1\u6d3e\u5f53\u505a\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u4e09\u4e2a\u8282\u70b9\u3002\u5c11\u6797\u6d3e\u4f5c\u4e3a\u534f\u8c03\u8005\u3002 2\u3001\u5c06\u8bc4\u4f30\u662f\u5426\u80fd\u8fdb\u653b\u5149\u660e\u9876\u4ee5\u53ca\u9884\u7559\u65f6\u95f4\u53ef\u4ee5\u7406\u89e3\u4e3a\u9700\u8981\u64cd\u4f5c\u7684\u5bf9\u8c61\u548c\u5bf9\u8c61\u72b6\u6001\uff0c\u662f\u5426\u5df2\u7ecf\u51c6\u5907\u597d\u4e86\uff0c\u80fd\u5426\u63d0\u4ea4\u65b0\u7684\u64cd\u4f5c\u3002 3\u3001\u53d1\u9001\u6d88\u606f\u3001\u98de\u9e3d\u4f20\u4e66\u53ef\u4ee5\u7406\u89e3\u4e3a\u7f51\u7edc\u6d88\u606f\u3002 4\u3001\u7b2c\u4e00\u4e2a\u9636\u6bb5\u4e2d\uff0c\u6bcf\u4e2a\u53c2\u4e0e\u8005\u6295\u7968\u8868\u51b3\u4e8b\u52a1\u662f\u653e\u5f03\u8fd8\u662f\u63d0\u4ea4\uff0c\u4e00\u65e6\u6295\u7968\u8981\u6c42\u63d0\u4ea4\u4e8b\u52a1\uff0c\u90a3\u4e48\u5c31\u4e0d\u5141\u8bb8\u653e\u5f03\u4e8b\u52a1\u3002 5\u3001\u7b2c\u4e8c\u4e2a\u9636\u6bb5\u4e2d\uff0c\u6bcf\u4e2a\u53c2\u4e0e\u8005\u6267\u884c\u6700\u7ec8\u7edf\u4e00\u7684\u51b3\u5b9a\uff0c\u63d0\u4ea4\u4e8b\u52a1\u6216\u8005\u653e\u5f03\u4e8b\u52a1\u3002\u8fd9\u4e2a\u5c31\u662f ACID \u7684\u539f\u5b50\u6027\u3002 NOTE: \u8fd9\u5c31\u662ftransaction 6\u3001\u7b2c\u4e00\u4e2a\u9636\u6bb5\u4e2d\uff0c\u9700\u8981\u9884\u7559\u8d44\u6e90\uff0c\u9884\u7559\u671f\u95f4\uff0c\u5176\u4ed6\u4eba\u4e0d\u80fd\u64cd\u4f5c\u8fd9\u4e2a\u8d44\u6e90\u3002","title":"\u6ce8\u610f\uff1a"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/02-%E7%94%A8%E5%A4%AA%E6%9E%81%E6%8B%B3%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/#24","text":"ACID \u7279\u6027\u662f CAP \u4e2d\u4e00\u81f4\u6027\u7684 \u8fb9\u754c \uff0c\u53ef\u4ee5\u79f0\u4f5c\u6700\u5f3a\u7684\u4e00\u81f4\u6027\uff0c\u5982\u679c\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u4e00\u81f4\u6027\uff0c\u5fc5\u7136\u4f1a\u5f71\u54cd\u5230 \u53ef\u7528\u6027 \u3002\u5982\u679c\u4e00\u4e2a\u8282\u70b9\u5931\u8d25\uff0c\u8fd9\u4e2a\u5206\u5e03\u5f0f\u4e8b\u52a1\u7684\u6267\u884c\u90fd\u662f\u5931\u8d25\u7684\u3002 \u7edd\u5927\u6570\u573a\u666f\u4e2d\uff0c\u5bf9\u4e00\u81f4\u6027\u8981\u6c42\u6ca1\u90a3\u4e48\u9ad8\uff0c\u5e76\u4e0d\u9700\u8981\u4fdd\u8bc1\u5f3a\u4e00\u81f4\u6027\uff0c\u77ed\u6682\u7684\u4e0d\u4e00\u81f4\u4e5f\u80fd\u63a5\u6536\uff0c\u6700\u540e\u80fd\u4fdd\u8bc1\u6570\u636e\u662f\u6b63\u786e\u7684\u5c31OK\u3002\u4e5f\u5c31\u662f\u8bf4\u6211\u4eec\u53ef\u4ee5\u7528 \u6700\u7ec8\u4e00\u81f4\u6027 \u65b9\u6848\u6765\u4fdd\u8bc1\u6570\u636e\u7684\u4e00\u81f4\u6027\u3002 \u53e6\u5916\u8981\u63d0\u5230\u7684\u5c31\u662f TCC \u534f\u8bae\uff08\u4e09\u9636\u6bb5\u63d0\u4ea4\u534f\u8bae\uff09\uff0c\u4ed6\u662f\u9488\u5bf9\u4e8c\u9636\u6bb5\u63d0\u4ea4\u4e2d\u7684\uff1a\u534f\u8c03\u8005\u6545\u969c\uff0c\u53c2\u4e0e\u8005\u957f\u671f\u9501\u5b9a\u8d44\u6e90\u7684 \u75db\u70b9 \u800c\u51fa\u7684\u534f\u8bae\u3002\u5f15\u5165\u4e86\u8be2\u95ee\u9636\u6bb5\u548c\u8d85\u65f6\u673a\u5236\uff0c\u51cf\u5c11\u8d44\u6e90\u88ab\u957f\u65f6\u95f4\u9501\u5b9a\u3002\u4f46\u662f\u9700\u8981\u66f4\u591a\u7684\u6d88\u606f\u8fdb\u884c\u534f\u5546\uff0c\u589e\u52a0\u4e86\u7cfb\u7edf\u8d1f\u8f7d\u548c\u54cd\u5e94\u5ef6\u8fdf\uff0c\u6240\u4ee5\u4e09\u9636\u6bb5\u63d0\u4ea4\u534f\u8bae\u5f88\u5c11\u88ab\u4f7f\u7528\u3002 NOTE: \u76f8\u6bd4\u4e8ePaxos\u534f\u8bae\uff0c2PC\u662f\u6bd4\u8f83\u8106\u5f31\u7684","title":"2.4 \u4e8c\u9636\u6bb5\u534f\u8bae\u5e26\u6765\u7684\u95ee\u9898"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/02-%E7%94%A8%E5%A4%AA%E6%9E%81%E6%8B%B3%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/#_8","text":"","title":"\u4e09\u3001\u592a\u6781\u7684\u67d4"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/02-%E7%94%A8%E5%A4%AA%E6%9E%81%E6%8B%B3%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/#31#base","text":"\u8bb2\u4e86\u592a\u6781\u7684\u521a\uff0c\u4e0b\u9762\u6765\u8bb2\u592a\u6781\u7684\u67d4\u3002\u8c08\u5230\u5206\u5e03\u5f0f\u4e8b\u52a1\u7684\u67d4\uff0c\u4e00\u5b9a\u4f1a\u63d0\u5230 BASE \u7406\u8bba\uff0c\u4fd7\u79f0 \u67d4\u6027\u4e8b\u52a1 \u3002 BASE \u7406\u8bba\u662f CAP \u7406\u8bba\u4e2d AP \u7684\u6269\u5c55\u3002\u5927\u90e8\u5206\u4e92\u8054\u7f51\u5206\u5e03\u5f0f\u7cfb\u7edf\u90fd\u5f3a\u8c03\u53ef\u7528\u6027\uff0c\u90fd\u4f1a\u8003\u8651\u5f15\u5165 BASE \u652f\u6301\u3002\u8fd9\u4e2a\u7406\u8bba\u975e\u5e38\u975e\u5e38\u91cd\u8981\uff0c\u6211\u8981\u544a\u8bc9\u4f60\u7684\u662f\uff0c\u638c\u63e1\u4e86\u8fd9\u4e2a\u7406\u8bba\uff0c\u8bbe\u8ba1\u51fa\u7b26\u5408\u81ea\u5df1\u4e1a\u52a1\u7684\u5206\u5e03\u5f0f\u67b6\u6784\u4e5f\u4f1a\u53d8\u5f97\u5bb9\u6613\u5f88\u591a\uff0c\u800c\u4e0d\u662f\u6478\u4e0d\u7740\u5934\u8111\u3002 BASE \u7684\u6838\u5fc3 \uff1a 1\u3001\u57fa\u672c\u53ef\u7528 BA \uff08Basically Available\uff09 NOTE: \u9700\u8981\u6ce8\u610f\uff0c\u4e0d\u662f\u767e\u5206\u4e4b\u767e\u7684\u53ef\u7528 2\u3001\u8f6f\u72b6\u6001 S \uff08Soft state\uff09 NOTE: \u5982\u4f55\u7406\u89e3\uff1f 3\u3001\u6700\u7ec8\u4e00\u81f4\u6027 E \uff08Eventually consistent\uff09 \u90a3\u4e3a\u4ec0\u4e48\u53eb\u5b83\u67d4\u6027\u4e8b\u52a1\uff1f\u5176\u5b9e\u5b83\u548c ACID \u662f\u76f8\u5bf9\u7684\uff0c\u4e0d\u9700\u8981\u4fdd\u8bc1**\u5f3a\u4e00\u81f4\u6027**\u3002","title":"3.1 BASE \u7684\u67d4"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/02-%E7%94%A8%E5%A4%AA%E6%9E%81%E6%8B%B3%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/#33","text":"NOTE: Redis cluster\u5c31\u662fBASE\u7684 \u600e\u4e48\u7406\u89e3\u57fa\u672c\u53ef\u7528\uff1f\u91cd\u70b9\u662f\u5728\u8fd9\u4e2a\u57fa\u672c\uff0c\u8fd9\u4e2a\u7406\u8bba\u5e76\u6ca1\u6709\u544a\u8bc9\u6211\u4eec\u600e\u4e48\u5b9a\u4e49\u57fa\u672c\uff0c\u8fd9\u662f\u4e00\u4e2a\u6a21\u7cca\u7684\u6982\u5ff5\u3002\u5176\u5b9e\u5c31\u662f\u8981 \u67d4 \u5230\u4ec0\u4e48\u7a0b\u5ea6\u3002 \u5728\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u628a\u57fa\u672c\u53ef\u7528\u7406\u89e3\u4e3a\u4fdd\u8bc1\u6838\u5fc3\u529f\u80fd\u53ef\u7528\uff0c\u5141\u8bb8\u635f\u5931\u90e8\u5206\u529f\u80fd\u7684\u53ef\u7528\u6027\u3002 \u57fa\u672c\u53ef\u7528 \u53ef\u4ee5\u7528\u56db\u79cd\u65b9\u6848\u6765\u5b9e\u73b0\u3002 1\u3001 \u6d41\u91cf\u524a\u5cf0 \uff1a\u6bd4\u5982\u591a\u4e2a\u79d2\u6740\u573a\u6b21\uff0c\u67d0\u4e1c\u7684 8 \u70b9\u79d2\u6740\u573a\uff0c12 \u70b9\u7684\u79d2\u6740\u573a\u3002 NOTE: \u53c2\u89c1: jianshu \u9ad8\u5e76\u53d1\u67b6\u6784\u7cfb\u5217\uff1a\u4ec0\u4e48\u662f\u6d41\u91cf\u524a\u5cf0\uff1f\u5982\u4f55\u89e3\u51b3\u79d2\u6740\u4e1a\u52a1\u7684\u524a\u5cf0\u573a\u666f zhihu \u79d2\u6740\u4e4b\u6d41\u91cf\u524a\u5cf0 2\u3001 \u5ef6\u8fdf\u54cd\u5e94 \uff1a\u6bd4\u5982\u53cc 11 \u671f\u95f4\u67d0\u5546\u57ce\u521b\u5efa\u7684\u8ba2\u5355\uff0c\u4f1a\u63d0\u793a\u5ba2\u6237\u8ba2\u5355\u6b63\u5728\u521b\u5efa\u4e2d\uff0c\u53ef\u80fd\u9700\u8981\u7b49\u4e2a\u5341\u51e0\u79d2\u3002 3\u3001 \u4f53\u9a8c\u964d\u7ea7 \uff1a\u6bd4\u5982\u67d0\u6b21\u6bd4\u8d5b\u6d3b\u52a8\uff0c\u6709\u5927\u91cf\u7528\u6237\u8fdb\u6d3b\u52a8\u9875\u67e5\u770b\u56fe\u7247\uff0c\u8fd9\u4e2a\u65f6\u5019\uff0c\u5927\u91cf\u56fe\u7247\u56e0\u4e3a\u7f51\u7edc\u8d85\u65f6\u800c\u65e0\u6cd5\u663e\u793a\uff0c\u8fd9\u4e2a\u65f6\u5019\u5c31\u53ef\u4ee5\u8003\u8651\u66ff\u6362\u539f\u6709\u56fe\u7247\uff0c\u8fd4\u56de\u6e05\u6670\u5ea6\u6ca1\u6709\u90a3\u4e48\u9ad8\u6216\u56fe\u7247\u6bd4\u8f83\u5c0f\u7684\u56fe\u7247\u3002 4\u3001 \u8fc7\u8f7d\u4fdd\u62a4 \uff1a\u6bd4\u5982\u6211\u4eec\u5e38\u7528\u7684\u6d88\u606f\u961f\u5217\u5360\u6ee1\u4e86\uff0c\u53ef\u4ee5\u8003\u8651\u4e22\u5f03\u540e\u6765\u7684\u8bf7\u6c42\uff0c\u6216\u6e05\u9664\u961f\u5217\u4e2d\u7684\u4e00\u4e9b\u8bf7\u6c42\uff0c\u4fdd\u62a4\u7cfb\u7edf\u4e0d\u8fc7\u8f7d\uff0c\u4f46\u8fd9\u90fd\u9700\u8981\u7ed3\u5408\u81ea\u8eab\u7684\u4e1a\u52a1\u573a\u666f\u6765\u8bbe\u8ba1\u3002","title":"3.3 \u57fa\u672c\u53ef\u7528"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/02-%E7%94%A8%E5%A4%AA%E6%9E%81%E6%8B%B3%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/#34","text":"\u6700\u7ec8\u4e00\u81f4\u6027 \uff1a\u7cfb\u7edf\u4e2d\u7684\u6240\u6709\u7684\u6570\u636e\u526f\u672c\u5728\u7ecf\u8fc7\u4e00\u6bb5\u65f6\u95f4\u7684\u540c\u6b65\u540e\uff0c\u6700\u7ec8\u80fd\u591f\u8fbe\u5230\u4e00\u4e2a\u4e00\u81f4\u7684\u72b6\u6001\u3002\u6700\u7ec8\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e00\u4e2a\u77ed\u6682\u7684\u5ef6\u8fdf\u3002 \u6700\u7ec8\u4e00\u81f4\u6027\u5728\u975e\u5e38\u591a\u7684\u4e92\u8054\u7f51\u4e1a\u52a1\u4e2d\u91c7\u7528\u3002\u4f46\u662f\u8ddf\u94b1\u6253\u4ea4\u9053\u6216\u91d1\u878d\u7cfb\u7edf\u4f1a\u91c7\u7528\u5f3a\u4e00\u81f4\u6027\u6216\u4e8b\u52a1\u3002 \u524d\u9762\u63d0\u5230\u4e86 ACID \u7684 \u5f3a\u4e00\u81f4\u6027 \uff0c\u800c \u6700\u7ec8\u4e00\u81f4\u6027 \u548c\u5b83\u662f\u4ec0\u4e48\u5173\u7cfb\uff1f \u5f3a\u4e00\u81f4\u6027\u5176\u5b9e\u4e5f\u662f\u6700\u7ec8\u4e00\u81f4\u6027\u7684\u4e00\u79cd\u3002\u90a3\u6700\u7ec8\u4e00\u81f4\u6027\u600e\u4e48\u7406\u89e3\uff1f\u5f3a\u4e00\u81f4\u6027\u53ef\u4ee5\u770b\u4f5c\u4e0d\u5b58\u5728\u5ef6\u8fdf\u7684\u4e00\u81f4\u6027\u3002\u5982\u679c\u65e0\u6cd5\u5bb9\u5fcd\u5ef6\u8fdf\u5c31\u7528\u5f3a\u4e00\u81f4\u6027\uff0c\u5426\u5219\u5c31\u7528\u6700\u7ec8\u4e00\u81f4\u6027\u3002","title":"3.4 \u6700\u7ec8\u4e00\u81f4\u6027"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/02-%E7%94%A8%E5%A4%AA%E6%9E%81%E6%8B%B3%E8%AE%B2%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA/#_9","text":"1\u3001\u592a\u6781\u62f3\u5206\u4e3a\u9634\u548c\u9633\u4e24\u65b9\u9762\uff0c\u5c31\u5982 CAP \u4e2d\u7684 C \u548c A\u3002 2\u3001ACID \u662f\u4f20\u7edf\u6570\u636e\u5e93\u7684\u8bbe\u8ba1\u7406\u5ff5\uff0c\u8ffd\u6c42\u5f3a\u4e00\u81f4\u6027\u3002\u56db\u4e2a\u6307\u6807\uff1a\u539f\u5b50\u6027\u3001\u4e00\u81f4\u6027\u3001\u9694\u79bb\u6027\u3001\u6301\u4e45\u6027\u3002\u662f CAP \u4e2d CP \u7684\u5ef6\u4f38\u3002 NOTE: \u603b\u7ed3\u5f97\u975e\u5e38\u597d 3\u3001BASE \u7406\u8bba\u662f CAP \u4e2d\u4e00\u81f4\u6027\u548c\u53ef\u7528\u6027\u6743\u8861\u7684\u7ed3\u679c\u3002\u662f CAP \u4e2d\u7684 AP \u7684\u5ef6\u4f38\u3002\u6ce8\u91cd\u53ef\u7528\u6027\u548c\u6027\u80fd\u4f18\u5148\uff0c\u6839\u636e\u4e1a\u52a1\u7684\u573a\u666f\u7279\u70b9\uff0c\u5b9e\u73b0\u5f39\u6027\u7684\u57fa\u672c\u53ef\u7528\uff0c\u7136\u540e\u5b9e\u73b0\u6570\u636e\u7684\u6700\u7ec8\u4e00\u81f4\u6027\u3002 4\u3001BASE \u7406\u8bba\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\uff0c\u89e3\u51b3\u4e86\u4e8b\u52a1\u6027\u7cfb\u7edf\u5728\u6027\u80fd\u3001\u5bb9\u9519\u3001\u53ef\u7528\u6027\u7b49\u65b9\u9762\u7684\u75db\u70b9\u3002 5\u3001BASE \u7406\u8bba\u5728 NoSQL \u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u662f NoSQL \u7cfb\u7edf\u8bbe\u8ba1\u7684\u4e8b\u5b9e\u4e0a\u7684\u7406\u8bba\u652f\u6491\u3002 NOTE: Redis\u5c31\u662f\u5178\u578b\u7684BASE","title":"\u4e94\u3001\u603b\u7ed3"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/03-Paxos%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/","text":"csdn \u8bf8\u845b\u4eae VS \u5e9e\u7edf\uff0c\u62ff\u4e0b Paxos \u5171\u8bc6\u7b97\u6cd5 \u4ece\u8fd9\u7bc7\u5f00\u59cb\uff0c\u5c06\u4f1a\u8bb2\u89e3\u5206\u5e03\u5f0f\u7684\u516b\u5927\u534f\u8bae/\u7b97\u6cd5\u3002\u672c\u7bc7\u4e3b\u8981\u8bb2\u89e3 Paxos \u5171\u8bc6\u7b97\u6cd5\u3002 \u672c\u6587\u4e3b\u8981\u5185\u5bb9\u5982\u4e0b\uff1a Paxos \u7b97\u6cd5 Paxos \u662f\u5206\u5e03\u5f0f\u7b97\u6cd5\u4e2d\u7684\u8001\u5927\u54e5\uff0c\u53ef\u4ee5\u8bf4 Paxos \u662f\u5206\u5e03\u5f0f\u5171\u8bc6\u7684\u4ee3\u540d\u8bcd\u3002\u6700\u5e38\u7528\u7684\u5206\u5e03\u5f0f\u5171\u8bc6\u7b97\u6cd5\u90fd\u662f\u57fa\u4e8e\u5b83\u6539\u8fdb\u3002\u6bd4\u5982 Raft \u7b97\u6cd5\uff08\u540e\u9762\u4e5f\u4f1a\u4ecb\u7ecd\uff09\u3002\u6240\u4ee5\u5b66\u4e60\u5206\u5e03\u5f0f\u7b97\u6cd5\u5fc5\u987b\u5148\u5b66\u4e60 Paxos \u7b97\u6cd5\u3002 Paxos \u7b97\u6cd5\u4e3b\u8981\u5305\u542b\u4e24\u4e2a\u90e8\u5206\uff1a 1\u3001 Basic Paxos \u7b97\u6cd5 \uff1a\u591a\u4e2a\u8282\u70b9\u4e4b\u95f4\u5982\u4f55\u5c31\u67d0\u4e2a\u503c\u8fbe\u6210\u5171\u8bc6\u3002\uff08\u8fd9\u4e2a\u503c\u6211\u4eec\u79f0\u4f5c \u63d0\u6848 Value \uff09 2\u3001 Multi-Paxos \u7b97\u6cd5 \uff1a\u6267\u884c\u591a\u4e2a Basic Paxos \u5b9e\u4f8b\uff0c\u5c31\u4e00\u7cfb\u5217\u503c\u8fbe\u6210\u5171\u8bc6\u3002 Basic Paxos \u7b97\u6cd5\u662f Multi-Paxos \u601d\u60f3\u7684\u6838\u5fc3\uff0cMulti \u7684\u610f\u601d\u5c31\u662f\u591a\u6b21\uff0c\u4e5f\u5c31\u662f\u8bf4\u591a\u6267\u884c\u51e0\u6b21 Basic Paxos \u7b97\u6cd5\u3002\u6240\u4ee5 Basic Paxos \u7b97\u6cd5\u662f\u91cd\u4e2d\u4e4b\u91cd\u3002 \u4e09\u56fd\u4e2d\u7684 Paxos \u4e09\u56fd\u4e2d\u5218\u5907\u96c6\u56e2\uff0c\u6709\u4e24\u5927\u519b\u5e08\uff1a\u8bf8\u845b\u4eae\u548c\u5e9e\u7edf\uff0c\u90fd\u662f\u975e\u5e38\u5389\u5bb3\u7684\u4eba\u7269\uff0c\u5f53\u4ed6\u4eec\u6709\u4e0d\u540c\u4f5c\u6218\u8ba1\u5212\u7ed9\u591a\u540d\u6b66\u5c06\u65f6\uff0c\u5982\u4f55\u8fbe\u6210\u4e00\u81f4\uff1f \u89d2\u8272 Paxos \u4e2d\u6709\u4e09\u79cd\u89d2\u8272\uff1a\u63d0\u8bae\u8005\u3001\u63a5\u53d7\u8005\u3001\u5b66\u4e60\u8005\u3002 \u5218\u5907\u7684\u5e10\u8425\u4e2d\u4eba\u7269\u4ecb\u7ecd\uff1a 1\u3001 \u4e3b\u516c\u4e00\u540d \uff1a \u5218\u5907 \uff0c\u4f5c\u4e3a\u8bf7\u6c42\u65b9\u6216 \u5ba2\u6237\u7aef \u3002 2\u3001 \u519b\u5e08\u4e24\u540d \uff1a \u8bf8\u845b\u4eae \u3001 \u5e9e\u7edf \uff0c\u4f5c\u4e3a \u63d0\u8bae\u8005 \u3002 3\u3001 \u6b66\u5c06\u4e09\u540d \uff1a \u5173\u7fbd \u3001 \u5f20\u98de \u3001 \u8d75\u4e91 \uff0c\u4f5c\u4e3a \u63a5\u53d7\u8005 \u3002 4\u3001 \u6587\u81e3\u4e24\u540d \uff1a \u6cd5\u6b63 \u3001 \u9a6c\u826f \uff0c\u4f5c\u4e3a \u5b66\u4e60\u8005 \u3002 \u63d0\u8bae\u8005\uff08Proposer\uff09 1\u3001\u63d0\u8bae\u4e00\u4e2a\u503c\uff0c\u7528\u4e8e\u6295\u7968\u8868\u51b3\u3002 NOTE: \"\u63d0\u8bae\"\u5373\"proposal\"\uff0c\u5b83\u53ea\u80fd\u591f\u7531\"Proposer\"\u53d1\u8d77 2\u3001\u63a5\u5165\u548c\u534f\u8c03\uff0c\u6536\u5230\u5ba2\u6237\u7aef\u7684\u8bf7\u6c42\u540e\uff0c\u53ef\u4ee5\u53d1\u8d77\u4e8c\u9636\u6bb5\u63d0\u4ea4\uff0c\u8fdb\u884c\u5171\u8bc6\u534f\u5546\u3002 NOTE: \u4e00\u3001\"\u4e8c\u9636\u6bb5\u63d0\u4ea4\"\u53732PC 3\u3001 \u6620\u5c04 \u5230\u4e0a\u9762\u7684\u6545\u4e8b\u4e2d\uff0c \u519b\u5e08 \u5c31\u662f\u7528\u6765\u90e8\u7f72\u4f5c\u6218\u8ba1\u5212\u7684\u3002 NOTE: \u4e5f\u5c31\u662f\u8bf4\uff0c \u519b\u5e08 \u5c31\u662f**proposer** \u63a5\u53d7\u8005\uff08Acceptor\uff09 1\u3001\u5bf9\u6bcf\u4e2a\u63d0\u8bae\u7684\u503c\u8fdb\u884c\u6295\u7968\uff0c\u5e76\u5b58\u50a8\u63a5\u53d7\u7684\u503c\u3002 NOTE: vote 2\u3001\u6295\u7968\u534f\u5546\u548c\u5b58\u50a8\u6570\u636e\uff0c\u5bf9\u63d0\u8bae\u7684\u503c\u8fdb\u884c\u6295\u7968\uff0c\u5e76\u63a5\u53d7\u8fbe\u6210\u5171\u8bc6\u7684\u503c\uff0c\u5b58\u50a8\u4fdd\u5b58\u3002 3\u3001 \u6620\u5c04 \u5230\u4e0a\u9762\u7684\u6545\u4e8b\u4e2d\uff0c \u6b66\u5c06 \u5c31\u662f\u7528\u6765\u63a5\u53d7\u519b\u5e08\u7684\u4f5c\u6218\u8ba1\u5212\u3002 4\u3001\u5176\u5b9e\uff0c\u96c6\u7fa4\u4e2d\u6240\u6709\u7684\u8282\u70b9\u90fd\u5728\u626e\u6f14\u63a5\u53d7\u8005\u7684\u89d2\u8272\uff0c\u53c2\u4e0e\u5171\u8bc6\u534f\u5546\uff0c\u5e76\u63a5\u53d7\u548c\u5b58\u50a8\u6570\u636e\u3002 NOTE: \u4e00\u3001\u90fd\u80fd\u591f\u8fdb\u884c\u6295\u7968\uff0c\u8fd9\u5728\u540e\u9762\u7684\"\u63a5\u53d7\u8005 or \u63d0\u8bae\u8005\"\u7ae0\u8282\u4e2d\uff0c\u8fdb\u884c\u4e86\u6df1\u5165\u8bf4\u660e \u5b66\u4e60\u8005\uff08Learner\uff09 1\u3001\u88ab\u544a\u77e5\u6295\u7968\u7684\u7ed3\u679c\uff0c\u63a5\u53d7\u8fbe\u6210\u5171\u8bc6\u7684\u503c\uff0c\u5b58\u50a8\u6570\u636e\uff0c 2\u3001\u4e0d\u53c2\u4e0e\u6295\u7968\u7684\u8fc7\u7a0b\uff0c\u5373\u4e0d\u53c2\u4e0e\u5171\u8bc6\u534f\u5546\u3002 3\u3001 \u6620\u5c04 \u5230\u4e0a\u9762\u7684\u6545\u4e8b\u4e2d\uff0c\u5c31\u662f\u4e24\u540d \u6587\u81e3 \u4f5c\u4e3a\u8bb0\u5f55\u4f5c\u6218\u65b9\u6848\u7684 \u5907\u80ce \u3002 \u63a5\u53d7\u8005 or \u63d0\u8bae\u8005 \u4e3a\u4ec0\u4e48\u8bf4\u8282\u70b9\u65e2\u53ef\u4ee5\u626e\u6f14\u63a5\u53d7\u8005\uff0c\u4e5f\u53ef\u4ee5\u626e\u6f14\u63d0\u8bae\u8005\u5462\uff1f \u4e0a\u7bc7\u6211\u5728\u8bb2\u89e3 BASE \u534f\u8bae\u7684\u65f6\u5019\uff0c\u8bb2\u5230 \u4e8c\u9636\u6bb5 \u63d0\u4ea4\u534f\u8bae\u3002\u5176\u4e2d\u6709\u4e00\u4e2a\u534f\u8c03\u8005\u7684\u8eab\u4efd\uff0c\u534f\u8c03\u8005\u65e2\u53ef\u4ee5\u662f\u63a5\u53d7\u8005\uff0c\u4e5f\u53ef\u4ee5\u662f\u63d0\u8bae\u8005\u3002 1\u3001 \u4f5c\u4e3a\u63a5\u53d7\u8005 \uff0c\u63a5\u6536\u5ba2\u6237\u7aef\u7684\u6d88\u606f\u3002\u6bd4\u5982 \u8bf8\u845b\u4eae \u9700\u8981\u63a5\u6536 \u5218\u5907 \u7684\u4f5c\u6218\u8981\u6c42\u3002 2\u3001 \u4f5c\u4e3a\u63d0\u8bae\u8005 \uff0c\u53d1\u8d77\u4e8c\u9636\u6bb5\u63d0\u4ea4\u3002\u7136\u540e\u8fd9\u4e2a\u8282\u70b9\u548c\u53e6\u5916\u5176\u4ed6\u8282\u70b9\u4f5c\u4e3a\u63a5\u53d7\u8005\u8fdb\u884c\u5171\u8bc6\u534f\u5546\u3002\u6bd4\u5982 \u8bf8\u845b\u4eae \u8981\u6c47\u603b\u6700\u7ec8\u7684\u4f5c\u6218\u8ba1\u5212\u7ed9 \u5218\u5907 \u3002 \u5982\u4e0b\u56fe\u6240\u793a\uff0c\u8282\u70b9 1 \u4f5c\u4e3a\u63d0\u8bae\u8005\u548c\u63a5\u53d7\u8005\uff0c\u8282\u70b9 2 \u548c\u8282\u70b9 3 \u4f5c\u4e3a\u63a5\u53d7\u8005\u3002 \u8bf8\u845b\u4eae VS \u5e9e\u7edf \u4e09\u56fd\u4e2d\u6709\u5218\u5907\u96c6\u56e2\uff08\u5360\u636e\u897f\u8700\uff09\u3001\u66f9\u64cd\u96c6\u56e2\uff08\u5360\u636e\u5317\u8fb9\uff09\u3001\u5b59\u6743\u96c6\u56e2\uff08\u5360\u636e\u6c5f\u5357\uff09\u3002 \u8bf8\u845b\u4eae \u548c \u5e9e\u7edf \u4f5c\u4e3a\u63d0\u8bae\u8005\uff0c\u5411\u4e09\u4e2a\u63a5\u53d7\u8005\u8fdb\u4f5c\u6218\u8ba1\u5212\u7684\u63d0\u6848\u3002\u63d0\u6848\u4e2d\u6709\u4e24\u4e2a\u5c5e\u6027\uff1a 1\u3001 \u63d0\u6848\u7f16\u53f7 \uff0c\u6bcf\u6b21\u519b\u5e08\u8fdb\u884c\u63d0\u6848\uff0c\u90fd\u4f1a\u6709\u4e2a\u7f16\u53f7\uff0c\u8fd9\u91cc\u7528 n \u8868\u793a\u3002 2\u3001 \u63d0\u8bae\u503c \uff0c\u4e5f\u5c31\u662f\u4f5c\u6218\u8ba1\u5212\uff0c\u8fd9\u91cc\u7528 v \u8868\u793a\u3002\u6240\u4ee5\u63d0\u6848\u5c31\u662f [n, v]\u3002 \u8bf8\u845b\u4eae \u7684\u4f5c\u6218\u8ba1\u5212\u662f\u4ece \u5317\u8fb9 \u8fdb\u653b\u66f9\u64cd\uff0c \u5e9e\u7edf \u7684\u4f5c\u6218\u8ba1\u5212\u662f\u4ece \u5357\u8fb9 \u8fdb\u653b\u66f9\u64cd\uff0c \u800c\u5173\u7fbd\u3001\u5f20\u98de\u3001\u8d75\u4e91\u5148\u540e\u6536\u5230\u4e86\u4ed6\u4eec\u7684\u4f5c\u6218\u8ba1\u5212\uff0c\u8be5\u542c\u8c01\u7684\u5462 \uff1f\u8fd9\u91cc\u5c31\u662f\u4e00\u4e2a \u5171\u8bc6 \u7684\u95ee\u9898\u3002\u800c Paxos \u7b97\u6cd5\u8fbe\u6210\u5171\u8bc6\u5206\u4e24\u4e2a\u9636\u6bb5\u3002 \u51c6\u5907\uff08Prepare\uff09\u9636\u6bb5 \u548c \u63a5\u53d7\uff08Accept\uff09\u9636\u6bb5 \u3002 \u51c6\u5907\u9636\u6bb5 \u8bf8\u845b\u4eae\u548c\u5e9e\u7edf\u4f5c\u4e3a\u63d0\u8bae\u8005\uff0c\u5206\u522b\u5411\u6240\u6709\u7684\u63a5\u53d7\u8005\uff08\u5173\u7fbd\u3001\u5f20\u98de\u3001\u8d75\u4e91\uff09\u53d1\u9001\u5305\u542b\u4f5c\u6218\u8ba1\u5212\u7f16\u53f7\uff08\u63d0\u6848\u7f16\u53f7\uff09\u7684 \u51c6\u5907\u8bf7\u6c42 \uff0c\u4f46\u4e0d\u5305\u542b\u4f5c\u6218\u8ba1\u5212\uff08\u63d0\u6848\u503c\uff09\u3002 \u53d1\u9001\u51c6\u5907\u8bf7\u6c42 1\u3001\u63d0\u8bae\u8005 \u8bf8\u845b\u4eae \u5148\u53d1\u9001\u7f16\u53f7\u4e3a 1 \u7684\u4f5c\u6218\u8ba1\u5212\u7684\u51c6\u5907\u8bf7\u6c42\uff0c \u5e9e\u7edf \u53d1\u9001\u7f16\u53f7\u4e3a 2 \u7684\u4f5c\u6218\u8ba1\u5212\u7684\u51c6\u5907\u8bf7\u6c42\u3002 2\u3001\u63a5\u53d7\u8005 \u5173\u7fbd \uff08\u8282\u70b9 X\uff09\u5728 8 \u70b9 \u6536\u5230\u6765\u81ea\u8bf8\u845b\u4eae\u53d1\u9001\u7684\u4f5c\u6218\u8ba1\u5212\u51c6\u5907\u8bf7\u6c42\uff0c\u5728 10 \u70b9 \u6536\u5230\u6765\u81ea\u5e9e\u7edf\u53d1\u9001\u7684\u4f5c\u6218\u8ba1\u5212\u51c6\u5907\u8bf7\u6c42\u3002 3\u3001\u63a5\u53d7\u8005 \u5f20\u98de \uff08\u8282\u70b9 Y\uff09\u5728 9 \u70b9 \u6536\u5230\u6765\u81ea\u8bf8\u845b\u4eae\u53d1\u9001\u7684\u4f5c\u6218\u8ba1\u5212\u51c6\u5907\u8bf7\u6c42\uff0c\u5728 11 \u70b9 \u6536\u5230\u6765\u81ea\u5e9e\u7edf\u53d1\u9001\u7684\u4f5c\u6218\u8ba1\u5212\u51c6\u5907\u8bf7\u6c42\u3002 4\u3001\u63a5\u53d7\u8005 \u8d75\u4e91 \uff08\u8282\u70b9 Z\uff09\u5728 12 \u70b9 \u6536\u5230\u6765\u81ea\u5e9e\u7edf\u53d1\u9001\u7684\u4f5c\u6218\u8ba1\u5212\u51c6\u5907\u8bf7\u6c42\uff0c\u5728 13 \u70b9 \u6536\u5230\u6765\u81ea\u8bf8\u845b\u4eae\u53d1\u9001\u7684\u4f5c\u6218\u8ba1\u5212\u51c6\u5907\u8bf7\u6c42\u3002 \u6ce8\u610f\uff1a\u51c6\u5907\u9636\u6bb5\u4e0d\u9700\u8981\u643a\u5e26\u5177\u4f53\u7684\u4f5c\u6218\u8ba1\u5212\uff0c\u6240\u4ee5\u4f5c\u6218\u8ba1\u5212\u53ef\u4ee5\u4e3a\u7a7a\uff0c\u4f46\u662f\u63d0\u8bae\u7f16\u53f7\u5fc5\u987b\u6709\u3002 \u6536\u5230\u51c6\u5907\u8bf7\u6c42\uff08\u7b2c\u4e00\u6b21\uff09 \u6309\u7167\u63a5\u53d7\u8bf7\u6c42\u7684\u65f6\u95f4\u987a\u5e8f\uff0c\u5173\u7fbd\u548c\u5f20\u98de\u6536\u5230\u8bf8\u845b\u4eae\u7684\u8bf7\u6c42 [1\uff0c\u7a7a] \uff0c\u8d75\u4e91\u6536\u5230\u5e9e\u7edf\u7684\u8bf7\u6c42 [2\uff0c\u7a7a] \u3002 \u56e0\u4e3a\u5173\u7fbd\u3001\u5f20\u98de\u4e4b\u524d\u6ca1\u6709\u6536\u5230\u63d0\u6848\uff0c\u6240\u4ee5\u8fd4\u56de\u4e00\u4e2a \u5c1a\u65e0\u63d0\u6848 \u7684\u54cd\u5e94\u3002\u4e5f\u5c31\u662f\u544a\u8bc9\u8bf8\u845b\u4eae\uff0c\u4e0d\u4f1a\u518d\u54cd\u5e94\u7f16\u53f7\u5c0f\u4e8e\u7b49\u4e8e 1 \u7684\u51c6\u5907\u8bf7\u6c42\u4e86\uff0c\u4e5f\u4e0d\u4f1a\u901a\u8fc7\u7f16\u53f7\u5c0f\u4e8e 1 \u7684\u63d0\u6848\u3002 \u54cd\u5e94\u7684\u65f6\u95f4\u70b9\u662f 14 \u70b9\u548c 15 \u70b9 \u3002 NOTE: \u4e00\u3001\u4f1a\u54cd\u5e94\u7f16\u53f7\u5927\u4e8e1\u7684\u63d0\u6848 \u4e8c\u3001\u5728 \u5c1a\u65e0\u63d0\u6848 \u7684\u54cd\u5e94\u4e2d\uff0c\u662f\u4f1a\u5c06\u63d0\u6848\u7f16\u53f71\u7ed9\u5e26\u4e0a\u7684 \u800c\u8d75\u4e91\u4e4b\u524d\u4e5f\u6ca1\u6709\u6536\u5230\u63d0\u6848\uff0c\u6240\u4ee5\u8fd4\u56de\u4e00\u4e2a \u5c1a\u65e0\u63d0\u6848 \u7684\u54cd\u5e94\u3002\u4e5f\u5c31\u662f\u544a\u8bc9\u5e9e\u7edf\uff0c\u4e0d\u4f1a\u518d\u54cd\u5e94\u7f16\u53f7\u5c0f\u4e8e\u7b49\u4e8e 2 \u7684\u51c6\u5907\u8bf7\u6c42\u4e86\uff0c\u4e5f\u4e0d\u4f1a\u901a\u8fc7\u7f16\u53f7\u5c0f\u4e8e 2 \u7684\u63d0\u6848\u3002 \u54cd\u5e94\u7684\u65f6\u95f4\u70b9\u662f 16 \u70b9 \u3002 \u6536\u5230\u51c6\u5907\u8bf7\u6c42\uff08\u7b2c\u4e8c\u6b21\uff09 \u800c\u5bf9\u4e8e\u5e9e\u7edf\u7684\u51c6\u5907\u8bf7\u6c42\uff0c\u5173\u7fbd\u3001\u5f20\u98de\u6536\u5230\u7f16\u53f7\u4e3a 2 \u7684\u51c6\u5907\u8bf7\u6c42\uff0c\u800c \u7f16\u53f7 2 \u5927\u4e8e\u4e4b\u524d\u63a5\u53d7\u5230\u7684\u7f16\u53f7 1 \uff0c\u800c\u4e14\u5173\u7fbd\u548c\u5f20\u98de\u6ca1\u6709\u901a\u8fc7\u4efb\u4f55\u63d0\u6848\uff0c\u6240\u4ee5\u8fd8\u662f\u4f1a\u8fd4\u56de\u7ed9\u5e9e\u7edf\u4e00\u4e2a \u5c1a\u65e0\u63d0\u6848 \u7684\u54cd\u5e94\u3002\u4e5f\u5c31\u662f\u544a\u8bc9\u5e9e\u7edf\u4e0d\u4f1a\u518d\u54cd\u5e94\u7f16\u53f7\u5c0f\u4e8e\u7b49\u4e8e 2 \u7684\u51c6\u5907\u8bf7\u6c42\u4e86\uff0c\u4e5f\u4e0d\u4f1a\u901a\u8fc7\u7f16\u53f7\u5c0f\u4e8e 2 \u7684\u63d0\u6848\u3002\u54cd\u5e94\u7684\u65f6\u95f4\u70b9\u662f 14 \u70b9\u548c 15 \u70b9 \u3002 NOTE: \u4e00\u3001\"\u800c\u4e14\u5173\u7fbd\u548c\u5f20\u98de\u6ca1\u6709\u901a\u8fc7\u4efb\u4f55\u63d0\u6848\" \u5982\u679c\u901a\u8fc7\u4e86\u63d0\u6848\uff0c\u90a3\u4e48\u6b64\u65f6\u4ed6\u4eec\u4f1a\u54cd\u5e94\u4ec0\u4e48\uff1f\u5728\"\u603b\u7ed3\"\u4e2d\u8fdb\u884c\u4e86\u8bf4\u660e\u3002 \u800c\u8d75\u4e91\u6700\u540e\u6536\u5230\u8bf8\u845b\u4eae\u7f16\u53f7\u4e3a 1 \u7684 \u51c6\u5907\u8bf7\u6c42 \u540e\uff0c\u56e0\u7f16\u53f7 1 \u5c0f\u4e8e\u4e4b\u524d\u54cd\u5e94\u7684\u51c6\u5907\u8bf7\u6c42\u7684\u63d0\u6848\u7f16\u53f7 2 \uff0c\u6240\u4ee5\u76f4\u63a5\u4e22\u5f03\u8be5\u51c6\u5907\u8bf7\u6c42\uff0c\u4e0d\u505a\u54cd\u5e94\uff0c\u5982\u4e0a\u56fe\u7684 \u274c \u56fe\u793a\u3002 \u63a5\u53d7\u9636\u6bb5 \u53d1\u9001\u63a5\u53d7\u8bf7\u6c42 \u8bf8\u845b\u4eae\u548c\u5e9e\u7edf\u6536\u5230\u51c6\u5907\u54cd\u5e94\u540e\uff0c\u4f1a\u5206\u522b\u53d1\u9001\u63a5\u53d7\u8bf7\u6c42\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u8bf8\u845b\u4eae \u8bf8\u845b\u4eae \u6536\u5230\u5927\u591a\u6570\u63a5\u53d7\u8005\uff08\u5173\u7fbd\u548c\u5f20\u98de\uff09\u7684 \u51c6\u5907\u54cd\u5e94 \u540e\uff0c\u6839\u636e\u54cd\u5e94\u4e2d\u63d0\u6848\u7f16\u53f7\u6700\u5927\u7684\u63d0\u6848\u7684\u503c\uff0c\u8bbe\u7f6e \u63a5\u53d7\u8bf7\u6c42 \u4e2d\u7684\u503c\u3002\u56e0\u4e3a\u5173\u7fbd\u548c\u5f20\u98de\u8fd4\u56de\u7684\u51c6\u5907\u54cd\u5e94\u90fd\u662f\u5c1a\u65e0\u63d0\u6848\uff0c\u6240\u4ee5\u8fd8\u662f\u53d1\u9001\u63d0\u6848\u7f16\u53f7\u4e3a 1 \uff0c\u63d0\u6848\u503c\u4e3a \u5317 \u7684 \u63a5\u53d7\u8bf7\u6c42 \uff0c \u5317 \u4ee3\u8868\u4ece\u5317\u8fb9\u8fdb\u653b\u66f9\u64cd\u3002\u53d1\u9001\u7684\u65f6\u95f4\u70b9\u662f 15 \u70b9\u8fc7 1 \u5206\u300116 \u70b9 \u3002 \u4e3a\u4ec0\u4e48\u662f 15 \u70b9\u8fc7 1 \u5206\uff1f \u56e0\u4e3a\u53ea\u8981\u6ee1\u8db3\u5927\u591a\u6570\u63a5\u53d7\u8005\u7684\u51c6\u5907\u8bf7\u6c42\u540e\uff0c\u5c31\u53ef\u4ee5\u53d1\u9001 \u63a5\u53d7\u8bf7\u6c42 \u4e86\u3002\u5173\u7fbd\u548c\u5f20\u98de\u54cd\u5e94\u7684\u65f6\u95f4\u70b9\u662f 14 \u70b9\u548c 15 \u70b9\uff0c\u6240\u4ee5 15 \u70b9\u4ee5\u540e\u5c31\u53ef\u4ee5\u53d1\u9001\u4e86\u3002 \u5e9e\u7edf \u800c \u5e9e\u7edf \u6536\u5230\u5927\u591a\u6570\u63a5\u53d7\u8005\uff08\u5173\u7fbd\u3001\u5f20\u98de\u548c\u8d75\u4e91\uff09\u7684 \u51c6\u5907\u54cd\u5e94 \u540e\uff0c\u6839\u636e\u54cd\u5e94\u4e2d\u63d0\u6848\u7f16\u53f7\u6700\u5927\u7684\u63d0\u6848\u7684\u503c\uff0c\u8bbe\u7f6e \u63a5\u53d7\u8bf7\u6c42 \u4e2d\u7684\u503c\u3002\u56e0\u4e3a\u5173\u7fbd\u3001\u5f20\u98de\u548c\u8d75\u4e91\u8fd4\u56de\u7684\u51c6\u5907\u54cd\u5e94\u90fd\u662f\u5c1a\u65e0\u63d0\u6848\uff0c\u6240\u4ee5\u8fd8\u662f\u53d1\u9001\u63d0\u6848\u7f16\u53f7\u4e3a 2 \uff0c\u63d0\u6848\u503c\u4e3a \u5357 \u7684 \u63a5\u53d7\u8bf7\u6c42 \uff0c \u5357 \u4ee3\u8868\u4ece\u5357\u8fb9\u8fdb\u653b\u66f9\u64cd\u3002\u53d1\u9001\u7684\u65f6\u95f4\u70b9\u662f 18 \u70b9\u300119 \u70b9\u300120 \u70b9 \u3002 \u6536\u5230\u63a5\u53d7\u8bf7\u6c42 \u5f53\u5173\u7fbd\u3001\u5f20\u98de\u3001\u8d75\u4e91\u6536\u5230\u8bf8\u845b\u4eae\u548c\u5e9e\u7edf\u7684 \u63a5\u53d7\u8bf7\u6c42 \u540e\uff0c\u4f1a\u8fdb\u884c\u5982\u4e0b\u5904\u7406\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u5173\u7fbd\u3001\u5f20\u98de\u3001\u8d75\u4e91\u6536\u5230\u8bf8\u845b\u4eae\u53d1\u9001\u7684\u63d0\u6848 [1\uff0c\u5317]\u65f6\u5019\uff0c\u56e0\u4e3a\u63d0\u6848\u7f16\u53f7 1 \u5c0f\u4e8e\u4ed6\u4eec\u627f\u8bfa\u7684\u80fd\u901a\u8fc7\u7684\u63d0\u6848\u7684\u6700\u5c0f\u63d0\u6848\u7f16\u53f7 2 \uff0c\u6240\u4ee5\u8bf8\u845b\u4eae\u7684\u63d0\u6848\u88ab\u62d2\u7edd\u4e86\u3002 \u800c\u5f53\u4ed6\u4eec\u6536\u5230\u5e9e\u7edf\u7684\u53d1\u9001\u7684\u63d0\u6848 [2\uff0c\u5357] \u7684\u65f6\u5019\uff0c\u56e0\u4e3a\u7f16\u53f7 2 \u4e0d\u5c0f\u4e8e\u4e4b\u524d\u627f\u8bfa\u7684\u7f16\u53f7 2\uff0c\u6240\u4ee5\u901a\u8fc7\u5e9e\u7edf\u7684\u63d0\u6848 [2\uff0c\u5357] \uff0c\u6240\u4ee5\u5173\u7fbd\u3001\u5f20\u98de\u3001\u8d75\u4e91\u4ed6\u4eec\u7684\u4f5c\u6218\u8ba1\u5212\u662f\u4ece\u5357\u8fb9\u8fdb\u653b\u66f9\u64cd\u3002 \u8fbe\u6210\u4e86\u5171\u8bc6 \u3002 \u5b66\u4e60\u8005\u767b\u573a \u5f53\u63a5\u53d7\u8005\u901a\u8fc7\u4e86\u4e00\u4e2a\u63d0\u6848\u65f6\uff0c\u5c31\u901a\u77e5\u6240\u6709\u7684\u5b66\u4e60\u8005\u3002\u5f53\u5b66\u4e60\u8005\u53d1\u73b0\u5927\u591a\u6570\u7684\u63a5\u53d7\u8005\u90fd\u901a\u8fc7\u4e86\u67d0\u4e2a\u63d0\u6848\uff0c\u90a3\u4e48\u5b66\u4e60\u8005\u4e5f\u4f1a\u901a\u8fc7\u8be5\u63d0\u6848\uff0c\u63a5\u53d7\u8be5\u63d0\u6848\u7684\u503c\u3002 \u4e5f\u5c31\u662f\u8bf4\u5173\u7fbd\u3001\u5f20\u98de\u3001\u8d75\u4e91\u8fbe\u6210\u4e86\u5171\u8bc6\u540e\uff0c\u5b66\u4e60\u8005 \u6cd5\u6b63 \u548c \u9a6c\u826f \u4e5f\u540c\u6837\u901a\u8fc7**\u4ece\u5357\u8fb9\u8fdb\u653b\u7684\u4f5c\u6218\u8ba1\u5212**\u3002 \u603b\u7ed3 1\u3001Basic Paxos \u4e5f\u662f\u901a\u8fc7\u4e8c\u9636\u6bb5\u63d0\u4ea4\u534f\u8bae\u8fbe\u6210\u5171\u8bc6\u3002\u51c6\u5907\u9636\u6bb5\u3001\u63a5\u53d7\u9636\u6bb5\u3002\u4e0d\u77e5\u9053\u4e8c\u9636\u6bb5\u63d0\u4ea4\u534f\u8bae\u7684\uff0c\u53ef\u4ee5\u770b\u6211\u524d\u9762\u7684\u6587\u7ae0\u3002\u300a\u7528\u592a\u6781\u62f3\u8bb2\u5206\u5e03\u5f0f\u7406\u8bba\uff0c\u8212\u670d\uff01\u300b 2\u3001Basic Paxos \u4e0d\u4ec5\u4ec5\u5b9e\u73b0\u4e86\u5171\u8bc6\uff0c\u8fd8\u5b9e\u73b0\u4e86\u5bb9\u9519\u3002\u6709\u5c11\u4e8e\u4e00\u534a\u7684\u8282\u70b9\u51fa\u73b0\u6545\u969c\u65f6\uff0c\u96c6\u7fa4\u4e5f\u80fd\u6b63\u5e38\u5de5\u4f5c\u3002\u6587\u4e2d\u4e5f\u591a\u6b21\u5f3a\u8c03\u4e86\u5927\u591a\u6570\u8282\u70b9\u90fd\u540c\u610f\u7684\u539f\u5219\uff0c\u800c\u8fd9\u4e2a\u539f\u5219\u8d4b\u4e88\u4e86 Basic Paxos \u5bb9\u9519\u7684\u80fd\u529b\u3002 3\u3001\u63d0\u6848\u7f16\u53f7\u4ee3\u8868\u4f18\u5148\u7ea7\uff0c\u4fdd\u8bc1\u4e86\u4e09\u4e2a\u627f\u8bfa\uff1a \u5982\u679c \u51c6\u5907\u8bf7\u6c42 \u7684\u63d0\u6848\u7f16\u53f7\uff0c \u5c0f\u4e8e\u7b49\u4e8e \u63a5\u53d7\u8005\u5df2\u7ecf\u54cd\u5e94\u7684 \u51c6\u5907\u8bf7\u6c42 \u7684\u63d0\u6848\u7f16\u53f7\uff0c\u90a3\u4e48\u63a5\u53d7\u8005\u5c06\u627f\u8bfa\u4e0d\u54cd\u5e94\u8fd9\u4e2a \u51c6\u5907\u8bf7\u6c42 \u3002 \u5982\u679c \u63a5\u53d7\u8bf7\u6c42 \u4e2d\u7684\u63d0\u6848\u7684\u63d0\u6848\u7f16\u53f7\uff0c \u5c0f\u4e8e \u63a5\u53d7\u8005\u5df2\u7ecf\u54cd\u5e94\u7684 \u51c6\u5907\u8bf7\u6c42 \u7684\u63d0\u6848\uff0c\u90a3\u4e48\u63a5\u53d7\u8005\u5c06\u627f\u8bfa\u4e0d\u901a\u8fc7\u8fd9\u4e2a\u63d0\u6848\u3002 \u5982\u679c\u63a5\u53d7\u8005\u4e4b\u524d\u6709\u901a\u8fc7\u63d0\u6848\uff0c\u90a3\u4e48\u63a5\u53d7\u8005\u5c06\u627f\u8bfa\uff0c\u4f1a\u5728 \u51c6\u5907\u8bf7\u6c42 \u7684\u54cd\u5e94\u4e2d\uff0c\u5305\u542b\u5df2\u7ecf\u901a\u8fc7\u7684\u6700\u5927\u7f16\u53f7\u7684\u63d0\u6848\u4fe1\u606f\u3002 \u52a0\u5206\u9898 \u5982\u679c\u5173\u7fbd\u548c\u5f20\u98de\u5df2\u7ecf\u901a\u8fc7\u4e86\u63d0\u6848 [2\uff0c\u5357] \uff0c\u800c\u8d75\u4e91\u8fd8\u672a\u901a\u8fc7\u4efb\u4f55\u63d0\u6848\uff0c\u5f53\u7b2c\u4e09\u540d\u519b\u5e08 \u7b80\u96cd \u63d0\u51fa\u4e00\u4e2a\u63d0\u6848\uff0c\u7f16\u53f7\u4e3a 8\uff0c\u4f5c\u6218\u8ba1\u5212\u4e3a\u4ece\u4e1c\u8fb9\u8fdb\u653b\u66f9\u64cd\uff0c\u4e5f\u5c31\u662f [8, \u4e1c] \u7684\u4f5c\u6218\u8ba1\u5212\uff0c\u90a3\u4e48\u6700\u7ec8\u5173\u7fbd\u3001\u5f20\u98de\u3001\u8d75\u4e91\u7684\u4f5c\u6218\u8ba1\u5212\u662f\u600e\u4e48\u6837\u7684\uff1f\u6b22\u8fce\u8bc4\u8bba\u533a\u7559\u8a00\u3002","title":"Introduction"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/03-Paxos%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#csdn#vs#paxos","text":"\u4ece\u8fd9\u7bc7\u5f00\u59cb\uff0c\u5c06\u4f1a\u8bb2\u89e3\u5206\u5e03\u5f0f\u7684\u516b\u5927\u534f\u8bae/\u7b97\u6cd5\u3002\u672c\u7bc7\u4e3b\u8981\u8bb2\u89e3 Paxos \u5171\u8bc6\u7b97\u6cd5\u3002 \u672c\u6587\u4e3b\u8981\u5185\u5bb9\u5982\u4e0b\uff1a","title":"csdn \u8bf8\u845b\u4eae VS \u5e9e\u7edf\uff0c\u62ff\u4e0b Paxos \u5171\u8bc6\u7b97\u6cd5"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/03-Paxos%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#paxos","text":"Paxos \u662f\u5206\u5e03\u5f0f\u7b97\u6cd5\u4e2d\u7684\u8001\u5927\u54e5\uff0c\u53ef\u4ee5\u8bf4 Paxos \u662f\u5206\u5e03\u5f0f\u5171\u8bc6\u7684\u4ee3\u540d\u8bcd\u3002\u6700\u5e38\u7528\u7684\u5206\u5e03\u5f0f\u5171\u8bc6\u7b97\u6cd5\u90fd\u662f\u57fa\u4e8e\u5b83\u6539\u8fdb\u3002\u6bd4\u5982 Raft \u7b97\u6cd5\uff08\u540e\u9762\u4e5f\u4f1a\u4ecb\u7ecd\uff09\u3002\u6240\u4ee5\u5b66\u4e60\u5206\u5e03\u5f0f\u7b97\u6cd5\u5fc5\u987b\u5148\u5b66\u4e60 Paxos \u7b97\u6cd5\u3002 Paxos \u7b97\u6cd5\u4e3b\u8981\u5305\u542b\u4e24\u4e2a\u90e8\u5206\uff1a 1\u3001 Basic Paxos \u7b97\u6cd5 \uff1a\u591a\u4e2a\u8282\u70b9\u4e4b\u95f4\u5982\u4f55\u5c31\u67d0\u4e2a\u503c\u8fbe\u6210\u5171\u8bc6\u3002\uff08\u8fd9\u4e2a\u503c\u6211\u4eec\u79f0\u4f5c \u63d0\u6848 Value \uff09 2\u3001 Multi-Paxos \u7b97\u6cd5 \uff1a\u6267\u884c\u591a\u4e2a Basic Paxos \u5b9e\u4f8b\uff0c\u5c31\u4e00\u7cfb\u5217\u503c\u8fbe\u6210\u5171\u8bc6\u3002 Basic Paxos \u7b97\u6cd5\u662f Multi-Paxos \u601d\u60f3\u7684\u6838\u5fc3\uff0cMulti \u7684\u610f\u601d\u5c31\u662f\u591a\u6b21\uff0c\u4e5f\u5c31\u662f\u8bf4\u591a\u6267\u884c\u51e0\u6b21 Basic Paxos \u7b97\u6cd5\u3002\u6240\u4ee5 Basic Paxos \u7b97\u6cd5\u662f\u91cd\u4e2d\u4e4b\u91cd\u3002","title":"Paxos \u7b97\u6cd5"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/03-Paxos%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#paxos_1","text":"\u4e09\u56fd\u4e2d\u5218\u5907\u96c6\u56e2\uff0c\u6709\u4e24\u5927\u519b\u5e08\uff1a\u8bf8\u845b\u4eae\u548c\u5e9e\u7edf\uff0c\u90fd\u662f\u975e\u5e38\u5389\u5bb3\u7684\u4eba\u7269\uff0c\u5f53\u4ed6\u4eec\u6709\u4e0d\u540c\u4f5c\u6218\u8ba1\u5212\u7ed9\u591a\u540d\u6b66\u5c06\u65f6\uff0c\u5982\u4f55\u8fbe\u6210\u4e00\u81f4\uff1f","title":"\u4e09\u56fd\u4e2d\u7684 Paxos"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/03-Paxos%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#_1","text":"Paxos \u4e2d\u6709\u4e09\u79cd\u89d2\u8272\uff1a\u63d0\u8bae\u8005\u3001\u63a5\u53d7\u8005\u3001\u5b66\u4e60\u8005\u3002 \u5218\u5907\u7684\u5e10\u8425\u4e2d\u4eba\u7269\u4ecb\u7ecd\uff1a 1\u3001 \u4e3b\u516c\u4e00\u540d \uff1a \u5218\u5907 \uff0c\u4f5c\u4e3a\u8bf7\u6c42\u65b9\u6216 \u5ba2\u6237\u7aef \u3002 2\u3001 \u519b\u5e08\u4e24\u540d \uff1a \u8bf8\u845b\u4eae \u3001 \u5e9e\u7edf \uff0c\u4f5c\u4e3a \u63d0\u8bae\u8005 \u3002 3\u3001 \u6b66\u5c06\u4e09\u540d \uff1a \u5173\u7fbd \u3001 \u5f20\u98de \u3001 \u8d75\u4e91 \uff0c\u4f5c\u4e3a \u63a5\u53d7\u8005 \u3002 4\u3001 \u6587\u81e3\u4e24\u540d \uff1a \u6cd5\u6b63 \u3001 \u9a6c\u826f \uff0c\u4f5c\u4e3a \u5b66\u4e60\u8005 \u3002","title":"\u89d2\u8272"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/03-Paxos%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#proposer","text":"1\u3001\u63d0\u8bae\u4e00\u4e2a\u503c\uff0c\u7528\u4e8e\u6295\u7968\u8868\u51b3\u3002 NOTE: \"\u63d0\u8bae\"\u5373\"proposal\"\uff0c\u5b83\u53ea\u80fd\u591f\u7531\"Proposer\"\u53d1\u8d77 2\u3001\u63a5\u5165\u548c\u534f\u8c03\uff0c\u6536\u5230\u5ba2\u6237\u7aef\u7684\u8bf7\u6c42\u540e\uff0c\u53ef\u4ee5\u53d1\u8d77\u4e8c\u9636\u6bb5\u63d0\u4ea4\uff0c\u8fdb\u884c\u5171\u8bc6\u534f\u5546\u3002 NOTE: \u4e00\u3001\"\u4e8c\u9636\u6bb5\u63d0\u4ea4\"\u53732PC 3\u3001 \u6620\u5c04 \u5230\u4e0a\u9762\u7684\u6545\u4e8b\u4e2d\uff0c \u519b\u5e08 \u5c31\u662f\u7528\u6765\u90e8\u7f72\u4f5c\u6218\u8ba1\u5212\u7684\u3002 NOTE: \u4e5f\u5c31\u662f\u8bf4\uff0c \u519b\u5e08 \u5c31\u662f**proposer**","title":"\u63d0\u8bae\u8005\uff08Proposer\uff09"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/03-Paxos%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#acceptor","text":"1\u3001\u5bf9\u6bcf\u4e2a\u63d0\u8bae\u7684\u503c\u8fdb\u884c\u6295\u7968\uff0c\u5e76\u5b58\u50a8\u63a5\u53d7\u7684\u503c\u3002 NOTE: vote 2\u3001\u6295\u7968\u534f\u5546\u548c\u5b58\u50a8\u6570\u636e\uff0c\u5bf9\u63d0\u8bae\u7684\u503c\u8fdb\u884c\u6295\u7968\uff0c\u5e76\u63a5\u53d7\u8fbe\u6210\u5171\u8bc6\u7684\u503c\uff0c\u5b58\u50a8\u4fdd\u5b58\u3002 3\u3001 \u6620\u5c04 \u5230\u4e0a\u9762\u7684\u6545\u4e8b\u4e2d\uff0c \u6b66\u5c06 \u5c31\u662f\u7528\u6765\u63a5\u53d7\u519b\u5e08\u7684\u4f5c\u6218\u8ba1\u5212\u3002 4\u3001\u5176\u5b9e\uff0c\u96c6\u7fa4\u4e2d\u6240\u6709\u7684\u8282\u70b9\u90fd\u5728\u626e\u6f14\u63a5\u53d7\u8005\u7684\u89d2\u8272\uff0c\u53c2\u4e0e\u5171\u8bc6\u534f\u5546\uff0c\u5e76\u63a5\u53d7\u548c\u5b58\u50a8\u6570\u636e\u3002 NOTE: \u4e00\u3001\u90fd\u80fd\u591f\u8fdb\u884c\u6295\u7968\uff0c\u8fd9\u5728\u540e\u9762\u7684\"\u63a5\u53d7\u8005 or \u63d0\u8bae\u8005\"\u7ae0\u8282\u4e2d\uff0c\u8fdb\u884c\u4e86\u6df1\u5165\u8bf4\u660e","title":"\u63a5\u53d7\u8005\uff08Acceptor\uff09"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/03-Paxos%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#learner","text":"1\u3001\u88ab\u544a\u77e5\u6295\u7968\u7684\u7ed3\u679c\uff0c\u63a5\u53d7\u8fbe\u6210\u5171\u8bc6\u7684\u503c\uff0c\u5b58\u50a8\u6570\u636e\uff0c 2\u3001\u4e0d\u53c2\u4e0e\u6295\u7968\u7684\u8fc7\u7a0b\uff0c\u5373\u4e0d\u53c2\u4e0e\u5171\u8bc6\u534f\u5546\u3002 3\u3001 \u6620\u5c04 \u5230\u4e0a\u9762\u7684\u6545\u4e8b\u4e2d\uff0c\u5c31\u662f\u4e24\u540d \u6587\u81e3 \u4f5c\u4e3a\u8bb0\u5f55\u4f5c\u6218\u65b9\u6848\u7684 \u5907\u80ce \u3002","title":"\u5b66\u4e60\u8005\uff08Learner\uff09"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/03-Paxos%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#or","text":"\u4e3a\u4ec0\u4e48\u8bf4\u8282\u70b9\u65e2\u53ef\u4ee5\u626e\u6f14\u63a5\u53d7\u8005\uff0c\u4e5f\u53ef\u4ee5\u626e\u6f14\u63d0\u8bae\u8005\u5462\uff1f \u4e0a\u7bc7\u6211\u5728\u8bb2\u89e3 BASE \u534f\u8bae\u7684\u65f6\u5019\uff0c\u8bb2\u5230 \u4e8c\u9636\u6bb5 \u63d0\u4ea4\u534f\u8bae\u3002\u5176\u4e2d\u6709\u4e00\u4e2a\u534f\u8c03\u8005\u7684\u8eab\u4efd\uff0c\u534f\u8c03\u8005\u65e2\u53ef\u4ee5\u662f\u63a5\u53d7\u8005\uff0c\u4e5f\u53ef\u4ee5\u662f\u63d0\u8bae\u8005\u3002 1\u3001 \u4f5c\u4e3a\u63a5\u53d7\u8005 \uff0c\u63a5\u6536\u5ba2\u6237\u7aef\u7684\u6d88\u606f\u3002\u6bd4\u5982 \u8bf8\u845b\u4eae \u9700\u8981\u63a5\u6536 \u5218\u5907 \u7684\u4f5c\u6218\u8981\u6c42\u3002 2\u3001 \u4f5c\u4e3a\u63d0\u8bae\u8005 \uff0c\u53d1\u8d77\u4e8c\u9636\u6bb5\u63d0\u4ea4\u3002\u7136\u540e\u8fd9\u4e2a\u8282\u70b9\u548c\u53e6\u5916\u5176\u4ed6\u8282\u70b9\u4f5c\u4e3a\u63a5\u53d7\u8005\u8fdb\u884c\u5171\u8bc6\u534f\u5546\u3002\u6bd4\u5982 \u8bf8\u845b\u4eae \u8981\u6c47\u603b\u6700\u7ec8\u7684\u4f5c\u6218\u8ba1\u5212\u7ed9 \u5218\u5907 \u3002 \u5982\u4e0b\u56fe\u6240\u793a\uff0c\u8282\u70b9 1 \u4f5c\u4e3a\u63d0\u8bae\u8005\u548c\u63a5\u53d7\u8005\uff0c\u8282\u70b9 2 \u548c\u8282\u70b9 3 \u4f5c\u4e3a\u63a5\u53d7\u8005\u3002","title":"\u63a5\u53d7\u8005 or \u63d0\u8bae\u8005"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/03-Paxos%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#vs","text":"\u4e09\u56fd\u4e2d\u6709\u5218\u5907\u96c6\u56e2\uff08\u5360\u636e\u897f\u8700\uff09\u3001\u66f9\u64cd\u96c6\u56e2\uff08\u5360\u636e\u5317\u8fb9\uff09\u3001\u5b59\u6743\u96c6\u56e2\uff08\u5360\u636e\u6c5f\u5357\uff09\u3002 \u8bf8\u845b\u4eae \u548c \u5e9e\u7edf \u4f5c\u4e3a\u63d0\u8bae\u8005\uff0c\u5411\u4e09\u4e2a\u63a5\u53d7\u8005\u8fdb\u4f5c\u6218\u8ba1\u5212\u7684\u63d0\u6848\u3002\u63d0\u6848\u4e2d\u6709\u4e24\u4e2a\u5c5e\u6027\uff1a 1\u3001 \u63d0\u6848\u7f16\u53f7 \uff0c\u6bcf\u6b21\u519b\u5e08\u8fdb\u884c\u63d0\u6848\uff0c\u90fd\u4f1a\u6709\u4e2a\u7f16\u53f7\uff0c\u8fd9\u91cc\u7528 n \u8868\u793a\u3002 2\u3001 \u63d0\u8bae\u503c \uff0c\u4e5f\u5c31\u662f\u4f5c\u6218\u8ba1\u5212\uff0c\u8fd9\u91cc\u7528 v \u8868\u793a\u3002\u6240\u4ee5\u63d0\u6848\u5c31\u662f [n, v]\u3002 \u8bf8\u845b\u4eae \u7684\u4f5c\u6218\u8ba1\u5212\u662f\u4ece \u5317\u8fb9 \u8fdb\u653b\u66f9\u64cd\uff0c \u5e9e\u7edf \u7684\u4f5c\u6218\u8ba1\u5212\u662f\u4ece \u5357\u8fb9 \u8fdb\u653b\u66f9\u64cd\uff0c \u800c\u5173\u7fbd\u3001\u5f20\u98de\u3001\u8d75\u4e91\u5148\u540e\u6536\u5230\u4e86\u4ed6\u4eec\u7684\u4f5c\u6218\u8ba1\u5212\uff0c\u8be5\u542c\u8c01\u7684\u5462 \uff1f\u8fd9\u91cc\u5c31\u662f\u4e00\u4e2a \u5171\u8bc6 \u7684\u95ee\u9898\u3002\u800c Paxos \u7b97\u6cd5\u8fbe\u6210\u5171\u8bc6\u5206\u4e24\u4e2a\u9636\u6bb5\u3002 \u51c6\u5907\uff08Prepare\uff09\u9636\u6bb5 \u548c \u63a5\u53d7\uff08Accept\uff09\u9636\u6bb5 \u3002","title":"\u8bf8\u845b\u4eae VS \u5e9e\u7edf"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/03-Paxos%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#_2","text":"\u8bf8\u845b\u4eae\u548c\u5e9e\u7edf\u4f5c\u4e3a\u63d0\u8bae\u8005\uff0c\u5206\u522b\u5411\u6240\u6709\u7684\u63a5\u53d7\u8005\uff08\u5173\u7fbd\u3001\u5f20\u98de\u3001\u8d75\u4e91\uff09\u53d1\u9001\u5305\u542b\u4f5c\u6218\u8ba1\u5212\u7f16\u53f7\uff08\u63d0\u6848\u7f16\u53f7\uff09\u7684 \u51c6\u5907\u8bf7\u6c42 \uff0c\u4f46\u4e0d\u5305\u542b\u4f5c\u6218\u8ba1\u5212\uff08\u63d0\u6848\u503c\uff09\u3002","title":"\u51c6\u5907\u9636\u6bb5"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/03-Paxos%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#_3","text":"1\u3001\u63d0\u8bae\u8005 \u8bf8\u845b\u4eae \u5148\u53d1\u9001\u7f16\u53f7\u4e3a 1 \u7684\u4f5c\u6218\u8ba1\u5212\u7684\u51c6\u5907\u8bf7\u6c42\uff0c \u5e9e\u7edf \u53d1\u9001\u7f16\u53f7\u4e3a 2 \u7684\u4f5c\u6218\u8ba1\u5212\u7684\u51c6\u5907\u8bf7\u6c42\u3002 2\u3001\u63a5\u53d7\u8005 \u5173\u7fbd \uff08\u8282\u70b9 X\uff09\u5728 8 \u70b9 \u6536\u5230\u6765\u81ea\u8bf8\u845b\u4eae\u53d1\u9001\u7684\u4f5c\u6218\u8ba1\u5212\u51c6\u5907\u8bf7\u6c42\uff0c\u5728 10 \u70b9 \u6536\u5230\u6765\u81ea\u5e9e\u7edf\u53d1\u9001\u7684\u4f5c\u6218\u8ba1\u5212\u51c6\u5907\u8bf7\u6c42\u3002 3\u3001\u63a5\u53d7\u8005 \u5f20\u98de \uff08\u8282\u70b9 Y\uff09\u5728 9 \u70b9 \u6536\u5230\u6765\u81ea\u8bf8\u845b\u4eae\u53d1\u9001\u7684\u4f5c\u6218\u8ba1\u5212\u51c6\u5907\u8bf7\u6c42\uff0c\u5728 11 \u70b9 \u6536\u5230\u6765\u81ea\u5e9e\u7edf\u53d1\u9001\u7684\u4f5c\u6218\u8ba1\u5212\u51c6\u5907\u8bf7\u6c42\u3002 4\u3001\u63a5\u53d7\u8005 \u8d75\u4e91 \uff08\u8282\u70b9 Z\uff09\u5728 12 \u70b9 \u6536\u5230\u6765\u81ea\u5e9e\u7edf\u53d1\u9001\u7684\u4f5c\u6218\u8ba1\u5212\u51c6\u5907\u8bf7\u6c42\uff0c\u5728 13 \u70b9 \u6536\u5230\u6765\u81ea\u8bf8\u845b\u4eae\u53d1\u9001\u7684\u4f5c\u6218\u8ba1\u5212\u51c6\u5907\u8bf7\u6c42\u3002 \u6ce8\u610f\uff1a\u51c6\u5907\u9636\u6bb5\u4e0d\u9700\u8981\u643a\u5e26\u5177\u4f53\u7684\u4f5c\u6218\u8ba1\u5212\uff0c\u6240\u4ee5\u4f5c\u6218\u8ba1\u5212\u53ef\u4ee5\u4e3a\u7a7a\uff0c\u4f46\u662f\u63d0\u8bae\u7f16\u53f7\u5fc5\u987b\u6709\u3002","title":"\u53d1\u9001\u51c6\u5907\u8bf7\u6c42"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/03-Paxos%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#_4","text":"\u6309\u7167\u63a5\u53d7\u8bf7\u6c42\u7684\u65f6\u95f4\u987a\u5e8f\uff0c\u5173\u7fbd\u548c\u5f20\u98de\u6536\u5230\u8bf8\u845b\u4eae\u7684\u8bf7\u6c42 [1\uff0c\u7a7a] \uff0c\u8d75\u4e91\u6536\u5230\u5e9e\u7edf\u7684\u8bf7\u6c42 [2\uff0c\u7a7a] \u3002 \u56e0\u4e3a\u5173\u7fbd\u3001\u5f20\u98de\u4e4b\u524d\u6ca1\u6709\u6536\u5230\u63d0\u6848\uff0c\u6240\u4ee5\u8fd4\u56de\u4e00\u4e2a \u5c1a\u65e0\u63d0\u6848 \u7684\u54cd\u5e94\u3002\u4e5f\u5c31\u662f\u544a\u8bc9\u8bf8\u845b\u4eae\uff0c\u4e0d\u4f1a\u518d\u54cd\u5e94\u7f16\u53f7\u5c0f\u4e8e\u7b49\u4e8e 1 \u7684\u51c6\u5907\u8bf7\u6c42\u4e86\uff0c\u4e5f\u4e0d\u4f1a\u901a\u8fc7\u7f16\u53f7\u5c0f\u4e8e 1 \u7684\u63d0\u6848\u3002 \u54cd\u5e94\u7684\u65f6\u95f4\u70b9\u662f 14 \u70b9\u548c 15 \u70b9 \u3002 NOTE: \u4e00\u3001\u4f1a\u54cd\u5e94\u7f16\u53f7\u5927\u4e8e1\u7684\u63d0\u6848 \u4e8c\u3001\u5728 \u5c1a\u65e0\u63d0\u6848 \u7684\u54cd\u5e94\u4e2d\uff0c\u662f\u4f1a\u5c06\u63d0\u6848\u7f16\u53f71\u7ed9\u5e26\u4e0a\u7684 \u800c\u8d75\u4e91\u4e4b\u524d\u4e5f\u6ca1\u6709\u6536\u5230\u63d0\u6848\uff0c\u6240\u4ee5\u8fd4\u56de\u4e00\u4e2a \u5c1a\u65e0\u63d0\u6848 \u7684\u54cd\u5e94\u3002\u4e5f\u5c31\u662f\u544a\u8bc9\u5e9e\u7edf\uff0c\u4e0d\u4f1a\u518d\u54cd\u5e94\u7f16\u53f7\u5c0f\u4e8e\u7b49\u4e8e 2 \u7684\u51c6\u5907\u8bf7\u6c42\u4e86\uff0c\u4e5f\u4e0d\u4f1a\u901a\u8fc7\u7f16\u53f7\u5c0f\u4e8e 2 \u7684\u63d0\u6848\u3002 \u54cd\u5e94\u7684\u65f6\u95f4\u70b9\u662f 16 \u70b9 \u3002","title":"\u6536\u5230\u51c6\u5907\u8bf7\u6c42\uff08\u7b2c\u4e00\u6b21\uff09"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/03-Paxos%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#_5","text":"\u800c\u5bf9\u4e8e\u5e9e\u7edf\u7684\u51c6\u5907\u8bf7\u6c42\uff0c\u5173\u7fbd\u3001\u5f20\u98de\u6536\u5230\u7f16\u53f7\u4e3a 2 \u7684\u51c6\u5907\u8bf7\u6c42\uff0c\u800c \u7f16\u53f7 2 \u5927\u4e8e\u4e4b\u524d\u63a5\u53d7\u5230\u7684\u7f16\u53f7 1 \uff0c\u800c\u4e14\u5173\u7fbd\u548c\u5f20\u98de\u6ca1\u6709\u901a\u8fc7\u4efb\u4f55\u63d0\u6848\uff0c\u6240\u4ee5\u8fd8\u662f\u4f1a\u8fd4\u56de\u7ed9\u5e9e\u7edf\u4e00\u4e2a \u5c1a\u65e0\u63d0\u6848 \u7684\u54cd\u5e94\u3002\u4e5f\u5c31\u662f\u544a\u8bc9\u5e9e\u7edf\u4e0d\u4f1a\u518d\u54cd\u5e94\u7f16\u53f7\u5c0f\u4e8e\u7b49\u4e8e 2 \u7684\u51c6\u5907\u8bf7\u6c42\u4e86\uff0c\u4e5f\u4e0d\u4f1a\u901a\u8fc7\u7f16\u53f7\u5c0f\u4e8e 2 \u7684\u63d0\u6848\u3002\u54cd\u5e94\u7684\u65f6\u95f4\u70b9\u662f 14 \u70b9\u548c 15 \u70b9 \u3002 NOTE: \u4e00\u3001\"\u800c\u4e14\u5173\u7fbd\u548c\u5f20\u98de\u6ca1\u6709\u901a\u8fc7\u4efb\u4f55\u63d0\u6848\" \u5982\u679c\u901a\u8fc7\u4e86\u63d0\u6848\uff0c\u90a3\u4e48\u6b64\u65f6\u4ed6\u4eec\u4f1a\u54cd\u5e94\u4ec0\u4e48\uff1f\u5728\"\u603b\u7ed3\"\u4e2d\u8fdb\u884c\u4e86\u8bf4\u660e\u3002 \u800c\u8d75\u4e91\u6700\u540e\u6536\u5230\u8bf8\u845b\u4eae\u7f16\u53f7\u4e3a 1 \u7684 \u51c6\u5907\u8bf7\u6c42 \u540e\uff0c\u56e0\u7f16\u53f7 1 \u5c0f\u4e8e\u4e4b\u524d\u54cd\u5e94\u7684\u51c6\u5907\u8bf7\u6c42\u7684\u63d0\u6848\u7f16\u53f7 2 \uff0c\u6240\u4ee5\u76f4\u63a5\u4e22\u5f03\u8be5\u51c6\u5907\u8bf7\u6c42\uff0c\u4e0d\u505a\u54cd\u5e94\uff0c\u5982\u4e0a\u56fe\u7684 \u274c \u56fe\u793a\u3002","title":"\u6536\u5230\u51c6\u5907\u8bf7\u6c42\uff08\u7b2c\u4e8c\u6b21\uff09"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/03-Paxos%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#_6","text":"","title":"\u63a5\u53d7\u9636\u6bb5"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/03-Paxos%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#_7","text":"\u8bf8\u845b\u4eae\u548c\u5e9e\u7edf\u6536\u5230\u51c6\u5907\u54cd\u5e94\u540e\uff0c\u4f1a\u5206\u522b\u53d1\u9001\u63a5\u53d7\u8bf7\u6c42\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff1a","title":"\u53d1\u9001\u63a5\u53d7\u8bf7\u6c42"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/03-Paxos%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#_8","text":"\u8bf8\u845b\u4eae \u6536\u5230\u5927\u591a\u6570\u63a5\u53d7\u8005\uff08\u5173\u7fbd\u548c\u5f20\u98de\uff09\u7684 \u51c6\u5907\u54cd\u5e94 \u540e\uff0c\u6839\u636e\u54cd\u5e94\u4e2d\u63d0\u6848\u7f16\u53f7\u6700\u5927\u7684\u63d0\u6848\u7684\u503c\uff0c\u8bbe\u7f6e \u63a5\u53d7\u8bf7\u6c42 \u4e2d\u7684\u503c\u3002\u56e0\u4e3a\u5173\u7fbd\u548c\u5f20\u98de\u8fd4\u56de\u7684\u51c6\u5907\u54cd\u5e94\u90fd\u662f\u5c1a\u65e0\u63d0\u6848\uff0c\u6240\u4ee5\u8fd8\u662f\u53d1\u9001\u63d0\u6848\u7f16\u53f7\u4e3a 1 \uff0c\u63d0\u6848\u503c\u4e3a \u5317 \u7684 \u63a5\u53d7\u8bf7\u6c42 \uff0c \u5317 \u4ee3\u8868\u4ece\u5317\u8fb9\u8fdb\u653b\u66f9\u64cd\u3002\u53d1\u9001\u7684\u65f6\u95f4\u70b9\u662f 15 \u70b9\u8fc7 1 \u5206\u300116 \u70b9 \u3002 \u4e3a\u4ec0\u4e48\u662f 15 \u70b9\u8fc7 1 \u5206\uff1f \u56e0\u4e3a\u53ea\u8981\u6ee1\u8db3\u5927\u591a\u6570\u63a5\u53d7\u8005\u7684\u51c6\u5907\u8bf7\u6c42\u540e\uff0c\u5c31\u53ef\u4ee5\u53d1\u9001 \u63a5\u53d7\u8bf7\u6c42 \u4e86\u3002\u5173\u7fbd\u548c\u5f20\u98de\u54cd\u5e94\u7684\u65f6\u95f4\u70b9\u662f 14 \u70b9\u548c 15 \u70b9\uff0c\u6240\u4ee5 15 \u70b9\u4ee5\u540e\u5c31\u53ef\u4ee5\u53d1\u9001\u4e86\u3002","title":"\u8bf8\u845b\u4eae"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/03-Paxos%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#_9","text":"\u800c \u5e9e\u7edf \u6536\u5230\u5927\u591a\u6570\u63a5\u53d7\u8005\uff08\u5173\u7fbd\u3001\u5f20\u98de\u548c\u8d75\u4e91\uff09\u7684 \u51c6\u5907\u54cd\u5e94 \u540e\uff0c\u6839\u636e\u54cd\u5e94\u4e2d\u63d0\u6848\u7f16\u53f7\u6700\u5927\u7684\u63d0\u6848\u7684\u503c\uff0c\u8bbe\u7f6e \u63a5\u53d7\u8bf7\u6c42 \u4e2d\u7684\u503c\u3002\u56e0\u4e3a\u5173\u7fbd\u3001\u5f20\u98de\u548c\u8d75\u4e91\u8fd4\u56de\u7684\u51c6\u5907\u54cd\u5e94\u90fd\u662f\u5c1a\u65e0\u63d0\u6848\uff0c\u6240\u4ee5\u8fd8\u662f\u53d1\u9001\u63d0\u6848\u7f16\u53f7\u4e3a 2 \uff0c\u63d0\u6848\u503c\u4e3a \u5357 \u7684 \u63a5\u53d7\u8bf7\u6c42 \uff0c \u5357 \u4ee3\u8868\u4ece\u5357\u8fb9\u8fdb\u653b\u66f9\u64cd\u3002\u53d1\u9001\u7684\u65f6\u95f4\u70b9\u662f 18 \u70b9\u300119 \u70b9\u300120 \u70b9 \u3002","title":"\u5e9e\u7edf"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/03-Paxos%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#_10","text":"\u5f53\u5173\u7fbd\u3001\u5f20\u98de\u3001\u8d75\u4e91\u6536\u5230\u8bf8\u845b\u4eae\u548c\u5e9e\u7edf\u7684 \u63a5\u53d7\u8bf7\u6c42 \u540e\uff0c\u4f1a\u8fdb\u884c\u5982\u4e0b\u5904\u7406\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u5173\u7fbd\u3001\u5f20\u98de\u3001\u8d75\u4e91\u6536\u5230\u8bf8\u845b\u4eae\u53d1\u9001\u7684\u63d0\u6848 [1\uff0c\u5317]\u65f6\u5019\uff0c\u56e0\u4e3a\u63d0\u6848\u7f16\u53f7 1 \u5c0f\u4e8e\u4ed6\u4eec\u627f\u8bfa\u7684\u80fd\u901a\u8fc7\u7684\u63d0\u6848\u7684\u6700\u5c0f\u63d0\u6848\u7f16\u53f7 2 \uff0c\u6240\u4ee5\u8bf8\u845b\u4eae\u7684\u63d0\u6848\u88ab\u62d2\u7edd\u4e86\u3002 \u800c\u5f53\u4ed6\u4eec\u6536\u5230\u5e9e\u7edf\u7684\u53d1\u9001\u7684\u63d0\u6848 [2\uff0c\u5357] \u7684\u65f6\u5019\uff0c\u56e0\u4e3a\u7f16\u53f7 2 \u4e0d\u5c0f\u4e8e\u4e4b\u524d\u627f\u8bfa\u7684\u7f16\u53f7 2\uff0c\u6240\u4ee5\u901a\u8fc7\u5e9e\u7edf\u7684\u63d0\u6848 [2\uff0c\u5357] \uff0c\u6240\u4ee5\u5173\u7fbd\u3001\u5f20\u98de\u3001\u8d75\u4e91\u4ed6\u4eec\u7684\u4f5c\u6218\u8ba1\u5212\u662f\u4ece\u5357\u8fb9\u8fdb\u653b\u66f9\u64cd\u3002 \u8fbe\u6210\u4e86\u5171\u8bc6 \u3002","title":"\u6536\u5230\u63a5\u53d7\u8bf7\u6c42"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/03-Paxos%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#_11","text":"\u5f53\u63a5\u53d7\u8005\u901a\u8fc7\u4e86\u4e00\u4e2a\u63d0\u6848\u65f6\uff0c\u5c31\u901a\u77e5\u6240\u6709\u7684\u5b66\u4e60\u8005\u3002\u5f53\u5b66\u4e60\u8005\u53d1\u73b0\u5927\u591a\u6570\u7684\u63a5\u53d7\u8005\u90fd\u901a\u8fc7\u4e86\u67d0\u4e2a\u63d0\u6848\uff0c\u90a3\u4e48\u5b66\u4e60\u8005\u4e5f\u4f1a\u901a\u8fc7\u8be5\u63d0\u6848\uff0c\u63a5\u53d7\u8be5\u63d0\u6848\u7684\u503c\u3002 \u4e5f\u5c31\u662f\u8bf4\u5173\u7fbd\u3001\u5f20\u98de\u3001\u8d75\u4e91\u8fbe\u6210\u4e86\u5171\u8bc6\u540e\uff0c\u5b66\u4e60\u8005 \u6cd5\u6b63 \u548c \u9a6c\u826f \u4e5f\u540c\u6837\u901a\u8fc7**\u4ece\u5357\u8fb9\u8fdb\u653b\u7684\u4f5c\u6218\u8ba1\u5212**\u3002","title":"\u5b66\u4e60\u8005\u767b\u573a"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/03-Paxos%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#_12","text":"1\u3001Basic Paxos \u4e5f\u662f\u901a\u8fc7\u4e8c\u9636\u6bb5\u63d0\u4ea4\u534f\u8bae\u8fbe\u6210\u5171\u8bc6\u3002\u51c6\u5907\u9636\u6bb5\u3001\u63a5\u53d7\u9636\u6bb5\u3002\u4e0d\u77e5\u9053\u4e8c\u9636\u6bb5\u63d0\u4ea4\u534f\u8bae\u7684\uff0c\u53ef\u4ee5\u770b\u6211\u524d\u9762\u7684\u6587\u7ae0\u3002\u300a\u7528\u592a\u6781\u62f3\u8bb2\u5206\u5e03\u5f0f\u7406\u8bba\uff0c\u8212\u670d\uff01\u300b 2\u3001Basic Paxos \u4e0d\u4ec5\u4ec5\u5b9e\u73b0\u4e86\u5171\u8bc6\uff0c\u8fd8\u5b9e\u73b0\u4e86\u5bb9\u9519\u3002\u6709\u5c11\u4e8e\u4e00\u534a\u7684\u8282\u70b9\u51fa\u73b0\u6545\u969c\u65f6\uff0c\u96c6\u7fa4\u4e5f\u80fd\u6b63\u5e38\u5de5\u4f5c\u3002\u6587\u4e2d\u4e5f\u591a\u6b21\u5f3a\u8c03\u4e86\u5927\u591a\u6570\u8282\u70b9\u90fd\u540c\u610f\u7684\u539f\u5219\uff0c\u800c\u8fd9\u4e2a\u539f\u5219\u8d4b\u4e88\u4e86 Basic Paxos \u5bb9\u9519\u7684\u80fd\u529b\u3002 3\u3001\u63d0\u6848\u7f16\u53f7\u4ee3\u8868\u4f18\u5148\u7ea7\uff0c\u4fdd\u8bc1\u4e86\u4e09\u4e2a\u627f\u8bfa\uff1a \u5982\u679c \u51c6\u5907\u8bf7\u6c42 \u7684\u63d0\u6848\u7f16\u53f7\uff0c \u5c0f\u4e8e\u7b49\u4e8e \u63a5\u53d7\u8005\u5df2\u7ecf\u54cd\u5e94\u7684 \u51c6\u5907\u8bf7\u6c42 \u7684\u63d0\u6848\u7f16\u53f7\uff0c\u90a3\u4e48\u63a5\u53d7\u8005\u5c06\u627f\u8bfa\u4e0d\u54cd\u5e94\u8fd9\u4e2a \u51c6\u5907\u8bf7\u6c42 \u3002 \u5982\u679c \u63a5\u53d7\u8bf7\u6c42 \u4e2d\u7684\u63d0\u6848\u7684\u63d0\u6848\u7f16\u53f7\uff0c \u5c0f\u4e8e \u63a5\u53d7\u8005\u5df2\u7ecf\u54cd\u5e94\u7684 \u51c6\u5907\u8bf7\u6c42 \u7684\u63d0\u6848\uff0c\u90a3\u4e48\u63a5\u53d7\u8005\u5c06\u627f\u8bfa\u4e0d\u901a\u8fc7\u8fd9\u4e2a\u63d0\u6848\u3002 \u5982\u679c\u63a5\u53d7\u8005\u4e4b\u524d\u6709\u901a\u8fc7\u63d0\u6848\uff0c\u90a3\u4e48\u63a5\u53d7\u8005\u5c06\u627f\u8bfa\uff0c\u4f1a\u5728 \u51c6\u5907\u8bf7\u6c42 \u7684\u54cd\u5e94\u4e2d\uff0c\u5305\u542b\u5df2\u7ecf\u901a\u8fc7\u7684\u6700\u5927\u7f16\u53f7\u7684\u63d0\u6848\u4fe1\u606f\u3002","title":"\u603b\u7ed3"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/03-Paxos%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#_13","text":"\u5982\u679c\u5173\u7fbd\u548c\u5f20\u98de\u5df2\u7ecf\u901a\u8fc7\u4e86\u63d0\u6848 [2\uff0c\u5357] \uff0c\u800c\u8d75\u4e91\u8fd8\u672a\u901a\u8fc7\u4efb\u4f55\u63d0\u6848\uff0c\u5f53\u7b2c\u4e09\u540d\u519b\u5e08 \u7b80\u96cd \u63d0\u51fa\u4e00\u4e2a\u63d0\u6848\uff0c\u7f16\u53f7\u4e3a 8\uff0c\u4f5c\u6218\u8ba1\u5212\u4e3a\u4ece\u4e1c\u8fb9\u8fdb\u653b\u66f9\u64cd\uff0c\u4e5f\u5c31\u662f [8, \u4e1c] \u7684\u4f5c\u6218\u8ba1\u5212\uff0c\u90a3\u4e48\u6700\u7ec8\u5173\u7fbd\u3001\u5f20\u98de\u3001\u8d75\u4e91\u7684\u4f5c\u6218\u8ba1\u5212\u662f\u600e\u4e48\u6837\u7684\uff1f\u6b22\u8fce\u8bc4\u8bba\u533a\u7559\u8a00\u3002","title":"\u52a0\u5206\u9898"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/04-Raft%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/","text":"csdn \u7528\u52a8\u56fe\u8bb2\u89e3\u5206\u5e03\u5f0f Raft \u4e00\u3001Raft \u6982\u8ff0 Raft \u7b97\u6cd5 \u662f\u5206\u5e03\u5f0f\u7cfb\u7edf\u5f00\u53d1\u9996\u9009\u7684 \u5171\u8bc6\u7b97\u6cd5 \u3002\u6bd4\u5982\u73b0\u5728\u6d41\u884c Etcd\u3001Consul\u3002 \u5982\u679c \u638c\u63e1 \u4e86\u8fd9\u4e2a\u7b97\u6cd5\uff0c\u5c31\u53ef\u4ee5\u8f83\u5bb9\u6613\u5730\u5904\u7406\u7edd\u5927\u90e8\u5206\u573a\u666f\u7684 \u5bb9\u9519 \u548c \u4e00\u81f4\u6027 \u9700\u6c42\u3002\u6bd4\u5982\u5206\u5e03\u5f0f\u914d\u7f6e\u7cfb\u7edf\u3001\u5206\u5e03\u5f0f NoSQL \u5b58\u50a8\u7b49\u7b49\uff0c\u8f7b\u677e\u7a81\u7834\u7cfb\u7edf\u7684\u5355\u673a\u9650\u5236\u3002 Raft \u7b97\u6cd5\u662f\u901a\u8fc7\u4e00\u5207\u4ee5\u9886\u5bfc\u8005\u4e3a\u51c6\u7684\u65b9\u5f0f\uff0c\u5b9e\u73b0\u4e00\u7cfb\u5217\u503c\u7684\u5171\u8bc6\u548c\u5404\u8282\u70b9\u65e5\u5fd7\u7684\u4e00\u81f4\u3002 NOTE: replication log \u4e8c\u3001Raft \u89d2\u8272 2.1 \u89d2\u8272 \u8ddf\u968f\u8005\uff08Follower\uff09 \u8ddf\u968f\u8005\uff08Follower\uff09 \uff1a \u666e\u901a\u7fa4\u4f17 \uff0c\u9ed8\u9ed8\u63a5\u6536\u548c\u6765\u81ea\u9886\u5bfc\u8005\u7684\u6d88\u606f\uff0c\u5f53\u9886\u5bfc\u8005\u5fc3\u8df3\u4fe1\u606f\u8d85\u65f6\u7684\u65f6\u5019\uff0c\u5c31\u4e3b\u52a8\u7ad9\u51fa\u6765\uff0c\u63a8\u8350\u81ea\u5df1\u5f53\u5019\u9009\u4eba\u3002 \u5019\u9009\u4eba\uff08Candidate\uff09 \u5019\u9009\u4eba\uff08Candidate\uff09 \uff1a \u5019\u9009\u4eba \u5c06\u5411\u5176\u4ed6\u8282\u70b9\u8bf7\u6c42\u6295\u7968 RPC \u6d88\u606f\uff0c\u901a\u77e5\u5176\u4ed6\u8282\u70b9\u6765\u6295\u7968\uff0c\u5982\u679c\u8d62\u5f97\u4e86\u5927\u591a\u6570\u6295\u7968\u9009\u7968\uff0c\u5c31\u664b\u5347\u5f53\u9886\u5bfc\u8005\u3002 \u9886\u5bfc\u8005\uff08Leader\uff09 \u9886\u5bfc\u8005\uff08Leader\uff09 \uff1a \u9738\u9053\u603b\u88c1 \uff0c\u4e00\u5207\u4ee5\u6211\u4e3a\u51c6\u3002\u5904\u7406\u5199\u8bf7\u6c42\u3001\u7ba1\u7406\u65e5\u5fd7\u590d\u5236\u548c\u4e0d\u65ad\u5730\u53d1\u9001\u5fc3\u8df3\u4fe1\u606f\uff0c\u901a\u77e5\u5176\u4ed6\u8282\u70b9\u201c\u6211\u662f\u9886\u5bfc\u8005\uff0c\u6211\u8fd8\u6d3b\u7740\uff0c\u4f60\u4eec\u4e0d\u8981\u201d\u53d1\u8d77\u65b0\u7684\u9009\u4e3e\uff0c\u4e0d\u7528\u627e\u65b0\u9886\u5bfc\u6765\u66ff\u4ee3\u6211\u3002 \u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5206\u522b\u7528\u4e09\u79cd\u56fe\u4ee3\u8868\u8ddf\u968f\u8005\u3001\u5019\u9009\u4eba\u548c\u9886\u5bfc\u8005\u3002 \u4e09\u3001\u5355\u8282\u70b9\u7cfb\u7edf 3.1 \u6570\u636e\u5e93\u670d\u52a1\u5668 3.2 \u5ba2\u6237\u7aef \u5de6\u8fb9\u7eff\u8272\u7684\u5b9e\u5fc3\u5708\u5c31\u662f\u5ba2\u6237\u7aef\uff0c\u53f3\u8fb9\u7684\u84dd\u8272\u5b9e\u5fc3\u5708\u5c31\u662f\u8282\u70b9 a\uff08Node a\uff09\u3002Term \u4ee3\u8868\u4efb\u671f\uff0c\u540e\u9762\u4f1a\u8bb2\u5230\u3002 3.3 \u5ba2\u6237\u7aef\u5411\u670d\u52a1\u5668\u53d1\u9001\u6570\u636e \u5ba2\u6237\u7aef\u5411\u5355\u8282\u70b9\u670d\u52a1\u5668\u53d1\u9001\u4e86\u4e00\u6761\u66f4\u65b0\u64cd\u4f5c\uff0c\u8bbe\u7f6e\u6570\u636e\u5e93\u4e2d\u5b58\u7684\u503c\u4e3a 8\u3002\u5355\u673a\u73af\u5883\u4e0b\uff08\u5355\u4e2a\u670d\u52a1\u5668\u8282\u70b9\uff09\uff0c\u5ba2\u6237\u7aef\u4ece\u670d\u52a1\u5668\u62ff\u5230\u7684\u503c\u4e5f\u662f 8\u3002\u4e00\u81f4\u6027\u975e\u5e38\u5bb9\u6613\u4fdd\u8bc1\u3002 3.4 \u591a\u8282\u70b9\u5982\u4f55\u4fdd\u8bc1\u4e00\u81f4\u6027\uff1f \u4f46\u5982\u679c\u6709\u591a\u4e2a\u670d\u52a1\u5668\u8282\u70b9\uff0c\u600e\u4e48\u4fdd\u8bc1\u4e00\u81f4\u6027\u5462\uff1f\u6bd4\u5982\u6709\u4e09\u4e2a\u8282\u70b9\uff1aa\uff0cb\uff0cc\u3002\u5982\u4e0b\u56fe\u6240\u793a\u3002\u8fd9\u4e09\u4e2a\u8282\u70b9\u7ec4\u6210\u4e00\u4e2a\u6570\u636e\u5e93\u96c6\u7fa4\u3002\u5ba2\u6237\u7aef\u5bf9\u8fd9\u4e09\u4e2a\u8282\u70b9\u8fdb\u884c\u66f4\u65b0\u64cd\u4f5c\uff0c\u5982\u4f55\u4fdd\u8bc1\u4e09\u4e2a\u8282\u70b9\u4e2d\u5b58\u7684\u503c\u4e00\u81f4\uff1f\u8fd9\u4e2a\u5c31\u662f\u5206\u5e03\u5f0f\u4e00\u81f4\u6027\u95ee\u9898\u3002Raft \u7b97\u6cd5\u5c31\u662f\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u7684\u3002\u5f53\u7136\u8fd8\u6709\u5176\u4ed6\u534f\u8bae\u4e5f\u53ef\u4ee5\u4fdd\u8bc1\uff0c\u672c\u7bc7\u53ea\u9488\u5bf9 Raft \u7b97\u6cd5\u3002 \u5728\u591a\u8282\u70b9\u96c6\u7fa4\u4e2d\uff0c\u5728\u8282\u70b9\u6545\u969c\u3001\u5206\u533a\u9519\u8bef\u7b49\u5f02\u5e38\u60c5\u51b5\u4e0b\uff0cRaft \u7b97\u6cd5\u5982\u4f55\u4fdd\u8bc1\u5728\u540c\u4e00\u4e2a\u65f6\u95f4\uff0c\u96c6\u7fa4\u4e2d\u53ea\u6709\u4e00\u4e2a\u9886\u5bfc\u8005\u5462\uff1f\u4e0b\u9762\u5c31\u5f00\u59cb\u8bb2\u89e3 Raft \u7b97\u6cd5\u9009\u4e3e\u9886\u5bfc\u8005\u7684\u8fc7\u7a0b\u3002 \u56db\u3001\u9009\u4e3e\u9886\u5bfc\u8fc7\u7a0b 4.1 \u521d\u59cb\u72b6\u6001 \u521d\u59cb\u72b6\u6001\u4e0b\uff0c\u96c6\u7fa4\u4e2d\u6240\u6709\u8282\u70b9\u90fd\u662f**\u8ddf\u968f\u8005**\u7684\u72b6\u6001\u3002 \u5982\u4e0b\u56fe\u6240\u793a\uff0c\u6709\u4e09\u4e2a\u8282\u70b9(Node) a\u3001b\u3001c\uff0c\u4efb\u671f\uff08Term\uff09\u90fd\u4e3a 0\u3002 4.2 \u6210\u4e3a\u5019\u9009\u8005 **Raft \u7b97\u6cd5**\u5b9e\u73b0\u4e86\u968f\u673a\u8d85\u65f6\u65f6\u95f4\u7684\u7279\u6027\uff0c\u6bcf\u4e2a\u8282\u70b9\u7b49\u5f85\u9886\u5bfc\u8005\u8282\u70b9\u5fc3\u8df3\u4fe1\u606f\u7684\u8d85\u65f6\u65f6\u95f4\u95f4\u9694\u662f\u968f\u673a\u7684\u3002\u6bd4\u5982 A \u8282\u70b9\u7b49\u5f85\u8d85\u65f6\u7684\u65f6\u95f4\u95f4\u9694 150 ms\uff0cB \u8282\u70b9 200 ms\uff0cC \u8282\u70b9 300 ms\u3002\u90a3\u4e48 a \u5148\u8d85\u65f6\uff0c\u6700\u5148\u56e0\u4e3a\u6ca1\u6709\u7b49\u5230\u9886\u5bfc\u8005\u7684\u5fc3\u8df3\u4fe1\u606f\uff0c\u53d1\u751f\u8d85\u65f6\u3002\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u4e09\u4e2a\u8282\u70b9\u7684**\u8d85\u65f6\u8ba1\u65f6\u5668**\u5f00\u59cb\u8fd0\u884c\u3002 \u5f53 A \u8282\u70b9\u7684\u8d85\u65f6\u65f6\u95f4\u5230\u4e86\u540e\uff0cA \u8282\u70b9\u6210\u4e3a**\u5019\u9009\u8005**\uff0c\u5e76\u589e\u52a0\u81ea\u5df1\u7684\u4efb\u671f\u7f16\u53f7\uff0cTerm \u503c\u4ece 0 \u66f4\u65b0\u4e3a 1\uff0c\u5e76\u7ed9\u81ea\u5df1\u6295\u4e86\u4e00\u7968\u3002 1\u3001Node A\uff1aTerm = 1, Vote Count = 1\u3002 2\u3001Node B\uff1aTerm = 0\u3002 3\u3001Node C\uff1aTerm = 0\u3002 4.3 \u6295\u7968 \u6211\u4eec\u6765\u770b\u4e0b**\u5019\u9009\u8005**\u5982\u4f55\u6210\u4e3a**\u9886\u5bfc\u8005**\u7684\u3002 1\u3001 \u7b2c\u4e00\u6b65 \uff1a\u8282\u70b9 A \u6210\u4e3a\u5019\u9009\u8005\u540e\uff0c\u5411\u5176\u4ed6\u8282\u70b9\u53d1\u9001\u8bf7\u6c42\u6295\u7968 RPC \u4fe1\u606f\uff0c\u8bf7\u5b83\u4eec\u9009\u4e3e\u81ea\u5df1\u4e3a\u9886\u5bfc\u8005\u3002 2\u3001 \u7b2c\u4e8c\u6b65 \uff1a\u8282\u70b9 B \u548c \u8282\u70b9 C \u63a5\u6536\u5230\u8282\u70b9 A \u53d1\u9001\u7684\u8bf7\u6c42\u6295\u7968\u4fe1\u606f\u540e\uff0c\u5728\u7f16\u53f7\u4e3a 1 \u7684\u8fd9\u5c4a\u4efb\u671f\u5185\uff0c\u8fd8\u6ca1\u6709\u8fdb\u884c\u8fc7\u6295\u7968\uff0c\u5c31\u628a\u9009\u7968\u6295\u7ed9\u8282\u70b9 A\uff0c\u5e76\u589e\u52a0\u81ea\u5df1\u7684\u4efb\u671f\u7f16\u53f7\u3002 3\u3001 \u7b2c\u4e09\u6b65 \uff1a\u8282\u70b9 A \u6536\u5230 3 \u6b21\u6295\u7968\uff0c\u5f97\u5230\u4e86\u5927\u591a\u6570\u8282\u70b9\u7684\u6295\u7968\uff0c\u4ece\u5019\u9009\u8005\u6210\u4e3a\u672c\u5c4a\u4efb\u671f\u5185\u7684\u65b0\u7684\u9886\u5bfc\u8005\u3002 4\u3001 \u7b2c\u56db\u6b65 \uff1a\u8282\u70b9 A \u4f5c\u4e3a\u9886\u5bfc\u8005\uff0c\u56fa\u5b9a\u7684\u65f6\u95f4\u95f4\u9694\u7ed9 \u8282\u70b9 B \u548c\u8282\u70b9 C \u53d1\u9001\u5fc3\u8df3\u4fe1\u606f\uff0c\u544a\u8bc9\u8282\u70b9 B \u548c C\uff0c\u6211\u662f\u9886\u5bfc\u8005\uff0c\u963b\u6b62\u5176\u4ed6\u8ddf\u968f\u8005\u53d1\u8d77\u65b0\u7684\u9009\u4e3e\u3002 5\u3001 \u7b2c\u4e94\u6b65 \uff1a\u8282\u70b9 B \u548c\u8282\u70b9 C \u53d1\u9001\u54cd\u5e94\u4fe1\u606f\u7ed9\u8282\u70b9 A\uff0c\u544a\u8bc9\u8282\u70b9 A \u6211\u662f\u6b63\u5e38\u7684\u3002 4.4 \u4efb\u671f \u82f1\u6587\u5355\u8bcd\u662f term\uff0c\u9886\u5bfc\u8005\u662f\u6709\u4efb\u671f\u7684\u3002 1\u3001 \u81ea\u52a8\u589e\u52a0 \uff1a \u8ddf\u968f\u8005**\u5728\u7b49\u5f85**\u9886\u5bfc\u8005**\u5fc3\u8df3\u4fe1\u606f\u8d85\u65f6\u540e\uff0c\u63a8\u8350\u81ea\u5df1\u4e3a**\u5019\u9009\u4eba \uff0c\u4f1a\u589e\u52a0\u81ea\u5df1\u7684**\u4efb\u671f\u53f7**\uff0c\u5982\u4e0a\u56fe\u6240\u793a\uff0c\u8282\u70b9 A \u4efb\u671f\u4e3a 0\uff0c\u63a8\u4e3e\u81ea\u5df1\u4e3a\u5019\u9009\u4eba\u65f6\uff0c\u4efb\u671f\u7f16\u53f7\u589e\u52a0\u4e3a 1\u3002 2\u3001 \u66f4\u65b0\u4e3a\u8f83\u5927\u503c \uff1a\u5f53\u8282\u70b9\u53d1\u73b0\u81ea\u5df1\u7684\u4efb\u671f\u7f16\u53f7\u6bd4\u5176\u4ed6\u8282\u70b9\u5c0f\u65f6\uff0c\u4f1a\u66f4\u65b0\u5230\u8f83\u5927\u7684\u7f16\u53f7\u503c\u3002\u6bd4\u5982\u8282\u70b9 A \u7684\u4efb\u671f\u4e3a 1\uff0c\u8bf7\u6c42\u6295\u7968\uff0c\u6295\u7968\u6d88\u606f\u4e2d\u5305\u542b\u4e86\u8282\u70b9 A \u7684\u4efb\u671f\u7f16\u53f7\uff0c\u4e14\u7f16\u53f7\u4e3a 1\uff0c\u8282\u70b9 B \u6536\u5230\u6d88\u606f\u540e\uff0c\u4f1a\u5c06\u81ea\u5df1\u7684\u4efb\u671f\u7f16\u53f7\u66f4\u65b0\u4e3a 1\u3002 3\u3001 \u6062\u590d\u4e3a\u8ddf\u968f\u8005 \uff1a\u5982\u679c\u4e00\u4e2a\u5019\u9009\u4eba\u6216\u8005\u9886\u5bfc\u8005\uff0c\u53d1\u73b0\u81ea\u5df1\u7684\u4efb\u671f\u7f16\u53f7\u6bd4\u5176\u4ed6\u8282\u70b9\u5c0f\uff0c\u90a3\u4e48\u5b83\u4f1a\u7acb\u5373\u6062\u590d\u6210\u8ddf\u968f\u8005\u72b6\u6001\u3002\u8fd9\u79cd\u573a\u666f\u51fa\u73b0\u5728\u5206\u533a\u9519\u8bef\u6062\u590d\u540e\uff0c\u4efb\u671f\u4e3a 3 \u7684\u9886\u5bfc\u8005\u53d7\u5230\u4efb\u671f\u7f16\u53f7\u4e3a 4 \u7684\u5fc3\u8df3\u6d88\u606f\uff0c\u90a3\u4e48\u524d\u8005\u5c06\u7acb\u5373\u6062\u590d\u6210\u8ddf\u968f\u8005\u72b6\u6001\u3002 4\u3001 \u62d2\u7edd\u6d88\u606f \uff1a\u5982\u679c\u4e00\u4e2a\u8282\u70b9\u63a5\u6536\u5230\u8f83\u5c0f\u7684\u4efb\u671f\u7f16\u53f7\u503c\u7684\u8bf7\u6c42\uff0c\u90a3\u4e48\u5b83\u4f1a\u76f4\u63a5\u62d2\u7edd\u8fd9\u4e2a\u8bf7\u6c42\uff0c\u6bd4\u5982\u4efb\u671f\u7f16\u53f7\u4e3a 6 \u7684\u8282\u70b9 A\uff0c\u6536\u5230\u4efb\u671f\u7f16\u53f7\u4e3a 5 \u7684\u8282\u70b9 B \u7684\u8bf7\u6c42\u6295\u7968 RPC \u6d88\u606f\uff0c\u90a3\u4e48\u8282\u70b9 A \u4f1a\u62d2\u7edd\u8fd9\u4e2a\u6d88\u606f\u3002 4.5 \u9009\u4e3e\u89c4\u5219 1\u3001\u4e00\u4e2a\u4efb\u671f\u5185\uff0c\u9886\u5bfc\u8005\u4e00\u76f4\u90fd\u4f1a\u9886\u5bfc\u8005\uff0c\u76f4\u5230\u81ea\u8eab\u51fa\u73b0\u95ee\u9898\uff08\u5982\u5b95\u673a\uff09\uff0c\u6216\u8005\u7f51\u7edc\u95ee\u9898\uff08\u5ef6\u8fdf\uff09\uff0c\u5176\u4ed6\u8282\u70b9\u53d1\u8d77\u4e00\u8f6e\u65b0\u7684\u9009\u4e3e\u3002 2\u3001\u5728\u4e00\u6b21\u9009\u4e3e\u4e2d\uff0c\u6bcf\u4e00\u4e2a\u670d\u52a1\u5668\u8282\u70b9\u6700\u591a\u4f1a\u5bf9\u4e00\u4e2a\u4efb\u671f\u7f16\u53f7\u6295\u51fa\u4e00\u5f20\u9009\u7968\uff0c\u6295\u5b8c\u4e86\u5c31\u6ca1\u4e86\u3002 4.6 \u5927\u591a\u6570 \u5047\u8bbe\u4e00\u4e2a\u96c6\u7fa4\u7531 N \u4e2a\u8282\u70b9\u7ec4\u6210\uff0c\u90a3\u4e48\u5927\u591a\u6570\u5c31\u662f\u81f3\u5c11 N/2+1\u3002\u4f8b\u5982\uff1a 3 \u4e2a\u8282\u70b9\u7684\u96c6\u7fa4\uff0c\u5927\u591a\u6570\u5c31\u662f 2\u3002 4.7 \u5fc3\u8df3\u8d85\u65f6 \u4e3a\u4e86\u9632\u6b62\u591a\u4e2a\u8282\u70b9\u540c\u65f6\u53d1\u8d77\u6295\u7968\uff0c\u4f1a\u7ed9\u6bcf\u4e2a\u8282\u70b9\u5206\u914d\u4e00\u4e2a\u968f\u673a\u7684\u9009\u4e3e\u8d85\u65f6\u65f6\u95f4\u3002\u8fd9\u4e2a\u65f6\u95f4\u5185\uff0c\u8282\u70b9\u4e0d\u80fd\u6210\u4e3a\u5019\u9009\u8005\uff0c\u53ea\u80fd\u7b49\u5230\u8d85\u65f6\u3002\u6bd4\u5982\u4e0a\u8ff0\u4f8b\u5b50\uff0c\u8282\u70b9 A \u5148\u8d85\u65f6\uff0c\u5148\u6210\u4e3a\u4e86\u5019\u9009\u8005\u3002\u8fd9\u79cd\u5de7\u5999\u7684\u8bbe\u8ba1\uff0c\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u53ea\u6709\u4e00\u4e2a\u670d\u52a1\u5668\u8282\u70b9\u5148\u53d1\u8d77\u9009\u4e3e\uff0c\u800c\u4e0d\u662f\u540c\u65f6\u53d1\u8d77\u9009\u4e3e\uff0c\u51cf\u5c11\u4e86\u56e0\u9009\u7968\u74dc\u5206\u5bfc\u81f4\u9009\u4e3e\u5931\u8d25\u7684\u60c5\u51b5\u3002 NOTE: \u54ea\u79cd\u60c5\u51b5\u4e0b\u4f1a\u51fa\u73b0\"\u9009\u7968\u74dc\u5206\u5bfc\u81f4\u9009\u4e3e\u5931\u8d25\u7684\u60c5\u51b5\"\uff1f \u5728Redis\u4e2d\uff0c\u4e5f\u6709\u7c7b\u4f3c\u7684\u5904\u7406\uff0c\u53c2\u89c1 cnblogs Redis\u6e90\u7801\u89e3\u6790\uff1a20sentinel(\u4e00)\u521d\u59cb\u5316\u3001\u5efa\u94fe \u4e94\u3001\u9886\u5bfc\u8005\u6545\u969c \u5982\u679c\u9886\u5bfc\u8005\u8282\u70b9\u51fa\u73b0\u6545\u969c\uff0c\u5219\u4f1a\u89e6\u53d1\u65b0\u7684\u4e00\u8f6e\u9009\u4e3e\u3002\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u9886\u5bfc\u8005\u8282\u70b9 B \u53d1\u751f\u6545\u969c\uff0c\u8282\u70b9 A \u548c \u8282\u70b9 B \u5c31\u4f1a\u91cd\u65b0\u9009\u4e3e Leader\u3002 1\u3001\u7b2c\u4e00\u6b65 \uff1a\u8282\u70b9 A \u53d1\u751f\u6545\u969c\uff0c\u8282\u70b9 B \u548c\u8282\u70b9 C \u6ca1\u6709\u6536\u5230\u9886\u5bfc\u8005\u8282\u70b9 A \u7684\u5fc3\u8df3\u4fe1\u606f\uff0c\u7b49\u5f85\u8d85\u65f6\u3002 2\u3001\u7b2c\u4e8c\u6b65\uff1a\u8282\u70b9 C \u5148\u53d1\u751f\u8d85\u65f6\uff0c\u8282\u70b9 C \u6210\u4e3a\u5019\u9009\u4eba\u3002 3\u3001\u7b2c\u4e09\u6b65\uff1a\u8282\u70b9 C \u5411\u8282\u70b9 A \u548c \u8282\u70b9 B \u53d1\u8d77\u8bf7\u6c42\u6295\u7968\u4fe1\u606f\u3002 4\u3001\u7b2c\u56db\u6b65\uff1a\u8282\u70b9 C \u54cd\u5e94\u6295\u7968\uff0c\u5c06\u7968\u6295\u7ed9\u4e86 C\uff0c\u800c\u8282\u70b9 A \u56e0\u4e3a\u53d1\u751f\u6545\u969c\u4e86\uff0c\u65e0\u6cd5\u54cd\u5e94 C \u7684\u6295\u7968\u8bf7\u6c42\u3002 5\u3001\u7b2c\u4e94\u6b65\uff1a\u8282\u70b9 C \u6536\u5230\u4e24\u7968\uff08\u5927\u591a\u6570\u7968\u6570\uff09\uff0c\u6210\u4e3a\u9886\u5bfc\u8005\u3002 6\u3001\u7b2c\u516d\u6b65\uff1a\u8282\u70b9 C \u5411\u8282\u70b9 A \u548c B \u53d1\u9001\u5fc3\u8df3\u4fe1\u606f\uff0c\u8282\u70b9 B \u54cd\u5e94\u5fc3\u8df3\u4fe1\u606f\uff0c\u8282\u70b9 A \u4e0d\u54cd\u5e94\u5fc3\u8df3\u4fe1\u606f\u3002 \u603b\u7ed3 Raft \u7b97\u6cd5\u901a\u8fc7\u4ee5\u4e0b\u51e0\u79cd\u65b9\u5f0f\u6765\u8fdb\u884c\u9886\u5bfc\u9009\u4e3e\uff0c\u4fdd\u8bc1\u4e86\u4e00\u4e2a\u4efb\u671f\u53ea\u6709\u4e00\u4f4d\u9886\u5bfc\uff0c\u6781\u5927\u51cf\u5c11\u4e86\u9009\u4e3e\u5931\u8d25\u7684\u60c5\u51b5\u3002 1\u3001\u4efb\u671f 2\u3001\u9886\u5bfc\u8005\u5fc3\u8df3\u4fe1\u606f 3\u3001\u968f\u673a\u9009\u4e3e\u8d85\u65f6\u65f6\u95f4 4\u3001\u5148\u6765\u5148\u670d\u52a1\u7684\u6295\u7968\u539f\u5219 5\u3001\u5927\u591a\u6570\u9009\u7968\u539f\u5219 \u672c\u7bc7\u901a\u8fc7\u52a8\u56fe\u7684\u65b9\u5f0f\u6765\u8bb2\u89e3 Raft \u7b97\u6cd5\u5982\u4f55\u9009\u4e3e\u9886\u5bfc\u8005\uff0c\u66f4\u5bb9\u6613\u7406\u89e3\u548c\u6d88\u5316\u3002","title":"Introduction"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/04-Raft%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#csdn#raft","text":"","title":"csdn \u7528\u52a8\u56fe\u8bb2\u89e3\u5206\u5e03\u5f0f Raft"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/04-Raft%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#raft","text":"Raft \u7b97\u6cd5 \u662f\u5206\u5e03\u5f0f\u7cfb\u7edf\u5f00\u53d1\u9996\u9009\u7684 \u5171\u8bc6\u7b97\u6cd5 \u3002\u6bd4\u5982\u73b0\u5728\u6d41\u884c Etcd\u3001Consul\u3002 \u5982\u679c \u638c\u63e1 \u4e86\u8fd9\u4e2a\u7b97\u6cd5\uff0c\u5c31\u53ef\u4ee5\u8f83\u5bb9\u6613\u5730\u5904\u7406\u7edd\u5927\u90e8\u5206\u573a\u666f\u7684 \u5bb9\u9519 \u548c \u4e00\u81f4\u6027 \u9700\u6c42\u3002\u6bd4\u5982\u5206\u5e03\u5f0f\u914d\u7f6e\u7cfb\u7edf\u3001\u5206\u5e03\u5f0f NoSQL \u5b58\u50a8\u7b49\u7b49\uff0c\u8f7b\u677e\u7a81\u7834\u7cfb\u7edf\u7684\u5355\u673a\u9650\u5236\u3002 Raft \u7b97\u6cd5\u662f\u901a\u8fc7\u4e00\u5207\u4ee5\u9886\u5bfc\u8005\u4e3a\u51c6\u7684\u65b9\u5f0f\uff0c\u5b9e\u73b0\u4e00\u7cfb\u5217\u503c\u7684\u5171\u8bc6\u548c\u5404\u8282\u70b9\u65e5\u5fd7\u7684\u4e00\u81f4\u3002 NOTE: replication log","title":"\u4e00\u3001Raft \u6982\u8ff0"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/04-Raft%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#raft_1","text":"","title":"\u4e8c\u3001Raft \u89d2\u8272"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/04-Raft%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#21","text":"","title":"2.1 \u89d2\u8272"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/04-Raft%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#follower","text":"\u8ddf\u968f\u8005\uff08Follower\uff09 \uff1a \u666e\u901a\u7fa4\u4f17 \uff0c\u9ed8\u9ed8\u63a5\u6536\u548c\u6765\u81ea\u9886\u5bfc\u8005\u7684\u6d88\u606f\uff0c\u5f53\u9886\u5bfc\u8005\u5fc3\u8df3\u4fe1\u606f\u8d85\u65f6\u7684\u65f6\u5019\uff0c\u5c31\u4e3b\u52a8\u7ad9\u51fa\u6765\uff0c\u63a8\u8350\u81ea\u5df1\u5f53\u5019\u9009\u4eba\u3002","title":"\u8ddf\u968f\u8005\uff08Follower\uff09"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/04-Raft%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#candidate","text":"\u5019\u9009\u4eba\uff08Candidate\uff09 \uff1a \u5019\u9009\u4eba \u5c06\u5411\u5176\u4ed6\u8282\u70b9\u8bf7\u6c42\u6295\u7968 RPC \u6d88\u606f\uff0c\u901a\u77e5\u5176\u4ed6\u8282\u70b9\u6765\u6295\u7968\uff0c\u5982\u679c\u8d62\u5f97\u4e86\u5927\u591a\u6570\u6295\u7968\u9009\u7968\uff0c\u5c31\u664b\u5347\u5f53\u9886\u5bfc\u8005\u3002","title":"\u5019\u9009\u4eba\uff08Candidate\uff09"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/04-Raft%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#leader","text":"\u9886\u5bfc\u8005\uff08Leader\uff09 \uff1a \u9738\u9053\u603b\u88c1 \uff0c\u4e00\u5207\u4ee5\u6211\u4e3a\u51c6\u3002\u5904\u7406\u5199\u8bf7\u6c42\u3001\u7ba1\u7406\u65e5\u5fd7\u590d\u5236\u548c\u4e0d\u65ad\u5730\u53d1\u9001\u5fc3\u8df3\u4fe1\u606f\uff0c\u901a\u77e5\u5176\u4ed6\u8282\u70b9\u201c\u6211\u662f\u9886\u5bfc\u8005\uff0c\u6211\u8fd8\u6d3b\u7740\uff0c\u4f60\u4eec\u4e0d\u8981\u201d\u53d1\u8d77\u65b0\u7684\u9009\u4e3e\uff0c\u4e0d\u7528\u627e\u65b0\u9886\u5bfc\u6765\u66ff\u4ee3\u6211\u3002 \u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5206\u522b\u7528\u4e09\u79cd\u56fe\u4ee3\u8868\u8ddf\u968f\u8005\u3001\u5019\u9009\u4eba\u548c\u9886\u5bfc\u8005\u3002","title":"\u9886\u5bfc\u8005\uff08Leader\uff09"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/04-Raft%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#_1","text":"","title":"\u4e09\u3001\u5355\u8282\u70b9\u7cfb\u7edf"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/04-Raft%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#31","text":"","title":"3.1 \u6570\u636e\u5e93\u670d\u52a1\u5668"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/04-Raft%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#32","text":"\u5de6\u8fb9\u7eff\u8272\u7684\u5b9e\u5fc3\u5708\u5c31\u662f\u5ba2\u6237\u7aef\uff0c\u53f3\u8fb9\u7684\u84dd\u8272\u5b9e\u5fc3\u5708\u5c31\u662f\u8282\u70b9 a\uff08Node a\uff09\u3002Term \u4ee3\u8868\u4efb\u671f\uff0c\u540e\u9762\u4f1a\u8bb2\u5230\u3002","title":"3.2 \u5ba2\u6237\u7aef"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/04-Raft%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#33","text":"\u5ba2\u6237\u7aef\u5411\u5355\u8282\u70b9\u670d\u52a1\u5668\u53d1\u9001\u4e86\u4e00\u6761\u66f4\u65b0\u64cd\u4f5c\uff0c\u8bbe\u7f6e\u6570\u636e\u5e93\u4e2d\u5b58\u7684\u503c\u4e3a 8\u3002\u5355\u673a\u73af\u5883\u4e0b\uff08\u5355\u4e2a\u670d\u52a1\u5668\u8282\u70b9\uff09\uff0c\u5ba2\u6237\u7aef\u4ece\u670d\u52a1\u5668\u62ff\u5230\u7684\u503c\u4e5f\u662f 8\u3002\u4e00\u81f4\u6027\u975e\u5e38\u5bb9\u6613\u4fdd\u8bc1\u3002","title":"3.3 \u5ba2\u6237\u7aef\u5411\u670d\u52a1\u5668\u53d1\u9001\u6570\u636e"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/04-Raft%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#34","text":"\u4f46\u5982\u679c\u6709\u591a\u4e2a\u670d\u52a1\u5668\u8282\u70b9\uff0c\u600e\u4e48\u4fdd\u8bc1\u4e00\u81f4\u6027\u5462\uff1f\u6bd4\u5982\u6709\u4e09\u4e2a\u8282\u70b9\uff1aa\uff0cb\uff0cc\u3002\u5982\u4e0b\u56fe\u6240\u793a\u3002\u8fd9\u4e09\u4e2a\u8282\u70b9\u7ec4\u6210\u4e00\u4e2a\u6570\u636e\u5e93\u96c6\u7fa4\u3002\u5ba2\u6237\u7aef\u5bf9\u8fd9\u4e09\u4e2a\u8282\u70b9\u8fdb\u884c\u66f4\u65b0\u64cd\u4f5c\uff0c\u5982\u4f55\u4fdd\u8bc1\u4e09\u4e2a\u8282\u70b9\u4e2d\u5b58\u7684\u503c\u4e00\u81f4\uff1f\u8fd9\u4e2a\u5c31\u662f\u5206\u5e03\u5f0f\u4e00\u81f4\u6027\u95ee\u9898\u3002Raft \u7b97\u6cd5\u5c31\u662f\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u7684\u3002\u5f53\u7136\u8fd8\u6709\u5176\u4ed6\u534f\u8bae\u4e5f\u53ef\u4ee5\u4fdd\u8bc1\uff0c\u672c\u7bc7\u53ea\u9488\u5bf9 Raft \u7b97\u6cd5\u3002 \u5728\u591a\u8282\u70b9\u96c6\u7fa4\u4e2d\uff0c\u5728\u8282\u70b9\u6545\u969c\u3001\u5206\u533a\u9519\u8bef\u7b49\u5f02\u5e38\u60c5\u51b5\u4e0b\uff0cRaft \u7b97\u6cd5\u5982\u4f55\u4fdd\u8bc1\u5728\u540c\u4e00\u4e2a\u65f6\u95f4\uff0c\u96c6\u7fa4\u4e2d\u53ea\u6709\u4e00\u4e2a\u9886\u5bfc\u8005\u5462\uff1f\u4e0b\u9762\u5c31\u5f00\u59cb\u8bb2\u89e3 Raft \u7b97\u6cd5\u9009\u4e3e\u9886\u5bfc\u8005\u7684\u8fc7\u7a0b\u3002","title":"3.4 \u591a\u8282\u70b9\u5982\u4f55\u4fdd\u8bc1\u4e00\u81f4\u6027\uff1f"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/04-Raft%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#_2","text":"","title":"\u56db\u3001\u9009\u4e3e\u9886\u5bfc\u8fc7\u7a0b"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/04-Raft%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#41","text":"\u521d\u59cb\u72b6\u6001\u4e0b\uff0c\u96c6\u7fa4\u4e2d\u6240\u6709\u8282\u70b9\u90fd\u662f**\u8ddf\u968f\u8005**\u7684\u72b6\u6001\u3002 \u5982\u4e0b\u56fe\u6240\u793a\uff0c\u6709\u4e09\u4e2a\u8282\u70b9(Node) a\u3001b\u3001c\uff0c\u4efb\u671f\uff08Term\uff09\u90fd\u4e3a 0\u3002","title":"4.1 \u521d\u59cb\u72b6\u6001"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/04-Raft%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#42","text":"**Raft \u7b97\u6cd5**\u5b9e\u73b0\u4e86\u968f\u673a\u8d85\u65f6\u65f6\u95f4\u7684\u7279\u6027\uff0c\u6bcf\u4e2a\u8282\u70b9\u7b49\u5f85\u9886\u5bfc\u8005\u8282\u70b9\u5fc3\u8df3\u4fe1\u606f\u7684\u8d85\u65f6\u65f6\u95f4\u95f4\u9694\u662f\u968f\u673a\u7684\u3002\u6bd4\u5982 A \u8282\u70b9\u7b49\u5f85\u8d85\u65f6\u7684\u65f6\u95f4\u95f4\u9694 150 ms\uff0cB \u8282\u70b9 200 ms\uff0cC \u8282\u70b9 300 ms\u3002\u90a3\u4e48 a \u5148\u8d85\u65f6\uff0c\u6700\u5148\u56e0\u4e3a\u6ca1\u6709\u7b49\u5230\u9886\u5bfc\u8005\u7684\u5fc3\u8df3\u4fe1\u606f\uff0c\u53d1\u751f\u8d85\u65f6\u3002\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u4e09\u4e2a\u8282\u70b9\u7684**\u8d85\u65f6\u8ba1\u65f6\u5668**\u5f00\u59cb\u8fd0\u884c\u3002 \u5f53 A \u8282\u70b9\u7684\u8d85\u65f6\u65f6\u95f4\u5230\u4e86\u540e\uff0cA \u8282\u70b9\u6210\u4e3a**\u5019\u9009\u8005**\uff0c\u5e76\u589e\u52a0\u81ea\u5df1\u7684\u4efb\u671f\u7f16\u53f7\uff0cTerm \u503c\u4ece 0 \u66f4\u65b0\u4e3a 1\uff0c\u5e76\u7ed9\u81ea\u5df1\u6295\u4e86\u4e00\u7968\u3002 1\u3001Node A\uff1aTerm = 1, Vote Count = 1\u3002 2\u3001Node B\uff1aTerm = 0\u3002 3\u3001Node C\uff1aTerm = 0\u3002","title":"4.2 \u6210\u4e3a\u5019\u9009\u8005"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/04-Raft%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#43","text":"\u6211\u4eec\u6765\u770b\u4e0b**\u5019\u9009\u8005**\u5982\u4f55\u6210\u4e3a**\u9886\u5bfc\u8005**\u7684\u3002 1\u3001 \u7b2c\u4e00\u6b65 \uff1a\u8282\u70b9 A \u6210\u4e3a\u5019\u9009\u8005\u540e\uff0c\u5411\u5176\u4ed6\u8282\u70b9\u53d1\u9001\u8bf7\u6c42\u6295\u7968 RPC \u4fe1\u606f\uff0c\u8bf7\u5b83\u4eec\u9009\u4e3e\u81ea\u5df1\u4e3a\u9886\u5bfc\u8005\u3002 2\u3001 \u7b2c\u4e8c\u6b65 \uff1a\u8282\u70b9 B \u548c \u8282\u70b9 C \u63a5\u6536\u5230\u8282\u70b9 A \u53d1\u9001\u7684\u8bf7\u6c42\u6295\u7968\u4fe1\u606f\u540e\uff0c\u5728\u7f16\u53f7\u4e3a 1 \u7684\u8fd9\u5c4a\u4efb\u671f\u5185\uff0c\u8fd8\u6ca1\u6709\u8fdb\u884c\u8fc7\u6295\u7968\uff0c\u5c31\u628a\u9009\u7968\u6295\u7ed9\u8282\u70b9 A\uff0c\u5e76\u589e\u52a0\u81ea\u5df1\u7684\u4efb\u671f\u7f16\u53f7\u3002 3\u3001 \u7b2c\u4e09\u6b65 \uff1a\u8282\u70b9 A \u6536\u5230 3 \u6b21\u6295\u7968\uff0c\u5f97\u5230\u4e86\u5927\u591a\u6570\u8282\u70b9\u7684\u6295\u7968\uff0c\u4ece\u5019\u9009\u8005\u6210\u4e3a\u672c\u5c4a\u4efb\u671f\u5185\u7684\u65b0\u7684\u9886\u5bfc\u8005\u3002 4\u3001 \u7b2c\u56db\u6b65 \uff1a\u8282\u70b9 A \u4f5c\u4e3a\u9886\u5bfc\u8005\uff0c\u56fa\u5b9a\u7684\u65f6\u95f4\u95f4\u9694\u7ed9 \u8282\u70b9 B \u548c\u8282\u70b9 C \u53d1\u9001\u5fc3\u8df3\u4fe1\u606f\uff0c\u544a\u8bc9\u8282\u70b9 B \u548c C\uff0c\u6211\u662f\u9886\u5bfc\u8005\uff0c\u963b\u6b62\u5176\u4ed6\u8ddf\u968f\u8005\u53d1\u8d77\u65b0\u7684\u9009\u4e3e\u3002 5\u3001 \u7b2c\u4e94\u6b65 \uff1a\u8282\u70b9 B \u548c\u8282\u70b9 C \u53d1\u9001\u54cd\u5e94\u4fe1\u606f\u7ed9\u8282\u70b9 A\uff0c\u544a\u8bc9\u8282\u70b9 A \u6211\u662f\u6b63\u5e38\u7684\u3002","title":"4.3 \u6295\u7968"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/04-Raft%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#44","text":"\u82f1\u6587\u5355\u8bcd\u662f term\uff0c\u9886\u5bfc\u8005\u662f\u6709\u4efb\u671f\u7684\u3002 1\u3001 \u81ea\u52a8\u589e\u52a0 \uff1a \u8ddf\u968f\u8005**\u5728\u7b49\u5f85**\u9886\u5bfc\u8005**\u5fc3\u8df3\u4fe1\u606f\u8d85\u65f6\u540e\uff0c\u63a8\u8350\u81ea\u5df1\u4e3a**\u5019\u9009\u4eba \uff0c\u4f1a\u589e\u52a0\u81ea\u5df1\u7684**\u4efb\u671f\u53f7**\uff0c\u5982\u4e0a\u56fe\u6240\u793a\uff0c\u8282\u70b9 A \u4efb\u671f\u4e3a 0\uff0c\u63a8\u4e3e\u81ea\u5df1\u4e3a\u5019\u9009\u4eba\u65f6\uff0c\u4efb\u671f\u7f16\u53f7\u589e\u52a0\u4e3a 1\u3002 2\u3001 \u66f4\u65b0\u4e3a\u8f83\u5927\u503c \uff1a\u5f53\u8282\u70b9\u53d1\u73b0\u81ea\u5df1\u7684\u4efb\u671f\u7f16\u53f7\u6bd4\u5176\u4ed6\u8282\u70b9\u5c0f\u65f6\uff0c\u4f1a\u66f4\u65b0\u5230\u8f83\u5927\u7684\u7f16\u53f7\u503c\u3002\u6bd4\u5982\u8282\u70b9 A \u7684\u4efb\u671f\u4e3a 1\uff0c\u8bf7\u6c42\u6295\u7968\uff0c\u6295\u7968\u6d88\u606f\u4e2d\u5305\u542b\u4e86\u8282\u70b9 A \u7684\u4efb\u671f\u7f16\u53f7\uff0c\u4e14\u7f16\u53f7\u4e3a 1\uff0c\u8282\u70b9 B \u6536\u5230\u6d88\u606f\u540e\uff0c\u4f1a\u5c06\u81ea\u5df1\u7684\u4efb\u671f\u7f16\u53f7\u66f4\u65b0\u4e3a 1\u3002 3\u3001 \u6062\u590d\u4e3a\u8ddf\u968f\u8005 \uff1a\u5982\u679c\u4e00\u4e2a\u5019\u9009\u4eba\u6216\u8005\u9886\u5bfc\u8005\uff0c\u53d1\u73b0\u81ea\u5df1\u7684\u4efb\u671f\u7f16\u53f7\u6bd4\u5176\u4ed6\u8282\u70b9\u5c0f\uff0c\u90a3\u4e48\u5b83\u4f1a\u7acb\u5373\u6062\u590d\u6210\u8ddf\u968f\u8005\u72b6\u6001\u3002\u8fd9\u79cd\u573a\u666f\u51fa\u73b0\u5728\u5206\u533a\u9519\u8bef\u6062\u590d\u540e\uff0c\u4efb\u671f\u4e3a 3 \u7684\u9886\u5bfc\u8005\u53d7\u5230\u4efb\u671f\u7f16\u53f7\u4e3a 4 \u7684\u5fc3\u8df3\u6d88\u606f\uff0c\u90a3\u4e48\u524d\u8005\u5c06\u7acb\u5373\u6062\u590d\u6210\u8ddf\u968f\u8005\u72b6\u6001\u3002 4\u3001 \u62d2\u7edd\u6d88\u606f \uff1a\u5982\u679c\u4e00\u4e2a\u8282\u70b9\u63a5\u6536\u5230\u8f83\u5c0f\u7684\u4efb\u671f\u7f16\u53f7\u503c\u7684\u8bf7\u6c42\uff0c\u90a3\u4e48\u5b83\u4f1a\u76f4\u63a5\u62d2\u7edd\u8fd9\u4e2a\u8bf7\u6c42\uff0c\u6bd4\u5982\u4efb\u671f\u7f16\u53f7\u4e3a 6 \u7684\u8282\u70b9 A\uff0c\u6536\u5230\u4efb\u671f\u7f16\u53f7\u4e3a 5 \u7684\u8282\u70b9 B \u7684\u8bf7\u6c42\u6295\u7968 RPC \u6d88\u606f\uff0c\u90a3\u4e48\u8282\u70b9 A \u4f1a\u62d2\u7edd\u8fd9\u4e2a\u6d88\u606f\u3002","title":"4.4 \u4efb\u671f"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/04-Raft%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#45","text":"1\u3001\u4e00\u4e2a\u4efb\u671f\u5185\uff0c\u9886\u5bfc\u8005\u4e00\u76f4\u90fd\u4f1a\u9886\u5bfc\u8005\uff0c\u76f4\u5230\u81ea\u8eab\u51fa\u73b0\u95ee\u9898\uff08\u5982\u5b95\u673a\uff09\uff0c\u6216\u8005\u7f51\u7edc\u95ee\u9898\uff08\u5ef6\u8fdf\uff09\uff0c\u5176\u4ed6\u8282\u70b9\u53d1\u8d77\u4e00\u8f6e\u65b0\u7684\u9009\u4e3e\u3002 2\u3001\u5728\u4e00\u6b21\u9009\u4e3e\u4e2d\uff0c\u6bcf\u4e00\u4e2a\u670d\u52a1\u5668\u8282\u70b9\u6700\u591a\u4f1a\u5bf9\u4e00\u4e2a\u4efb\u671f\u7f16\u53f7\u6295\u51fa\u4e00\u5f20\u9009\u7968\uff0c\u6295\u5b8c\u4e86\u5c31\u6ca1\u4e86\u3002","title":"4.5 \u9009\u4e3e\u89c4\u5219"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/04-Raft%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#46","text":"\u5047\u8bbe\u4e00\u4e2a\u96c6\u7fa4\u7531 N \u4e2a\u8282\u70b9\u7ec4\u6210\uff0c\u90a3\u4e48\u5927\u591a\u6570\u5c31\u662f\u81f3\u5c11 N/2+1\u3002\u4f8b\u5982\uff1a 3 \u4e2a\u8282\u70b9\u7684\u96c6\u7fa4\uff0c\u5927\u591a\u6570\u5c31\u662f 2\u3002","title":"4.6 \u5927\u591a\u6570"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/04-Raft%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#47","text":"\u4e3a\u4e86\u9632\u6b62\u591a\u4e2a\u8282\u70b9\u540c\u65f6\u53d1\u8d77\u6295\u7968\uff0c\u4f1a\u7ed9\u6bcf\u4e2a\u8282\u70b9\u5206\u914d\u4e00\u4e2a\u968f\u673a\u7684\u9009\u4e3e\u8d85\u65f6\u65f6\u95f4\u3002\u8fd9\u4e2a\u65f6\u95f4\u5185\uff0c\u8282\u70b9\u4e0d\u80fd\u6210\u4e3a\u5019\u9009\u8005\uff0c\u53ea\u80fd\u7b49\u5230\u8d85\u65f6\u3002\u6bd4\u5982\u4e0a\u8ff0\u4f8b\u5b50\uff0c\u8282\u70b9 A \u5148\u8d85\u65f6\uff0c\u5148\u6210\u4e3a\u4e86\u5019\u9009\u8005\u3002\u8fd9\u79cd\u5de7\u5999\u7684\u8bbe\u8ba1\uff0c\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u53ea\u6709\u4e00\u4e2a\u670d\u52a1\u5668\u8282\u70b9\u5148\u53d1\u8d77\u9009\u4e3e\uff0c\u800c\u4e0d\u662f\u540c\u65f6\u53d1\u8d77\u9009\u4e3e\uff0c\u51cf\u5c11\u4e86\u56e0\u9009\u7968\u74dc\u5206\u5bfc\u81f4\u9009\u4e3e\u5931\u8d25\u7684\u60c5\u51b5\u3002 NOTE: \u54ea\u79cd\u60c5\u51b5\u4e0b\u4f1a\u51fa\u73b0\"\u9009\u7968\u74dc\u5206\u5bfc\u81f4\u9009\u4e3e\u5931\u8d25\u7684\u60c5\u51b5\"\uff1f \u5728Redis\u4e2d\uff0c\u4e5f\u6709\u7c7b\u4f3c\u7684\u5904\u7406\uff0c\u53c2\u89c1 cnblogs Redis\u6e90\u7801\u89e3\u6790\uff1a20sentinel(\u4e00)\u521d\u59cb\u5316\u3001\u5efa\u94fe","title":"4.7 \u5fc3\u8df3\u8d85\u65f6"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/04-Raft%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#_3","text":"\u5982\u679c\u9886\u5bfc\u8005\u8282\u70b9\u51fa\u73b0\u6545\u969c\uff0c\u5219\u4f1a\u89e6\u53d1\u65b0\u7684\u4e00\u8f6e\u9009\u4e3e\u3002\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u9886\u5bfc\u8005\u8282\u70b9 B \u53d1\u751f\u6545\u969c\uff0c\u8282\u70b9 A \u548c \u8282\u70b9 B \u5c31\u4f1a\u91cd\u65b0\u9009\u4e3e Leader\u3002 1\u3001\u7b2c\u4e00\u6b65 \uff1a\u8282\u70b9 A \u53d1\u751f\u6545\u969c\uff0c\u8282\u70b9 B \u548c\u8282\u70b9 C \u6ca1\u6709\u6536\u5230\u9886\u5bfc\u8005\u8282\u70b9 A \u7684\u5fc3\u8df3\u4fe1\u606f\uff0c\u7b49\u5f85\u8d85\u65f6\u3002 2\u3001\u7b2c\u4e8c\u6b65\uff1a\u8282\u70b9 C \u5148\u53d1\u751f\u8d85\u65f6\uff0c\u8282\u70b9 C \u6210\u4e3a\u5019\u9009\u4eba\u3002 3\u3001\u7b2c\u4e09\u6b65\uff1a\u8282\u70b9 C \u5411\u8282\u70b9 A \u548c \u8282\u70b9 B \u53d1\u8d77\u8bf7\u6c42\u6295\u7968\u4fe1\u606f\u3002 4\u3001\u7b2c\u56db\u6b65\uff1a\u8282\u70b9 C \u54cd\u5e94\u6295\u7968\uff0c\u5c06\u7968\u6295\u7ed9\u4e86 C\uff0c\u800c\u8282\u70b9 A \u56e0\u4e3a\u53d1\u751f\u6545\u969c\u4e86\uff0c\u65e0\u6cd5\u54cd\u5e94 C \u7684\u6295\u7968\u8bf7\u6c42\u3002 5\u3001\u7b2c\u4e94\u6b65\uff1a\u8282\u70b9 C \u6536\u5230\u4e24\u7968\uff08\u5927\u591a\u6570\u7968\u6570\uff09\uff0c\u6210\u4e3a\u9886\u5bfc\u8005\u3002 6\u3001\u7b2c\u516d\u6b65\uff1a\u8282\u70b9 C \u5411\u8282\u70b9 A \u548c B \u53d1\u9001\u5fc3\u8df3\u4fe1\u606f\uff0c\u8282\u70b9 B \u54cd\u5e94\u5fc3\u8df3\u4fe1\u606f\uff0c\u8282\u70b9 A \u4e0d\u54cd\u5e94\u5fc3\u8df3\u4fe1\u606f\u3002","title":"\u4e94\u3001\u9886\u5bfc\u8005\u6545\u969c"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/04-Raft%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/#_4","text":"Raft \u7b97\u6cd5\u901a\u8fc7\u4ee5\u4e0b\u51e0\u79cd\u65b9\u5f0f\u6765\u8fdb\u884c\u9886\u5bfc\u9009\u4e3e\uff0c\u4fdd\u8bc1\u4e86\u4e00\u4e2a\u4efb\u671f\u53ea\u6709\u4e00\u4f4d\u9886\u5bfc\uff0c\u6781\u5927\u51cf\u5c11\u4e86\u9009\u4e3e\u5931\u8d25\u7684\u60c5\u51b5\u3002 1\u3001\u4efb\u671f 2\u3001\u9886\u5bfc\u8005\u5fc3\u8df3\u4fe1\u606f 3\u3001\u968f\u673a\u9009\u4e3e\u8d85\u65f6\u65f6\u95f4 4\u3001\u5148\u6765\u5148\u670d\u52a1\u7684\u6295\u7968\u539f\u5219 5\u3001\u5927\u591a\u6570\u9009\u7968\u539f\u5219 \u672c\u7bc7\u901a\u8fc7\u52a8\u56fe\u7684\u65b9\u5f0f\u6765\u8bb2\u89e3 Raft \u7b97\u6cd5\u5982\u4f55\u9009\u4e3e\u9886\u5bfc\u8005\uff0c\u66f4\u5bb9\u6613\u7406\u89e3\u548c\u6d88\u5316\u3002","title":"\u603b\u7ed3"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/05-Consistent-hash/","text":"csdn \u97e9\u4fe1\u5927\u62db\uff1a\u4e00\u81f4\u6027\u54c8\u5e0c","title":"Introduction"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/05-Consistent-hash/#csdn","text":"","title":"csdn \u97e9\u4fe1\u5927\u62db\uff1a\u4e00\u81f4\u6027\u54c8\u5e0c"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/06-Gossip%E5%8D%8F%E8%AE%AE/","text":"","title":"Introduction"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/07-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%9D%91/","text":"csdn \u8fd9\u4e09\u5e74\u88ab\u5206\u5e03\u5f0f\u5751\u60e8\u4e86\uff0c\u66dd\u5149\u5341\u5927\u5751 \u672c\u7bc7\u4e3b\u8981\u5185\u5bb9\u5982\u4e0b\uff1a \u4e00\u3001\u5206\u5e03\u5f0f\u6d88\u606f\u961f\u5217\u7684\u5751 1. \u6d88\u606f\u961f\u5217\u7684\u5751\u4e4b\u975e\u5e42\u7b49 NOTE: \u6211\u662f\u901a\u8fc7\u4e0b\u9762\u7684\u5185\u5bb9\u660e\u767d\u4e86\"idempotent\u5e42\u7b49\"\u7684\u91cd\u8981\u4ef7\u503c \uff081\uff09\u5e42\u7b49\u6027\u6982\u5ff5 \u6240\u8c13\u5e42\u7b49\u6027\u5c31\u662f\u65e0\u8bba\u591a\u5c11\u6b21\u64cd\u4f5c\u548c\u7b2c\u4e00\u6b21\u7684\u64cd\u4f5c\u7ed3\u679c\u4e00\u6837\u3002\u5982\u679c\u6d88\u606f\u88ab\u591a\u6b21\u6d88\u8d39\uff0c\u5f88\u6709\u53ef\u80fd\u9020\u6210\u6570\u636e\u7684\u4e0d\u4e00\u81f4\u3002\u800c\u5982\u679c\u6d88\u606f\u4e0d\u53ef\u907f\u514d\u5730\u88ab\u6d88\u8d39\u591a\u6b21\uff0c\u5982\u679c\u6211\u4eec\u5f00\u53d1\u4eba\u5458\u80fd\u901a\u8fc7\u6280\u672f\u624b\u6bb5\u4fdd\u8bc1\u6570\u636e\u7684\u524d\u540e\u4e00\u81f4\u6027\uff0c\u90a3\u4e5f\u662f\u53ef\u4ee5\u63a5\u53d7\u7684\uff0c\u8fd9\u8ba9\u6211\u60f3\u8d77\u4e86 Java \u5e76\u53d1\u7f16\u7a0b\u4e2d\u7684 ABA \u95ee\u9898\uff0c\u5982\u679c\u51fa\u73b0\u4e86 ABA \u95ee\u9898 \uff0c\u82e5\u80fd\u4fdd\u8bc1\u6240\u6709\u6570\u636e\u7684\u524d\u540e\u4e00\u81f4\u6027\u4e5f\u80fd\u63a5\u53d7\u3002 NOTE: ABA problem\u548cidempotent\u6709\u4ec0\u4e48\u76f8\u4f3c\u4e4b\u5904\uff1f \uff082\uff09\u573a\u666f\u5206\u6790 \uff083\uff09\u907f\u5751\u6307\u5357 NOTE: \u4e0b\u9762\u63cf\u8ff0\u4e86\u591a\u79cd\u5b9e\u73b0idempotent\u7684trick\uff0c\u5176\u5b9e\u5f52\u7ed3\u4e3a\u4e00\u53e5\u8bdd\u5c31\u662f: \"\u901a\u8fc7\u6807\u5fd7\u5224\u65ad\u6765\u8fc7\u6ee4\u6389\u91cd\u590d\u7684\u6d88\u606f\uff0c\u4ece\u800c\u5b9e\u73b0idempotent\" 1\u3001\u5fae\u4fe1\u652f\u4ed8\u7ed3\u679c\u901a\u77e5\u573a\u666f NOTE: \u8fd9\u662f\u5178\u578b\u7684\u901a\u8fc7state machine\u6765\u5b9e\u73b0\"idempotent\u5e42\u7b49\"\uff0c\u4ece\u800c\u8f7b\u677e\u5e94\u4ed8\u91cd\u590d\u6d88\u606f\u7684\u95ee\u9898\uff0c\u5728Redis\u4e2d\uff0c\u5e7f\u6cdb\u5e94\u7528\u8fd9\u79cdtrick \u5fae\u4fe1\u5b98\u65b9\u6587\u6863\u4e0a\u63d0\u5230\u5fae\u4fe1\u652f\u4ed8\u901a\u77e5\u7ed3\u679c\u53ef\u80fd\u4f1a\u63a8\u9001\u591a\u6b21\uff0c\u9700\u8981\u5f00\u53d1\u8005\u81ea\u884c\u4fdd\u8bc1\u5e42\u7b49\u6027\u3002\u7b2c\u4e00\u6b21\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u4fee\u6539\u8ba2\u5355\u72b6\u6001\uff08\u5982\u652f\u4ed8\u4e2d -> \u652f\u4ed8\u6210\u529f\uff09\uff0c\u7b2c\u4e8c\u6b21\u5c31\u6839\u636e\u8ba2\u5355\u72b6\u6001\u6765\u5224\u65ad\uff0c\u5982\u679c\u4e0d\u662f\u652f\u4ed8\u4e2d\uff0c\u5219\u4e0d\u8fdb\u884c\u8ba2\u5355\u5904\u7406\u903b\u8f91\u3002 2\u3001\u63d2\u5165\u6570\u636e\u5e93\u573a\u666f NOTE: \u5176\u4e2d\u8fd9\u79cd\u65b9\u5f0f\u662f\u7ecf\u5e38\u4f1a\u4f7f\u7528\u5230\u7684 \u6bcf\u6b21\u63d2\u5165\u6570\u636e\u65f6\uff0c\u5148\u68c0\u67e5\u4e0b\u6570\u636e\u5e93\u4e2d\u662f\u5426\u6709\u8fd9\u6761\u6570\u636e\u7684\u4e3b\u952e id\uff0c\u5982\u679c\u6709\uff0c\u5219\u8fdb\u884c\u66f4\u65b0\u64cd\u4f5c\u3002 3\u3001\u5199 Redis \u573a\u666f Redis \u7684 Set \u64cd\u4f5c\u5929\u7136\u5e42\u7b49\u6027\uff0c\u6240\u4ee5\u4e0d\u7528\u8003\u8651 Redis \u5199\u6570\u636e\u7684\u95ee\u9898\u3002 NOTE: Redis Set \u662f\u80fd\u591f\u5224\u65ad\uff0c\u662f\u5426\u5b58\u5728\u7684\u3002 4\u3001\u5176\u4ed6\u573a\u666f\u65b9\u6848 - \u751f\u4ea7\u8005\u53d1\u9001\u6bcf\u6761\u6570\u636e\u65f6\uff0c\u589e\u52a0\u4e00\u4e2a\u5168\u5c40\u552f\u4e00 id\uff0c\u7c7b\u4f3c\u8ba2\u5355 id\u3002\u6bcf\u6b21\u6d88\u8d39\u65f6\uff0c\u5148\u53bb Redis \u67e5\u4e0b\u662f\u5426\u6709\u8fd9\u4e2a id\uff0c\u5982\u679c\u6ca1\u6709\uff0c\u5219\u8fdb\u884c\u6b63\u5e38\u5904\u7406\u6d88\u606f\uff0c\u4e14\u5c06 id \u5b58\u5230 Redis\u3002\u5982\u679c\u67e5\u5230\u6709\u8fd9\u4e2a id\uff0c\u8bf4\u660e\u4e4b\u524d\u6d88\u8d39\u8fc7\uff0c\u5219\u4e0d\u8981\u8fdb\u884c\u91cd\u590d\u5904\u7406\u8fd9\u6761\u6d88\u606f\u3002 NOTE: \u5176\u5b9e\u8fd8\u662f\"\u901a\u8fc7\u6807\u5fd7\u5224\u65ad\u6765\u8fc7\u6ee4\u6389\u91cd\u590d\u7684\u6d88\u606f\uff0c\u4ece\u800c\u5b9e\u73b0idempotent\" - \u4e0d\u540c\u4e1a\u52a1\u573a\u666f\uff0c\u53ef\u80fd\u4f1a\u6709\u4e0d\u540c\u7684\u5e42\u7b49\u6027\u65b9\u6848\uff0c\u5927\u5bb6\u9009\u62e9\u5408\u9002\u7684\u5373\u53ef\uff0c\u4e0a\u9762\u7684\u51e0\u79cd\u65b9\u6848\u53ea\u662f\u63d0\u4f9b\u5e38\u89c1\u7684\u89e3\u51b3\u601d\u8def\u3002","title":"Introduction"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/07-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%9D%91/#csdn","text":"\u672c\u7bc7\u4e3b\u8981\u5185\u5bb9\u5982\u4e0b\uff1a","title":"csdn \u8fd9\u4e09\u5e74\u88ab\u5206\u5e03\u5f0f\u5751\u60e8\u4e86\uff0c\u66dd\u5149\u5341\u5927\u5751"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/07-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%9D%91/#_1","text":"","title":"\u4e00\u3001\u5206\u5e03\u5f0f\u6d88\u606f\u961f\u5217\u7684\u5751"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/07-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%9D%91/#1","text":"NOTE: \u6211\u662f\u901a\u8fc7\u4e0b\u9762\u7684\u5185\u5bb9\u660e\u767d\u4e86\"idempotent\u5e42\u7b49\"\u7684\u91cd\u8981\u4ef7\u503c","title":"1. \u6d88\u606f\u961f\u5217\u7684\u5751\u4e4b\u975e\u5e42\u7b49"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/07-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%9D%91/#1_1","text":"\u6240\u8c13\u5e42\u7b49\u6027\u5c31\u662f\u65e0\u8bba\u591a\u5c11\u6b21\u64cd\u4f5c\u548c\u7b2c\u4e00\u6b21\u7684\u64cd\u4f5c\u7ed3\u679c\u4e00\u6837\u3002\u5982\u679c\u6d88\u606f\u88ab\u591a\u6b21\u6d88\u8d39\uff0c\u5f88\u6709\u53ef\u80fd\u9020\u6210\u6570\u636e\u7684\u4e0d\u4e00\u81f4\u3002\u800c\u5982\u679c\u6d88\u606f\u4e0d\u53ef\u907f\u514d\u5730\u88ab\u6d88\u8d39\u591a\u6b21\uff0c\u5982\u679c\u6211\u4eec\u5f00\u53d1\u4eba\u5458\u80fd\u901a\u8fc7\u6280\u672f\u624b\u6bb5\u4fdd\u8bc1\u6570\u636e\u7684\u524d\u540e\u4e00\u81f4\u6027\uff0c\u90a3\u4e5f\u662f\u53ef\u4ee5\u63a5\u53d7\u7684\uff0c\u8fd9\u8ba9\u6211\u60f3\u8d77\u4e86 Java \u5e76\u53d1\u7f16\u7a0b\u4e2d\u7684 ABA \u95ee\u9898\uff0c\u5982\u679c\u51fa\u73b0\u4e86 ABA \u95ee\u9898 \uff0c\u82e5\u80fd\u4fdd\u8bc1\u6240\u6709\u6570\u636e\u7684\u524d\u540e\u4e00\u81f4\u6027\u4e5f\u80fd\u63a5\u53d7\u3002 NOTE: ABA problem\u548cidempotent\u6709\u4ec0\u4e48\u76f8\u4f3c\u4e4b\u5904\uff1f","title":"\uff081\uff09\u5e42\u7b49\u6027\u6982\u5ff5"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/07-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%9D%91/#2","text":"","title":"\uff082\uff09\u573a\u666f\u5206\u6790"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/07-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%9D%91/#3","text":"NOTE: \u4e0b\u9762\u63cf\u8ff0\u4e86\u591a\u79cd\u5b9e\u73b0idempotent\u7684trick\uff0c\u5176\u5b9e\u5f52\u7ed3\u4e3a\u4e00\u53e5\u8bdd\u5c31\u662f: \"\u901a\u8fc7\u6807\u5fd7\u5224\u65ad\u6765\u8fc7\u6ee4\u6389\u91cd\u590d\u7684\u6d88\u606f\uff0c\u4ece\u800c\u5b9e\u73b0idempotent\" 1\u3001\u5fae\u4fe1\u652f\u4ed8\u7ed3\u679c\u901a\u77e5\u573a\u666f NOTE: \u8fd9\u662f\u5178\u578b\u7684\u901a\u8fc7state machine\u6765\u5b9e\u73b0\"idempotent\u5e42\u7b49\"\uff0c\u4ece\u800c\u8f7b\u677e\u5e94\u4ed8\u91cd\u590d\u6d88\u606f\u7684\u95ee\u9898\uff0c\u5728Redis\u4e2d\uff0c\u5e7f\u6cdb\u5e94\u7528\u8fd9\u79cdtrick \u5fae\u4fe1\u5b98\u65b9\u6587\u6863\u4e0a\u63d0\u5230\u5fae\u4fe1\u652f\u4ed8\u901a\u77e5\u7ed3\u679c\u53ef\u80fd\u4f1a\u63a8\u9001\u591a\u6b21\uff0c\u9700\u8981\u5f00\u53d1\u8005\u81ea\u884c\u4fdd\u8bc1\u5e42\u7b49\u6027\u3002\u7b2c\u4e00\u6b21\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u4fee\u6539\u8ba2\u5355\u72b6\u6001\uff08\u5982\u652f\u4ed8\u4e2d -> \u652f\u4ed8\u6210\u529f\uff09\uff0c\u7b2c\u4e8c\u6b21\u5c31\u6839\u636e\u8ba2\u5355\u72b6\u6001\u6765\u5224\u65ad\uff0c\u5982\u679c\u4e0d\u662f\u652f\u4ed8\u4e2d\uff0c\u5219\u4e0d\u8fdb\u884c\u8ba2\u5355\u5904\u7406\u903b\u8f91\u3002 2\u3001\u63d2\u5165\u6570\u636e\u5e93\u573a\u666f NOTE: \u5176\u4e2d\u8fd9\u79cd\u65b9\u5f0f\u662f\u7ecf\u5e38\u4f1a\u4f7f\u7528\u5230\u7684 \u6bcf\u6b21\u63d2\u5165\u6570\u636e\u65f6\uff0c\u5148\u68c0\u67e5\u4e0b\u6570\u636e\u5e93\u4e2d\u662f\u5426\u6709\u8fd9\u6761\u6570\u636e\u7684\u4e3b\u952e id\uff0c\u5982\u679c\u6709\uff0c\u5219\u8fdb\u884c\u66f4\u65b0\u64cd\u4f5c\u3002 3\u3001\u5199 Redis \u573a\u666f Redis \u7684 Set \u64cd\u4f5c\u5929\u7136\u5e42\u7b49\u6027\uff0c\u6240\u4ee5\u4e0d\u7528\u8003\u8651 Redis \u5199\u6570\u636e\u7684\u95ee\u9898\u3002 NOTE: Redis Set \u662f\u80fd\u591f\u5224\u65ad\uff0c\u662f\u5426\u5b58\u5728\u7684\u3002 4\u3001\u5176\u4ed6\u573a\u666f\u65b9\u6848 - \u751f\u4ea7\u8005\u53d1\u9001\u6bcf\u6761\u6570\u636e\u65f6\uff0c\u589e\u52a0\u4e00\u4e2a\u5168\u5c40\u552f\u4e00 id\uff0c\u7c7b\u4f3c\u8ba2\u5355 id\u3002\u6bcf\u6b21\u6d88\u8d39\u65f6\uff0c\u5148\u53bb Redis \u67e5\u4e0b\u662f\u5426\u6709\u8fd9\u4e2a id\uff0c\u5982\u679c\u6ca1\u6709\uff0c\u5219\u8fdb\u884c\u6b63\u5e38\u5904\u7406\u6d88\u606f\uff0c\u4e14\u5c06 id \u5b58\u5230 Redis\u3002\u5982\u679c\u67e5\u5230\u6709\u8fd9\u4e2a id\uff0c\u8bf4\u660e\u4e4b\u524d\u6d88\u8d39\u8fc7\uff0c\u5219\u4e0d\u8981\u8fdb\u884c\u91cd\u590d\u5904\u7406\u8fd9\u6761\u6d88\u606f\u3002 NOTE: \u5176\u5b9e\u8fd8\u662f\"\u901a\u8fc7\u6807\u5fd7\u5224\u65ad\u6765\u8fc7\u6ee4\u6389\u91cd\u590d\u7684\u6d88\u606f\uff0c\u4ece\u800c\u5b9e\u73b0idempotent\" - \u4e0d\u540c\u4e1a\u52a1\u573a\u666f\uff0c\u53ef\u80fd\u4f1a\u6709\u4e0d\u540c\u7684\u5e42\u7b49\u6027\u65b9\u6848\uff0c\u5927\u5bb6\u9009\u62e9\u5408\u9002\u7684\u5373\u53ef\uff0c\u4e0a\u9762\u7684\u51e0\u79cd\u65b9\u6848\u53ea\u662f\u63d0\u4f9b\u5e38\u89c1\u7684\u89e3\u51b3\u601d\u8def\u3002","title":"\uff083\uff09\u907f\u5751\u6307\u5357"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/08-Quorum-NWR/","text":"csdn \u592a\u4e0a\u8001\u541b\u7684\u70bc\u4e39\u7089\u4e4b\u5206\u5e03\u5f0f Quorum NWR NOTE: \u4e00\u822c\u53eb\u505aNRW \u4e00\u3001\u4e09\u4e2a\u70bc\u4e39\u7089\u600e\u4e48\u5206\u914d\u7684 \u6620\u5c04\u5230\u6211\u4eec\u4e92\u8054\u7f51\u7cfb\u7edf\u4e2d\uff1a\u4e39\u7089\u7c7b\u4f3c\u4e8e\u670d\u52a1\u5668\u8282\u70b9\u6216\u6570\u636e\u5e93\u8282\u70b9\uff0c\u901a\u8fc7\u591a\u4e2a\u8282\u70b9\u6765\u76f8\u4e92\u5907\u4efd\u6570\u636e\u6765\u4fdd\u8bc1\u7cfb\u7edf\u7684 \u9ad8\u53ef\u7528\u6027 \uff08High Availability\uff09\u3002 \u4e8c\u3001\u5982\u4f55\u4fdd\u8bc1\u4e39\u836f\u54c1\u8d28\u4e00\u6837 2.1 \u4e00\u81f4\u6027 \u592a\u4e0a\u8001\u541b\u8bf4\u7684\u54c1\u8d28\u4fdd\u6301\u4e00\u81f4\u5230\u5e95\u600e\u4e48\u56de\u4e8b\uff1f \u4e00\u53f7\u4e39\u7089\u91cc\u9762\u7684\u5ef6\u5e74\u4e39\u548c\u4e8c\u53f7\u4e39\u7089\u7684\u5ef6\u5e74\u4e39\u5982\u4f55\u4fdd\u8bc1\u54c1\u8d28\u4e00\u81f4\u5462\uff1f \u8fd9\u4e0d\u5c31\u662f\u6211\u4eec\u5e38\u5e38\u8bf4\u7684 \u5206\u5e03\u5f0f\u4e00\u81f4\u6027 \u5417\uff1f\u4e24\u9897\u4e39\u836f\u5206\u5e03\u5728\u4e0d\u540c\u7684\u4e39\u7089\u4e2d\uff0c\u9700\u8981\u4fdd\u8bc1\u54c1\u8d28\u4e00\u81f4\u3002 \u5982\u4e0b\u56fe\u6240\u793a\uff0c\u8fd9\u4e24\u9897\u5ef6\u5e74\u4e39\u7684\u4e00\u5927\u4e00\u5c0f\uff0c\u989c\u8272\u4e5f\u6709\u4e0d\u540c\uff0c\u8fd9\u5c31\u662f\u54c1\u8d28\u4e0d\u4e00\u6837\u3002 2.2 \u6700\u7ec8\u4e00\u81f4\u6027\u548c\u5f3a\u4e00\u81f4\u6027 \u5206\u5e03\u5f0f\u4e2d\u7684\u4e00\u81f4\u6027\u53c8\u5206\u4e3a \u6700\u7ec8\u4e00\u81f4\u6027 \u548c \u5f3a\u4e00\u81f4\u6027 \u3002 \u6240\u8c13 \u5f3a\u4e00\u81f4\u6027 \u5c31\u662f\u5199\u64cd\u4f5c\u5b8c\u6210\u540e\uff0c\u4efb\u4f55\u540e\u7eed\u8bbf\u95ee\u90fd\u80fd\u8bfb\u5230\u66f4\u65b0\u540e\u7684\u503c\u3002\u8fd9\u5c31\u662f CP \u7cfb\u7edf\u6240\u8981\u6c42\u7684\u4e00\u81f4\u6027\u548c\u5206\u533a\u5bb9\u9519\u6027\u3002 \u800c \u6700\u7ec8\u4e00\u81f4\u6027 \u5c31\u662f\u4e0d\u4fdd\u8bc1\u540e\u7eed\u8bbf\u95ee\u90fd\u80fd\u8bfb\u5230\u66f4\u65b0\u540e\u7684\u503c\uff0c\u4f46\u662f\u7ecf\u8fc7\u4e00\u6bb5\u65f6\u95f4\u540e\uff0c\u518d\u53bb\u8bfb\uff0c\u5c31\u80fd\u5f97\u5230\u76f8\u540c\u7684\u503c\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c\u5728\u8fd9\u6bb5\u65f6\u95f4\u5185\uff0c\u53ef\u80fd\u8bfb\u5230\u65e7\u7684\u6570\u636e\u3002\u8fd9\u5c31\u662f AP \u7cfb\u7edf\u6240\u8981\u6c42\u7684\u53ef\u7528\u6027\u548c\u5206\u533a\u5bb9\u9519\u6027\u3002 \u4e09\u3001\u53ef\u63a7\u7684\u54c1\u8d28\uff1aQuorum NWR \u534f\u8bae Quorum NWR \u5047\u5982\u5ef6\u5e74\u4e39\u5fc5\u987b\u4fdd\u8bc1\u54c1\u8d28\u7684\u5f3a\u4e00\u81f4\u6027\uff0c\u800c\u5065\u6b65\u4e39\u53ea\u9700\u8981\u4fdd\u8bc1\u54c1\u8d28\u7684\u6700\u7ec8\u4e00\u81f4\u6027\uff0c\u8fd9\u4e2a\u8be5\u600e\u4e48\u63a7\u5236\u5462\uff1f \u8fd9\u4e2a\u53ef\u6ca1\u6709\u96be\u5012\u8001\u541b\uff0c\u56e0\u4e3a\u8001\u541b\u61c2\u5f97\u5206\u5e03\u5f0f\u534f\u8bae\uff1a Quorum NWR \u3002 Quorum \u8fd9\u4e2a\u5355\u8bcd\u7684\u610f\u601d\uff1a(\u4f1a\u8bae\u7684)\u6cd5\u5b9a\u4eba\u6570\u3002\u4e3b\u8981\u662f\u770b\u540e\u9762\u4e09\u4e2a\u5927\u5199\u5b57\u6bcd\uff1a N \u3001 W \u3001 R \u3002\u7531 NWR \u6765\u63a7\u5236\u4e00\u81f4\u6027\u3002 3.1 \u53c2\u6570 N \u6211\u4eec\u8fd8\u662f\u6765\u770b\u4e0b\u4e39\u7089\u4e2d\u7684\u60c5\u51b5\uff0c\u4e24\u9897\u5ef6\u5e74\u4e39\u662f\u4e92\u4e3a\u5907\u4efd\u7684\uff0c\u76f8\u5f53\u4e8e\u6709\u4e24\u4e2a\u526f\u672c\u3002 N \u79f0\u4f5c\u526f\u672c\u6570\uff0c\u53c8\u53eb\u505a\u590d\u5236\u56e0\u5b50\uff08Replication Factor\uff09 \u3002\u8868\u793a\u540c\u4e00\u4efd\u6570\u636e\u6709\u591a\u5c11\u4e2a\u526f\u672c\uff0c\u6240\u4ee5\uff1a\u5ef6\u5e74\u4e39\u7684 N = 2\u3002\u4f9d\u6b21\u7c7b\u63a8\uff1a\u5065\u6b65\u4e39\u7684 N = 2\uff0c\u6062\u590d\u4e39\u7684 N = 2\u3002\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u90a3 N \u53ef\u4ee5\u53d8\u5417\uff1f \u5982\u4e0b\u56fe\u6240\u793a\uff1a\u6bd4\u5982\u6211\u60f3\u70bc 3 \u9897\u5ef6\u5e74\u4e39\uff0c\u4e5f\u5c31\u662f\u6bcf\u4e2a\u4e39\u7089\u90fd\u6709\u5ef6\u5e74\u4e39\uff0c\u90a3\u5c31\u628a N \u6539\u6210 3 \u5c31\u53ef\u4ee5\u4e86\u3002\u800c\u5065\u6b65\u4e39\u53ea\u9700\u8981\u70bc\u4e00\u9897\u8db3\u4ee5\uff0c\u90a3\u4e00\u53f7\u4e39\u7089\u70bc\u5c31\u53ef\u4ee5\u4e86\uff0c\u6240\u4ee5N = 1\u3002 3.2 \u53c2\u6570 W \u6307\u5b9a\u4e86\u526f\u672c\u6570 N \u4e4b\u540e\uff0c\u5c31\u53ef\u4ee5\u5bf9\u526f\u672c\u6570\u636e\u8fdb\u884c\u8bfb\u5199\u64cd\u4f5c\u3002 1\u3001 \u8bfb\u64cd\u4f5c \uff1a\u67e5\u770b\u6240\u5728\u4e39\u7089\u5185\u4e39\u836f\u7684\u60c5\u51b5\u3002 2\u3001 \u5199\u64cd\u4f5c \uff1a\u7ed9\u4e39\u836f\u6dfb\u52a0\u836f\u6750\u3001\u63d0\u9ad8\u6e29\u5ea6\u3002 \u90a3\u591a\u4e2a\u4e39\u836f\u8be5\u5982\u4f55\u6267\u884c\u8bfb\u5199\u64cd\u4f5c\u5462\uff1f\u5bf9\u4e8e\u5199\u64cd\u4f5c\uff0c\u6211\u4eec\u6709 W \u53c2\u6570\uff0c\u5bf9\u4e8e\u8bfb\u64cd\u4f5c\uff0c\u6211\u4eec\u6709 R \u53c2\u6570\u3002 W \u79f0\u4e3a\u5199\u4e00\u81f4\u6027\u7ea7\u522b\uff08Write Consistency Level\uff09 \uff0c\u8868\u793a\u6210\u529f\u5b8c\u6210 W \u4e2a\u526f\u672c\u66f4\u65b0\uff0c\u624d\u5b8c\u6210\u5199\u64cd\u4f5c\u3002 \u6bd4\u5982\u8bbe\u7f6e\u5ef6\u5e74\u4e39\u7684 W = 2\uff0c\u8868\u793a\u5bf9\u5ef6\u5e74\u4e39\u6267\u884c\u5199\u64cd\u4f5c\u65f6\uff0c\u5b8c\u6210\u4e86 2 \u4e2a\u526f\u672c\u7684\u66f4\u65b0\u65f6\uff0c\u624d\u5b8c\u6210\u5199\u64cd\u4f5c\u3002 \u5982\u4e0b\u56fe\u6240\u793a\uff1a\u4e00\u53f7\u4e39\u7089\u548c\u4e8c\u53f7\u4e39\u7089\u4e2d\u7684\u5ef6\u5e74\u4e39\u90fd\u52a0\u5165\u4e86\u83b2\u82b1\uff0c\u800c\u4e09\u53f7\u4e39\u7089\u4e2d\u7684\u5ef6\u5e74\u4e39\u672a\u52a0\u5165\u83b2\u82b1\u3002\u4e5f\u5c31\u662f\u53ea\u5b8c\u6210\u4e86\u4e24\u4e2a\u526f\u672c\u7684\u66f4\u65b0\uff0c\u7b26\u5408 W = 2 \u8fd9\u4e2a\u6761\u4ef6\uff0c\u5373\u5199\u64cd\u4f5c\u5b8c\u6210\u3002 \u4f46\u662f\u5927\u5bb6\u53d1\u73b0\u95ee\u9898\u6ca1\uff0c\u4e09\u53f7\u4e39\u7089\u7684\u5ef6\u5e74\u4e39\u672a\u52a0\u5165\u83b2\u82b1\uff0c\u90a3\u600e\u4e48\u4fdd\u8bc1\u592a\u4e0a\u8001\u541b\u67e5\u770b\u4e39\u836f\u60c5\u51b5\u65f6\uff0c\u5f97\u77e5\u662f\u5df2\u52a0\u5165\u83b2\u82b1\u5462\uff1f\u4e5f\u5c31\u662f\u5982\u4f55\u4fdd\u8bc1**\u8bfb\u5199\u7684\u5f3a\u4e00\u81f4\u6027**\uff0c\u8fd9\u5c31\u8981\u7528\u5230\u7b2c\u4e09\u4e2a\u53c2\u6570\u4e86\uff1aR\u3002 3.3 \u53c2\u6570 R R \u79f0\u4e3a\u8bfb\u4e00\u81f4\u6027\u7ea7\u522b\uff08Read Consistency Level\uff09\uff0c\u8868\u793a\u8bfb\u53d6\u4e00\u4e2a\u6570\u636e\u5bf9\u8c61\u65f6\uff0c\u9700\u8981\u8bfb R \u4e2a\u526f\u672c\uff0c\u7136\u540e\u8fd4\u56de R \u4e2a\u526f\u672c\u4e2d\u6700\u65b0\u7684\u90a3\u4efd\u6570\u636e\u3002 \u56de\u5230\u70bc\u4e39\u7684\u95ee\u9898\u4e2d\uff0c\u8bbe\u7f6e\u5ef6\u5e74\u4e39\u7684 R = 2\uff0c\u4e5f\u5c31\u662f\u67e5\u770b\u5ef6\u5e74\u4e39\u7684\u60c5\u51b5\u65f6\uff0c\u53ea\u9700\u8981\u67e5\u770b\u4e24\u4e2a\u4e39\u7089\u5185\u7684\u5ef6\u5e74\u4e39\u7684\u60c5\u51b5\uff0c\u7136\u540e\u8fd4\u56de\u6700\u65b0\u7684\u5ef6\u5e74\u4e39\u7684\u60c5\u51b5\u5c31\u53ef\u4ee5\u4e86\u3002 1\u3001\u5047\u8bbe\u67e5\u770b\u7684\u662f\u4e00\u53f7\u548c\u4e8c\u53f7\u4e39\u7089\u5185\u7684\u5ef6\u5e74\u4e39\uff0c\u8fd4\u56de\u7684\u60c5\u51b5\u90fd\u662f\uff1a\u5df2\u52a0\u5165\u83b2\u82b1\u3002\u8fd9\u79cd\u573a\u666f\u662f\u4e00\u81f4\u6027\u7684\u3002 2\u3001\u5047\u8bbe\u67e5\u770b\u7684\u662f\u4e00\u53f7\u548c\u4e09\u53f7\u4e39\u7089\u5185\u7684\u5ef6\u5e74\u4e39\uff0c\u4e00\u53f7\u4e39\u7089\u7684\u5ef6\u5e74\u4e39\u662f\u5df2\u52a0\u5165\u83b2\u82b1\uff0c\u4e09\u53f7\u4e39\u7089\u662f\u672a\u52a0\u5165\u83b2\u82b1\uff0c\u4f46\u662f\u4e09\u53f7\u4e39\u7089\u5185\u7684\u5ef6\u5e74\u4e39\u6700\u540e\u4e00\u6b21\u64cd\u4f5c\u65f6\u95f4\u662f\u65e9\u4e8e\u4e00\u53f7\u4e39\u7089\u7684\uff0c\u6240\u4ee5\u8fd4\u56de\u4e00\u53f7\u4e39\u7089\u5185\u5ef6\u5e74\u4e39\u7684\u60c5\u51b5\uff1a\u5df2\u52a0\u5165\u83b2\u82b1\u3002\u8fd9\u79cd\u573a\u666f\u4e5f\u662f\u4e00\u81f4\u6027\u7684\u3002 3.4 \u53c2\u6570\u7ec4\u5408 \u53c2\u6570 N\u3001W\u3001R \u7684\u4e0d\u540c\u7ec4\u5408\u5c06\u4f1a\u5e26\u6765\u4e0d\u540c\u7684\u4e00\u81f4\u6027\u6548\u679c\u3002 1\u3001\u6bd4\u5982\u4e0a\u9762\u7684\u4f8b\u5b50\uff0cN = 3\uff0cW = 2\uff0cR = 2\uff0cW + R > N\uff0c\u5bf9\u4e8e\u5ba2\u6237\u7aef\u6765\u8bb2\uff0c\u6574\u4e2a\u7cfb\u7edf\u80fd\u4fdd\u8bc1\u5f3a\u4e00\u81f4\u6027\uff0c\u4e00\u5b9a\u80fd\u8fd4\u56de\u66f4\u65b0\u540e\u7684\u90a3\u4efd\u6570\u636e\u3002 2\u3001\u5f53 W + R <= N \u65f6\uff0c\u5bf9\u4e8e\u5ba2\u6237\u7aef\u6765\u8bb2\uff0c\u6574\u4e2a\u7cfb\u7edf\u53ea\u80fd\u4fdd\u8bc1\u6700\u7ec8\u4e00\u81f4\u6027\uff0c\u8bbf\u95ee\u6570\u636e\u671f\u95f4\u53ef\u80fd\u4f1a\u8fd4\u56de\u65e7\u6570\u636e\u3002 \u53c2\u6570\u4e0d\u540c\uff0c\u6548\u679c\u4e0d\u540c\uff0c\u5206\u5e03\u5f0f\u7cfb\u7edf\u9700\u8981\u573a\u666f\u6765\u914d\u7f6e\u3002 \u56db\u3001\u5e94\u7528 InfluxDB \u4f01\u4e1a\u7248\u662f\u65f6\u5e8f\u6570\u636e\u5e93\uff0c\u5b83\u6709\u56db\u79cd\u5199\u4e00\u81f4\u6027\u7ea7\u522b\uff1a 1\u3001 any \uff1aW + R < N\uff0cW = 1\uff0c\u4efb\u4f55\u4e00\u4e2a\u8282\u70b9\u5199\u5165\u6210\u529f\u540e\uff0c\u6216\u8005\u5199\u5165 Hinted-handoff \u7f13\u5b58\uff08\u7b49\u4e0b\u6b21\u91cd\u4f20\uff09\uff0c\u8fd4\u56de\u6210\u529f\u7ed9\u5ba2\u6237\u7aef\u3002 2\u3001 one \uff1aW + R < N\uff0cW = 1\uff0c\u4efb\u4f55\u4e00\u4e2a\u8282\u70b9\u5199\u5165\u6210\u529f\u540e\uff0c\u7acb\u5373\u8fd4\u56de\u6210\u529f\u7ed9\u5ba2\u6237\u7aef\uff0c\u4e0d\u5305\u62ec\u5199\u5165 Hinted-handoff \u7f13\u5b58 3\u3001 quorum \uff1aW + R > N\uff0c\u5927\u591a\u6570\u8282\u70b9\u5199\u5165\u6210\u529f\u540e\uff0c\u5c31\u8fd4\u56de\u6210\u529f\u7ed9\u5ba2\u6237\u7aef\u3002\uff08\u8981\u6c42 N \u5927\u4e8e2\uff09 4\u3001 all \uff1aW = N\uff0c\u6240\u6709\u8282\u70b9\u90fd\u5199\u5165\u6210\u529f\u540e\uff0c\u8fd4\u56de\u6210\u529f\u3002 \u53e6\u5916\u5bf9\u4e8e \u65f6\u5e8f\u6570\u636e\u5e93 InfluxDB \u6765\u8bf4\uff0c\u8bfb\u64cd\u4f5c\u9700\u8981\u8bfb\u53d6\u5927\u91cf\u6570\u636e\uff0c\u4e3a\u4e86\u4fdd\u8bc1\u8bfb\u53d6\u7684\u9ad8\u6548\uff0c\u5b83\u4e0d\u652f\u6301\u8bfb\u4e00\u81f4\u6027\u7ea7\u522b\uff08R = N\uff09\uff0c\u4f46\u662f\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e\u5199\u4e00\u81f4\u6027\u7ea7\u522b\u4e3a all\uff0c\u6765\u5b9e\u73b0\u5f3a\u4e00\u81f4\u6027\u3002 InfluxDb \u5b9e\u73b0\u4e86 Quorum NWR\uff0c\u5f53\u7ebf\u4e0a\u4e1a\u52a1\u9700\u8981\u4e34\u65f6\u505a\u4e9b\u4e00\u81f4\u6027\u8c03\u6574\u65f6\uff0c\u8bbe\u7f6e\u4e0d\u540c\u7684\u5199\u4e00\u81f4\u6027\u7ea7\u522b\u5373\u53ef\u5b8c\u6210\u5feb\u901f\u5207\u6362\u3002 \u4e94\u3001\u603b\u7ed3 \u672c\u6587\u901a\u8fc7\u592a\u4e0a\u8001\u541b\u548c\u592a\u767d\u91d1\u661f\u5173\u4e8e\u70bc\u4e39\u7684\u5bf9\u8bdd\uff0c\u5f15\u7533\u51fa\u81ea\u5b9a\u4e49\u4e00\u81f4\u6027\u7684\u5206\u5e03\u5f0f\u534f\u8bae\uff1aQuorum NWR \u534f\u8bae\u3002 1\u3001\u4e39\u7089\u6bd4\u55bb\u8282\u70b9\uff0c\u4e39\u836f\u6bd4\u4f5c\u6570\u636e\uff0c\u591a\u4e2a\u4e39\u836f\u79f0\u4f5c\u526f\u672c\u3002 2\u3001 N \u4ee3\u8868\u526f\u672c\u6570\uff0c W \u4ee3\u8868\u5199\u591a\u5c11\u4e2a\u526f\u672c\u6570\uff0c R \u4ee3\u8868\u8bfb\u591a\u5c11\u4e2a\u526f\u672c\u6570\u3002 3\u3001\u5f53 N \u5927\u4e8e\u8282\u70b9\u6570\u65f6\uff0c\u5c31\u4f1a\u51fa\u73b0\u4e00\u4e2a\u8282\u70b9\u5b58\u5728\u591a\u4e2a\u526f\u672c\u7684\u60c5\u51b5\uff0c\u8fd9\u4e2a\u8282\u70b9\u6545\u969c\u65f6\uff0c\u591a\u4e2a\u526f\u672c\u4f1a\u53d7\u5230\u5f71\u54cd\u3002 4\u3001W + R > N \u65f6\uff0c\u4ee3\u8868\u5f3a\u4e00\u81f4\u6027\u3002 5\u3001W = N \u65f6\uff0c\u8bfb\u6027\u80fd\u597d\u3002R = N\uff0c\u5199\u6027\u80fd\u597d\u3002 NOTE: read\u3001write tradeoff 6\u3001W = R = (N+1)/2\uff0c\u5bb9\u9519\u80fd\u529b\u597d\uff0c\u80fd\u5bb9\u5fcd \u5c11\u6570\u8282\u70b9\uff08\u4e5f\u5c31\u662f(N-1)/2\uff09 \u4e2a\u8282\u70b9\u6545\u969c\u3002 NOTE: \u6b64\u65f6W+R>N\uff0c\u663e\u7136\u80fd\u591f\u4fdd\u8bc1strong consistency 7\u3001\u5982\u4f55\u8bbe\u7f6e N\u3001W\u3001R \u503c\uff0c\u53d6\u51b3\u4e8e\u6211\u4eec\u7684\u7cfb\u7edf\u8be5\u5f80\u54ea\u65b9\u9762\u4f18\u5316\u3002 8\u3001 Quorum NWR \u5206\u5e03\u5f0f\u7b97\u6cd5\u7ed9\u4e1a\u52a1\u63d0\u4f9b\u4e86\u6309\u9700\u9009\u62e9\u4e00\u81f4\u6027\u7ea7\u522b\u7684\u7075\u6d3b\u5ea6\uff0c\u5f25\u8865\u4e86 AP \u578b\u7cfb\u7edf\u7f3a\u4e4f\u5f3a\u4e00\u81f4\u6027\u7684\u7f3a\u70b9\u3002","title":"Introduction"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/08-Quorum-NWR/#csdn#quorum#nwr","text":"NOTE: \u4e00\u822c\u53eb\u505aNRW","title":"csdn \u592a\u4e0a\u8001\u541b\u7684\u70bc\u4e39\u7089\u4e4b\u5206\u5e03\u5f0f Quorum NWR"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/08-Quorum-NWR/#_1","text":"\u6620\u5c04\u5230\u6211\u4eec\u4e92\u8054\u7f51\u7cfb\u7edf\u4e2d\uff1a\u4e39\u7089\u7c7b\u4f3c\u4e8e\u670d\u52a1\u5668\u8282\u70b9\u6216\u6570\u636e\u5e93\u8282\u70b9\uff0c\u901a\u8fc7\u591a\u4e2a\u8282\u70b9\u6765\u76f8\u4e92\u5907\u4efd\u6570\u636e\u6765\u4fdd\u8bc1\u7cfb\u7edf\u7684 \u9ad8\u53ef\u7528\u6027 \uff08High Availability\uff09\u3002","title":"\u4e00\u3001\u4e09\u4e2a\u70bc\u4e39\u7089\u600e\u4e48\u5206\u914d\u7684"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/08-Quorum-NWR/#_2","text":"","title":"\u4e8c\u3001\u5982\u4f55\u4fdd\u8bc1\u4e39\u836f\u54c1\u8d28\u4e00\u6837"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/08-Quorum-NWR/#21","text":"\u592a\u4e0a\u8001\u541b\u8bf4\u7684\u54c1\u8d28\u4fdd\u6301\u4e00\u81f4\u5230\u5e95\u600e\u4e48\u56de\u4e8b\uff1f \u4e00\u53f7\u4e39\u7089\u91cc\u9762\u7684\u5ef6\u5e74\u4e39\u548c\u4e8c\u53f7\u4e39\u7089\u7684\u5ef6\u5e74\u4e39\u5982\u4f55\u4fdd\u8bc1\u54c1\u8d28\u4e00\u81f4\u5462\uff1f \u8fd9\u4e0d\u5c31\u662f\u6211\u4eec\u5e38\u5e38\u8bf4\u7684 \u5206\u5e03\u5f0f\u4e00\u81f4\u6027 \u5417\uff1f\u4e24\u9897\u4e39\u836f\u5206\u5e03\u5728\u4e0d\u540c\u7684\u4e39\u7089\u4e2d\uff0c\u9700\u8981\u4fdd\u8bc1\u54c1\u8d28\u4e00\u81f4\u3002 \u5982\u4e0b\u56fe\u6240\u793a\uff0c\u8fd9\u4e24\u9897\u5ef6\u5e74\u4e39\u7684\u4e00\u5927\u4e00\u5c0f\uff0c\u989c\u8272\u4e5f\u6709\u4e0d\u540c\uff0c\u8fd9\u5c31\u662f\u54c1\u8d28\u4e0d\u4e00\u6837\u3002","title":"2.1 \u4e00\u81f4\u6027"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/08-Quorum-NWR/#22","text":"\u5206\u5e03\u5f0f\u4e2d\u7684\u4e00\u81f4\u6027\u53c8\u5206\u4e3a \u6700\u7ec8\u4e00\u81f4\u6027 \u548c \u5f3a\u4e00\u81f4\u6027 \u3002 \u6240\u8c13 \u5f3a\u4e00\u81f4\u6027 \u5c31\u662f\u5199\u64cd\u4f5c\u5b8c\u6210\u540e\uff0c\u4efb\u4f55\u540e\u7eed\u8bbf\u95ee\u90fd\u80fd\u8bfb\u5230\u66f4\u65b0\u540e\u7684\u503c\u3002\u8fd9\u5c31\u662f CP \u7cfb\u7edf\u6240\u8981\u6c42\u7684\u4e00\u81f4\u6027\u548c\u5206\u533a\u5bb9\u9519\u6027\u3002 \u800c \u6700\u7ec8\u4e00\u81f4\u6027 \u5c31\u662f\u4e0d\u4fdd\u8bc1\u540e\u7eed\u8bbf\u95ee\u90fd\u80fd\u8bfb\u5230\u66f4\u65b0\u540e\u7684\u503c\uff0c\u4f46\u662f\u7ecf\u8fc7\u4e00\u6bb5\u65f6\u95f4\u540e\uff0c\u518d\u53bb\u8bfb\uff0c\u5c31\u80fd\u5f97\u5230\u76f8\u540c\u7684\u503c\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c\u5728\u8fd9\u6bb5\u65f6\u95f4\u5185\uff0c\u53ef\u80fd\u8bfb\u5230\u65e7\u7684\u6570\u636e\u3002\u8fd9\u5c31\u662f AP \u7cfb\u7edf\u6240\u8981\u6c42\u7684\u53ef\u7528\u6027\u548c\u5206\u533a\u5bb9\u9519\u6027\u3002","title":"2.2 \u6700\u7ec8\u4e00\u81f4\u6027\u548c\u5f3a\u4e00\u81f4\u6027"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/08-Quorum-NWR/#quorum#nwr","text":"","title":"\u4e09\u3001\u53ef\u63a7\u7684\u54c1\u8d28\uff1aQuorum NWR \u534f\u8bae"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/08-Quorum-NWR/#quorum#nwr_1","text":"\u5047\u5982\u5ef6\u5e74\u4e39\u5fc5\u987b\u4fdd\u8bc1\u54c1\u8d28\u7684\u5f3a\u4e00\u81f4\u6027\uff0c\u800c\u5065\u6b65\u4e39\u53ea\u9700\u8981\u4fdd\u8bc1\u54c1\u8d28\u7684\u6700\u7ec8\u4e00\u81f4\u6027\uff0c\u8fd9\u4e2a\u8be5\u600e\u4e48\u63a7\u5236\u5462\uff1f \u8fd9\u4e2a\u53ef\u6ca1\u6709\u96be\u5012\u8001\u541b\uff0c\u56e0\u4e3a\u8001\u541b\u61c2\u5f97\u5206\u5e03\u5f0f\u534f\u8bae\uff1a Quorum NWR \u3002 Quorum \u8fd9\u4e2a\u5355\u8bcd\u7684\u610f\u601d\uff1a(\u4f1a\u8bae\u7684)\u6cd5\u5b9a\u4eba\u6570\u3002\u4e3b\u8981\u662f\u770b\u540e\u9762\u4e09\u4e2a\u5927\u5199\u5b57\u6bcd\uff1a N \u3001 W \u3001 R \u3002\u7531 NWR \u6765\u63a7\u5236\u4e00\u81f4\u6027\u3002","title":"Quorum NWR"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/08-Quorum-NWR/#31#n","text":"\u6211\u4eec\u8fd8\u662f\u6765\u770b\u4e0b\u4e39\u7089\u4e2d\u7684\u60c5\u51b5\uff0c\u4e24\u9897\u5ef6\u5e74\u4e39\u662f\u4e92\u4e3a\u5907\u4efd\u7684\uff0c\u76f8\u5f53\u4e8e\u6709\u4e24\u4e2a\u526f\u672c\u3002 N \u79f0\u4f5c\u526f\u672c\u6570\uff0c\u53c8\u53eb\u505a\u590d\u5236\u56e0\u5b50\uff08Replication Factor\uff09 \u3002\u8868\u793a\u540c\u4e00\u4efd\u6570\u636e\u6709\u591a\u5c11\u4e2a\u526f\u672c\uff0c\u6240\u4ee5\uff1a\u5ef6\u5e74\u4e39\u7684 N = 2\u3002\u4f9d\u6b21\u7c7b\u63a8\uff1a\u5065\u6b65\u4e39\u7684 N = 2\uff0c\u6062\u590d\u4e39\u7684 N = 2\u3002\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u90a3 N \u53ef\u4ee5\u53d8\u5417\uff1f \u5982\u4e0b\u56fe\u6240\u793a\uff1a\u6bd4\u5982\u6211\u60f3\u70bc 3 \u9897\u5ef6\u5e74\u4e39\uff0c\u4e5f\u5c31\u662f\u6bcf\u4e2a\u4e39\u7089\u90fd\u6709\u5ef6\u5e74\u4e39\uff0c\u90a3\u5c31\u628a N \u6539\u6210 3 \u5c31\u53ef\u4ee5\u4e86\u3002\u800c\u5065\u6b65\u4e39\u53ea\u9700\u8981\u70bc\u4e00\u9897\u8db3\u4ee5\uff0c\u90a3\u4e00\u53f7\u4e39\u7089\u70bc\u5c31\u53ef\u4ee5\u4e86\uff0c\u6240\u4ee5N = 1\u3002","title":"3.1 \u53c2\u6570 N"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/08-Quorum-NWR/#32#w","text":"\u6307\u5b9a\u4e86\u526f\u672c\u6570 N \u4e4b\u540e\uff0c\u5c31\u53ef\u4ee5\u5bf9\u526f\u672c\u6570\u636e\u8fdb\u884c\u8bfb\u5199\u64cd\u4f5c\u3002 1\u3001 \u8bfb\u64cd\u4f5c \uff1a\u67e5\u770b\u6240\u5728\u4e39\u7089\u5185\u4e39\u836f\u7684\u60c5\u51b5\u3002 2\u3001 \u5199\u64cd\u4f5c \uff1a\u7ed9\u4e39\u836f\u6dfb\u52a0\u836f\u6750\u3001\u63d0\u9ad8\u6e29\u5ea6\u3002 \u90a3\u591a\u4e2a\u4e39\u836f\u8be5\u5982\u4f55\u6267\u884c\u8bfb\u5199\u64cd\u4f5c\u5462\uff1f\u5bf9\u4e8e\u5199\u64cd\u4f5c\uff0c\u6211\u4eec\u6709 W \u53c2\u6570\uff0c\u5bf9\u4e8e\u8bfb\u64cd\u4f5c\uff0c\u6211\u4eec\u6709 R \u53c2\u6570\u3002 W \u79f0\u4e3a\u5199\u4e00\u81f4\u6027\u7ea7\u522b\uff08Write Consistency Level\uff09 \uff0c\u8868\u793a\u6210\u529f\u5b8c\u6210 W \u4e2a\u526f\u672c\u66f4\u65b0\uff0c\u624d\u5b8c\u6210\u5199\u64cd\u4f5c\u3002 \u6bd4\u5982\u8bbe\u7f6e\u5ef6\u5e74\u4e39\u7684 W = 2\uff0c\u8868\u793a\u5bf9\u5ef6\u5e74\u4e39\u6267\u884c\u5199\u64cd\u4f5c\u65f6\uff0c\u5b8c\u6210\u4e86 2 \u4e2a\u526f\u672c\u7684\u66f4\u65b0\u65f6\uff0c\u624d\u5b8c\u6210\u5199\u64cd\u4f5c\u3002 \u5982\u4e0b\u56fe\u6240\u793a\uff1a\u4e00\u53f7\u4e39\u7089\u548c\u4e8c\u53f7\u4e39\u7089\u4e2d\u7684\u5ef6\u5e74\u4e39\u90fd\u52a0\u5165\u4e86\u83b2\u82b1\uff0c\u800c\u4e09\u53f7\u4e39\u7089\u4e2d\u7684\u5ef6\u5e74\u4e39\u672a\u52a0\u5165\u83b2\u82b1\u3002\u4e5f\u5c31\u662f\u53ea\u5b8c\u6210\u4e86\u4e24\u4e2a\u526f\u672c\u7684\u66f4\u65b0\uff0c\u7b26\u5408 W = 2 \u8fd9\u4e2a\u6761\u4ef6\uff0c\u5373\u5199\u64cd\u4f5c\u5b8c\u6210\u3002 \u4f46\u662f\u5927\u5bb6\u53d1\u73b0\u95ee\u9898\u6ca1\uff0c\u4e09\u53f7\u4e39\u7089\u7684\u5ef6\u5e74\u4e39\u672a\u52a0\u5165\u83b2\u82b1\uff0c\u90a3\u600e\u4e48\u4fdd\u8bc1\u592a\u4e0a\u8001\u541b\u67e5\u770b\u4e39\u836f\u60c5\u51b5\u65f6\uff0c\u5f97\u77e5\u662f\u5df2\u52a0\u5165\u83b2\u82b1\u5462\uff1f\u4e5f\u5c31\u662f\u5982\u4f55\u4fdd\u8bc1**\u8bfb\u5199\u7684\u5f3a\u4e00\u81f4\u6027**\uff0c\u8fd9\u5c31\u8981\u7528\u5230\u7b2c\u4e09\u4e2a\u53c2\u6570\u4e86\uff1aR\u3002","title":"3.2 \u53c2\u6570 W"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/08-Quorum-NWR/#33#r","text":"R \u79f0\u4e3a\u8bfb\u4e00\u81f4\u6027\u7ea7\u522b\uff08Read Consistency Level\uff09\uff0c\u8868\u793a\u8bfb\u53d6\u4e00\u4e2a\u6570\u636e\u5bf9\u8c61\u65f6\uff0c\u9700\u8981\u8bfb R \u4e2a\u526f\u672c\uff0c\u7136\u540e\u8fd4\u56de R \u4e2a\u526f\u672c\u4e2d\u6700\u65b0\u7684\u90a3\u4efd\u6570\u636e\u3002 \u56de\u5230\u70bc\u4e39\u7684\u95ee\u9898\u4e2d\uff0c\u8bbe\u7f6e\u5ef6\u5e74\u4e39\u7684 R = 2\uff0c\u4e5f\u5c31\u662f\u67e5\u770b\u5ef6\u5e74\u4e39\u7684\u60c5\u51b5\u65f6\uff0c\u53ea\u9700\u8981\u67e5\u770b\u4e24\u4e2a\u4e39\u7089\u5185\u7684\u5ef6\u5e74\u4e39\u7684\u60c5\u51b5\uff0c\u7136\u540e\u8fd4\u56de\u6700\u65b0\u7684\u5ef6\u5e74\u4e39\u7684\u60c5\u51b5\u5c31\u53ef\u4ee5\u4e86\u3002 1\u3001\u5047\u8bbe\u67e5\u770b\u7684\u662f\u4e00\u53f7\u548c\u4e8c\u53f7\u4e39\u7089\u5185\u7684\u5ef6\u5e74\u4e39\uff0c\u8fd4\u56de\u7684\u60c5\u51b5\u90fd\u662f\uff1a\u5df2\u52a0\u5165\u83b2\u82b1\u3002\u8fd9\u79cd\u573a\u666f\u662f\u4e00\u81f4\u6027\u7684\u3002 2\u3001\u5047\u8bbe\u67e5\u770b\u7684\u662f\u4e00\u53f7\u548c\u4e09\u53f7\u4e39\u7089\u5185\u7684\u5ef6\u5e74\u4e39\uff0c\u4e00\u53f7\u4e39\u7089\u7684\u5ef6\u5e74\u4e39\u662f\u5df2\u52a0\u5165\u83b2\u82b1\uff0c\u4e09\u53f7\u4e39\u7089\u662f\u672a\u52a0\u5165\u83b2\u82b1\uff0c\u4f46\u662f\u4e09\u53f7\u4e39\u7089\u5185\u7684\u5ef6\u5e74\u4e39\u6700\u540e\u4e00\u6b21\u64cd\u4f5c\u65f6\u95f4\u662f\u65e9\u4e8e\u4e00\u53f7\u4e39\u7089\u7684\uff0c\u6240\u4ee5\u8fd4\u56de\u4e00\u53f7\u4e39\u7089\u5185\u5ef6\u5e74\u4e39\u7684\u60c5\u51b5\uff1a\u5df2\u52a0\u5165\u83b2\u82b1\u3002\u8fd9\u79cd\u573a\u666f\u4e5f\u662f\u4e00\u81f4\u6027\u7684\u3002","title":"3.3 \u53c2\u6570 R"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/08-Quorum-NWR/#34","text":"\u53c2\u6570 N\u3001W\u3001R \u7684\u4e0d\u540c\u7ec4\u5408\u5c06\u4f1a\u5e26\u6765\u4e0d\u540c\u7684\u4e00\u81f4\u6027\u6548\u679c\u3002 1\u3001\u6bd4\u5982\u4e0a\u9762\u7684\u4f8b\u5b50\uff0cN = 3\uff0cW = 2\uff0cR = 2\uff0cW + R > N\uff0c\u5bf9\u4e8e\u5ba2\u6237\u7aef\u6765\u8bb2\uff0c\u6574\u4e2a\u7cfb\u7edf\u80fd\u4fdd\u8bc1\u5f3a\u4e00\u81f4\u6027\uff0c\u4e00\u5b9a\u80fd\u8fd4\u56de\u66f4\u65b0\u540e\u7684\u90a3\u4efd\u6570\u636e\u3002 2\u3001\u5f53 W + R <= N \u65f6\uff0c\u5bf9\u4e8e\u5ba2\u6237\u7aef\u6765\u8bb2\uff0c\u6574\u4e2a\u7cfb\u7edf\u53ea\u80fd\u4fdd\u8bc1\u6700\u7ec8\u4e00\u81f4\u6027\uff0c\u8bbf\u95ee\u6570\u636e\u671f\u95f4\u53ef\u80fd\u4f1a\u8fd4\u56de\u65e7\u6570\u636e\u3002 \u53c2\u6570\u4e0d\u540c\uff0c\u6548\u679c\u4e0d\u540c\uff0c\u5206\u5e03\u5f0f\u7cfb\u7edf\u9700\u8981\u573a\u666f\u6765\u914d\u7f6e\u3002","title":"3.4 \u53c2\u6570\u7ec4\u5408"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/08-Quorum-NWR/#_3","text":"InfluxDB \u4f01\u4e1a\u7248\u662f\u65f6\u5e8f\u6570\u636e\u5e93\uff0c\u5b83\u6709\u56db\u79cd\u5199\u4e00\u81f4\u6027\u7ea7\u522b\uff1a 1\u3001 any \uff1aW + R < N\uff0cW = 1\uff0c\u4efb\u4f55\u4e00\u4e2a\u8282\u70b9\u5199\u5165\u6210\u529f\u540e\uff0c\u6216\u8005\u5199\u5165 Hinted-handoff \u7f13\u5b58\uff08\u7b49\u4e0b\u6b21\u91cd\u4f20\uff09\uff0c\u8fd4\u56de\u6210\u529f\u7ed9\u5ba2\u6237\u7aef\u3002 2\u3001 one \uff1aW + R < N\uff0cW = 1\uff0c\u4efb\u4f55\u4e00\u4e2a\u8282\u70b9\u5199\u5165\u6210\u529f\u540e\uff0c\u7acb\u5373\u8fd4\u56de\u6210\u529f\u7ed9\u5ba2\u6237\u7aef\uff0c\u4e0d\u5305\u62ec\u5199\u5165 Hinted-handoff \u7f13\u5b58 3\u3001 quorum \uff1aW + R > N\uff0c\u5927\u591a\u6570\u8282\u70b9\u5199\u5165\u6210\u529f\u540e\uff0c\u5c31\u8fd4\u56de\u6210\u529f\u7ed9\u5ba2\u6237\u7aef\u3002\uff08\u8981\u6c42 N \u5927\u4e8e2\uff09 4\u3001 all \uff1aW = N\uff0c\u6240\u6709\u8282\u70b9\u90fd\u5199\u5165\u6210\u529f\u540e\uff0c\u8fd4\u56de\u6210\u529f\u3002 \u53e6\u5916\u5bf9\u4e8e \u65f6\u5e8f\u6570\u636e\u5e93 InfluxDB \u6765\u8bf4\uff0c\u8bfb\u64cd\u4f5c\u9700\u8981\u8bfb\u53d6\u5927\u91cf\u6570\u636e\uff0c\u4e3a\u4e86\u4fdd\u8bc1\u8bfb\u53d6\u7684\u9ad8\u6548\uff0c\u5b83\u4e0d\u652f\u6301\u8bfb\u4e00\u81f4\u6027\u7ea7\u522b\uff08R = N\uff09\uff0c\u4f46\u662f\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e\u5199\u4e00\u81f4\u6027\u7ea7\u522b\u4e3a all\uff0c\u6765\u5b9e\u73b0\u5f3a\u4e00\u81f4\u6027\u3002 InfluxDb \u5b9e\u73b0\u4e86 Quorum NWR\uff0c\u5f53\u7ebf\u4e0a\u4e1a\u52a1\u9700\u8981\u4e34\u65f6\u505a\u4e9b\u4e00\u81f4\u6027\u8c03\u6574\u65f6\uff0c\u8bbe\u7f6e\u4e0d\u540c\u7684\u5199\u4e00\u81f4\u6027\u7ea7\u522b\u5373\u53ef\u5b8c\u6210\u5feb\u901f\u5207\u6362\u3002","title":"\u56db\u3001\u5e94\u7528"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/08-Quorum-NWR/#_4","text":"\u672c\u6587\u901a\u8fc7\u592a\u4e0a\u8001\u541b\u548c\u592a\u767d\u91d1\u661f\u5173\u4e8e\u70bc\u4e39\u7684\u5bf9\u8bdd\uff0c\u5f15\u7533\u51fa\u81ea\u5b9a\u4e49\u4e00\u81f4\u6027\u7684\u5206\u5e03\u5f0f\u534f\u8bae\uff1aQuorum NWR \u534f\u8bae\u3002 1\u3001\u4e39\u7089\u6bd4\u55bb\u8282\u70b9\uff0c\u4e39\u836f\u6bd4\u4f5c\u6570\u636e\uff0c\u591a\u4e2a\u4e39\u836f\u79f0\u4f5c\u526f\u672c\u3002 2\u3001 N \u4ee3\u8868\u526f\u672c\u6570\uff0c W \u4ee3\u8868\u5199\u591a\u5c11\u4e2a\u526f\u672c\u6570\uff0c R \u4ee3\u8868\u8bfb\u591a\u5c11\u4e2a\u526f\u672c\u6570\u3002 3\u3001\u5f53 N \u5927\u4e8e\u8282\u70b9\u6570\u65f6\uff0c\u5c31\u4f1a\u51fa\u73b0\u4e00\u4e2a\u8282\u70b9\u5b58\u5728\u591a\u4e2a\u526f\u672c\u7684\u60c5\u51b5\uff0c\u8fd9\u4e2a\u8282\u70b9\u6545\u969c\u65f6\uff0c\u591a\u4e2a\u526f\u672c\u4f1a\u53d7\u5230\u5f71\u54cd\u3002 4\u3001W + R > N \u65f6\uff0c\u4ee3\u8868\u5f3a\u4e00\u81f4\u6027\u3002 5\u3001W = N \u65f6\uff0c\u8bfb\u6027\u80fd\u597d\u3002R = N\uff0c\u5199\u6027\u80fd\u597d\u3002 NOTE: read\u3001write tradeoff 6\u3001W = R = (N+1)/2\uff0c\u5bb9\u9519\u80fd\u529b\u597d\uff0c\u80fd\u5bb9\u5fcd \u5c11\u6570\u8282\u70b9\uff08\u4e5f\u5c31\u662f(N-1)/2\uff09 \u4e2a\u8282\u70b9\u6545\u969c\u3002 NOTE: \u6b64\u65f6W+R>N\uff0c\u663e\u7136\u80fd\u591f\u4fdd\u8bc1strong consistency 7\u3001\u5982\u4f55\u8bbe\u7f6e N\u3001W\u3001R \u503c\uff0c\u53d6\u51b3\u4e8e\u6211\u4eec\u7684\u7cfb\u7edf\u8be5\u5f80\u54ea\u65b9\u9762\u4f18\u5316\u3002 8\u3001 Quorum NWR \u5206\u5e03\u5f0f\u7b97\u6cd5\u7ed9\u4e1a\u52a1\u63d0\u4f9b\u4e86\u6309\u9700\u9009\u62e9\u4e00\u81f4\u6027\u7ea7\u522b\u7684\u7075\u6d3b\u5ea6\uff0c\u5f25\u8865\u4e86 AP \u578b\u7cfb\u7edf\u7f3a\u4e4f\u5f3a\u4e00\u81f4\u6027\u7684\u7f3a\u70b9\u3002","title":"\u4e94\u3001\u603b\u7ed3"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/%E6%9C%8D%E5%8A%A1%E9%9B%AA%E5%B4%A9/","text":"csdn \u4e1c\u6c49\u672b\u5e74\uff0c\u4ed6\u4eec\u628a\u300c\u670d\u52a1\u96ea\u5d29\u300d\u73a9\u5230\u4e86\u6781\u81f4\uff08\u5e72\u8d27\uff09 \u4f46\u968f\u7740\u4e1a\u52a1\u7684\u589e\u957f\uff0c\u670d\u52a1\u7684\u6570\u91cf\u4e5f\u662f\u968f\u4e4b\u589e\u591a\uff0c\u903b\u8f91\u4e5f\u4f1a\u66f4\u52a0\u590d\u6742\uff0c\u4e00\u4e2a\u670d\u52a1\u7684\u67d0\u4e2a\u903b\u8f91\u9700\u8981\u4f9d\u8d56\u591a\u4e2a\u5176\u4ed6\u670d\u52a1\u624d\u80fd\u5b8c\u6210\u3002\u5047\u5982\u4e00\u4e2a\u88ab\u4f9d\u8d56\u7684\u670d\u52a1\u4e0d\u80fd\u5411\u4e0a\u6e38\u7684\u670d\u52a1\u63d0\u4f9b\u670d\u52a1\uff0c\u5219\u5f88\u53ef\u80fd\u9020\u6210\u96ea\u5d29\u6548\u5e94\uff0c\u6700\u540e\u5bfc\u81f4\u6574\u4e2a\u670d\u52a1\u4e0d\u53ef\u8bbf\u95ee\u3002 \u5c31\u50cf\u96ea\u5c71\u4e0a\u67d0\u4e00\u5904\u51fa\u73b0\u79ef\u96ea\u5d29\u584c\u7684\u73b0\u8c61\uff0c\u6162\u6162\u5730\u5e26\u52a8\u5176\u4ed6\u7247\u533a\u7684\u79ef\u96ea\u5d29\u584c\uff0c\u4ea7\u751f\u4e86\u7ea7\u8054\u53cd\u5e94\uff0c\u6700\u540e\u9020\u6210\u5927\u7247\u7684\u79ef\u96ea\u5d29\u584c\uff0c\u8fd9\u5c31\u662f\u5e38\u89c1\u7684**\u96ea\u5d29**\u573a\u666f\u3002 \u5c0f\u7ed3\uff1a \u4e00\u4e2a\u670d\u52a1\u5931\u8d25\uff0c\u5bfc\u81f4\u6574\u6761\u94fe\u8def\u7684\u670d\u52a1\u90fd\u5931\u8d25\u7684\u573a\u666f\uff0c\u79f0\u4e3a**\u670d\u52a1\u96ea\u5d29**\u3002 NOTE: \u4e0a\u8ff0\u89e3\u91ca\u8ba9\u6211\u60f3\u5230\u4e86single point of failure \u4e94\u3001\u5982\u4f55\u9632\u6b62\u96ea\u5d29 \u65b9\u6848 \u51fa\u95ee\u9898\u524d\u9884\u9632\uff1a\u9650\u6d41\u3001\u4e3b\u52a8\u964d\u7ea7\u3001\u9694\u79bb \u51fa\u95ee\u9898\u540e\u4fee\u590d\uff1a\u7194\u65ad\u3001\u88ab\u52a8\u964d\u7ea7 \u672c\u7bc7\u4e3b\u8981\u6765\u8bb2\u89e3\u7194\u65ad\u673a\u5236\u3002 \u540e\u7eed\u51e0\u7bc7\u4f1a\u8bb2\u89e3\u5176\u4ed6\u65b9\u6848\u3002 \u516d\u3001\u7194\u65ad\u539f\u7406\u548c\u7b97\u6cd5","title":"Introduction"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/%E6%9C%8D%E5%8A%A1%E9%9B%AA%E5%B4%A9/#csdn","text":"\u4f46\u968f\u7740\u4e1a\u52a1\u7684\u589e\u957f\uff0c\u670d\u52a1\u7684\u6570\u91cf\u4e5f\u662f\u968f\u4e4b\u589e\u591a\uff0c\u903b\u8f91\u4e5f\u4f1a\u66f4\u52a0\u590d\u6742\uff0c\u4e00\u4e2a\u670d\u52a1\u7684\u67d0\u4e2a\u903b\u8f91\u9700\u8981\u4f9d\u8d56\u591a\u4e2a\u5176\u4ed6\u670d\u52a1\u624d\u80fd\u5b8c\u6210\u3002\u5047\u5982\u4e00\u4e2a\u88ab\u4f9d\u8d56\u7684\u670d\u52a1\u4e0d\u80fd\u5411\u4e0a\u6e38\u7684\u670d\u52a1\u63d0\u4f9b\u670d\u52a1\uff0c\u5219\u5f88\u53ef\u80fd\u9020\u6210\u96ea\u5d29\u6548\u5e94\uff0c\u6700\u540e\u5bfc\u81f4\u6574\u4e2a\u670d\u52a1\u4e0d\u53ef\u8bbf\u95ee\u3002 \u5c31\u50cf\u96ea\u5c71\u4e0a\u67d0\u4e00\u5904\u51fa\u73b0\u79ef\u96ea\u5d29\u584c\u7684\u73b0\u8c61\uff0c\u6162\u6162\u5730\u5e26\u52a8\u5176\u4ed6\u7247\u533a\u7684\u79ef\u96ea\u5d29\u584c\uff0c\u4ea7\u751f\u4e86\u7ea7\u8054\u53cd\u5e94\uff0c\u6700\u540e\u9020\u6210\u5927\u7247\u7684\u79ef\u96ea\u5d29\u584c\uff0c\u8fd9\u5c31\u662f\u5e38\u89c1\u7684**\u96ea\u5d29**\u573a\u666f\u3002 \u5c0f\u7ed3\uff1a \u4e00\u4e2a\u670d\u52a1\u5931\u8d25\uff0c\u5bfc\u81f4\u6574\u6761\u94fe\u8def\u7684\u670d\u52a1\u90fd\u5931\u8d25\u7684\u573a\u666f\uff0c\u79f0\u4e3a**\u670d\u52a1\u96ea\u5d29**\u3002 NOTE: \u4e0a\u8ff0\u89e3\u91ca\u8ba9\u6211\u60f3\u5230\u4e86single point of failure","title":"csdn \u4e1c\u6c49\u672b\u5e74\uff0c\u4ed6\u4eec\u628a\u300c\u670d\u52a1\u96ea\u5d29\u300d\u73a9\u5230\u4e86\u6781\u81f4\uff08\u5e72\u8d27\uff09"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/%E6%9C%8D%E5%8A%A1%E9%9B%AA%E5%B4%A9/#_1","text":"","title":"\u4e94\u3001\u5982\u4f55\u9632\u6b62\u96ea\u5d29"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/%E6%9C%8D%E5%8A%A1%E9%9B%AA%E5%B4%A9/#_2","text":"\u51fa\u95ee\u9898\u524d\u9884\u9632\uff1a\u9650\u6d41\u3001\u4e3b\u52a8\u964d\u7ea7\u3001\u9694\u79bb \u51fa\u95ee\u9898\u540e\u4fee\u590d\uff1a\u7194\u65ad\u3001\u88ab\u52a8\u964d\u7ea7 \u672c\u7bc7\u4e3b\u8981\u6765\u8bb2\u89e3\u7194\u65ad\u673a\u5236\u3002 \u540e\u7eed\u51e0\u7bc7\u4f1a\u8bb2\u89e3\u5176\u4ed6\u65b9\u6848\u3002","title":"\u65b9\u6848"},{"location":"Distributed-computing/Guide/CSDN-%E6%82%9F%E7%A9%BA%E8%AE%B2%E6%9E%B6%E6%9E%84-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E5%88%97/%E6%9C%8D%E5%8A%A1%E9%9B%AA%E5%B4%A9/#_3","text":"","title":"\u516d\u3001\u7194\u65ad\u539f\u7406\u548c\u7b97\u6cd5"},{"location":"Distributed-computing/Guide/Distributed-system-challenge/","text":"The challenge of Distributed Systems \u672c\u6587\u5bf9Distributed Systems\u7684challenge\u3001trouble\u8fdb\u884c\u603b\u7ed3\uff0c\u5b83\u4eec\u5176\u5b9e\u53cd\u6620\u7684\u662f\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u7279\u5f81\uff0c\u53ea\u6709\u628a\u63e1\u4e86\u5b83\u4eec\u624d\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u5404\u79cd\u6280\u672f\u3002 \u7d20\u6750 \u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\uff0c\u5bf9challenge of Distributed Systems\u8fdb\u884c\u4e86\u603b\u7ed3\u3002 Book-Designing-Data-Intensive-Applications \u5728\u8fd9\u672c\u4e66\u7684 PART-II-Distributed-Data\\CHAPTER-8-The-Trouble-with-Distributed-Systems \u4e2d\u8fdb\u884c\u4e86\u975e\u5e38\u597d\u7684\u603b\u7ed3\uff0c\u53c2\u89c1\u5bf9\u5e94\u7684\u7ae0\u8282\u3002 ZooKeeper ZooKeeper: A Distributed Coordination Service for Distributed Applications \u901a\u8fc7\u5b83\u7684\u5b9e\u73b0\u6211\u4eec\u662f\u53ef\u4ee5\u770b\u51fadistributed computing\u4e2d\u7684\u5404\u79cdtrouble\u7684\u3002 embedded Distributed Software Design: Challenges and Solutions NOTE: \u603b\u7ed3\u7684\u8fd8\u53ef\u4ee5 martinfowler Patterns of Distributed Systems \u5176\u4e2d\u4e5f\u8fdb\u884c\u4e86\u63a2\u8ba8\u3002 wikipedia Distributed computing 2\u3001 lack of a global clock NOTE: \u8fd9\u662f\u5206\u5e03\u5f0f\u8ba1\u7b97\u7684\u6311\u6218 3\u3001independent failure of components. NOTE: 1\u3001\u7b80\u800c\u8a00\u4e4b\uff0c\u53bb\u4e2d\u5fc3\u5316\u7684 Byzantine fault \u5b83\u662f\u975e\u5e38\u80fd\u591f\u4f53\u73b0\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u7279\u6027\u3001\u8c03\u6574\u3001\u56f0\u96be\u7684\uff0c\u53c2\u89c1: Byzantine-Fault-Tolerance \u7ae0\u8282\u3002 Lack of a global clock and ordering ordering \u662fcomputational \u7684\u524d\u63d0\uff0c\u800c\u5206\u5e03\u5f0f\u7cfb\u7edf\" Lack of a global clock \"\uff0c\u90a3\u8fd9\u8981\u5982\u4f55\u89e3\u51b3\u5462\uff1f\u8fd9\u5728 Distributed-computing\\Theory\\Time&Ordering \u7ae0\u8282\u8fdb\u884c\u4e86\u8ba8\u8bba\u3002","title":"Introduction"},{"location":"Distributed-computing/Guide/Distributed-system-challenge/#the#challenge#of#distributed#systems","text":"\u672c\u6587\u5bf9Distributed Systems\u7684challenge\u3001trouble\u8fdb\u884c\u603b\u7ed3\uff0c\u5b83\u4eec\u5176\u5b9e\u53cd\u6620\u7684\u662f\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u7279\u5f81\uff0c\u53ea\u6709\u628a\u63e1\u4e86\u5b83\u4eec\u624d\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u5404\u79cd\u6280\u672f\u3002","title":"The challenge of Distributed Systems"},{"location":"Distributed-computing/Guide/Distributed-system-challenge/#_1","text":"\u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\uff0c\u5bf9challenge of Distributed Systems\u8fdb\u884c\u4e86\u603b\u7ed3\u3002","title":"\u7d20\u6750"},{"location":"Distributed-computing/Guide/Distributed-system-challenge/#book-designing-data-intensive-applications","text":"\u5728\u8fd9\u672c\u4e66\u7684 PART-II-Distributed-Data\\CHAPTER-8-The-Trouble-with-Distributed-Systems \u4e2d\u8fdb\u884c\u4e86\u975e\u5e38\u597d\u7684\u603b\u7ed3\uff0c\u53c2\u89c1\u5bf9\u5e94\u7684\u7ae0\u8282\u3002","title":"Book-Designing-Data-Intensive-Applications"},{"location":"Distributed-computing/Guide/Distributed-system-challenge/#zookeeper","text":"ZooKeeper: A Distributed Coordination Service for Distributed Applications \u901a\u8fc7\u5b83\u7684\u5b9e\u73b0\u6211\u4eec\u662f\u53ef\u4ee5\u770b\u51fadistributed computing\u4e2d\u7684\u5404\u79cdtrouble\u7684\u3002","title":"ZooKeeper"},{"location":"Distributed-computing/Guide/Distributed-system-challenge/#embedded#distributed#software#design#challenges#and#solutions","text":"NOTE: \u603b\u7ed3\u7684\u8fd8\u53ef\u4ee5","title":"embedded Distributed Software Design: Challenges and Solutions"},{"location":"Distributed-computing/Guide/Distributed-system-challenge/#martinfowler#patterns#of#distributed#systems","text":"\u5176\u4e2d\u4e5f\u8fdb\u884c\u4e86\u63a2\u8ba8\u3002","title":"martinfowler Patterns of Distributed Systems"},{"location":"Distributed-computing/Guide/Distributed-system-challenge/#wikipedia#distributed#computing","text":"2\u3001 lack of a global clock NOTE: \u8fd9\u662f\u5206\u5e03\u5f0f\u8ba1\u7b97\u7684\u6311\u6218 3\u3001independent failure of components. NOTE: 1\u3001\u7b80\u800c\u8a00\u4e4b\uff0c\u53bb\u4e2d\u5fc3\u5316\u7684","title":"wikipedia Distributed computing"},{"location":"Distributed-computing/Guide/Distributed-system-challenge/#byzantine#fault","text":"\u5b83\u662f\u975e\u5e38\u80fd\u591f\u4f53\u73b0\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u7279\u6027\u3001\u8c03\u6574\u3001\u56f0\u96be\u7684\uff0c\u53c2\u89c1: Byzantine-Fault-Tolerance \u7ae0\u8282\u3002","title":"Byzantine fault"},{"location":"Distributed-computing/Guide/Distributed-system-challenge/#lack#of#a#global#clock#and#ordering","text":"ordering \u662fcomputational \u7684\u524d\u63d0\uff0c\u800c\u5206\u5e03\u5f0f\u7cfb\u7edf\" Lack of a global clock \"\uff0c\u90a3\u8fd9\u8981\u5982\u4f55\u89e3\u51b3\u5462\uff1f\u8fd9\u5728 Distributed-computing\\Theory\\Time&Ordering \u7ae0\u8282\u8fdb\u884c\u4e86\u8ba8\u8bba\u3002","title":"Lack of a global clock and ordering"},{"location":"Distributed-computing/Guide/martinfowler-Patterns-of-Distributed-Systems/","text":"martinfowler Patterns of Distributed Systems","title":"Introduction"},{"location":"Distributed-computing/Guide/martinfowler-Patterns-of-Distributed-Systems/#martinfowler#patterns#of#distributed#systems","text":"","title":"martinfowler Patterns of Distributed Systems"},{"location":"Distributed-computing/Technique/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u5bf9\u5728distributed computing\u9886\u57df\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684\u4e00\u4e9btechnique\u8fdb\u884c\u603b\u7ed3\u3002","title":"Introduction"},{"location":"Distributed-computing/Technique/#_1","text":"\u672c\u7ae0\u5bf9\u5728distributed computing\u9886\u57df\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684\u4e00\u4e9btechnique\u8fdb\u884c\u603b\u7ed3\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Distributed-computing/Technique/Distributed-transaction/","text":"Distributed transaction wikipedia Distributed transaction \u6587\u7ae0: tianshouzhi 1.0 \u5206\u5e03\u5f0f\u4e8b\u52a1\u6982\u8ff0 ucsd Lecture 8: Transactions, ACID, 2PC, 2PL, Serializability stackoverflow What is a \u201cdistributed transaction\u201d? A Usually, transactions occur on one database server: BEGIN TRANSACTION SELECT something FROM myTable UPDATE something IN myTable COMMIT A distributed transaction involves multiple servers: BEGIN TRANSACTION UPDATE amount = amount - 100 IN bankAccounts WHERE accountNr = 1 UPDATE amount = amount + 100 IN someRemoteDatabaseAtSomeOtherBank . bankAccounts WHERE accountNr = 2 COMMIT The difficulty comes from the fact that the servers must communicate to ensure that transactional properties such as atomicity are satisfied on both servers: If the transaction succeeds, the values must be updated on both servers. If the transaction fails, the transaction must be rollbacked on both servers. It must never happen that the values are updated on one server but not updated on the other. cnblogs \u5206\u5e03\u5f0f\u4e8b\u52a1\uff0c\u4e24\u9636\u6bb5\u63d0\u4ea4\u534f\u8bae\uff0c\u4e09\u9636\u6bb5\u63d0\u4ea4\u534f\u8bae","title":"Introduction"},{"location":"Distributed-computing/Technique/Distributed-transaction/#distributed#transaction","text":"","title":"Distributed transaction"},{"location":"Distributed-computing/Technique/Distributed-transaction/#wikipedia#distributed#transaction","text":"","title":"wikipedia Distributed transaction"},{"location":"Distributed-computing/Technique/Distributed-transaction/#tianshouzhi#10","text":"","title":"\u6587\u7ae0: tianshouzhi 1.0 \u5206\u5e03\u5f0f\u4e8b\u52a1\u6982\u8ff0"},{"location":"Distributed-computing/Technique/Distributed-transaction/#ucsd#lecture#8#transactions#acid#2pc#2pl#serializability","text":"","title":"ucsd Lecture 8: Transactions, ACID, 2PC, 2PL, Serializability"},{"location":"Distributed-computing/Technique/Distributed-transaction/#stackoverflow#what#is#a#distributed#transaction","text":"A Usually, transactions occur on one database server: BEGIN TRANSACTION SELECT something FROM myTable UPDATE something IN myTable COMMIT A distributed transaction involves multiple servers: BEGIN TRANSACTION UPDATE amount = amount - 100 IN bankAccounts WHERE accountNr = 1 UPDATE amount = amount + 100 IN someRemoteDatabaseAtSomeOtherBank . bankAccounts WHERE accountNr = 2 COMMIT The difficulty comes from the fact that the servers must communicate to ensure that transactional properties such as atomicity are satisfied on both servers: If the transaction succeeds, the values must be updated on both servers. If the transaction fails, the transaction must be rollbacked on both servers. It must never happen that the values are updated on one server but not updated on the other.","title":"stackoverflow What is a \u201cdistributed transaction\u201d?"},{"location":"Distributed-computing/Technique/Distributed-transaction/#cnblogs","text":"","title":"cnblogs \u5206\u5e03\u5f0f\u4e8b\u52a1\uff0c\u4e24\u9636\u6bb5\u63d0\u4ea4\u534f\u8bae\uff0c\u4e09\u9636\u6bb5\u63d0\u4ea4\u534f\u8bae"},{"location":"Distributed-computing/Technique/Heartbeat/","text":"Heartbeat Heartbeat\u662fdistributed computing\u4e2d\u5e7f\u6cdb\u91c7\u7528\u7684\u4e00\u79cdtechnique\u3002 \u7d20\u6750 Heartbeat (computing) https://en.wanweibaike.com/wiki-Heartbeat%20protocol In computer science , a heartbeat is a periodic signal generated by hardware or software to indicate normal operation or to synchronize other parts of a computer system .[ 1] Usually a heartbeat is sent between machines at a regular interval in the order of seconds. If the endpoint does not receive a heartbeat for a time\u2014usually a few heartbeat intervals\u2014the machine that should have sent the heartbeat is assumed to have failed.[ 2] Heartbeat message https://en.wanweibaike.com/wiki-Heartbeat%20message A heartbeat message in signal processing is a message sent from an originator to a destination that enables the destination to identify if and when the originator fails or is no longer available.[ 1] Heartbeat messages are typically sent non-stop on a periodic or recurring basis from the originator's start-up until the originator's shutdown. When the destination identifies a lack of heartbeat messages during an anticipated arrival period, the destination may determine that the originator has failed, shutdown, or is generally no longer available. Heartbeat messages may be used for high-availability and fault tolerance purposes. Heartbeat network https://en.wanweibaike.com/wiki-Heartbeat%20network In computer clusters , a heartbeat network is a private network which is shared only by the nodes in the cluster, and is not accessible from outside the cluster. It is used by cluster nodes in order to monitor each node's status and communicate with each other messages necessary for maintaining operation of the cluster. Case study Case: Linux-HA https://en.wanweibaike.com/wiki-Heartbeat%20(program ) Case: zookeeper \u5728 ZooKeeper Getting Started Guide \u4e2d\u6709\u8fd9\u6837\u7684\u4ecb\u7ecd: tickTime : the basic time unit in milliseconds used by ZooKeeper. It is used to do heartbeats and the minimum session timeout will be twice the tickTime .","title":"Introduction"},{"location":"Distributed-computing/Technique/Heartbeat/#heartbeat","text":"Heartbeat\u662fdistributed computing\u4e2d\u5e7f\u6cdb\u91c7\u7528\u7684\u4e00\u79cdtechnique\u3002","title":"Heartbeat"},{"location":"Distributed-computing/Technique/Heartbeat/#_1","text":"","title":"\u7d20\u6750"},{"location":"Distributed-computing/Technique/Heartbeat/#heartbeat#computing","text":"https://en.wanweibaike.com/wiki-Heartbeat%20protocol In computer science , a heartbeat is a periodic signal generated by hardware or software to indicate normal operation or to synchronize other parts of a computer system .[ 1] Usually a heartbeat is sent between machines at a regular interval in the order of seconds. If the endpoint does not receive a heartbeat for a time\u2014usually a few heartbeat intervals\u2014the machine that should have sent the heartbeat is assumed to have failed.[ 2]","title":"Heartbeat (computing)"},{"location":"Distributed-computing/Technique/Heartbeat/#heartbeat#message","text":"https://en.wanweibaike.com/wiki-Heartbeat%20message A heartbeat message in signal processing is a message sent from an originator to a destination that enables the destination to identify if and when the originator fails or is no longer available.[ 1] Heartbeat messages are typically sent non-stop on a periodic or recurring basis from the originator's start-up until the originator's shutdown. When the destination identifies a lack of heartbeat messages during an anticipated arrival period, the destination may determine that the originator has failed, shutdown, or is generally no longer available. Heartbeat messages may be used for high-availability and fault tolerance purposes.","title":"Heartbeat message"},{"location":"Distributed-computing/Technique/Heartbeat/#heartbeat#network","text":"https://en.wanweibaike.com/wiki-Heartbeat%20network In computer clusters , a heartbeat network is a private network which is shared only by the nodes in the cluster, and is not accessible from outside the cluster. It is used by cluster nodes in order to monitor each node's status and communicate with each other messages necessary for maintaining operation of the cluster.","title":"Heartbeat network"},{"location":"Distributed-computing/Technique/Heartbeat/#case#study","text":"","title":"Case study"},{"location":"Distributed-computing/Technique/Heartbeat/#case#linux-ha","text":"https://en.wanweibaike.com/wiki-Heartbeat%20(program )","title":"Case: Linux-HA"},{"location":"Distributed-computing/Technique/Heartbeat/#case#zookeeper","text":"\u5728 ZooKeeper Getting Started Guide \u4e2d\u6709\u8fd9\u6837\u7684\u4ecb\u7ecd: tickTime : the basic time unit in milliseconds used by ZooKeeper. It is used to do heartbeats and the minimum session timeout will be twice the tickTime .","title":"Case: zookeeper"},{"location":"Distributed-computing/Technique/Rendezvous/","text":"Rendezvous \u5728 ZooKeeper Programmer's Guide \u4e2d\u63d0\u53ca\u4e86rendezvous \uff0c\u56de\u60f3\u8d77\u4e4b\u524d\u9047\u5230\u8fc7\u7684rendezvous hash\uff0c\u89c9\u5f97\u6709\u5fc5\u8981\u5bf9rendezvous\u8fdb\u884c\u603b\u7ed3\u3002 wikipedia Rendezvous protocol A rendezvous protocol is a computer network protocol that enables resources or P2P network peers to find each other. wikipedia Synchronous rendezvous Barrier (computer science) wikipedia Rendezvous hashing Rendezvous problem infogalactic Rendezvous problem The rendezvous dilemma can be formulated in this way: If they both choose to wait, of course, they will never meet. If they both choose to walk there are chances that they meet and chances that they do not. If one chooses to wait and the other chooses to walk, then there is a theoretical certainty that they will meet eventually; in practice, though, it may take too long for it to be guaranteed. The question posed, then, is: what strategies should they choose to maximize their probability of meeting? Examples of this class of problems are known as rendezvous problems . These problems were first introduced informally by Steve Alpern in 1976,[ 1] and he formalised the continuous version of the problem in 1995.[ 2] This has led to much recent research in rendezvous search.[ 3] Even the symmetric rendezvous problem played in n discrete locations (sometimes called the Mozart Cafe Rendezvous Problem )[ 4] has turned out to be very difficult to solve, and in 1990 Richard Weber and Eddie Anderson conjectured the optimal strategy.[ 5] Only recently has the conjecture been proved for n = 3 by Richard Weber .[ 6] This was the first non-trivial symmetric rendezvous search problem to be fully solved. As well as being problems of theoretical interest, rendezvous problems include real-world problems with applications in the fields of synchronization , operating system design, operations research , and even search and rescue operations planning.","title":"Introduction"},{"location":"Distributed-computing/Technique/Rendezvous/#rendezvous","text":"\u5728 ZooKeeper Programmer's Guide \u4e2d\u63d0\u53ca\u4e86rendezvous \uff0c\u56de\u60f3\u8d77\u4e4b\u524d\u9047\u5230\u8fc7\u7684rendezvous hash\uff0c\u89c9\u5f97\u6709\u5fc5\u8981\u5bf9rendezvous\u8fdb\u884c\u603b\u7ed3\u3002","title":"Rendezvous"},{"location":"Distributed-computing/Technique/Rendezvous/#wikipedia#rendezvous#protocol","text":"A rendezvous protocol is a computer network protocol that enables resources or P2P network peers to find each other.","title":"wikipedia Rendezvous protocol"},{"location":"Distributed-computing/Technique/Rendezvous/#wikipedia#synchronous#rendezvous","text":"Barrier (computer science)","title":"wikipedia Synchronous rendezvous"},{"location":"Distributed-computing/Technique/Rendezvous/#wikipedia#rendezvous#hashing","text":"","title":"wikipedia Rendezvous hashing"},{"location":"Distributed-computing/Technique/Rendezvous/#rendezvous#problem","text":"","title":"Rendezvous problem"},{"location":"Distributed-computing/Technique/Rendezvous/#infogalactic#rendezvous#problem","text":"The rendezvous dilemma can be formulated in this way: If they both choose to wait, of course, they will never meet. If they both choose to walk there are chances that they meet and chances that they do not. If one chooses to wait and the other chooses to walk, then there is a theoretical certainty that they will meet eventually; in practice, though, it may take too long for it to be guaranteed. The question posed, then, is: what strategies should they choose to maximize their probability of meeting? Examples of this class of problems are known as rendezvous problems . These problems were first introduced informally by Steve Alpern in 1976,[ 1] and he formalised the continuous version of the problem in 1995.[ 2] This has led to much recent research in rendezvous search.[ 3] Even the symmetric rendezvous problem played in n discrete locations (sometimes called the Mozart Cafe Rendezvous Problem )[ 4] has turned out to be very difficult to solve, and in 1990 Richard Weber and Eddie Anderson conjectured the optimal strategy.[ 5] Only recently has the conjecture been proved for n = 3 by Richard Weber .[ 6] This was the first non-trivial symmetric rendezvous search problem to be fully solved. As well as being problems of theoretical interest, rendezvous problems include real-world problems with applications in the fields of synchronization , operating system design, operations research , and even search and rescue operations planning.","title":"infogalactic Rendezvous problem"},{"location":"Distributed-computing/Theory/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u8ba8\u8bbadistributed computing\u7684theory\uff0c\u5185\u5bb9\u6bd4\u8f83\u5e9e\u6742\u3002","title":"Introduction"},{"location":"Distributed-computing/Theory/#_1","text":"\u672c\u7ae0\u8ba8\u8bbadistributed computing\u7684theory\uff0c\u5185\u5bb9\u6bd4\u8f83\u5e9e\u6742\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Distributed-computing/Theory/TODO-medium-How%20Does%20Distributed%20Consensus%20Work/","text":"medium How Does Distributed Consensus Work","title":"medium [How Does Distributed Consensus Work](https://medium.com/s/story/lets-take-a-crack-at-understanding-distributed-consensus-dad23d0dc95)"},{"location":"Distributed-computing/Theory/TODO-medium-How%20Does%20Distributed%20Consensus%20Work/#medium#how#does#distributed#consensus#work","text":"","title":"medium How Does Distributed Consensus Work"},{"location":"Distributed-computing/Theory/TODO/","text":"stackoverflow When do I use a consensus algorithm like Paxos vs using a something like a Vector Clock?","title":"stackoverflow [When do I use a consensus algorithm like Paxos vs using a something like a Vector Clock?](https://stackoverflow.com/questions/43554164/when-do-i-use-a-consensus-algorithm-like-paxos-vs-using-a-something-like-a-vecto)"},{"location":"Distributed-computing/Theory/TODO/#stackoverflow#when#do#i#use#a#consensus#algorithm#like#paxos#vs#using#a#something#like#a#vector#clock","text":"","title":"stackoverflow When do I use a consensus algorithm like Paxos vs using a something like a Vector Clock?"},{"location":"Distributed-computing/Theory/CAP/","text":"CAP wikipedia CAP theorem In theoretical computer science , the CAP theorem , also named Brewer's theorem after computer scientist Eric Brewer , states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees:[ 1] [ 2] [ 3] 1\u3001 Consistency : Every read receives the most recent write or an error NOTE: \u5373\u4e0d\u4f1a\u4e22\u5931write\uff0c\u5982\u679c\u4e22\u5931\u4e86write\uff0c\u5219\u53ef\u80fd\u5bfc\u81f4inconsistency\uff0c\u5176\u5b9e\u8fd9\u4e2aconsistency\u7684\u5b9a\u4e49\u975e\u5e38\u7c7b\u4f3c\u4e8e 6.3.1. Strict Consistency 2\u3001 Availability : Every request receives a (non-error) response \u2013 without the guarantee that it contains the most recent write 3\u3001 Partition tolerance : The system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network between nodes When a network partition failure happens should we decide to 1\u3001Cancel the operation and thus decrease the availability but ensure consistency. NOTE: CP 2\u3001Proceed with the operation and thus provide availability but risk inconsistency. NOTE: AP In particular, the CAP theorem implies that in the presence of a network partition , one has to choose between consistency and availability . Note that consistency as defined in the CAP theorem is quite different from the consistency guaranteed in ACID database transactions [ 4] . Explanation No distributed system is safe from network failures, thus network partitioning generally has to be tolerated\uff08\u5bb9\u9519\u7684\uff09. In the presence of a partition\uff08\u5206\u88c2\uff09, one is then left with two options: consistency or availability . 1\u3001When choosing consistency over availability, the system will return an error or a time-out if particular information cannot be guaranteed to be up to date due to network partitioning. 2\u3001When choosing availability over consistency, the system will always process the query and try to return the most recent available version of the information, even if it cannot guarantee it is up to date due to network partitioning . In the absence of network failure \u2013 that is, when the distributed system is running normally \u2013 both availability and consistency can be satisfied. CAP is frequently misunderstood as if one has to choose to abandon one of the three guarantees at all times. In fact, the choice is really between consistency and availability only when a network partition or failure happens; at all other times, no trade-off has to be made.[ 5] [ 6] Database systems designed with traditional ACID guarantees in mind such as RDBMS choose consistency over availability , whereas systems designed around the BASE philosophy, common in the NoSQL movement for example, choose availability over consistency.[ 7] NOTE: \u4e00\u3001\u5173\u4e8eCAP\u3001ACID\u3001BASE\uff0c\u53c2\u89c1\u5982\u4e0b\u6587\u7ae0: 1\u3001csdn \u7528\u592a\u6781\u62f3\u8bb2\u5206\u5e03\u5f0f\u7406\u8bba\uff0c\u771f\u8212\u670d\uff01 The PACELC theorem builds on CAP by stating that even in the absence of partitioning, another trade-off between latency and consistency occurs. See also PACELC theorem Consistency model Fallacies of distributed computing Paxos (computer science) Project management triangle Raft (computer science) Trilemma stackoverflow CAP theorem - Availability and Partition Tolerance While I try to understand the \"Availability\" (A) and \"Partition tolerance\" (P) in CAP, I found it difficult to understand the explanations from various articles. I get a feeling that A and P can go together (I know this is not the case, and that's why I fail to understand!). Explaining in simple terms, what are A and P and the difference between them? COMMENTS : here is an article which explains CAP in plain english ksat.me/a-plain-english-introduction-to-cap-theorem \u2013 Tushar Saha Jun 25 at 6:14 A Consistency means that data is the same across the cluster, so you can read or write from/to any node and get the same data. Availability means the ability to access the cluster even if a node in the cluster goes down. Partition tolerance means that the cluster continues to function even if there is a \"partition\" (communication break) between two nodes (both nodes are up, but can't communicate). In order to get both availability and partition tolerance, you have to give up consistency. Consider if you have two nodes, X and Y, in a master-master setup. Now, there is a break between network communication between X and Y, so they can't sync updates. At this point you can either: A) Allow the nodes to get out of sync (giving up consistency), or B) Consider the cluster to be \"down\" (giving up availability) All the combinations available are: 1\u3001 CA - data is consistent between all nodes - as long as all nodes are online - and you can read/write from any node and be sure that the data is the same, but if you ever develop a partition between nodes, the data will be out of sync (and won't re-sync once the partition is resolved). NOTE: \u5b9e\u9645\u4e2d\uff0c\u5e76\u4e0d\u5b58\u5728 2\u3001 CP - data is consistent between all nodes, and maintains partition tolerance (preventing data desync) by becoming unavailable when a node goes down. 3\u3001 AP - nodes remain online even if they can't communicate with each other and will resync data once the partition is resolved, but you aren't guaranteed that all nodes will have the same data (either during or after the partition) You should note that CA systems don't practically exist (even if some systems claim to be so). CAP\u7684\u76f4\u89c2\u7406\u89e3 \"\u9c7c\u548c\u718a\u638c\u4e0d\u53ef\u517c\u5f97\": 1) \u4e4b\u524d\u5728\u5f00\u53d1\u4e00\u4e2aapplication\u7684\u65f6\u5019\uff0c\u6d89\u53ca\u5230\u5728performance\u548creliability\u4e2d\u8fdb\u884c\u6289\u62e9: \u5982\u679c\u5b9e\u65f6(\u540c\u6b65)\u5730\u5c06\u8bb0\u5f55\u5199\u5165\u5230\u6587\u4ef6\uff0c\u5219performance\u8f83\u5dee\uff0creliability\u8f83\u597d\uff0c\u8fd9\u5c31\u662f\u727a\u7272performance\u6765\u83b7\u53d6reliability\u3002 \u5982\u679c\u975e\u5b9e\u65f6(\u5f02\u6b65)\u5730\u5c06\u8bb0\u5f55\u5199\u5165\u5230\u6587\u4ef6\uff0c\u5219performance\u8f83\u597d\uff0creliability\u8f83\u5dee\uff0c\u8fd9\u5c31\u662f\u727a\u7272reliability\u6765\u83b7\u53d6performance\u3002","title":"Introduction"},{"location":"Distributed-computing/Theory/CAP/#cap","text":"","title":"CAP"},{"location":"Distributed-computing/Theory/CAP/#wikipedia#cap#theorem","text":"In theoretical computer science , the CAP theorem , also named Brewer's theorem after computer scientist Eric Brewer , states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees:[ 1] [ 2] [ 3] 1\u3001 Consistency : Every read receives the most recent write or an error NOTE: \u5373\u4e0d\u4f1a\u4e22\u5931write\uff0c\u5982\u679c\u4e22\u5931\u4e86write\uff0c\u5219\u53ef\u80fd\u5bfc\u81f4inconsistency\uff0c\u5176\u5b9e\u8fd9\u4e2aconsistency\u7684\u5b9a\u4e49\u975e\u5e38\u7c7b\u4f3c\u4e8e 6.3.1. Strict Consistency 2\u3001 Availability : Every request receives a (non-error) response \u2013 without the guarantee that it contains the most recent write 3\u3001 Partition tolerance : The system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network between nodes When a network partition failure happens should we decide to 1\u3001Cancel the operation and thus decrease the availability but ensure consistency. NOTE: CP 2\u3001Proceed with the operation and thus provide availability but risk inconsistency. NOTE: AP In particular, the CAP theorem implies that in the presence of a network partition , one has to choose between consistency and availability . Note that consistency as defined in the CAP theorem is quite different from the consistency guaranteed in ACID database transactions [ 4] .","title":"wikipedia CAP theorem"},{"location":"Distributed-computing/Theory/CAP/#explanation","text":"No distributed system is safe from network failures, thus network partitioning generally has to be tolerated\uff08\u5bb9\u9519\u7684\uff09. In the presence of a partition\uff08\u5206\u88c2\uff09, one is then left with two options: consistency or availability . 1\u3001When choosing consistency over availability, the system will return an error or a time-out if particular information cannot be guaranteed to be up to date due to network partitioning. 2\u3001When choosing availability over consistency, the system will always process the query and try to return the most recent available version of the information, even if it cannot guarantee it is up to date due to network partitioning . In the absence of network failure \u2013 that is, when the distributed system is running normally \u2013 both availability and consistency can be satisfied. CAP is frequently misunderstood as if one has to choose to abandon one of the three guarantees at all times. In fact, the choice is really between consistency and availability only when a network partition or failure happens; at all other times, no trade-off has to be made.[ 5] [ 6] Database systems designed with traditional ACID guarantees in mind such as RDBMS choose consistency over availability , whereas systems designed around the BASE philosophy, common in the NoSQL movement for example, choose availability over consistency.[ 7] NOTE: \u4e00\u3001\u5173\u4e8eCAP\u3001ACID\u3001BASE\uff0c\u53c2\u89c1\u5982\u4e0b\u6587\u7ae0: 1\u3001csdn \u7528\u592a\u6781\u62f3\u8bb2\u5206\u5e03\u5f0f\u7406\u8bba\uff0c\u771f\u8212\u670d\uff01 The PACELC theorem builds on CAP by stating that even in the absence of partitioning, another trade-off between latency and consistency occurs.","title":"Explanation"},{"location":"Distributed-computing/Theory/CAP/#see#also","text":"PACELC theorem Consistency model Fallacies of distributed computing Paxos (computer science) Project management triangle Raft (computer science) Trilemma","title":"See also"},{"location":"Distributed-computing/Theory/CAP/#stackoverflow#cap#theorem#-#availability#and#partition#tolerance","text":"While I try to understand the \"Availability\" (A) and \"Partition tolerance\" (P) in CAP, I found it difficult to understand the explanations from various articles. I get a feeling that A and P can go together (I know this is not the case, and that's why I fail to understand!). Explaining in simple terms, what are A and P and the difference between them? COMMENTS : here is an article which explains CAP in plain english ksat.me/a-plain-english-introduction-to-cap-theorem \u2013 Tushar Saha Jun 25 at 6:14","title":"stackoverflow CAP theorem - Availability and Partition Tolerance"},{"location":"Distributed-computing/Theory/CAP/#a","text":"Consistency means that data is the same across the cluster, so you can read or write from/to any node and get the same data. Availability means the ability to access the cluster even if a node in the cluster goes down. Partition tolerance means that the cluster continues to function even if there is a \"partition\" (communication break) between two nodes (both nodes are up, but can't communicate). In order to get both availability and partition tolerance, you have to give up consistency. Consider if you have two nodes, X and Y, in a master-master setup. Now, there is a break between network communication between X and Y, so they can't sync updates. At this point you can either: A) Allow the nodes to get out of sync (giving up consistency), or B) Consider the cluster to be \"down\" (giving up availability) All the combinations available are: 1\u3001 CA - data is consistent between all nodes - as long as all nodes are online - and you can read/write from any node and be sure that the data is the same, but if you ever develop a partition between nodes, the data will be out of sync (and won't re-sync once the partition is resolved). NOTE: \u5b9e\u9645\u4e2d\uff0c\u5e76\u4e0d\u5b58\u5728 2\u3001 CP - data is consistent between all nodes, and maintains partition tolerance (preventing data desync) by becoming unavailable when a node goes down. 3\u3001 AP - nodes remain online even if they can't communicate with each other and will resync data once the partition is resolved, but you aren't guaranteed that all nodes will have the same data (either during or after the partition) You should note that CA systems don't practically exist (even if some systems claim to be so).","title":"A"},{"location":"Distributed-computing/Theory/CAP/#cap_1","text":"\"\u9c7c\u548c\u718a\u638c\u4e0d\u53ef\u517c\u5f97\": 1) \u4e4b\u524d\u5728\u5f00\u53d1\u4e00\u4e2aapplication\u7684\u65f6\u5019\uff0c\u6d89\u53ca\u5230\u5728performance\u548creliability\u4e2d\u8fdb\u884c\u6289\u62e9: \u5982\u679c\u5b9e\u65f6(\u540c\u6b65)\u5730\u5c06\u8bb0\u5f55\u5199\u5165\u5230\u6587\u4ef6\uff0c\u5219performance\u8f83\u5dee\uff0creliability\u8f83\u597d\uff0c\u8fd9\u5c31\u662f\u727a\u7272performance\u6765\u83b7\u53d6reliability\u3002 \u5982\u679c\u975e\u5b9e\u65f6(\u5f02\u6b65)\u5730\u5c06\u8bb0\u5f55\u5199\u5165\u5230\u6587\u4ef6\uff0c\u5219performance\u8f83\u597d\uff0creliability\u8f83\u5dee\uff0c\u8fd9\u5c31\u662f\u727a\u7272reliability\u6765\u83b7\u53d6performance\u3002","title":"CAP\u7684\u76f4\u89c2\u7406\u89e3"},{"location":"Distributed-computing/Theory/Consensus/","text":"Consensus \"consensus \"\u5373\"\u5171\u8bc6\"\uff0c\u5b83\u662fdistributed computing\u4e2d\u7684\u5178\u578b\u95ee\u9898\u3002 \u76ee\u524d\u91c7\u7528\u7684\u57fa\u672c\u601d\u60f3\u662f(\u5728Designing Data-Intensive Applications ): \u4e8b\u5b9e\u662f\u7531\u591a\u6570\u51b3\u5b9a\u7684\uff0c\u4f7f\u7528\u6295\u7968\u7684\u65b9\u5f0f \u8fd9\u4e2a\u601d\u60f3\uff0c\u5728Book Designing Data-Intensive Applications \u7684 CHAPTER 8 The Trouble with Distributed Systems # Knowledge, Truth, and Lies \u7ae0\u8282\u8fdb\u884c\u4e86\u4ecb\u7ecd\u3002 \u5728 \"wikipedia Consensus (computer science) # Problem description\" \u4e2d\uff0c\u4e5f\u4ecb\u7ecd\u4e86\u8fd9\u79cd\u601d\u60f3: One approach to generating consensus is for all processes (agents) to agree on a majority value . In this context, a majority requires at least one more than half of available votes (where each process is given a vote). wikipedia Consensus (computer science) NOTE\uff1a Consensus\u7684\u4e2d\u6587\u610f\u601d\u662f\u5171\u8bc6 A fundamental problem in distributed computing and multi-agent systems is to achieve overall system reliability in the presence of a number of faulty processes. This often requires processes to agree on some data value that is needed during computation. Examples of applications of consensus 1\u3001include whether to commit a transaction to a database, 2\u3001agreeing on the identity of a leader , 3\u3001 state machine replication , and 4\u3001 atomic broadcasts . The real world applications include clock synchronization , PageRank , opinion formation(\u610f\u89c1\u5f62\u6210), smart power grids , state estimation , control of UAVs (and multiple robots/agents in general), load balancing , blockchain and others. NOTE : \u5728redis\u4e2d\uff0cConsensus \u6240\u6307\u5c31\u662fcluster\u4e2d\u7684\u6240\u6709\u7684node\u5bf9cluster state\uff08\u54ea\u4e9b\u8282\u70b9\u662fmaster\uff0c\u54ea\u4e9b\u8282\u70b9\u662fslave\uff09\u7684agreement\uff1b\u663e\u7136\uff0c\u8fbe\u6210agreement\u7684\u6700\u7b80\u5355\u7684\u65b9\u5f0f\u5c31\u662fvote\uff1b Problem description The consensus problem requires agreement among a number of processes (or agents) for a single data value. Some of the processes (agents) may fail or be unreliable in other ways, so consensus protocols must be fault tolerant or resilient\uff08\u5f39\u6027\u7684\uff0c\u53ef\u5feb\u901f\u6062\u590d\u7684\uff09. The processes must somehow put forth their candidate values, communicate with one another, and agree on a single consensus value\uff08\u53d1\u8d77\u4e00\u8f6e\u6295\u7968\uff09. The consensus problem is a fundamental problem in control of multi-agent systems. One approach to generating consensus is for all processes (agents) to agree on a majority value . In this context, a majority requires at least one more than half of available votes (where each process is given a vote). However, one or more faulty processes may skew\uff08\u626d\u66f2\uff0c\u6b6a\u659c\uff09 the resultant outcome such that consensus may not be reached or reached incorrectly. NOTE: \u7136\u800c,\u4e00\u4e2a\u6216\u591a\u4e2a\u6709\u7f3a\u9677\u7684\u8fc7\u7a0b\u53ef\u80fd\u4f1a\u626d\u66f2\u7ed3\u679c,\u5373\u8fbe\u6210\u5171\u8bc6\u7684\u7ed3\u679c\u53ef\u80fd\u65e0\u6cd5\u8fbe\u5230\u6216\u8fbe\u5230\u9519\u8bef\u3002 Protocols that solve consensus problems are designed to deal with limited numbers of faulty processes . These protocols must satisfy a number of requirements to be useful. For instance, a trivial protocol could have all processes output binary value 1. This is not useful and thus the requirement is modified such that the output must somehow depend on the input. That is, the output value of a consensus protocol must be the input value of some process. Another requirement is that a process may decide upon and output a value only once and this decision is irrevocable. A process is called correct in an execution if it does not experience a failure. A consensus protocol tolerating halting failures must satisfy the following properties. TODO jianshu Quorum\u4ecb\u7ecd zhihu \u5982\u4f55\u8bc4\u4ef7Heidi Howard\u7684\u535a\u58eb\u8bba\u6587Distributed consensus revised\uff1f","title":"Introduction"},{"location":"Distributed-computing/Theory/Consensus/#consensus","text":"\"consensus \"\u5373\"\u5171\u8bc6\"\uff0c\u5b83\u662fdistributed computing\u4e2d\u7684\u5178\u578b\u95ee\u9898\u3002 \u76ee\u524d\u91c7\u7528\u7684\u57fa\u672c\u601d\u60f3\u662f(\u5728Designing Data-Intensive Applications ): \u4e8b\u5b9e\u662f\u7531\u591a\u6570\u51b3\u5b9a\u7684\uff0c\u4f7f\u7528\u6295\u7968\u7684\u65b9\u5f0f \u8fd9\u4e2a\u601d\u60f3\uff0c\u5728Book Designing Data-Intensive Applications \u7684 CHAPTER 8 The Trouble with Distributed Systems # Knowledge, Truth, and Lies \u7ae0\u8282\u8fdb\u884c\u4e86\u4ecb\u7ecd\u3002 \u5728 \"wikipedia Consensus (computer science) # Problem description\" \u4e2d\uff0c\u4e5f\u4ecb\u7ecd\u4e86\u8fd9\u79cd\u601d\u60f3: One approach to generating consensus is for all processes (agents) to agree on a majority value . In this context, a majority requires at least one more than half of available votes (where each process is given a vote).","title":"Consensus"},{"location":"Distributed-computing/Theory/Consensus/#wikipedia#consensus#computer#science","text":"NOTE\uff1a Consensus\u7684\u4e2d\u6587\u610f\u601d\u662f\u5171\u8bc6 A fundamental problem in distributed computing and multi-agent systems is to achieve overall system reliability in the presence of a number of faulty processes. This often requires processes to agree on some data value that is needed during computation. Examples of applications of consensus 1\u3001include whether to commit a transaction to a database, 2\u3001agreeing on the identity of a leader , 3\u3001 state machine replication , and 4\u3001 atomic broadcasts . The real world applications include clock synchronization , PageRank , opinion formation(\u610f\u89c1\u5f62\u6210), smart power grids , state estimation , control of UAVs (and multiple robots/agents in general), load balancing , blockchain and others. NOTE : \u5728redis\u4e2d\uff0cConsensus \u6240\u6307\u5c31\u662fcluster\u4e2d\u7684\u6240\u6709\u7684node\u5bf9cluster state\uff08\u54ea\u4e9b\u8282\u70b9\u662fmaster\uff0c\u54ea\u4e9b\u8282\u70b9\u662fslave\uff09\u7684agreement\uff1b\u663e\u7136\uff0c\u8fbe\u6210agreement\u7684\u6700\u7b80\u5355\u7684\u65b9\u5f0f\u5c31\u662fvote\uff1b","title":"wikipedia Consensus (computer science)"},{"location":"Distributed-computing/Theory/Consensus/#problem#description","text":"The consensus problem requires agreement among a number of processes (or agents) for a single data value. Some of the processes (agents) may fail or be unreliable in other ways, so consensus protocols must be fault tolerant or resilient\uff08\u5f39\u6027\u7684\uff0c\u53ef\u5feb\u901f\u6062\u590d\u7684\uff09. The processes must somehow put forth their candidate values, communicate with one another, and agree on a single consensus value\uff08\u53d1\u8d77\u4e00\u8f6e\u6295\u7968\uff09. The consensus problem is a fundamental problem in control of multi-agent systems. One approach to generating consensus is for all processes (agents) to agree on a majority value . In this context, a majority requires at least one more than half of available votes (where each process is given a vote). However, one or more faulty processes may skew\uff08\u626d\u66f2\uff0c\u6b6a\u659c\uff09 the resultant outcome such that consensus may not be reached or reached incorrectly. NOTE: \u7136\u800c,\u4e00\u4e2a\u6216\u591a\u4e2a\u6709\u7f3a\u9677\u7684\u8fc7\u7a0b\u53ef\u80fd\u4f1a\u626d\u66f2\u7ed3\u679c,\u5373\u8fbe\u6210\u5171\u8bc6\u7684\u7ed3\u679c\u53ef\u80fd\u65e0\u6cd5\u8fbe\u5230\u6216\u8fbe\u5230\u9519\u8bef\u3002 Protocols that solve consensus problems are designed to deal with limited numbers of faulty processes . These protocols must satisfy a number of requirements to be useful. For instance, a trivial protocol could have all processes output binary value 1. This is not useful and thus the requirement is modified such that the output must somehow depend on the input. That is, the output value of a consensus protocol must be the input value of some process. Another requirement is that a process may decide upon and output a value only once and this decision is irrevocable. A process is called correct in an execution if it does not experience a failure. A consensus protocol tolerating halting failures must satisfy the following properties.","title":"Problem description"},{"location":"Distributed-computing/Theory/Consensus/#todo","text":"jianshu Quorum\u4ecb\u7ecd zhihu \u5982\u4f55\u8bc4\u4ef7Heidi Howard\u7684\u535a\u58eb\u8bba\u6587Distributed consensus revised\uff1f","title":"TODO"},{"location":"Distributed-computing/Theory/Consensus/VS-consensus-VS-consistency/","text":"","title":"VS-consensus-VS-consistency"},{"location":"Distributed-computing/Theory/Consensus/Leader-election/","text":"Leader election wikipedia Leader election In distributed computing , leader election is the process of designating a single process as the organizer of some task distributed among several computers (nodes). Before the task is begun, all network nodes are either unaware which node will serve as the \"leader\" (or coordinator ) of the task, or unable to communicate with the current coordinator. After a leader election algorithm has been run, however, each node throughout the network recognizes a particular, unique node as the task leader. The network nodes communicate among themselves in order to decide which of them will get into the \"leader\" state. For that, they need some method in order to break the symmetry among them. For example, if each node has unique and comparable identities, then the nodes can compare their identities, and decide that the node with the highest identity is the leader. The definition of this problem is often attributed to LeLann, who formalized it as a method to create a new token in a token ring network in which the token has been lost. Leader election algorithms are designed to be economical in terms of total bytes transmitted, and time. The algorithm suggested by Gallager, Humblet, and Spira[ 1] for general undirected graphs has had a strong impact on the design of distributed algorithms in general, and won the Dijkstra Prize for an influential paper in distributed computing. Many other algorithms have been suggested for different kinds of network graphs , such as undirected rings, unidirectional rings, complete graphs, grids, directed Euler graphs, and others. A general method that decouples the issue of the graph family from the design of the leader election algorithm was suggested by Korach, Kutten , and Moran .[ 2]","title":"Introduction"},{"location":"Distributed-computing/Theory/Consensus/Leader-election/#leader#election","text":"","title":"Leader election"},{"location":"Distributed-computing/Theory/Consensus/Leader-election/#wikipedia#leader#election","text":"In distributed computing , leader election is the process of designating a single process as the organizer of some task distributed among several computers (nodes). Before the task is begun, all network nodes are either unaware which node will serve as the \"leader\" (or coordinator ) of the task, or unable to communicate with the current coordinator. After a leader election algorithm has been run, however, each node throughout the network recognizes a particular, unique node as the task leader. The network nodes communicate among themselves in order to decide which of them will get into the \"leader\" state. For that, they need some method in order to break the symmetry among them. For example, if each node has unique and comparable identities, then the nodes can compare their identities, and decide that the node with the highest identity is the leader. The definition of this problem is often attributed to LeLann, who formalized it as a method to create a new token in a token ring network in which the token has been lost. Leader election algorithms are designed to be economical in terms of total bytes transmitted, and time. The algorithm suggested by Gallager, Humblet, and Spira[ 1] for general undirected graphs has had a strong impact on the design of distributed algorithms in general, and won the Dijkstra Prize for an influential paper in distributed computing. Many other algorithms have been suggested for different kinds of network graphs , such as undirected rings, unidirectional rings, complete graphs, grids, directed Euler graphs, and others. A general method that decouples the issue of the graph family from the design of the leader election algorithm was suggested by Korach, Kutten , and Moran .[ 2]","title":"wikipedia Leader election"},{"location":"Distributed-computing/Theory/Consensus/Quorum/","text":"Quorum \u4e00\u3001\"quorum\"\u7684\u610f\u601d\u662f\"\u6cd5\u5b9a\u4eba\u6570\"\uff0c\u4e5f\u5c31\u662f\u8bf4\u591a\u5c11\u4eba\u540c\u610f\u4e86\u624d\u4f5c\u6570\uff1b\u4e00\u822c\u6211\u4eec\u4f1a\u5c06\u5b83\u7684\u503c\u8bbe\u7f6e\u4e3amajority\uff0c\u90a3\u4e48\u6b64\u65f6\u7684\u542b\u4e49\u5c31\u662f: \"\u5927\u591a\u6570\u4eba\u540c\u610f\u624d\u7b97\u6570\"\u3001\"\u5c11\u6570\u670d\u4ece\u591a\u6570\"\u3002 \u4e8c\u3001\u5b83\u5728distributed computing\u4e2d\u6709\u7740\u975e\u5e38\u5e7f\u6cdb\u7684\u4f7f\u7528 Multiple model \u4e00\u3001\u901a\u8fc7**\u6295\u7968**\u6765\u8fbe\u6210\u5171\u8bc6\u3002 \u4e8c\u3001\u65e2\u7136\u6709multiple entity\uff0c\u90a3\u4e48\u7531\u5b83\u4eec\u6765\u8fdb\u884cvote\u4ece\u800c\u8fdb\u884c\u51b3\u7b56\u662f\u81ea\u7136\u800c\u7136\u7684\u60f3\u6cd5\u4e86\u3002 \u5c11\u6570\u670d\u4ece\u591a\u6570 wikipedia Quorum (distributed computing) A quorum is the minimum number of votes that a distributed transaction has to obtain in order to be allowed to perform an operation in a distributed system . A quorum -based technique is implemented to enforce consistent operation in a distributed system. Quorum-based techniques in distributed database systems Quorum-based voting can be used as a replica control method,[ 1] as well as a commit method to ensure transaction atomicity in the presence of network partitioning .[ 1] Quorum-based voting in commit protocols In a distributed database system, a transaction could be executing its operations at multiple sites. Since atomicity requires every distributed transaction to be atomic , the transaction must have the same fate ( commit or abort ) at every site. In case of network partitioning, sites are partitioned and the partitions may not be able to communicate with each other. This is where a quorum-based technique comes in. The fundamental idea is that a transaction is executed if the majority of sites vote to execute it . Every site in the system is assigned a vote V_i V_i . Let us assume that the total number of votes in the system is V V and the abort and commit quorums are V_a V_a and V_c V_c , respectively. Then the following rules must be obeyed in the implementation of the commit protocol: V_a + V_c > V V_a + V_c > V , where 0 < V_c, V_a 0 < V_c, V_a $ \\leq $ V V . Before a transaction commits, it must obtain a commit quorum V_c V_c . The total of at least one site that is prepared to commit and zero or more sites waiting $ \\geq $ V_c V_c .[ 2] Before a transaction aborts, it must obtain an abort quorum V_a V_a The total of zero or more sites that are prepared to abort or any sites waiting $ \\geq $ Va. The first rule ensures that a transaction cannot be committed and aborted at the same time. The next two rules indicate the votes that a transaction has to obtain before it can terminate one way or the other. Quorum-based voting for replica control In replicated databases, a data object has copies present at several sites. To ensure serializability , no two transactions should be allowed to read or write a data item concurrently. In case of replicated databases, a quorum-based replica control protocol can be used to ensure that no two copies of a data item are read or written by two transactions concurrently. The quorum-based voting for replica control is due to [Gifford, 1979].[ 3] Each copy of a replicated data item is assigned a vote. Each operation then has to obtain a read quorum (Vr) or a write quorum (Vw) to read or write a data item, respectively. If a given data item has a total of V votes, the quorums have to obey the following rules: Vr + Vw > V Vw > V/2 The first rule ensures that a data item is not read and written by two transactions concurrently. Additionally, it ensures that a read quorum contains at least one site with the newest version of the data item. The second rule ensures that two write operations from two transactions cannot occur concurrently on the same data item. The two rules ensure that one-copy serializability is maintained. See also CAP theorem Database transaction Replication (computer science) Atomicity (database systems) Case study Case: ZooKeeper \u5728 ZooKeeper Getting Started Guide \u4e2d\u7684\"Running Replicated ZooKeeper\"\u7ae0\u8282\u8fdb\u884c\u4e86\u4ecb\u7ecd: But in production, you should run ZooKeeper in replicated mode. A replicated group of servers in the same application is called a quorum , and in replicated mode, all servers in the quorum have copies of the same configuration file.","title":"Introduction"},{"location":"Distributed-computing/Theory/Consensus/Quorum/#quorum","text":"\u4e00\u3001\"quorum\"\u7684\u610f\u601d\u662f\"\u6cd5\u5b9a\u4eba\u6570\"\uff0c\u4e5f\u5c31\u662f\u8bf4\u591a\u5c11\u4eba\u540c\u610f\u4e86\u624d\u4f5c\u6570\uff1b\u4e00\u822c\u6211\u4eec\u4f1a\u5c06\u5b83\u7684\u503c\u8bbe\u7f6e\u4e3amajority\uff0c\u90a3\u4e48\u6b64\u65f6\u7684\u542b\u4e49\u5c31\u662f: \"\u5927\u591a\u6570\u4eba\u540c\u610f\u624d\u7b97\u6570\"\u3001\"\u5c11\u6570\u670d\u4ece\u591a\u6570\"\u3002 \u4e8c\u3001\u5b83\u5728distributed computing\u4e2d\u6709\u7740\u975e\u5e38\u5e7f\u6cdb\u7684\u4f7f\u7528","title":"Quorum"},{"location":"Distributed-computing/Theory/Consensus/Quorum/#multiple#model","text":"\u4e00\u3001\u901a\u8fc7**\u6295\u7968**\u6765\u8fbe\u6210\u5171\u8bc6\u3002 \u4e8c\u3001\u65e2\u7136\u6709multiple entity\uff0c\u90a3\u4e48\u7531\u5b83\u4eec\u6765\u8fdb\u884cvote\u4ece\u800c\u8fdb\u884c\u51b3\u7b56\u662f\u81ea\u7136\u800c\u7136\u7684\u60f3\u6cd5\u4e86\u3002 \u5c11\u6570\u670d\u4ece\u591a\u6570","title":"Multiple model"},{"location":"Distributed-computing/Theory/Consensus/Quorum/#wikipedia#quorum#distributed#computing","text":"A quorum is the minimum number of votes that a distributed transaction has to obtain in order to be allowed to perform an operation in a distributed system . A quorum -based technique is implemented to enforce consistent operation in a distributed system.","title":"wikipedia Quorum (distributed computing)"},{"location":"Distributed-computing/Theory/Consensus/Quorum/#quorum-based#techniques#in#distributed#database#systems","text":"Quorum-based voting can be used as a replica control method,[ 1] as well as a commit method to ensure transaction atomicity in the presence of network partitioning .[ 1]","title":"Quorum-based techniques in distributed database systems"},{"location":"Distributed-computing/Theory/Consensus/Quorum/#quorum-based#voting#in#commit#protocols","text":"In a distributed database system, a transaction could be executing its operations at multiple sites. Since atomicity requires every distributed transaction to be atomic , the transaction must have the same fate ( commit or abort ) at every site. In case of network partitioning, sites are partitioned and the partitions may not be able to communicate with each other. This is where a quorum-based technique comes in. The fundamental idea is that a transaction is executed if the majority of sites vote to execute it . Every site in the system is assigned a vote V_i V_i . Let us assume that the total number of votes in the system is V V and the abort and commit quorums are V_a V_a and V_c V_c , respectively. Then the following rules must be obeyed in the implementation of the commit protocol: V_a + V_c > V V_a + V_c > V , where 0 < V_c, V_a 0 < V_c, V_a $ \\leq $ V V . Before a transaction commits, it must obtain a commit quorum V_c V_c . The total of at least one site that is prepared to commit and zero or more sites waiting $ \\geq $ V_c V_c .[ 2] Before a transaction aborts, it must obtain an abort quorum V_a V_a The total of zero or more sites that are prepared to abort or any sites waiting $ \\geq $ Va. The first rule ensures that a transaction cannot be committed and aborted at the same time. The next two rules indicate the votes that a transaction has to obtain before it can terminate one way or the other.","title":"Quorum-based voting in commit protocols"},{"location":"Distributed-computing/Theory/Consensus/Quorum/#quorum-based#voting#for#replica#control","text":"In replicated databases, a data object has copies present at several sites. To ensure serializability , no two transactions should be allowed to read or write a data item concurrently. In case of replicated databases, a quorum-based replica control protocol can be used to ensure that no two copies of a data item are read or written by two transactions concurrently. The quorum-based voting for replica control is due to [Gifford, 1979].[ 3] Each copy of a replicated data item is assigned a vote. Each operation then has to obtain a read quorum (Vr) or a write quorum (Vw) to read or write a data item, respectively. If a given data item has a total of V votes, the quorums have to obey the following rules: Vr + Vw > V Vw > V/2 The first rule ensures that a data item is not read and written by two transactions concurrently. Additionally, it ensures that a read quorum contains at least one site with the newest version of the data item. The second rule ensures that two write operations from two transactions cannot occur concurrently on the same data item. The two rules ensure that one-copy serializability is maintained.","title":"Quorum-based voting for replica control"},{"location":"Distributed-computing/Theory/Consensus/Quorum/#see#also","text":"CAP theorem Database transaction Replication (computer science) Atomicity (database systems)","title":"See also"},{"location":"Distributed-computing/Theory/Consensus/Quorum/#case#study","text":"","title":"Case study"},{"location":"Distributed-computing/Theory/Consensus/Quorum/#case#zookeeper","text":"\u5728 ZooKeeper Getting Started Guide \u4e2d\u7684\"Running Replicated ZooKeeper\"\u7ae0\u8282\u8fdb\u884c\u4e86\u4ecb\u7ecd: But in production, you should run ZooKeeper in replicated mode. A replicated group of servers in the same application is called a quorum , and in replicated mode, all servers in the quorum have copies of the same configuration file.","title":"Case: ZooKeeper"},{"location":"Distributed-computing/Theory/Consistency/","text":"Consistency model \u672c\u7ae0\u8ba8\u8bbaconsistency model\u3002 \u542b\u4e49 \"consistency\"\u7684\u542b\u4e49\u662f\"\u4e00\u81f4\"\u3002 multiple\u7684\u5404\u4e2aentity\u4e4b\u95f4\u4fdd\u6301consistency\uff0c\u5728computer science\u4e2d\uff0c\u4e3b\u8981\u662fdata \u4fdd\u6301consistency\u3002 \u4e0b\u9762\u662f\u4e00\u4e9b\u4f8b\u5b50: Memory and database \u5728drdobbs Generic: Change the Way You Write Exception-Safe Code \u2014 Forever \u4e2d\u63cf\u8ff0\u4e86\u8fd9\u6837\u7684\u60c5\u51b5: void User :: AddFriend ( User & newFriend ) { // Add the new friend to the vector of friends // If this throws, the friend is not added to // the vector, nor the database friends_ . push_back ( & newFriend ); // Add the new friend to the database pDB_ -> AddFriend ( GetName (), newFriend . GetName ()); } This definitely causes consistency in the case of vector::push_back failing. Unfortunately, as you consult UserDatabase::AddFriend 's documentation, you discover with annoyance that it can throw an exception, too! Now you might end up with the friend in the vector, but not in the database! \u4ec0\u4e48\u65f6\u5019\u9700\u8981\u8003\u8651consistency\uff1f HA\u7684\u4e00\u79cd\u65b9\u5f0f\u662freplication\uff0c\u8fd9\u5c31\u540c\u65f6\u5b58\u5728\u7740\u591a\u4e2aentity\uff0c\u5982\u679c\u662fdata system\u7684\u8bdd\uff0c\u5c31\u6d89\u53ca\u5982\u4f55\u4fdd\u8bc1\u8fd9\u4e9bentity\u4e4b\u95f4\u7684data\u7684consistency\u3002 TODO csdn Zookeeper\u4e13\u9898\u2014\u20143\u3001\u5206\u5e03\u5f0f\u4e00\u81f4\u6027\uff0c\u51e0\u79cd\u5b9e\u73b0\u7684\u4f18\u7f3a\u70b9 jianshu \u53d6\u4ee3 ZooKeeper\uff01\u9ad8\u5e76\u53d1\u4e0b\u7684\u5206\u5e03\u5f0f\u4e00\u81f4\u6027\u5f00\u6e90\u7ec4\u4ef6 StateSynchronizer mwhittaker Replicated Data Consistency Explained Through Baseball","title":"Introduction"},{"location":"Distributed-computing/Theory/Consistency/#consistency#model","text":"\u672c\u7ae0\u8ba8\u8bbaconsistency model\u3002","title":"Consistency model"},{"location":"Distributed-computing/Theory/Consistency/#_1","text":"\"consistency\"\u7684\u542b\u4e49\u662f\"\u4e00\u81f4\"\u3002 multiple\u7684\u5404\u4e2aentity\u4e4b\u95f4\u4fdd\u6301consistency\uff0c\u5728computer science\u4e2d\uff0c\u4e3b\u8981\u662fdata \u4fdd\u6301consistency\u3002 \u4e0b\u9762\u662f\u4e00\u4e9b\u4f8b\u5b50:","title":"\u542b\u4e49"},{"location":"Distributed-computing/Theory/Consistency/#memory#and#database","text":"\u5728drdobbs Generic: Change the Way You Write Exception-Safe Code \u2014 Forever \u4e2d\u63cf\u8ff0\u4e86\u8fd9\u6837\u7684\u60c5\u51b5: void User :: AddFriend ( User & newFriend ) { // Add the new friend to the vector of friends // If this throws, the friend is not added to // the vector, nor the database friends_ . push_back ( & newFriend ); // Add the new friend to the database pDB_ -> AddFriend ( GetName (), newFriend . GetName ()); } This definitely causes consistency in the case of vector::push_back failing. Unfortunately, as you consult UserDatabase::AddFriend 's documentation, you discover with annoyance that it can throw an exception, too! Now you might end up with the friend in the vector, but not in the database!","title":"Memory and database"},{"location":"Distributed-computing/Theory/Consistency/#consistency","text":"HA\u7684\u4e00\u79cd\u65b9\u5f0f\u662freplication\uff0c\u8fd9\u5c31\u540c\u65f6\u5b58\u5728\u7740\u591a\u4e2aentity\uff0c\u5982\u679c\u662fdata system\u7684\u8bdd\uff0c\u5c31\u6d89\u53ca\u5982\u4f55\u4fdd\u8bc1\u8fd9\u4e9bentity\u4e4b\u95f4\u7684data\u7684consistency\u3002","title":"\u4ec0\u4e48\u65f6\u5019\u9700\u8981\u8003\u8651consistency\uff1f"},{"location":"Distributed-computing/Theory/Consistency/#todo","text":"csdn Zookeeper\u4e13\u9898\u2014\u20143\u3001\u5206\u5e03\u5f0f\u4e00\u81f4\u6027\uff0c\u51e0\u79cd\u5b9e\u73b0\u7684\u4f18\u7f3a\u70b9 jianshu \u53d6\u4ee3 ZooKeeper\uff01\u9ad8\u5e76\u53d1\u4e0b\u7684\u5206\u5e03\u5f0f\u4e00\u81f4\u6027\u5f00\u6e90\u7ec4\u4ef6 StateSynchronizer mwhittaker Replicated Data Consistency Explained Through Baseball","title":"TODO"},{"location":"Distributed-computing/Theory/Consistency/Classification/TODO-Sequential-consistency-VS-linearizability/","text":"Sequential consistency VS linearizability \u5728\u5982\u4e0b\u6587\u7ae0\u4e2d\uff0c\u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u4e86\u8bf4\u660e: 1\u3001 https://spcl.inf.ethz.ch/Teaching/2015-dphpc/recitation/linearizability_solution.pdf 2\u3001 https://people.engr.tamu.edu/j-welch/papers/tocs94.pdf 3\u3001cs.cmu-10-consistency","title":"TODO-Sequential-consistency-VS-linearizability"},{"location":"Distributed-computing/Theory/Consistency/Classification/TODO-Sequential-consistency-VS-linearizability/#sequential#consistency#vs#linearizability","text":"\u5728\u5982\u4e0b\u6587\u7ae0\u4e2d\uff0c\u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u4e86\u8bf4\u660e: 1\u3001 https://spcl.inf.ethz.ch/Teaching/2015-dphpc/recitation/linearizability_solution.pdf 2\u3001 https://people.engr.tamu.edu/j-welch/papers/tocs94.pdf 3\u3001cs.cmu-10-consistency","title":"Sequential consistency VS linearizability"},{"location":"Distributed-computing/Theory/Consistency/Classification/Linearizability/","text":"Linearizability \"linearizability\"\u7684\u542b\u4e49\u662f\"\u53ef\u7ebf\u6027\u5316\"\uff0c\u5176\u5b9e\u5c31\u662f\u6211\u4eec\u5e73\u65f6\u6240\u8bf4\u7684\"\u4e32\u884c\"\u3002 wikipedia Linearizability In concurrent programming , an operation (or set of operations) is linearizable if it consists of an ordered list of invocation and response events ( callbacks ), that may be extended by adding response events such that: The extended list can be re-expressed as a sequential history (is serializable ), and That sequential history is a subset of the original unextended list. Informally, this means that the unmodified list of events is linearizable if and only if its invocations were serializable , but some of the responses of the serial schedule have yet to return.[ 1] NOTE: \u6ca1\u6709\u5b8c\u5168\u7406\u89e3\uff1b\u76f4\u89c2\u7406\u89e3\u662f: response event list \u662f\u7531invocation list\u51b3\u5b9a\uff0c\u5e76\u4e14\u5b83\u4eec\u7684order\u662f\u5339\u914d\u7684\u3002 In a concurrent system, processes can access a shared object at the same time. Because multiple processes are accessing a single object, there may arise a situation in which while one process is accessing the object, another process changes its contents. This example demonstrates the need for linearizability. In a linearizable system although operations overlap on a shared object , each operation appears to take place instantaneously(\u77ac\u95f4\uff0c\u5176\u5b9e\u5c31\u662fatomic). NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u603b\u7ed3\u5f97\u975e\u5e38\u597d\u7684\u3002 \u4f7f\u7528multiple model\u6765\u8fdb\u884c\u7406\u89e3: 1\u3001\u4e00\u65e6\u6709\u4e00\u4e2a\u8fdb\u884c\u4e86write\uff0c\u5219\u5fc5\u987b\u8fdb\u884c concurrency control\u3002 2\u3001\u4e0a\u8ff0shared object\uff0c\u5176\u5b9e\u5c31\u662fshared data Linearizability is a strong correctness condition , which constrains what outputs are possible when an object is accessed by multiple processes concurrently. It is a safety property which ensures that operations do not complete in an unexpected or unpredictable manner. If a system is linearizable it allows a programmer to reason about the system.[ 2] NOTE: Linearizability \u5176\u5b9e\u5c31\u662f make it computational \u4e2d\u7684 ordering \uff0c\u5b83\u80fd\u591f\u4fdd\u8bc1 correct/ideal/expected result \uff0c\u5b83\u662fprogrammer\u7406\u89e3system\u7684\u65b9\u5f0f\u3002 TRANSLATION : \u5728\u5e76\u53d1\u7cfb\u7edf\u4e2d\uff0c\u8fdb\u7a0b\u53ef\u4ee5\u540c\u65f6\u8bbf\u95ee\u5171\u4eab\u5bf9\u8c61\u3002\u7531\u4e8e\u591a\u4e2a\u8fdb\u7a0b\u6b63\u5728\u8bbf\u95ee\u5355\u4e2a\u5bf9\u8c61\uff0c\u56e0\u6b64\u53ef\u80fd\u4f1a\u51fa\u73b0\u8fd9\u6837\u7684\u60c5\u51b5\uff1a\u5f53\u4e00\u4e2a\u8fdb\u7a0b\u6b63\u5728\u8bbf\u95ee\u8be5\u5bf9\u8c61\u65f6\uff0c\u53e6\u4e00\u4e2a\u8fdb\u7a0b\u4f1a\u66f4\u6539\u5176\u5185\u5bb9\u3002\u6b64\u793a\u4f8b\u6f14\u793a\u4e86\u7ebf\u6027\u5316\u7684\u5fc5\u8981\u6027\u3002\u5728\u53ef\u7ebf\u6027\u5316\u7cfb\u7edf\u4e2d\uff0c\u5c3d\u7ba1\u64cd\u4f5c\u5728\u5171\u4eab\u5bf9\u8c61\u4e0a\u91cd\u53e0\uff0c\u4f46\u6bcf\u4e2a\u64cd\u4f5c\u4f3c\u4e4e\u90fd\u662f\u5373\u65f6\u8fdb\u884c\u7684\u3002\u7ebf\u6027\u5316\u662f\u4e00\u79cd\u5f3a\u6b63\u786e\u6027\u6761\u4ef6\uff0c\u5b83\u9650\u5236\u4e86\u5f53\u591a\u4e2a\u8fdb\u7a0b\u540c\u65f6\u8bbf\u95ee\u5bf9\u8c61\u65f6\u53ef\u80fd\u7684\u8f93\u51fa\u3002\u5b83\u662f\u4e00\u79cd\u5b89\u5168\u5c5e\u6027\uff0c\u53ef\u786e\u4fdd\u64cd\u4f5c\u4e0d\u4f1a\u4ee5\u610f\u5916\u6216\u4e0d\u53ef\u9884\u6d4b\u7684\u65b9\u5f0f\u5b8c\u6210\u3002\u5982\u679c\u7cfb\u7edf\u662f\u53ef\u7ebf\u6027\u5316\u7684\uff0c\u5219\u5141\u8bb8\u7a0b\u5e8f\u5458\u63a8\u7406\u7cfb\u7edf\u3002 In grey a linear sub-history, processes beginning in b do not have a linearizable history because b0 or b1 may complete in either order before b2 occurs. History of linearizability NOTE: \u8fd9\u6bb5\u8bdd\u6d89\u53ca\u4e86linearizability\u548catomic\u4e4b\u95f4\u7684\u5173\u7cfb\uff1b\u4ece\u540e\u6587\u53ef\u4ee5\u770b\u51fa\uff0c\u4e24\u8005\u662f\u4e0d\u540c\u7684\u6982\u5ff5 Linearizability was first introduced as a consistency model by Herlihy and Wing in 1987. It encompassed(\u5305\u542b) more restrictive definitions of atomic , such as \"an atomic operation is one which cannot be (or is not) interrupted by concurrent operations\", which are usually vague(\u6a21\u7cca\u7684) about when an operation is considered to begin and end. An atomic object can be understood immediately and completely from its sequential definition, as a set of operations run in parallel which always appear to occur one after the other; no inconsistencies may emerge. Specifically, linearizability guarantees that the invariants of a system are observed and preserved by all operations: if all operations individually preserve an invariant, the system as a whole will. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\uff0c\u7b80\u800c\u8a00\u4e4b: atomic -> linearizability linearizability !-> atomic Definition of linearizability NOTE: \u672a\u8bfb Primitive atomic instructions NOTE:\u8fd9\u90e8\u5206\u5185\u5bb9 \u653e\u5230\u4e86\u5de5\u7a0bhardware\u7684atomic-instruction\u7ae0\u8282\u4e2d Processors have instructions that can be used to implement locking and lock-free and wait-free algorithms . High-level atomic operations NOTE: \u524d\u9762\u7684\"Primitive atomic instructions\"\uff0c\u5176\u5b9e\u662f\u975e\u5e38\u5e95\u5c42\u7684\uff0c\u5b66\u4e60\u96be\u5ea6\u662f\u8f83\u5927\u7684\uff0c\u56e0\u6b64\uff0c\u9700\u8981\u521b\u5efa\u5408\u7406\u7684abstraction\uff0c\u6216\u8005\u63d0\u4f9bhigh-level interface\u3002 Lock based NOTE: \u5176\u5b9e\u5c31\u662f\u57fa\u4e8elock\u6765\u5b9e\u73b0 The easiest way to achieve linearizability is running groups of primitive operations in a critical section . Strictly, independent operations can then be carefully permitted to overlap their critical sections, provided this does not violate linearizability. Such an approach must balance the cost of large numbers of locks against the benefits of increased parallelism. Native atomic primitives based NOTE: \u5176\u5b9e\u5c31\u662f\u57fa\u4e8eNative atomic primitives \u6765\u5b9e\u73b0 Another approach, favoured by researchers (but not yet widely used in the software industry), is to design a linearizable object using the native atomic primitives provided by the hardware. This has the potential to maximise available parallelism and minimise synchronisation costs, but requires mathematical proofs which show that the objects behave correctly. Transactional memory based NOTE: \u57fa\u4e8etransactional memory\u7684\u5b9e\u73b0 A promising hybrid of these two is to provide a transactional memory abstraction. As with critical sections, the user marks sequential code that must be run in isolation from other threads. The implementation then ensures the code executes atomically. This style of abstraction is common when interacting with databases; for instance, when using the Spring Framework , annotating a method with @Transactional will ensure all enclosed database interactions occur in a single database transaction . Transactional memory goes a step further, ensuring that all memory interactions occur atomically. As with database transactions, issues arise regarding composition of transactions, especially database and in-memory transactions. All-or-nothing interface NOTE: \u5176\u5b9e\uff0c\u6211\u89c9\u5f97\u8fd9\u662f\u66f4\u52a0\u5bbd\u6cdb\u7684transaction\u6982\u5ff5 A common theme when designing linearizable objects is to provide an all-or-nothing interface : either an operation succeeds completely, or it fails and does nothing. ( ACID databases refer to this principle as atomicity .) If the operation fails (usually due to concurrent operations), the user must retry, usually performing a different operation. For example: 1\u3001 Compare-and-swap writes a new value into a location only if the latter's contents matches a supplied old value. This is commonly used in a read-modify-CAS sequence: the user reads the location, computes a new value to write, and writes it with a CAS (compare-and-swap); if the value changes concurrently, the CAS will fail and the user tries again. 2\u3001 Load-link/store-conditional encodes this pattern more directly: the user reads the location with load-link, computes a new value to write, and writes it with store-conditional; if the value has changed concurrently, the SC (store-conditional) will fail and the user tries again. 3\u3001In a database transaction , if the transaction cannot be completed due to a concurrent operation (e.g. in a deadlock ), the transaction will be aborted and the user must try again. Examples of linearizability NOTE: \u8fd9\u6bb5\u4ecb\u7ecd\u4e86\u4f7f\u7528 TODO \u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\u63d0\u53ca\u4e86atomicity\u5185\u5bb9: 1) Using Redis as a Time Series Database: Why and How draft Serialization NOTE: \u672c\u8282\u6807\u9898\u7684\u542b\u4e49\u662f\"\u4e32\u884c\"\uff0c\u4e32\u884c\u7684\u82f1\u8bed\u662fserial\uff0c\u663e\u7136\uff0c\u4e32\u884c\u548c\u5e76\u884c\u662f\u76f8\u53cd\u7684\u3002 \u53c2\u89c1\u6587\u7ae0\uff1a\u7ef4\u57fa\u767e\u79d1 Linearizability \u3002 \u5b9e\u73b0Serialization\u7684\u65b9\u5f0f\u6709\uff1a atomic\u64cd\u4f5c \u52a0\u9501\u673a\u5236 What is the difference between linearizability and serializability? \u7ef4\u57fa\u767e\u79d1 Mutual exclusion","title":"Introduction"},{"location":"Distributed-computing/Theory/Consistency/Classification/Linearizability/#linearizability","text":"\"linearizability\"\u7684\u542b\u4e49\u662f\"\u53ef\u7ebf\u6027\u5316\"\uff0c\u5176\u5b9e\u5c31\u662f\u6211\u4eec\u5e73\u65f6\u6240\u8bf4\u7684\"\u4e32\u884c\"\u3002","title":"Linearizability"},{"location":"Distributed-computing/Theory/Consistency/Classification/Linearizability/#wikipedia#linearizability","text":"In concurrent programming , an operation (or set of operations) is linearizable if it consists of an ordered list of invocation and response events ( callbacks ), that may be extended by adding response events such that: The extended list can be re-expressed as a sequential history (is serializable ), and That sequential history is a subset of the original unextended list. Informally, this means that the unmodified list of events is linearizable if and only if its invocations were serializable , but some of the responses of the serial schedule have yet to return.[ 1] NOTE: \u6ca1\u6709\u5b8c\u5168\u7406\u89e3\uff1b\u76f4\u89c2\u7406\u89e3\u662f: response event list \u662f\u7531invocation list\u51b3\u5b9a\uff0c\u5e76\u4e14\u5b83\u4eec\u7684order\u662f\u5339\u914d\u7684\u3002 In a concurrent system, processes can access a shared object at the same time. Because multiple processes are accessing a single object, there may arise a situation in which while one process is accessing the object, another process changes its contents. This example demonstrates the need for linearizability. In a linearizable system although operations overlap on a shared object , each operation appears to take place instantaneously(\u77ac\u95f4\uff0c\u5176\u5b9e\u5c31\u662fatomic). NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u603b\u7ed3\u5f97\u975e\u5e38\u597d\u7684\u3002 \u4f7f\u7528multiple model\u6765\u8fdb\u884c\u7406\u89e3: 1\u3001\u4e00\u65e6\u6709\u4e00\u4e2a\u8fdb\u884c\u4e86write\uff0c\u5219\u5fc5\u987b\u8fdb\u884c concurrency control\u3002 2\u3001\u4e0a\u8ff0shared object\uff0c\u5176\u5b9e\u5c31\u662fshared data Linearizability is a strong correctness condition , which constrains what outputs are possible when an object is accessed by multiple processes concurrently. It is a safety property which ensures that operations do not complete in an unexpected or unpredictable manner. If a system is linearizable it allows a programmer to reason about the system.[ 2] NOTE: Linearizability \u5176\u5b9e\u5c31\u662f make it computational \u4e2d\u7684 ordering \uff0c\u5b83\u80fd\u591f\u4fdd\u8bc1 correct/ideal/expected result \uff0c\u5b83\u662fprogrammer\u7406\u89e3system\u7684\u65b9\u5f0f\u3002 TRANSLATION : \u5728\u5e76\u53d1\u7cfb\u7edf\u4e2d\uff0c\u8fdb\u7a0b\u53ef\u4ee5\u540c\u65f6\u8bbf\u95ee\u5171\u4eab\u5bf9\u8c61\u3002\u7531\u4e8e\u591a\u4e2a\u8fdb\u7a0b\u6b63\u5728\u8bbf\u95ee\u5355\u4e2a\u5bf9\u8c61\uff0c\u56e0\u6b64\u53ef\u80fd\u4f1a\u51fa\u73b0\u8fd9\u6837\u7684\u60c5\u51b5\uff1a\u5f53\u4e00\u4e2a\u8fdb\u7a0b\u6b63\u5728\u8bbf\u95ee\u8be5\u5bf9\u8c61\u65f6\uff0c\u53e6\u4e00\u4e2a\u8fdb\u7a0b\u4f1a\u66f4\u6539\u5176\u5185\u5bb9\u3002\u6b64\u793a\u4f8b\u6f14\u793a\u4e86\u7ebf\u6027\u5316\u7684\u5fc5\u8981\u6027\u3002\u5728\u53ef\u7ebf\u6027\u5316\u7cfb\u7edf\u4e2d\uff0c\u5c3d\u7ba1\u64cd\u4f5c\u5728\u5171\u4eab\u5bf9\u8c61\u4e0a\u91cd\u53e0\uff0c\u4f46\u6bcf\u4e2a\u64cd\u4f5c\u4f3c\u4e4e\u90fd\u662f\u5373\u65f6\u8fdb\u884c\u7684\u3002\u7ebf\u6027\u5316\u662f\u4e00\u79cd\u5f3a\u6b63\u786e\u6027\u6761\u4ef6\uff0c\u5b83\u9650\u5236\u4e86\u5f53\u591a\u4e2a\u8fdb\u7a0b\u540c\u65f6\u8bbf\u95ee\u5bf9\u8c61\u65f6\u53ef\u80fd\u7684\u8f93\u51fa\u3002\u5b83\u662f\u4e00\u79cd\u5b89\u5168\u5c5e\u6027\uff0c\u53ef\u786e\u4fdd\u64cd\u4f5c\u4e0d\u4f1a\u4ee5\u610f\u5916\u6216\u4e0d\u53ef\u9884\u6d4b\u7684\u65b9\u5f0f\u5b8c\u6210\u3002\u5982\u679c\u7cfb\u7edf\u662f\u53ef\u7ebf\u6027\u5316\u7684\uff0c\u5219\u5141\u8bb8\u7a0b\u5e8f\u5458\u63a8\u7406\u7cfb\u7edf\u3002 In grey a linear sub-history, processes beginning in b do not have a linearizable history because b0 or b1 may complete in either order before b2 occurs.","title":"wikipedia Linearizability"},{"location":"Distributed-computing/Theory/Consistency/Classification/Linearizability/#history#of#linearizability","text":"NOTE: \u8fd9\u6bb5\u8bdd\u6d89\u53ca\u4e86linearizability\u548catomic\u4e4b\u95f4\u7684\u5173\u7cfb\uff1b\u4ece\u540e\u6587\u53ef\u4ee5\u770b\u51fa\uff0c\u4e24\u8005\u662f\u4e0d\u540c\u7684\u6982\u5ff5 Linearizability was first introduced as a consistency model by Herlihy and Wing in 1987. It encompassed(\u5305\u542b) more restrictive definitions of atomic , such as \"an atomic operation is one which cannot be (or is not) interrupted by concurrent operations\", which are usually vague(\u6a21\u7cca\u7684) about when an operation is considered to begin and end. An atomic object can be understood immediately and completely from its sequential definition, as a set of operations run in parallel which always appear to occur one after the other; no inconsistencies may emerge. Specifically, linearizability guarantees that the invariants of a system are observed and preserved by all operations: if all operations individually preserve an invariant, the system as a whole will. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\uff0c\u7b80\u800c\u8a00\u4e4b: atomic -> linearizability linearizability !-> atomic","title":"History of linearizability"},{"location":"Distributed-computing/Theory/Consistency/Classification/Linearizability/#definition#of#linearizability","text":"NOTE: \u672a\u8bfb","title":"Definition of linearizability"},{"location":"Distributed-computing/Theory/Consistency/Classification/Linearizability/#primitive#atomic#instructions","text":"NOTE:\u8fd9\u90e8\u5206\u5185\u5bb9 \u653e\u5230\u4e86\u5de5\u7a0bhardware\u7684atomic-instruction\u7ae0\u8282\u4e2d Processors have instructions that can be used to implement locking and lock-free and wait-free algorithms .","title":"Primitive atomic instructions"},{"location":"Distributed-computing/Theory/Consistency/Classification/Linearizability/#high-level#atomic#operations","text":"NOTE: \u524d\u9762\u7684\"Primitive atomic instructions\"\uff0c\u5176\u5b9e\u662f\u975e\u5e38\u5e95\u5c42\u7684\uff0c\u5b66\u4e60\u96be\u5ea6\u662f\u8f83\u5927\u7684\uff0c\u56e0\u6b64\uff0c\u9700\u8981\u521b\u5efa\u5408\u7406\u7684abstraction\uff0c\u6216\u8005\u63d0\u4f9bhigh-level interface\u3002","title":"High-level atomic operations"},{"location":"Distributed-computing/Theory/Consistency/Classification/Linearizability/#lock#based","text":"NOTE: \u5176\u5b9e\u5c31\u662f\u57fa\u4e8elock\u6765\u5b9e\u73b0 The easiest way to achieve linearizability is running groups of primitive operations in a critical section . Strictly, independent operations can then be carefully permitted to overlap their critical sections, provided this does not violate linearizability. Such an approach must balance the cost of large numbers of locks against the benefits of increased parallelism.","title":"Lock based"},{"location":"Distributed-computing/Theory/Consistency/Classification/Linearizability/#native#atomic#primitives#based","text":"NOTE: \u5176\u5b9e\u5c31\u662f\u57fa\u4e8eNative atomic primitives \u6765\u5b9e\u73b0 Another approach, favoured by researchers (but not yet widely used in the software industry), is to design a linearizable object using the native atomic primitives provided by the hardware. This has the potential to maximise available parallelism and minimise synchronisation costs, but requires mathematical proofs which show that the objects behave correctly.","title":"Native atomic primitives based"},{"location":"Distributed-computing/Theory/Consistency/Classification/Linearizability/#transactional#memory#based","text":"NOTE: \u57fa\u4e8etransactional memory\u7684\u5b9e\u73b0 A promising hybrid of these two is to provide a transactional memory abstraction. As with critical sections, the user marks sequential code that must be run in isolation from other threads. The implementation then ensures the code executes atomically. This style of abstraction is common when interacting with databases; for instance, when using the Spring Framework , annotating a method with @Transactional will ensure all enclosed database interactions occur in a single database transaction . Transactional memory goes a step further, ensuring that all memory interactions occur atomically. As with database transactions, issues arise regarding composition of transactions, especially database and in-memory transactions.","title":"Transactional memory based"},{"location":"Distributed-computing/Theory/Consistency/Classification/Linearizability/#all-or-nothing#interface","text":"NOTE: \u5176\u5b9e\uff0c\u6211\u89c9\u5f97\u8fd9\u662f\u66f4\u52a0\u5bbd\u6cdb\u7684transaction\u6982\u5ff5 A common theme when designing linearizable objects is to provide an all-or-nothing interface : either an operation succeeds completely, or it fails and does nothing. ( ACID databases refer to this principle as atomicity .) If the operation fails (usually due to concurrent operations), the user must retry, usually performing a different operation. For example: 1\u3001 Compare-and-swap writes a new value into a location only if the latter's contents matches a supplied old value. This is commonly used in a read-modify-CAS sequence: the user reads the location, computes a new value to write, and writes it with a CAS (compare-and-swap); if the value changes concurrently, the CAS will fail and the user tries again. 2\u3001 Load-link/store-conditional encodes this pattern more directly: the user reads the location with load-link, computes a new value to write, and writes it with store-conditional; if the value has changed concurrently, the SC (store-conditional) will fail and the user tries again. 3\u3001In a database transaction , if the transaction cannot be completed due to a concurrent operation (e.g. in a deadlock ), the transaction will be aborted and the user must try again.","title":"All-or-nothing interface"},{"location":"Distributed-computing/Theory/Consistency/Classification/Linearizability/#examples#of#linearizability","text":"NOTE: \u8fd9\u6bb5\u4ecb\u7ecd\u4e86\u4f7f\u7528","title":"Examples of linearizability"},{"location":"Distributed-computing/Theory/Consistency/Classification/Linearizability/#todo","text":"\u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\u63d0\u53ca\u4e86atomicity\u5185\u5bb9: 1) Using Redis as a Time Series Database: Why and How","title":"TODO"},{"location":"Distributed-computing/Theory/Consistency/Classification/Linearizability/#draft","text":"","title":"draft"},{"location":"Distributed-computing/Theory/Consistency/Classification/Linearizability/#serialization","text":"NOTE: \u672c\u8282\u6807\u9898\u7684\u542b\u4e49\u662f\"\u4e32\u884c\"\uff0c\u4e32\u884c\u7684\u82f1\u8bed\u662fserial\uff0c\u663e\u7136\uff0c\u4e32\u884c\u548c\u5e76\u884c\u662f\u76f8\u53cd\u7684\u3002 \u53c2\u89c1\u6587\u7ae0\uff1a\u7ef4\u57fa\u767e\u79d1 Linearizability \u3002 \u5b9e\u73b0Serialization\u7684\u65b9\u5f0f\u6709\uff1a atomic\u64cd\u4f5c \u52a0\u9501\u673a\u5236 What is the difference between linearizability and serializability? \u7ef4\u57fa\u767e\u79d1 Mutual exclusion","title":"Serialization"},{"location":"Distributed-computing/Theory/Consistency/Classification/Sequential-consistency/","text":"Sequential consistency \u603b\u7ed3 \u76f8\u8f83\u4e8estrict consistency model\uff0c\u653e\u677e\u4e86**\u6240\u6709\u7684write\u7acb\u5373\u751f\u6548**\u7684\u9650\u5236\uff0c\u56e0\u6b64\u5728Consistency model abstract machine\u4e0a\u7684\u8fd0\u884c\u7ed3\u679c\u53ef\u80fd\u5b58\u5728dirty data\u7684\uff1b Constrain: \u5b9e\u9645\u7684\u8fd0\u884c\u7ed3\u679c\u80fd\u591f\u4f7f\u7528\u4e0b\u9762\u63cf\u8ff0\u7684\u65b9\u5f0f\u8fdb\u884c**\u6a21\u62df**: \u80fd\u591f\u4f7f\u7528C++11 std::atomic \u7684 sequential consistency\u6765\u8fdb\u884c\u6a21\u62df\uff0c\u4e0b\u9762\u7b80\u5355\u5730\u63cf\u8ff0\u4e00\u4e0b\u5bf9\u5e94\u5173\u7cfb: 1\u3001\u6240\u6709entity\u6309\u7167program\u4e2d\u7684\u6b21\u5e8f\u8fd0\u884c 2\u3001\u6bcf\u4e2aentity\u5bf9\u5e94\u4e00\u4e2athread\uff0cshared data\u5bf9\u5e94process\u4e2d\u6240\u6709\u7ebf\u7a0b\u5171\u4eab\u7684\u6570\u636e 3\u3001\u5728\u4e00\u4e2aprocess\u4e2d\u8fdb\u884c\u8fd0\u884c\uff0c\u53ef\u4ee5\u5f97\u5230\u5728Consistency model abstract machine\u4e0a\u7684\u8fd0\u884c\u7ed3\u679c\u3002 NOTE: \u5173\u4e8e\u4e0a\u8ff0\u8fc7\u7a0b\u7684\u5177\u4f53\u4f8b\u5b50\uff0c\u53c2\u89c1: stackoverflow C++11 introduced a standardized memory model. What does it mean? And how is it going to affect C++ programming? # A \uff0c\u5176\u4e2d\u6709\u975e\u5e38\u597d\u7684\u63cf\u8ff0\u3002 \u73b0\u5728\u5728\u56de\u8fc7\u5934\u6765\u770b\u770b Lamport(1979) Sequential consistency \u7ed9\u51fa\u7684sequential consistency model\u7684\u5b9a\u4e49: \"... the result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program.\"[ 1] \u5176\u4e2d\u7684\"as if\"\u544a\u8bc9\u6211\u4eec\u9700\u8981\u8fdb\u884c\u6a21\u62df\u3002 \u7ec4\u5408\u5206\u6790 \u6709\u591a\u5c11\u79cd\u53ef\u80fd\u7684\u7ec4\u5408\uff1f\u5728\u4e0b\u9762\u8fd9\u4e9b\u6587\u7ae0\u4e2d\uff0c\u7ed9\u51fa\u4e86\u7c7b\u4f3c\u7684\u5206\u6790: stackoverflow Sequential Consistency in Distributed Systems # A An execution e of operations is sequentially consistent if and only if it can be permutated into a sequence s of these operations such that: 1\u3001the sequence s respects the program order of each process. That is, for any two operations o1 and o2 which are of the same process and if o1 precedes o2 in e , then o1 should be placed before o2 in s ; 2\u3001in the sequence s , each read operation returns the value of the last preceding write operation over the same variable. For (a), s can be\uff1a W(x)b [P2], R(x)b [P3], R(x)b [P4], W(x)a [P1], R(x)a [P3], R(x)a [P4] For \u00a9, s can be: W(x)a [P1], R(x)a [P2], R(x)a [P3], R(x)a [P4], W(x)b [P3], R(x)b [P1], R(x)b [P2], R(x)b [P4] However, for (b): the operations R(x)b, R(x)a from P3 require that W(x)b come before W(x)a the operations R(x)a, R(x)b from P4 require that W(x)a come before W(x)b It is impossible to construct such a sequence s . cs.cmu 10-consistency wikipedia Sequential consistency \u987a\u5e8f\u4e00\u81f4\u6027 sequential order \u5148\u540e\u987a\u5e8f\uff0c\u76f8\u7ee7\u987a\u5e8f Sequential consistency is one of the consistency models used in the domain of concurrent computing (e.g. in distributed shared memory , distributed transactions , etc.). It was first defined as the property that requires that \"... the result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program.\"[ 1] To understand this statement, it is essential to understand one key property of sequential consistency : execution order of program in the same processor (or thread) is the same as the program order , while\uff08\u5c3d\u7ba1\uff09 execution order of program between processors (or threads) is undefined. In an example like this: processor 1: <-- A1 run --> <-- B1 run --> <-- C1 run --> processor 2: <-- A2 run --> <-- B2 run --> Time ---------------------------------------------------------------------> execution order between A1, B1 and C1 is preserved, that is, A1 runs before B1, and B1 before C1. The same for A2 and B2. But, as execution order between processors is undefined, B2 might run before or after C1 (B2 might physically run before C1, but the effect of B2 might be seen after that of C1, which is the same as \"B2 run after C1\") Conceptually, there is single global memory and a \"switch\" that connects an arbitrary processor to memory at any time step. Each processor issues memory operations in program order and the switch provides the global serialization among all memory operations[ 2] The sequential consistency is weaker than strict consistency , which requires a read from a location to return the value of the last write to that location; strict consistency demands that operations be seen in the order in which they were actually issued. book-Distributed-operating-systems \u5728\u8fd9\u672c\u4e66\u7684 6-Distributed-Shared-Memory\\6.3.2-Sequential-Consistency \u4e2d\u5bf9sequential \u53c2\u89c1 Theory\\book-Distributed-operating-systems\\6-Distributed-Shared-Memory\\6.3.2-Sequential-Consistency \u7ae0\u8282\u3002 csdn \u4e00\u81f4\u6027\u6a21\u578b\u4e4bSequential Consistency Sequential Consistency\u7684\u5b9a\u4e49 Sequential Consistency\u7684\u7cbe\u786e\u5b9a\u4e49\u6765\u81ea\u4e8eLeslie Lamport\u8001\u54e5(\u4ee5\u540e\u6211\u4eec\u4f1a\u591a\u6b21\u63d0\u5230\u4ed6)\u3002\u4ed6\u672c\u6765\u662f\u5b9a\u4e49\u4e86\u57fa\u4e8e\u5171\u4eab\u5185\u5b58\u7684\u591aCPU\u5e76\u884c\u8ba1\u7b97\u7684\u4e00\u81f4\u6027\u6a21\u578b\uff0c\u4f46\u662f\u4e5f\u53ef\u4ee5\u63a8\u5e7f\u5230\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\uff0c\u5b9e\u9645\u4e0a\u591aCPU\u5e76\u884c\u8ba1\u7b97\u4e5f\u90fd\u53ef\u4ee5\u8ba4\u4e3a\u662f\u5206\u5e03\u5f0f\u7cfb\u7edf\u3002\u6a21\u578b\u7684\u5b9a\u4e49\u662f: the result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program \u653e\u5230\u5206\u5e03\u5f0f\u7cfb\u7edf\u91cc\uff0c\u610f\u601d\u5c31\u662f\u4e0d\u7ba1\u7cfb\u7edf\u600e\u4e48\u8fd0\u884c\uff0c\u5f97\u5230\u7684\u7ed3\u679c\u5c31\u597d\u50cf\u628a\u6240\u6709\u8282\u70b9\u7684\u6240\u6709\u64cd\u4f5c\u6309\u7167\u67d0\u4e2asequential order\u6392\u5e8f\u540e\u8fd0\u884c\uff0c\u4f46\u662f\u5728\u8fd9\u4e2asequential order\u987a\u5e8f\u4e2d\uff0c\u6765\u81ea\u540c\u4e00\u4e2a\u8282\u70b9\u7684\u64cd\u4f5c\u4ecd\u7136\u4fdd\u6301\u7740\u5b83\u4eec\u5728\u8282\u70b9\u4e2d\u88ab\u6307\u5b9a\u7684\u987a\u5e8f\uff08\u4e5f\u5c31\u662f\u4ed6\u4eec\u5728program\u4e2d\u6307\u5b9a\u7684\u987a\u5e8f\uff09\u3002 Sequential Consistency\u7684\u4f8b\u5b50 Leslie Lamport\u8001\u54e5\u7684\u8bf4\u6cd5\u4e00\u8d2f\u7684\u4f76\u5c48\u8071\u7259\uff0c\u6211\u4eec\u901a\u8fc7\u51e0\u4e2a\u4f8b\u5b50\u6765\u770b\u4e00\u4e0b\u3002\u56fe\u4e2d\u4ece\u5de6\u5411\u53f3\u8868\u793a\u7269\u7406\u65f6\u95f4\uff0cW(a)\u8868\u793a\u5199\u5165\u6570\u636ea\uff0cR(a)\u8868\u793a\u8bfb\u51fa\u6570\u636ea\u3002 \u53ef\u4ee5\u770b\u51fa\uff0c\u8fd9\u4e24\u4e2a\u7cfb\u7edf\u90fd\u4e0d\u662f\u5f88\u5b8c\u7f8e\uff0c\u4f46\u662f\u5b83\u4eec\u7684\u6a21\u578b\u90fd\u53ef\u4ee5\u770b\u505a**Sequential Consistency**\uff0c\u56e0\u4e3a\u901a\u8fc7\u5982\u4e0b\u53d8\u6362\uff0c\u603b\u662f\u53ef\u4ee5\u81ea\u5706\u5176\u8bf4\uff0c\u4e5f\u5c31\u662f\u53ef\u4ee5\u627e\u5230\u7b26\u5408\u5b9a\u4e49\u7684**sequential order**\u3002 Sequential Consistency\u548c\u786c\u4ef6 \u4e5f\u8bb8\u6709\u4eba\u4f1a\u95ee\uff0c\u540c\u4e00\u4e2a\u8fdb\u7a0b\u4e2d\u4fdd\u7559\u64cd\u4f5c\u987a\u5e8f\u4e0d\u662f\u663e\u800c\u6613\u89c1\u7684\u4e48?\u5b9e\u9645\u4e0a\u968f\u7740\u786c\u4ef6\u6280\u672f\uff0c\u5c24\u5176\u662f\u591a\u6838\u3001\u591aCPU\u6280\u672f\u7684\u53d1\u5c55\uff0c\u4e00\u4e2aCPU\u6838\u5fc3\u8fd0\u884c\u7684\u8fdb\u7a0b\uff0c\u4e0d\u4e00\u5b9a\u80fd\u89c2\u6d4b\u5230\u53e6\u4e00\u4e2a\u6838\u5fc3\u8fdb\u7a0b\u7684\u64cd\u4f5c\u987a\u5e8f\u3002 \u5728\u8bba\u6587\u4e2d\uff0cLeslie Lamport\u8001\u54e5\u4e3e\u4e86\u8fd9\u6837\u4e00\u4e2a\u4f8b\u5b50\uff0c\u6709\u4e00\u4e2a\u4e92\u65a5\u7b97\u6cd5\uff0c\u8981\u6c42\u4e24\u4e2a\u8fdb\u7a0b\u4e0d\u80fd\u540c\u65f6\u6267\u884c\u4e34\u754c\u533a\u65b9\u6cd5\uff0ca\u548cb\u4e24\u4e2a\u53d8\u91cf\u521d\u59cb\u503c\u4e3a0\u3002\u6b63\u5e38\u60c5\u51b5\u4e0b\uff0c\u6700\u591a\u4e00\u4e2a\u8fdb\u7a0b\u6267\u884c\u4e34\u754c\u533a\u65b9\u6cd5\u3002 \u8fdb\u7a0b1\u6267\u884c\u5e8f\u5217\u5982\u4e0b\uff1a a = 1 if (b!=0){ \u4e34\u754c\u533a\u65b9\u6cd5 } \u8fdb\u7a0b2\u6267\u884c\u5e8f\u5217\u5982\u4e0b\uff1a b = 1 if (a!=0){ \u4e34\u754c\u533a\u65b9\u6cd5 } \u8fd9\u4e2a\u7a0b\u5e8f\u5728\u591a\u6838CPU\u673a\u5668\u4e0a\u8fd0\u884c\u65f6\uff0c\u6709\u53ef\u80fd\u4e24\u4e2a\u8fdb\u7a0b\u540c\u65f6\u8fdb\u5165\u4e34\u754c\u533a\u3002\u4e3a\u4ec0\u4e48\u5462? \u6211\u4eec\u5148\u770b\u4e00\u4e0b\u73b0\u4ee3CPU\u7684\u67b6\u6784 CPU\u4e00\u822c\u5177\u6709\u591a\u4e2a\u6838\u5fc3\uff0c\u6bcf\u4e2a\u6838\u5fc3\u90fd\u6709\u81ea\u5df1\u7684L1 cache\u548cL2 cache\uff0ccache\u4e4b\u4e0a\u8fd8\u6709Load Buffer\u548cStore Buffer\u3002\u5199\u5165\u65f6\uff0c\u5904\u7406\u5668\u5f88\u6709\u53ef\u80fd\u4ec5\u4ec5\u5c06\u6570\u636e\u5199\u5165Store Buffer\uff0c\u7a0d\u540e\u518d\u5c06Store Buffer\u4e2d\u7684\u6570\u636e\u7edf\u4e00\u5199\u56decache\uff0c\u6709\u53ef\u80fd\u518d\u8fc7\u4e00\u4f1a\u513f\u624d\u5c06cache\u7684\u6570\u636e\u5199\u56de\u5185\u5b58\u3002\u540c\u6837\uff0c\u4e00\u4e2a\u6838\u5fc3\u8bfb\u53d6\u7684\u6570\u636e\u8bf4\u4e0d\u5b9a\u4e5f\u5df2\u7ecf\u88ab\u53e6\u4e00\u4e2a\u6838\u5fc3\u4fee\u6539\u8fc7\uff0c\u53ea\u662f\u5b83\u4e0d\u77e5\u9053\u800c\u5df2\u3002 \u6240\u4ee5\u4e0a\u8ff0\u8fdb\u7a0b\u5bf9a\u548cb\u7684\u8d4b\u503c\uff0c\u5f88\u6709\u53ef\u80fd\u6ca1\u88ab\u5bf9\u65b9\u611f\u77e5\u3002 \u4e3a\u4e86\u4fdd\u8bc1**Sequential Consistency**\uff0cLeslie Lamport\u8001\u54e5\u5728\u8bba\u6587\u4e2d\u63d0\u51fa\u4e86\u4e24\u4e2a\u8981\u6c42: Each processor issues memory requests in the order specified by its program Memory requests from all processors issued to an individual memory module are serviced from a single FIFO queue. Issuing a memory request consists of entering the request on this queue. \u4f46\u662f\u5982\u679c\u5728\u786c\u4ef6\u5c42\u6ee1\u8db3Sequential Consistency\uff0c\u80af\u5b9a\u4f1a\u5927\u5927\u964d\u4f4e\u6548\u7387\uff0c\u6240\u4ee5\u4e00\u822c\u8fd9\u4e9b\u5de5\u4f5c\u5c31\u4f1a\u4ea4\u7ed9\u4e0a\u5c42\u7684\u8f6f\u4ef6\u5f00\u53d1\u4eba\u5458\u6765\u505a\u3002 Case study Case: zookeeper zookeeper\u627f\u8bfaSequential Consistency\uff0c\u5728 ZooKeeper overview#Guarantees \u4e2d\u5bf9\u6b64\u8fdb\u884c\u4e86\u8bf4\u660e: Sequential Consistency - Updates from a client will be applied in the order that they were sent. Case: C++ memory model \u53c2\u89c1\u5de5\u7a0bprogramming-language\u7684 C++\\Language-reference\\Basic-concept\\Abstract-machine\\Memory-model \u7ae0\u8282\u3002","title":"Introduction"},{"location":"Distributed-computing/Theory/Consistency/Classification/Sequential-consistency/#sequential#consistency","text":"","title":"Sequential consistency"},{"location":"Distributed-computing/Theory/Consistency/Classification/Sequential-consistency/#_1","text":"\u76f8\u8f83\u4e8estrict consistency model\uff0c\u653e\u677e\u4e86**\u6240\u6709\u7684write\u7acb\u5373\u751f\u6548**\u7684\u9650\u5236\uff0c\u56e0\u6b64\u5728Consistency model abstract machine\u4e0a\u7684\u8fd0\u884c\u7ed3\u679c\u53ef\u80fd\u5b58\u5728dirty data\u7684\uff1b Constrain: \u5b9e\u9645\u7684\u8fd0\u884c\u7ed3\u679c\u80fd\u591f\u4f7f\u7528\u4e0b\u9762\u63cf\u8ff0\u7684\u65b9\u5f0f\u8fdb\u884c**\u6a21\u62df**: \u80fd\u591f\u4f7f\u7528C++11 std::atomic \u7684 sequential consistency\u6765\u8fdb\u884c\u6a21\u62df\uff0c\u4e0b\u9762\u7b80\u5355\u5730\u63cf\u8ff0\u4e00\u4e0b\u5bf9\u5e94\u5173\u7cfb: 1\u3001\u6240\u6709entity\u6309\u7167program\u4e2d\u7684\u6b21\u5e8f\u8fd0\u884c 2\u3001\u6bcf\u4e2aentity\u5bf9\u5e94\u4e00\u4e2athread\uff0cshared data\u5bf9\u5e94process\u4e2d\u6240\u6709\u7ebf\u7a0b\u5171\u4eab\u7684\u6570\u636e 3\u3001\u5728\u4e00\u4e2aprocess\u4e2d\u8fdb\u884c\u8fd0\u884c\uff0c\u53ef\u4ee5\u5f97\u5230\u5728Consistency model abstract machine\u4e0a\u7684\u8fd0\u884c\u7ed3\u679c\u3002 NOTE: \u5173\u4e8e\u4e0a\u8ff0\u8fc7\u7a0b\u7684\u5177\u4f53\u4f8b\u5b50\uff0c\u53c2\u89c1: stackoverflow C++11 introduced a standardized memory model. What does it mean? And how is it going to affect C++ programming? # A \uff0c\u5176\u4e2d\u6709\u975e\u5e38\u597d\u7684\u63cf\u8ff0\u3002 \u73b0\u5728\u5728\u56de\u8fc7\u5934\u6765\u770b\u770b Lamport(1979) Sequential consistency \u7ed9\u51fa\u7684sequential consistency model\u7684\u5b9a\u4e49: \"... the result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program.\"[ 1] \u5176\u4e2d\u7684\"as if\"\u544a\u8bc9\u6211\u4eec\u9700\u8981\u8fdb\u884c\u6a21\u62df\u3002","title":"\u603b\u7ed3"},{"location":"Distributed-computing/Theory/Consistency/Classification/Sequential-consistency/#_2","text":"\u6709\u591a\u5c11\u79cd\u53ef\u80fd\u7684\u7ec4\u5408\uff1f\u5728\u4e0b\u9762\u8fd9\u4e9b\u6587\u7ae0\u4e2d\uff0c\u7ed9\u51fa\u4e86\u7c7b\u4f3c\u7684\u5206\u6790:","title":"\u7ec4\u5408\u5206\u6790"},{"location":"Distributed-computing/Theory/Consistency/Classification/Sequential-consistency/#stackoverflow#sequential#consistency#in#distributed#systems#a","text":"An execution e of operations is sequentially consistent if and only if it can be permutated into a sequence s of these operations such that: 1\u3001the sequence s respects the program order of each process. That is, for any two operations o1 and o2 which are of the same process and if o1 precedes o2 in e , then o1 should be placed before o2 in s ; 2\u3001in the sequence s , each read operation returns the value of the last preceding write operation over the same variable. For (a), s can be\uff1a W(x)b [P2], R(x)b [P3], R(x)b [P4], W(x)a [P1], R(x)a [P3], R(x)a [P4] For \u00a9, s can be: W(x)a [P1], R(x)a [P2], R(x)a [P3], R(x)a [P4], W(x)b [P3], R(x)b [P1], R(x)b [P2], R(x)b [P4] However, for (b): the operations R(x)b, R(x)a from P3 require that W(x)b come before W(x)a the operations R(x)a, R(x)b from P4 require that W(x)a come before W(x)b It is impossible to construct such a sequence s .","title":"stackoverflow Sequential Consistency in Distributed Systems # A"},{"location":"Distributed-computing/Theory/Consistency/Classification/Sequential-consistency/#cscmu#10-consistency","text":"","title":"cs.cmu 10-consistency"},{"location":"Distributed-computing/Theory/Consistency/Classification/Sequential-consistency/#wikipedia#sequential#consistency","text":"\u987a\u5e8f\u4e00\u81f4\u6027 sequential order \u5148\u540e\u987a\u5e8f\uff0c\u76f8\u7ee7\u987a\u5e8f Sequential consistency is one of the consistency models used in the domain of concurrent computing (e.g. in distributed shared memory , distributed transactions , etc.). It was first defined as the property that requires that \"... the result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program.\"[ 1] To understand this statement, it is essential to understand one key property of sequential consistency : execution order of program in the same processor (or thread) is the same as the program order , while\uff08\u5c3d\u7ba1\uff09 execution order of program between processors (or threads) is undefined. In an example like this: processor 1: <-- A1 run --> <-- B1 run --> <-- C1 run --> processor 2: <-- A2 run --> <-- B2 run --> Time ---------------------------------------------------------------------> execution order between A1, B1 and C1 is preserved, that is, A1 runs before B1, and B1 before C1. The same for A2 and B2. But, as execution order between processors is undefined, B2 might run before or after C1 (B2 might physically run before C1, but the effect of B2 might be seen after that of C1, which is the same as \"B2 run after C1\") Conceptually, there is single global memory and a \"switch\" that connects an arbitrary processor to memory at any time step. Each processor issues memory operations in program order and the switch provides the global serialization among all memory operations[ 2] The sequential consistency is weaker than strict consistency , which requires a read from a location to return the value of the last write to that location; strict consistency demands that operations be seen in the order in which they were actually issued.","title":"wikipedia Sequential consistency"},{"location":"Distributed-computing/Theory/Consistency/Classification/Sequential-consistency/#book-distributed-operating-systems","text":"\u5728\u8fd9\u672c\u4e66\u7684 6-Distributed-Shared-Memory\\6.3.2-Sequential-Consistency \u4e2d\u5bf9sequential \u53c2\u89c1 Theory\\book-Distributed-operating-systems\\6-Distributed-Shared-Memory\\6.3.2-Sequential-Consistency \u7ae0\u8282\u3002","title":"book-Distributed-operating-systems"},{"location":"Distributed-computing/Theory/Consistency/Classification/Sequential-consistency/#csdn#sequential#consistency","text":"","title":"csdn \u4e00\u81f4\u6027\u6a21\u578b\u4e4bSequential Consistency"},{"location":"Distributed-computing/Theory/Consistency/Classification/Sequential-consistency/#sequential#consistency_1","text":"Sequential Consistency\u7684\u7cbe\u786e\u5b9a\u4e49\u6765\u81ea\u4e8eLeslie Lamport\u8001\u54e5(\u4ee5\u540e\u6211\u4eec\u4f1a\u591a\u6b21\u63d0\u5230\u4ed6)\u3002\u4ed6\u672c\u6765\u662f\u5b9a\u4e49\u4e86\u57fa\u4e8e\u5171\u4eab\u5185\u5b58\u7684\u591aCPU\u5e76\u884c\u8ba1\u7b97\u7684\u4e00\u81f4\u6027\u6a21\u578b\uff0c\u4f46\u662f\u4e5f\u53ef\u4ee5\u63a8\u5e7f\u5230\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\uff0c\u5b9e\u9645\u4e0a\u591aCPU\u5e76\u884c\u8ba1\u7b97\u4e5f\u90fd\u53ef\u4ee5\u8ba4\u4e3a\u662f\u5206\u5e03\u5f0f\u7cfb\u7edf\u3002\u6a21\u578b\u7684\u5b9a\u4e49\u662f: the result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program \u653e\u5230\u5206\u5e03\u5f0f\u7cfb\u7edf\u91cc\uff0c\u610f\u601d\u5c31\u662f\u4e0d\u7ba1\u7cfb\u7edf\u600e\u4e48\u8fd0\u884c\uff0c\u5f97\u5230\u7684\u7ed3\u679c\u5c31\u597d\u50cf\u628a\u6240\u6709\u8282\u70b9\u7684\u6240\u6709\u64cd\u4f5c\u6309\u7167\u67d0\u4e2asequential order\u6392\u5e8f\u540e\u8fd0\u884c\uff0c\u4f46\u662f\u5728\u8fd9\u4e2asequential order\u987a\u5e8f\u4e2d\uff0c\u6765\u81ea\u540c\u4e00\u4e2a\u8282\u70b9\u7684\u64cd\u4f5c\u4ecd\u7136\u4fdd\u6301\u7740\u5b83\u4eec\u5728\u8282\u70b9\u4e2d\u88ab\u6307\u5b9a\u7684\u987a\u5e8f\uff08\u4e5f\u5c31\u662f\u4ed6\u4eec\u5728program\u4e2d\u6307\u5b9a\u7684\u987a\u5e8f\uff09\u3002","title":"Sequential Consistency\u7684\u5b9a\u4e49"},{"location":"Distributed-computing/Theory/Consistency/Classification/Sequential-consistency/#sequential#consistency_2","text":"Leslie Lamport\u8001\u54e5\u7684\u8bf4\u6cd5\u4e00\u8d2f\u7684\u4f76\u5c48\u8071\u7259\uff0c\u6211\u4eec\u901a\u8fc7\u51e0\u4e2a\u4f8b\u5b50\u6765\u770b\u4e00\u4e0b\u3002\u56fe\u4e2d\u4ece\u5de6\u5411\u53f3\u8868\u793a\u7269\u7406\u65f6\u95f4\uff0cW(a)\u8868\u793a\u5199\u5165\u6570\u636ea\uff0cR(a)\u8868\u793a\u8bfb\u51fa\u6570\u636ea\u3002 \u53ef\u4ee5\u770b\u51fa\uff0c\u8fd9\u4e24\u4e2a\u7cfb\u7edf\u90fd\u4e0d\u662f\u5f88\u5b8c\u7f8e\uff0c\u4f46\u662f\u5b83\u4eec\u7684\u6a21\u578b\u90fd\u53ef\u4ee5\u770b\u505a**Sequential Consistency**\uff0c\u56e0\u4e3a\u901a\u8fc7\u5982\u4e0b\u53d8\u6362\uff0c\u603b\u662f\u53ef\u4ee5\u81ea\u5706\u5176\u8bf4\uff0c\u4e5f\u5c31\u662f\u53ef\u4ee5\u627e\u5230\u7b26\u5408\u5b9a\u4e49\u7684**sequential order**\u3002","title":"Sequential Consistency\u7684\u4f8b\u5b50"},{"location":"Distributed-computing/Theory/Consistency/Classification/Sequential-consistency/#sequential#consistency_3","text":"\u4e5f\u8bb8\u6709\u4eba\u4f1a\u95ee\uff0c\u540c\u4e00\u4e2a\u8fdb\u7a0b\u4e2d\u4fdd\u7559\u64cd\u4f5c\u987a\u5e8f\u4e0d\u662f\u663e\u800c\u6613\u89c1\u7684\u4e48?\u5b9e\u9645\u4e0a\u968f\u7740\u786c\u4ef6\u6280\u672f\uff0c\u5c24\u5176\u662f\u591a\u6838\u3001\u591aCPU\u6280\u672f\u7684\u53d1\u5c55\uff0c\u4e00\u4e2aCPU\u6838\u5fc3\u8fd0\u884c\u7684\u8fdb\u7a0b\uff0c\u4e0d\u4e00\u5b9a\u80fd\u89c2\u6d4b\u5230\u53e6\u4e00\u4e2a\u6838\u5fc3\u8fdb\u7a0b\u7684\u64cd\u4f5c\u987a\u5e8f\u3002 \u5728\u8bba\u6587\u4e2d\uff0cLeslie Lamport\u8001\u54e5\u4e3e\u4e86\u8fd9\u6837\u4e00\u4e2a\u4f8b\u5b50\uff0c\u6709\u4e00\u4e2a\u4e92\u65a5\u7b97\u6cd5\uff0c\u8981\u6c42\u4e24\u4e2a\u8fdb\u7a0b\u4e0d\u80fd\u540c\u65f6\u6267\u884c\u4e34\u754c\u533a\u65b9\u6cd5\uff0ca\u548cb\u4e24\u4e2a\u53d8\u91cf\u521d\u59cb\u503c\u4e3a0\u3002\u6b63\u5e38\u60c5\u51b5\u4e0b\uff0c\u6700\u591a\u4e00\u4e2a\u8fdb\u7a0b\u6267\u884c\u4e34\u754c\u533a\u65b9\u6cd5\u3002 \u8fdb\u7a0b1\u6267\u884c\u5e8f\u5217\u5982\u4e0b\uff1a a = 1 if (b!=0){ \u4e34\u754c\u533a\u65b9\u6cd5 } \u8fdb\u7a0b2\u6267\u884c\u5e8f\u5217\u5982\u4e0b\uff1a b = 1 if (a!=0){ \u4e34\u754c\u533a\u65b9\u6cd5 } \u8fd9\u4e2a\u7a0b\u5e8f\u5728\u591a\u6838CPU\u673a\u5668\u4e0a\u8fd0\u884c\u65f6\uff0c\u6709\u53ef\u80fd\u4e24\u4e2a\u8fdb\u7a0b\u540c\u65f6\u8fdb\u5165\u4e34\u754c\u533a\u3002\u4e3a\u4ec0\u4e48\u5462? \u6211\u4eec\u5148\u770b\u4e00\u4e0b\u73b0\u4ee3CPU\u7684\u67b6\u6784 CPU\u4e00\u822c\u5177\u6709\u591a\u4e2a\u6838\u5fc3\uff0c\u6bcf\u4e2a\u6838\u5fc3\u90fd\u6709\u81ea\u5df1\u7684L1 cache\u548cL2 cache\uff0ccache\u4e4b\u4e0a\u8fd8\u6709Load Buffer\u548cStore Buffer\u3002\u5199\u5165\u65f6\uff0c\u5904\u7406\u5668\u5f88\u6709\u53ef\u80fd\u4ec5\u4ec5\u5c06\u6570\u636e\u5199\u5165Store Buffer\uff0c\u7a0d\u540e\u518d\u5c06Store Buffer\u4e2d\u7684\u6570\u636e\u7edf\u4e00\u5199\u56decache\uff0c\u6709\u53ef\u80fd\u518d\u8fc7\u4e00\u4f1a\u513f\u624d\u5c06cache\u7684\u6570\u636e\u5199\u56de\u5185\u5b58\u3002\u540c\u6837\uff0c\u4e00\u4e2a\u6838\u5fc3\u8bfb\u53d6\u7684\u6570\u636e\u8bf4\u4e0d\u5b9a\u4e5f\u5df2\u7ecf\u88ab\u53e6\u4e00\u4e2a\u6838\u5fc3\u4fee\u6539\u8fc7\uff0c\u53ea\u662f\u5b83\u4e0d\u77e5\u9053\u800c\u5df2\u3002 \u6240\u4ee5\u4e0a\u8ff0\u8fdb\u7a0b\u5bf9a\u548cb\u7684\u8d4b\u503c\uff0c\u5f88\u6709\u53ef\u80fd\u6ca1\u88ab\u5bf9\u65b9\u611f\u77e5\u3002 \u4e3a\u4e86\u4fdd\u8bc1**Sequential Consistency**\uff0cLeslie Lamport\u8001\u54e5\u5728\u8bba\u6587\u4e2d\u63d0\u51fa\u4e86\u4e24\u4e2a\u8981\u6c42: Each processor issues memory requests in the order specified by its program Memory requests from all processors issued to an individual memory module are serviced from a single FIFO queue. Issuing a memory request consists of entering the request on this queue. \u4f46\u662f\u5982\u679c\u5728\u786c\u4ef6\u5c42\u6ee1\u8db3Sequential Consistency\uff0c\u80af\u5b9a\u4f1a\u5927\u5927\u964d\u4f4e\u6548\u7387\uff0c\u6240\u4ee5\u4e00\u822c\u8fd9\u4e9b\u5de5\u4f5c\u5c31\u4f1a\u4ea4\u7ed9\u4e0a\u5c42\u7684\u8f6f\u4ef6\u5f00\u53d1\u4eba\u5458\u6765\u505a\u3002","title":"Sequential Consistency\u548c\u786c\u4ef6"},{"location":"Distributed-computing/Theory/Consistency/Classification/Sequential-consistency/#case#study","text":"","title":"Case study"},{"location":"Distributed-computing/Theory/Consistency/Classification/Sequential-consistency/#case#zookeeper","text":"zookeeper\u627f\u8bfaSequential Consistency\uff0c\u5728 ZooKeeper overview#Guarantees \u4e2d\u5bf9\u6b64\u8fdb\u884c\u4e86\u8bf4\u660e: Sequential Consistency - Updates from a client will be applied in the order that they were sent.","title":"Case: zookeeper"},{"location":"Distributed-computing/Theory/Consistency/Classification/Sequential-consistency/#case#c#memory#model","text":"\u53c2\u89c1\u5de5\u7a0bprogramming-language\u7684 C++\\Language-reference\\Basic-concept\\Abstract-machine\\Memory-model \u7ae0\u8282\u3002","title":"Case: C++ memory model"},{"location":"Distributed-computing/Theory/Consistency/Classification/Strong-consistency/","text":"wikipedia Strong consistency Strong consistency is one of the consistency models used in the domain of the concurrent programming (e.g., in distributed shared memory , distributed transactions ). The protocol is said to support strong consistency if: All accesses are seen by all parallel processes (or nodes, processors, etc.) in the same order (sequentially) Therefore, only one consistent state can be observed, as opposed to weak consistency , where different parallel processes (or nodes, etc.) can perceive\uff08\u611f\u77e5\uff09 variables in different states.","title":"Introduction"},{"location":"Distributed-computing/Theory/Consistency/Classification/Strong-consistency/#wikipedia#strong#consistency","text":"Strong consistency is one of the consistency models used in the domain of the concurrent programming (e.g., in distributed shared memory , distributed transactions ). The protocol is said to support strong consistency if: All accesses are seen by all parallel processes (or nodes, processors, etc.) in the same order (sequentially) Therefore, only one consistent state can be observed, as opposed to weak consistency , where different parallel processes (or nodes, etc.) can perceive\uff08\u611f\u77e5\uff09 variables in different states.","title":"wikipedia Strong consistency"},{"location":"Distributed-computing/Theory/Consistency/Classification/Weak-consistency/","text":"Eventual consistency wikipedia Eventual consistency Example redis \u5728\u300a consistency-of-redis.md \u300b\u4e2d\u5bf9\u8fd9\u90e8\u5206\u5185\u5bb9\u8fdb\u884c\u4e86\u603b\u7ed3","title":"Introduction"},{"location":"Distributed-computing/Theory/Consistency/Classification/Weak-consistency/#eventual#consistency","text":"","title":"Eventual consistency"},{"location":"Distributed-computing/Theory/Consistency/Classification/Weak-consistency/#wikipedia#eventual#consistency","text":"","title":"wikipedia Eventual consistency"},{"location":"Distributed-computing/Theory/Consistency/Classification/Weak-consistency/#example","text":"","title":"Example"},{"location":"Distributed-computing/Theory/Consistency/Classification/Weak-consistency/#redis","text":"\u5728\u300a consistency-of-redis.md \u300b\u4e2d\u5bf9\u8fd9\u90e8\u5206\u5185\u5bb9\u8fdb\u884c\u4e86\u603b\u7ed3","title":"redis"},{"location":"Distributed-computing/Theory/Consistency/Classification/Weak-consistency/Causal-consistency/","text":"Causal consistency","title":"Introduction"},{"location":"Distributed-computing/Theory/Consistency/Classification/Weak-consistency/Causal-consistency/#causal#consistency","text":"","title":"Causal consistency"},{"location":"Distributed-computing/Theory/Consistency/Classification/Weak-consistency/Eventual-consistency/","text":"Eventual consistency wikipedia Eventual consistency Example redis \u5728\u300a consistency-of-redis.md \u300b\u4e2d\u5bf9\u8fd9\u90e8\u5206\u5185\u5bb9\u8fdb\u884c\u4e86\u603b\u7ed3","title":"Introduction"},{"location":"Distributed-computing/Theory/Consistency/Classification/Weak-consistency/Eventual-consistency/#eventual#consistency","text":"","title":"Eventual consistency"},{"location":"Distributed-computing/Theory/Consistency/Classification/Weak-consistency/Eventual-consistency/#wikipedia#eventual#consistency","text":"","title":"wikipedia Eventual consistency"},{"location":"Distributed-computing/Theory/Consistency/Classification/Weak-consistency/Eventual-consistency/#example","text":"","title":"Example"},{"location":"Distributed-computing/Theory/Consistency/Classification/Weak-consistency/Eventual-consistency/#redis","text":"\u5728\u300a consistency-of-redis.md \u300b\u4e2d\u5bf9\u8fd9\u90e8\u5206\u5185\u5bb9\u8fdb\u884c\u4e86\u603b\u7ed3","title":"redis"},{"location":"Distributed-computing/Theory/Consistency/Data-consistency/","text":"Data consistency \u8fd9\u662f\u5728\u9605\u8bfbwikipedia MVCC \u65f6\uff0c\u53d1\u73b0\u7684\u3002 wikipedia Data consistency","title":"Introduction"},{"location":"Distributed-computing/Theory/Consistency/Data-consistency/#data#consistency","text":"\u8fd9\u662f\u5728\u9605\u8bfbwikipedia MVCC \u65f6\uff0c\u53d1\u73b0\u7684\u3002","title":"Data consistency"},{"location":"Distributed-computing/Theory/Consistency/Data-consistency/#wikipedia#data#consistency","text":"","title":"wikipedia Data consistency"},{"location":"Distributed-computing/Theory/Consistency/NRW-quorum-consistency/","text":"NRW quorum consistency \u53c2\u8003\u6587\u7ae0 1\u3001zhihu \u5173\u4e8e\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684NWR quorum consistency NOTE: \u4e00\u3001\u6211\u662f\u5728\u8fd9\u7bc7\u6587\u7ae0\u4e2d\uff0c\u9996\u6b21\u53d1\u73b0NRW quorum consistency\u7684 \u4e8c\u3001\u5176\u5b9e\u8fd9\u7bc7\u6587\u7ae0\uff0c\u6c47\u603b\u4e86\u5904\u7406replication lag\u7684\u4e24\u79cd\u65b9\u6cd5: 1\u3001NRW 2\u3001read-your-own-writes \u5728\u8fd9\u7247\u6587\u7ae0\u91cc\u9762 https://www.allthingsdistributed.com/2008/12/eventually_consistent.html \u4f5c\u8005Werner Vogels\u7ed9\u4e86\u8fd9\u4e2aNWR\u7684\u5b9a\u4e49 N = the number of nodes that store replicas of the data W = the number of replicas that need to acknowledge the receipt of the update before the update completes R = the number of replicas that are contacted when a data object is accessed through a read operation N \u5b58\u50a8\u4e86\u6570\u636e\u526f\u672c\u7684\u8282\u70b9 W \u5728\u66f4\u65b0\u5b8c\u6210\u4e4b\u524d\u9700\u8981\u66f4\u65b0\u7684\u526f\u672c\u6570\u91cf R \u5728\u8bfb\u64cd\u4f5c\u4e2d\u9700\u8981\u8054\u7cfb\u7684\u526f\u672c\u6570\u91cf \u5e76\u4e14 \u7ed9\u51fa\u4e86\u7ed3\u8bba If W+R > N, then the write set and the read set always overlap and one can guarantee strong consistency. \u5982\u679cW+R >N \uff0c \u90a3\u4e48\u5199\u7684\u8fd9\u4e2a\u8282\u70b9\u96c6\u5408\u548c\u8bfb\u7684\u8282\u70b9\u96c6\u5408\u80af\u5b9a\u4f1a\u91cd\u5408\uff08\u5c0f\u5b66\u65f6\u5019\u5b66\u4e60\u7684\u62bd\u5c49\u539f\u7406\uff09\uff0c\u80af\u5b9a\u80fd\u4fdd\u8bc1\u5f3a\u4e00\u81f4\u3002 \u5bf9\u4e0a\u9762\u8fd9\u4e2a\u573a\u666f\u7a0d\u7a0d\u6539\u4e00\u4e0b\uff0c\u53d8\u6210\u4e00\u4e2a\u6211\u4eec\u53f8\u7a7a\u89c1\u60ef\u7684\u4e00\u4e2a\u573a\u666f\u3002\u8fd9\u4e2aDB NODE A\u662f\u4e00\u4e2amaster\u8282\u70b9\uff0cDB NODE B\u662f\u4e00\u4e2aslave\u8282\u70b9\u3002\u4e00\u4e2a\u901a\u7528\u7684\u505a\u6cd5\u662f\u6211\u4eec\u53eawrite master\uff0c\u901a\u8fc7master-slave\u4e4b\u95f4\u7684\u5f02\u6b65\u540c\u6b65\u673a\u5236\u5c06\u6570\u636ecopy\u5230slave\u4e0a\uff0c\u7136\u540e\u6211\u4eec\u518dread slave\uff0c\u6839\u636e\u6211\u4eec\u4e0a\u9762\u7684\u5206\u6790\u6765\u770b\u786e\u5b9e\u662f\u4e0d\u80fd\u4fdd\u8bc1read\u7684\u6570\u636e\u4e00\u5b9a\u662f\u6700\u65b0\u7684\uff0c\u5f88\u53ef\u80fd\u7684\u539f\u56e0\u5c31\u662fmaster\u8282\u70b9\u5728\u540c\u6b65\u8fd9\u4e2adata\u6570\u636e\u5230slave\u7684\u65f6\u5019\u6709\u4e86\u65f6\u5ef6\uff0c\u8fd9\u4e2a\u95ee\u9898\u5728\u5b66\u672f\u754c\u4e2d\u4e5f\u662f\u6709\u7814\u7a76\u7684\uff0c\u53ebProblems with Replication Lag \u4e3a\u4e86\u4fdd\u8bc1\u5f3a\u4e00\u81f4\uff0c\u4fdd\u8bc1\u6211\u4eec\u8bfb\u7684\u6570\u636e\u662f\u6700\u65b0\u7684\uff0c\u90a3\u6211\u4eec\u5199master DB\u4e4b\u540e\u518d\u8bfb master DB\u4e0d\u5c31\u5b8c\u4e86\u5417\uff1f \u5bf9\uff0c\u6ca1\u9519\uff0c\u662f\u53ef\u4ee5\u8fd9\u4e48\u5e72 write\u7684\u65f6\u5019\u5199master DB \uff0c read\u7684\u65f6\u5019\u8bfb master DB \uff0c\u8fd9\u4e2a\u65f6\u5019\u662f\u53ef\u4ee5\u4fdd\u8bc1\u5f3a\u4e00\u81f4\u7684\u3002\u5176\u5b9e\u8fd9\u4e2a\u662f\u6709\u4e00\u4e2a\u7406\u8bba\u4f9d\u636e\uff0c\u53ebread-your-own-writes.\u5c31\u662f\u6211\u53ea\u8bfb\u6211\u5199\u8fc7\u7684\u90a3\u4e2a\u8282\u70b9\uff0c\u8fd9\u4e2a\u4e5f\u662f\u89e3\u51b3Replication Lag\u7684\u4e00\u4e2a\u65b9\u6cd5\u3002 NOTE: \u4f7f\u7528 read-your-own-writes \u6765\u89e3\u51b3 Replication Lag\u95ee\u9898 2\u3001zhihu NWR\u6a21\u578b\u4e0b\u7684\u4e00\u81f4\u6027\u95ee\u9898 NOTE: \u6ca1\u6709\u6df1\u5165\u9605\u8bfb\uff0c\u5176\u4e2d\u7ed3\u5408Dynamo\u6765\u8fdb\u884c\u5206\u6790\u7684 3\u3001 \u592a\u4e0a\u8001\u541b\u7684\u70bc\u4e39\u7089\u4e4b\u5206\u5e03\u5f0f Quorum NWR NOTE: \u8fd9\u7bc7\u6587\u7ae0\uff0c\u57fa\u672c\u4e0a\u8bb2\u6e05\u695a\u4e86 Quorum NWR\uff0c\u5df2\u7ecf\u6536\u5f55\u4e86 4\u3001geeksforgeeks Quorum Consistency in Cassandra \u62bd\u5c49\u539f\u7406 \u53ef\u4ee5\u4f7f\u7528\u62bd\u5c49\u539f\u7406\u6765\u9a8c\u8bc1NRW quorum\u7684\u6b63\u786e\u6027\u3002","title":"Introduction"},{"location":"Distributed-computing/Theory/Consistency/NRW-quorum-consistency/#nrw#quorum#consistency","text":"","title":"NRW quorum consistency"},{"location":"Distributed-computing/Theory/Consistency/NRW-quorum-consistency/#_1","text":"","title":"\u53c2\u8003\u6587\u7ae0"},{"location":"Distributed-computing/Theory/Consistency/NRW-quorum-consistency/#1zhihu#nwr#quorum#consistency","text":"NOTE: \u4e00\u3001\u6211\u662f\u5728\u8fd9\u7bc7\u6587\u7ae0\u4e2d\uff0c\u9996\u6b21\u53d1\u73b0NRW quorum consistency\u7684 \u4e8c\u3001\u5176\u5b9e\u8fd9\u7bc7\u6587\u7ae0\uff0c\u6c47\u603b\u4e86\u5904\u7406replication lag\u7684\u4e24\u79cd\u65b9\u6cd5: 1\u3001NRW 2\u3001read-your-own-writes \u5728\u8fd9\u7247\u6587\u7ae0\u91cc\u9762 https://www.allthingsdistributed.com/2008/12/eventually_consistent.html \u4f5c\u8005Werner Vogels\u7ed9\u4e86\u8fd9\u4e2aNWR\u7684\u5b9a\u4e49 N = the number of nodes that store replicas of the data W = the number of replicas that need to acknowledge the receipt of the update before the update completes R = the number of replicas that are contacted when a data object is accessed through a read operation N \u5b58\u50a8\u4e86\u6570\u636e\u526f\u672c\u7684\u8282\u70b9 W \u5728\u66f4\u65b0\u5b8c\u6210\u4e4b\u524d\u9700\u8981\u66f4\u65b0\u7684\u526f\u672c\u6570\u91cf R \u5728\u8bfb\u64cd\u4f5c\u4e2d\u9700\u8981\u8054\u7cfb\u7684\u526f\u672c\u6570\u91cf \u5e76\u4e14 \u7ed9\u51fa\u4e86\u7ed3\u8bba If W+R > N, then the write set and the read set always overlap and one can guarantee strong consistency. \u5982\u679cW+R >N \uff0c \u90a3\u4e48\u5199\u7684\u8fd9\u4e2a\u8282\u70b9\u96c6\u5408\u548c\u8bfb\u7684\u8282\u70b9\u96c6\u5408\u80af\u5b9a\u4f1a\u91cd\u5408\uff08\u5c0f\u5b66\u65f6\u5019\u5b66\u4e60\u7684\u62bd\u5c49\u539f\u7406\uff09\uff0c\u80af\u5b9a\u80fd\u4fdd\u8bc1\u5f3a\u4e00\u81f4\u3002 \u5bf9\u4e0a\u9762\u8fd9\u4e2a\u573a\u666f\u7a0d\u7a0d\u6539\u4e00\u4e0b\uff0c\u53d8\u6210\u4e00\u4e2a\u6211\u4eec\u53f8\u7a7a\u89c1\u60ef\u7684\u4e00\u4e2a\u573a\u666f\u3002\u8fd9\u4e2aDB NODE A\u662f\u4e00\u4e2amaster\u8282\u70b9\uff0cDB NODE B\u662f\u4e00\u4e2aslave\u8282\u70b9\u3002\u4e00\u4e2a\u901a\u7528\u7684\u505a\u6cd5\u662f\u6211\u4eec\u53eawrite master\uff0c\u901a\u8fc7master-slave\u4e4b\u95f4\u7684\u5f02\u6b65\u540c\u6b65\u673a\u5236\u5c06\u6570\u636ecopy\u5230slave\u4e0a\uff0c\u7136\u540e\u6211\u4eec\u518dread slave\uff0c\u6839\u636e\u6211\u4eec\u4e0a\u9762\u7684\u5206\u6790\u6765\u770b\u786e\u5b9e\u662f\u4e0d\u80fd\u4fdd\u8bc1read\u7684\u6570\u636e\u4e00\u5b9a\u662f\u6700\u65b0\u7684\uff0c\u5f88\u53ef\u80fd\u7684\u539f\u56e0\u5c31\u662fmaster\u8282\u70b9\u5728\u540c\u6b65\u8fd9\u4e2adata\u6570\u636e\u5230slave\u7684\u65f6\u5019\u6709\u4e86\u65f6\u5ef6\uff0c\u8fd9\u4e2a\u95ee\u9898\u5728\u5b66\u672f\u754c\u4e2d\u4e5f\u662f\u6709\u7814\u7a76\u7684\uff0c\u53ebProblems with Replication Lag \u4e3a\u4e86\u4fdd\u8bc1\u5f3a\u4e00\u81f4\uff0c\u4fdd\u8bc1\u6211\u4eec\u8bfb\u7684\u6570\u636e\u662f\u6700\u65b0\u7684\uff0c\u90a3\u6211\u4eec\u5199master DB\u4e4b\u540e\u518d\u8bfb master DB\u4e0d\u5c31\u5b8c\u4e86\u5417\uff1f \u5bf9\uff0c\u6ca1\u9519\uff0c\u662f\u53ef\u4ee5\u8fd9\u4e48\u5e72 write\u7684\u65f6\u5019\u5199master DB \uff0c read\u7684\u65f6\u5019\u8bfb master DB \uff0c\u8fd9\u4e2a\u65f6\u5019\u662f\u53ef\u4ee5\u4fdd\u8bc1\u5f3a\u4e00\u81f4\u7684\u3002\u5176\u5b9e\u8fd9\u4e2a\u662f\u6709\u4e00\u4e2a\u7406\u8bba\u4f9d\u636e\uff0c\u53ebread-your-own-writes.\u5c31\u662f\u6211\u53ea\u8bfb\u6211\u5199\u8fc7\u7684\u90a3\u4e2a\u8282\u70b9\uff0c\u8fd9\u4e2a\u4e5f\u662f\u89e3\u51b3Replication Lag\u7684\u4e00\u4e2a\u65b9\u6cd5\u3002 NOTE: \u4f7f\u7528 read-your-own-writes \u6765\u89e3\u51b3 Replication Lag\u95ee\u9898","title":"1\u3001zhihu \u5173\u4e8e\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684NWR quorum consistency"},{"location":"Distributed-computing/Theory/Consistency/NRW-quorum-consistency/#2zhihu#nwr","text":"NOTE: \u6ca1\u6709\u6df1\u5165\u9605\u8bfb\uff0c\u5176\u4e2d\u7ed3\u5408Dynamo\u6765\u8fdb\u884c\u5206\u6790\u7684","title":"2\u3001zhihu NWR\u6a21\u578b\u4e0b\u7684\u4e00\u81f4\u6027\u95ee\u9898"},{"location":"Distributed-computing/Theory/Consistency/NRW-quorum-consistency/#3#quorum#nwr","text":"NOTE: \u8fd9\u7bc7\u6587\u7ae0\uff0c\u57fa\u672c\u4e0a\u8bb2\u6e05\u695a\u4e86 Quorum NWR\uff0c\u5df2\u7ecf\u6536\u5f55\u4e86","title":"3\u3001\u592a\u4e0a\u8001\u541b\u7684\u70bc\u4e39\u7089\u4e4b\u5206\u5e03\u5f0f Quorum NWR"},{"location":"Distributed-computing/Theory/Consistency/NRW-quorum-consistency/#4geeksforgeeks#quorum#consistency#in#cassandra","text":"","title":"4\u3001geeksforgeeks Quorum Consistency in Cassandra"},{"location":"Distributed-computing/Theory/Consistency/NRW-quorum-consistency/#_2","text":"\u53ef\u4ee5\u4f7f\u7528\u62bd\u5c49\u539f\u7406\u6765\u9a8c\u8bc1NRW quorum\u7684\u6b63\u786e\u6027\u3002","title":"\u62bd\u5c49\u539f\u7406"},{"location":"Distributed-computing/Theory/Consistency/Primer/","text":"Primer \u5982\u4f55\u638c\u63e1consistency model? Consistency model\u662f\u5173\u4e8edata/memory\u7684model \u5bf9data/memory\u7684operation\u5305\u62ec 1\u3001read 2\u3001write Consistency model\u662f\u4e00\u4e2a\u975e\u5e38\u5f3a\u5927\u7684\u3001\u9002\u7528\u9762\u975e\u5e38\u5e7f\u6cdb\u7684model \u5b83\u80fd\u591f\u5bf9multiple model\u4e2d\u7684shared data\u8fdb\u884c\u63cf\u8ff0\uff0cmultiple model\u80fd\u591f\u63cf\u8ff0\u975e\u5e38\u591a\u7684\uff0c\u4e0b\u9762\u53ef\u4ee5\u770b\u5230: Multiple model Multiple model\u4e2d\u7684entity\u53ef\u4ee5\u4e3a: 1\u3001multicore: processor core 2\u3001multithread: thread 3\u3001multiprocess: process 4\u3001distributed computing: node \u56e0\u6b64\u4e0b\u9762\u7684\u63cf\u8ff0\u7684\u5185\u5bb9\uff0c\u6709\u7684\u65f6\u5019\u5e76\u4e0d\u9488\u5bf9\u4e0a\u8ff0\u60c5\u51b5\u4e2d\u7684\u4e00\u79cd\uff0c\u800c\u662f\u63cf\u8ff0\u7684\u4e00\u79cd\u901a\u7528\u7684\u539f\u7406\u3002 \u7406\u60f3 \u4e0e \u73b0\u5b9e \u73b0\u5b9e : \u5404\u4e2aentity\u4e4b\u95f4\u5b58\u5728\u7740**\u901a\u4fe1\u5ef6\u65f6**( delay )\uff0c\u4e00\u65e6entity\u5bf9\u81ea\u5df1\u7684**copy of shared data**\u505a\u51fa\u4e86write\uff0c\u5728\u5b8c\u6210\u548c\u5176\u4ed6\u7684entity\u7684**synchronization**\u4e4b\u524d( \u540c\u6b65\u5ef6\u65f6 )\uff0c\u5404\u4e2aentity\u7684**copy of shared data**\u662f\u5b58\u5728\u7740\u5dee\u5f02\u7684\uff0c\u5373\u5404\u4e2aentity\u6240\u770b\u5230\u7684**shared data**\u662f\u4e0d\u540c\u7684\u3002 NOTE: replication lag \u7406\u60f3 : \u5404\u4e2aentity\u4e4b\u95f4\u7684**\u901a\u4fe1\u5ef6\u65f6**\u4e3a0\uff0c\u6216\u8005\u8bf4\uff0c\u5b83\u4eec\u4e4b\u95f4\u4e0d\u5b58\u5728\u5ef6\u65f6\uff0c\u8fd9\u6837\u5c31\u4e0d\u5b58\u5728**\u540c\u6b65\u5ef6\u65f6**\uff0c\u8fd9\u6837\u4efb\u4f55\u4e00\u4e2aentity\u5bf9\u81ea\u5df1\u7684copy of shared data\u7684\u4fee\u6539\u80fd\u591f\u7acb\u5373\u540c\u6b65\u5230\u5176\u4ed6\u7684entity\u4e2d\uff0c\u5404\u4e2aentity\u7684**copy of shared data**\u662f\u4e0d\u5b58\u5728\u5dee\u5f02\u7684\uff0c\u5373\u5404\u4e2aentity\u6240\u770b\u5230\u7684**shared data**\u662f\u76f8\u540c\u7684\u3002\u5176\u5b9e\u8fd9\u5c31\u662f**strict consistency**\u3002 Consistency model abstract machine \u4e3a\u4e86\u4fbf\u4e8e\u7406\u89e3\u5404\u79cdconsistency model\uff0c\u672c\u8282\u63d0\u51fa\u4e00\u4e2aconsistency model abstract machine\uff0c\u5b83\u5176\u5b9e\u5c31\u662fprogrammer\u7ad9\u5728\u5168\u5c40\u7684\u89c6\u89d2\u6765\u770b\u5f85\u5e76\u8fd0\u884c\u6574\u4e2amultiple model\uff0c\u4ece\u800c\u5f97\u5230\u8f93\u51fa\uff0c\u5b83\u7684\u7b80\u5355\u5b9a\u4e49\u5982\u4e0b: 1\u3001\u6240\u6709\u7684entity\u90fd\u72ec\u7acb\u5730\u8fd0\u884c\u5728\u8fd9\u53f0abstract machine\u4e0a NOTE: \u53ef\u4ee5\u8ba4\u4e3a\u6bcf\u4e2aentity\u6709\u4e00\u4e2a\u81ea\u5df1\u7684processor 2\u3001shared data NOTE: \u53ef\u4ee5\u8ba4\u4e3a\u662fmemory 3\u3001\u6bcf\u4e2aentity\u90fd\u6709\u81ea\u5df1\u7684copy of shared data NOTE: \u53ef\u4ee5\u8ba4\u4e3a\u6bcf\u4e2aentity\u90fd\u5168\u91cf\u7684cache shared data 4\u3001\u6bcf\u4e2aentity\u9700\u8981\u5c06\u81ea\u5df1\u5bf9shared data\u7684\u66f4\u6539\u540c\u6b65\u5230shared data\u4e2d 5\u3001 \u5168\u5c40\u65f6\u949f \u3001abstract machine\u7684\u65f6\u949f\uff0c\u5176\u5b9e\u5c31\u662f\u5899\u4e0a\u65f6\u949f\uff0cprogrammer\u4ee5\u8fd9\u4e2a\u65f6\u949f\u4e3a\u53c2\u8003\uff0c\u6765\u770babstract machine\u4e0a\u7684\u5404\u4e2aentity\uff0c\u5176\u5b9e\u5c31\u662f\u4eceprogrammer\u7684\u89d2\u5ea6\u6765\u770b\u5404\u4e2aentity\u7684\u6d3b\u52a8\u3002\u8fd9\u4f4dprogrammer\u63d0\u4f9b\u4e86\u89c2\u5bdf\u6574\u4e2a\u7cfb\u7edf\u7684\u89c6\u89d2\u3002 \u6a21\u62dfstrict consistency model **Strict consistency model**\u662f\u6700\u6700\u7406\u60f3\u7684\u60c5\u51b5\uff0c\u540c\u65f6\u5b83\u4e5f\u662f\u975e\u5e38\u5bb9\u6613\u7406\u89e3\u7684\u3002 \u5728strict consistency\u4e2d\uff0c\u7531\u4e8e: 1\u3001 \u6240\u6709\u7684write\u90fd\u7acb\u5373\u751f\u6548 (\u5728\u5b9e\u9645\u4e2d\uff0c\u53ef\u80fd\u65e0\u6cd5\u8fbe\u6210) 2\u3001entity\u7684copy of shared data\u80fd\u591f\u4e00\u76f4\u4fdd\u6301\u76f8\u540c \u56e0\u6b64**Strict consistency model**\u53ef\u4ee5\u8fd9\u6837\u6a21\u62df: \u6240\u6709\u7684**entity**\u90fd\u5728\u540c\u4e00\u4e2aprocessor\u4e0a\u6267\u884c\uff0c\u4f7f\u7528\u540c\u4e00\u4e2ashared data\uff0c\u8fd9\u4e2aprocessor\u6ca1\u6709cache\uff0c\u6240\u6709\u7684write\u7acb\u5373\u751f\u6548\u3002 Consistency model\u662fcontract 1\u3001\u5b83\u662f\u4e00\u79cdcontract\uff0c\u9075\u5faa\u4e00\u5b9a\u7684contract\uff0c\u5219\u53ef\u4ee5\u5b9e\u73b0\u9884\u671f\u7684\u6548\u679c\uff1b Level and tradeoff NOTE: \u6709\u54ea\u4e9bconstrain\u3001\u76f8\u8f83\u4e8e\u4e0a\u4e00\u5c42\uff0c\u653e\u677e\u4e86\u54ea\u4e9bconstrain\u3002 2\u3001\u6bcf\u79cdconsistency model\u90fd\u6709constrain\uff0c\u4ece\u800c\u51b3\u5b9a\u4e86consistency model\u7684strong\u3001weak 1\u3001strict consistency\u662f\u6700\u6700\u7406\u60f3\u7684\u3001\u5bb9\u6613\u7406\u89e3\u60c5\u51b5\uff0c\u53ef\u4ee5\u4ee5\u5b83\u6765\u4f5c\u4e3a\u5bf9\u6bd4 2\u3001\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\uff0cstrict consistency\u7684\u5b9e\u73b0\u662f\u6bd4\u8f83\u56f0\u96be\u7684\uff0c\u5e76\u4e14\u6709\u7684application\u4e0d\u9700\u8981strict consistency\uff0c\u56e0\u6b64\u53ef\u4ee5\u653e\u5bbdconstrain\uff0c\u4f7f\u7528weak consistency model 3\u3001programmer\u9700\u8981\u8fdb\u884ctradeoff csdn \u5f3a\u4e00\u81f4\u6027\u3001\u987a\u5e8f\u4e00\u81f4\u6027\u3001\u5f31\u4e00\u81f4\u6027\u548c\u5171\u8bc6 NOTE: \u6bd4\u8f83\u96be\u4ee5\u7406\u89e3\u7684: \"\u7cfb\u7edf\u4e2d\u8fdb\u7a0b\u770b\u5230\u7684\u987a\u5e8f\"\u548c \"\u5168\u5c40\u65f6\u949f\u4e0b\u7684\u987a\u5e8f\" \u56e0\u4e3a\u7cfb\u7edf\u4e2d\u7684\u6bcf\u4e2anode\u90fd\u6709\u4e00\u4efdcopy\uff0c\u56e0\u6b64\u5b83\u4eec\u5404\u81ea\u5bf9\u81ea\u5df1\u7684copy\u90fd\u8fdb\u884c\u4e86read\u3001write\uff0c\u56e0\u6b64\u6bcf\u4e2anode\u90fd\u6709\u81ea\u5df1\u7684\u5b9e\u9645\u64cd\u4f5c\u7684\u987a\u5e8f\uff0c\u8fd9\u5c31\u662f\u6240\u8c13\"\u7cfb\u7edf\u4e2d\u8fdb\u7a0b\u770b\u5230\u7684\u987a\u5e8f\"\uff1b \"\u5168\u5c40\u65f6\u949f\u4e0b\u7684\u987a\u5e8f\"\u5219\u662f\u6307\u7406\u60f3\u7684\u72b6\u6001(\u987a\u5e8f)\uff0c\u8fd9\u79cd\u7406\u60f3\u72b6\u6001\u53ef\u4ee5\u8fd9\u6837\u6765\u6a21\u62df: \u7531\u4e8e\u6240\u6709\u7684node share data\uff0c\u6240\u6709\u7684node\u90fd\u662f\u5728\u5bf9shared data\u8fdb\u884coperation(read\u3001write)\uff0c\u56e0\u6b64\u53ef\u4ee5\u8ba4\u4e3a\u6240\u6709\u7684node\u90fd\u5728\u540c\u4e00\u4e2aprocessor\u4e0a\u6267\u884c\uff0c\u663e\u7136\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5b83\u4eec\u5c31\u6709\u4e00\u4e2a\u5168\u5c40\u65f6\u949f\u4e86\uff0c\u5728\u8fd9\u4e2a\u7406\u60f3\u7684\u72b6\u6001(\u987a\u5e8f)\u4e0b\uff0c\u6240\u6709\u7684write\u90fd\u7acb\u5373\u751f\u6548(\u5728\u5b9e\u9645\u4e2d\uff0c\u53ef\u80fd\u65e0\u6cd5\u8fbe\u6210)\uff0c\u5c31\u597d\u6bd4\u5728\u540c\u4e00\u4e2aprocessor\uff0c\u8fd9\u5c31\u662f\"\u5f3a\u4e00\u81f4\u6027\uff08Strong Consistency\uff09\"\u3002 \u7531\u4e8e\u7cfb\u7edf\u4e2d\u7684\u6bcf\u4e2anode\u90fd\u6709\u4e00\u4efdcopy\uff0cchange\u5728\u5b83\u4eec\u4e4b\u95f4\u540c\u6b65\u662f\u9700\u8981\u65f6\u95f4\u7684\u3002 \u4e0d\u662f\u6240\u6709\u7684consistency model\u90fd\u53c2\u8003\u4e86**\u5168\u5c40\u65f6\u949f**\u7684\uff0c\"Sequential Consistency\"\u5c31\u6ca1\u6709\u53c2\u8003\"\u5168\u5c40\u65f6\u949f\"\uff0c\u8c8c\u4f3c\u53ea\u6709strong consistency\u624d\u53c2\u8003\u4e86\"\u5168\u5c40\u65f6\u949f\"\u3002Sequential Consistency\u8c8c\u4f3c\u4ec5\u4ec5\u5f3a\u8c03sequence\uff1b 1. \u4e00\u81f4\u6027\uff08Consistency\uff09 \u4e00\u81f4\u6027\uff08Consistency\uff09\u662f\u6307\u591a\u526f\u672c\uff08Replications\uff09\u95ee\u9898\u4e2d\u7684\u6570\u636e\u4e00\u81f4\u6027\u3002\u53ef\u4ee5\u5206\u4e3a\u5f3a\u4e00\u81f4\u6027\u3001\u987a\u5e8f\u4e00\u81f4\u6027\u4e0e\u5f31\u4e00\u81f4\u6027\u3002 1.1 \u5f3a\u4e00\u81f4\u6027\uff08Strong Consistency\uff09 \u4e5f\u79f0\u4e3a\uff1a \u539f\u5b50\u4e00\u81f4\u6027\uff08Atomic Consistency\uff09 \u7ebf\u6027\u4e00\u81f4\u6027\uff08Linearizable Consistency\uff09 \u4e24\u4e2a\u8981\u6c42\uff1a \u4efb\u4f55\u4e00\u6b21\u8bfb\u90fd\u80fd\u8bfb\u5230\u67d0\u4e2a\u6570\u636e\u7684\u6700\u8fd1\u4e00\u6b21\u5199\u7684\u6570\u636e\u3002 \u7cfb\u7edf\u4e2d\u7684\u6240\u6709\u8fdb\u7a0b\uff0c\u770b\u5230\u7684\u64cd\u4f5c\u987a\u5e8f\uff0c\u90fd\u548c**\u5168\u5c40\u65f6\u949f\u4e0b\u7684\u987a\u5e8f\u4e00\u81f4**\u3002 \u7b80\u8a00\u4e4b\uff0c\u5728\u4efb\u610f\u65f6\u523b\uff0c\u6240\u6709\u8282\u70b9\u4e2d\u7684\u6570\u636e\u662f\u4e00\u6837\u7684\u3002 \u4f8b\u5982\uff0c\u5bf9\u4e8e\u5173\u7cfb\u578b\u6570\u636e\u5e93\uff0c\u8981\u6c42\u66f4\u65b0\u8fc7\u7684\u6570\u636e\u80fd\u88ab\u540e\u7eed\u7684\u8bbf\u95ee\u90fd\u80fd\u770b\u5230\uff0c\u8fd9\u662f\u5f3a\u4e00\u81f4\u6027\u3002 NOTE: 1.2 \u987a\u5e8f\u4e00\u81f4\u6027\uff08Sequential Consistency\uff09 the result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program. - - Lamport \u4e24\u4e2a\u8981\u6c42\uff1a \u4efb\u4f55\u4e00\u6b21\u8bfb\u90fd\u80fd\u8bfb\u5230\u67d0\u4e2a\u6570\u636e\u7684\u6700\u8fd1\u4e00\u6b21\u5199\u7684\u6570\u636e\u3002 \u7cfb\u7edf\u7684\u6240\u6709\u8fdb\u7a0b\u7684\u987a\u5e8f\u4e00\u81f4\uff0c\u800c\u4e14\u662f\u5408\u7406\u7684\u3002\u5373\u4e0d\u9700\u8981\u548c\u5168\u5c40\u65f6\u949f\u4e0b\u7684\u987a\u5e8f\u4e00\u81f4\uff0c\u9519\u7684\u8bdd\u4e00\u8d77\u9519\uff0c\u5bf9\u7684\u8bdd\u4e00\u8d77\u5bf9\u3002 \u4e3e\u4e2a\u6817\u5b50\uff1a NOTE: \u4e0a\u56fe\u5176\u5b9e\u662f\u4e00\u4e2aspace-time diagram\uff0c\u6a2a\u8f74\u8868\u793a\u7684\u5168\u5c40\u65f6\u949f; \u4e0b\u9762\u662f\u6a21\u62df\u7a0b\u5e8f: Global int x = 0 , y = 0 ; Process 1 Process 2 x = 4 ; y = 2 cout << y ; cout << x ; Write(x, 4)\uff1a\u5199\u5165x=4 Read(x, 0)\uff1a\u8bfb\u51fax=0 1\uff09\u56fea\u662f\u6ee1\u8db3**\u987a\u5e8f\u4e00\u81f4\u6027**\uff0c\u4f46\u662f\u4e0d\u6ee1\u8db3**\u5f3a\u4e00\u81f4\u6027**\u7684\u3002\u539f\u56e0\u5728\u4e8e\uff0c\u4ece**\u5168\u5c40\u65f6\u949f**\u7684\u89c2\u70b9\u6765\u770b\uff0cP2\u8fdb\u7a0b\u5bf9\u53d8\u91cf X \u7684\u8bfb\u64cd\u4f5c\u5728P1\u8fdb\u7a0b\u5bf9\u53d8\u91cfX\u7684\u5199\u64cd\u4f5c\u4e4b\u540e\uff0c\u7136\u800c\u8bfb\u51fa\u6765\u7684\u5374\u662f\u65e7\u7684\u6570\u636e\u3002\u4f46\u662f\u8fd9\u4e2a\u56fe\u5374\u662f\u6ee1\u8db3**\u987a\u5e8f\u4e00\u81f4\u6027**\u7684\uff0c\u56e0\u4e3a\u4e24\u4e2a\u8fdb\u7a0bP1\uff0cP2\u7684\u4e00\u81f4\u6027\u5e76\u6ca1\u6709\u51b2\u7a81\u3002\u4ece\u8fd9\u4e24\u4e2a\u8fdb\u7a0b\u7684\u89d2\u5ea6\u6765\u770b\uff0c**\u987a\u5e8f**\u5e94\u8be5\u662f\u8fd9\u6837\u7684\uff1aWrite(y,2) , Read(x,0) , Write(x,4), Read(y,2)\uff0c\u6bcf\u4e2a\u8fdb\u7a0b\u5185\u90e8\u7684\u8bfb\u5199\u987a\u5e8f\u90fd\u662f\u5408\u7406\u7684\uff0c\u4f46\u662f\u8fd9\u4e2a\u987a\u5e8f\u4e0e**\u5168\u5c40\u65f6\u949f**\u4e0b\u770b\u5230\u7684\u987a\u5e8f\u5e76\u4e0d\u4e00\u6837\u3002 NOTE: \u8f93\u51fa\u4e3a 02 ; \"\u4ece**\u5168\u5c40\u65f6\u949f**\u7684\u89c2\u70b9\u6765\u770b\uff0cP2\u8fdb\u7a0b\u5bf9\u53d8\u91cf X \u7684\u8bfb\u64cd\u4f5c\u5728P1\u8fdb\u7a0b\u5bf9\u53d8\u91cfX\u7684\u5199\u64cd\u4f5c\u4e4b\u540e\uff0c\u7136\u800c\u8bfb\u51fa\u6765\u7684\u5374\u662f\u65e7\u7684\u6570\u636e\"\u7684\u53ef\u80fd\u65b9\u5f0f\u7684: Write(x, 4) \u7531\u4e8e\u7f51\u7edc\u5ef6\u65f6\uff0c\u5bfc\u81f4\u66f4\u65b0\u7684\u6570\u636e\u6ca1\u6709\u53ca\u65f6\u540c\u6b65\u5230P2\u4e2d\uff0c\u56e0\u6b64P2\u8bfb\u51fa\u7684\u662f\u65e7\u503c \u6bcf\u4e2aP\u4e2d\uff0cload\u548cstore\u662f\u7ef4\u6301\u7684\uff1b \u4e0a\u8ff0 \"\u4ece\u8fd9\u4e24\u4e2a\u8fdb\u7a0b\u7684\u89d2\u5ea6\u6765\u770b\uff0c**\u987a\u5e8f**\u5e94\u8be5\u662f\u8fd9\u6837\u7684\uff1aWrite(y,2) , Read(x,0) , Write(x,4), Read(y,2)\"\uff0c\u5176\u4e2d\u5f97\u51fa\u7684\u987a\u5e8f\u5176\u5b9e\u4e5f\u662f\u5c06\u4e24\u4e2aprocess\u6a21\u62df\u5230\u540c\u4e00\u4e2aprocessor\u4e0a\u5f97\u51fa\u7684\uff0c\u51c6\u786e\u6765\u8bf4\uff0c\u4f7f\u7528C++11 std::atomic \u7684 sequential consistency\u6765\u8fdb\u884c\u6a21\u62df\uff0c\u53c2\u89c1 \"introduction\" \u7ae0\u8282\uff1b 2\uff09\u56feb\u6ee1\u8db3**\u5f3a\u4e00\u81f4\u6027**\uff0c\u56e0\u4e3a\u6bcf\u4e2a\u8bfb\u64cd\u4f5c\u90fd\u8bfb\u5230\u4e86\u8be5\u53d8\u91cf\u7684\u6700\u65b0\u5199\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u4e24\u4e2a\u8fdb\u7a0b\u770b\u5230\u7684\u64cd\u4f5c\u987a\u5e8f\u4e0e\u5168\u5c40\u65f6\u949f\u7684\u987a\u5e8f\u4e00\u6837\uff0c\u90fd\u662fWrite(y,2) , Read(x,4) , Write(x,4), Read(y,2)\u3002 NOTE: \"Write(y,2) , Read(x,4) , Write(x,4), Read(y,2)\"\u662f\u6709\u8bef\u7684 \uff0c\u5e94\u8be5\u662f\"Write(y,2) , Write(x,4), Read(x,4) , Read(y,2)\" 3\uff09\u56fec\u4e0d\u6ee1\u8db3\u987a\u5e8f\u4e00\u81f4\u6027\uff0c\u5f53\u7136\u4e5f\u5c31\u4e0d\u6ee1\u8db3\u5f3a\u4e00\u81f4\u6027\u4e86\u3002\u56e0\u4e3a\u4ece\u8fdb\u7a0bP1\u7684\u89d2\u5ea6\u770b\uff0c\u5b83\u5bf9\u53d8\u91cfY\u7684\u8bfb\u64cd\u4f5c\u8fd4\u56de\u4e86\u7ed3\u679c0\u3002\u90a3\u4e48\u5c31\u662f\u8bf4\uff0cP1\u8fdb\u7a0b\u7684\u5bf9\u53d8\u91cfY\u7684\u8bfb\u64cd\u4f5c\u5728P2\u8fdb\u7a0b\u5bf9\u53d8\u91cfY\u7684\u5199\u64cd\u4f5c\u4e4b\u524d\uff0c\u8fd9\u610f\u5473\u7740\u5b83\u8ba4\u4e3a\u7684\u987a\u5e8f\u662f\u8fd9\u6837\u7684\uff1awrite(x,4) , Read(y,0) , Write(y,2), Read(x,0)\uff0c\u663e\u7136\u8fd9\u4e2a\u987a\u5e8f\u53c8\u662f\u4e0d\u80fd\u88ab\u6ee1\u8db3\u7684\uff0c\u56e0\u4e3a\u6700\u540e\u4e00\u4e2a\u5bf9\u53d8\u91cfx\u7684\u8bfb\u64cd\u4f5c\u8bfb\u51fa\u6765\u4e5f\u662f\u65e7\u7684\u6570\u636e\u3002\u56e0\u6b64\u8fd9\u4e2a\u987a\u5e8f\u662f\u6709\u51b2\u7a81\u7684\uff0c\u4e0d\u6ee1\u8db3\u987a\u5e8f\u4e00\u81f4\u6027\u3002 NOTE: \u8f93\u51fa\u4e3a 00 ; \u53ef\u80fd\u65b9\u5f0f: Write(x, 4) \u7531\u4e8e\u7f51\u7edc\u5ef6\u65f6\uff0c\u5bfc\u81f4\u66f4\u65b0\u7684\u6570\u636e\u6ca1\u6709\u53ca\u65f6\u540c\u6b65\u5230P2\u4e2d\uff0c\u56e0\u6b64P2\u8bfb\u51fa\u7684\u662f\u65e7\u503c Write(y, 2) \u7531\u4e8e\u7f51\u7edc\u5ef6\u65f6\uff0c\u5bfc\u81f4\u66f4\u65b0\u7684\u6570\u636e\u6ca1\u6709\u53ca\u65f6\u540c\u6b65\u5230P2\u4e2d\uff0c\u56e0\u6b64P1\u8bfb\u51fa\u7684\u662f\u65e7\u503c \u6b64\u65f6\u6bcf\u4e2aP\u4e2d\uff0cload\u548cstore\u7684order\u88abreorder\u4e86\uff1b 1.3 \u5f31\u4e00\u81f4\u6027 \u6570\u636e\u66f4\u65b0\u540e\uff0c\u5982\u679c\u80fd\u5bb9\u5fcd\u540e\u7eed\u7684\u8bbf\u95ee\u53ea\u80fd\u8bbf\u95ee\u5230\u90e8\u5206\u6216\u8005\u5168\u90e8\u8bbf\u95ee\u4e0d\u5230\uff0c\u5219\u662f**\u5f31\u4e00\u81f4\u6027**\u3002 \u6700\u7ec8\u4e00\u81f4\u6027**\u5c31\u5c5e\u4e8e**\u5f31\u4e00\u81f4\u6027 \u3002 \u6700\u7ec8\u4e00\u81f4\u6027 \u4e0d\u4fdd\u8bc1\u5728\u4efb\u610f\u65f6\u523b\u4efb\u610f\u8282\u70b9\u4e0a\u7684\u540c\u4e00\u4efd\u6570\u636e\u90fd\u662f\u76f8\u540c\u7684\uff0c\u4f46\u662f\u968f\u7740\u65f6\u95f4\u7684\u8fc1\u79fb\uff0c\u4e0d\u540c\u8282\u70b9\u4e0a\u7684\u540c\u4e00\u4efd\u6570\u636e\u603b\u662f\u5728\u5411\u8d8b\u540c\u7684\u65b9\u5411\u53d8\u5316\u3002 \u7b80\u5355\u8bf4\uff0c\u5c31\u662f\u5728\u4e00\u6bb5\u65f6\u95f4\u540e\uff0c\u8282\u70b9\u95f4\u7684\u6570\u636e\u4f1a\u6700\u7ec8\u8fbe\u5230\u4e00\u81f4\u72b6\u6001\u3002 **\u6700\u7ec8\u4e00\u81f4\u6027**\u6839\u636e\u66f4\u65b0\u6570\u636e\u540e\u5404\u8fdb\u7a0b\u8bbf\u95ee\u5230\u6570\u636e\u7684\u65f6\u95f4\u548c\u65b9\u5f0f\u7684\u4e0d\u540c\uff0c\u53c8\u53ef\u4ee5\u533a\u5206\u4e3a\uff1a 1\u3001\u56e0\u679c\u4e00\u81f4\u6027\uff08Casual Consistency\uff09\u3002\u5982\u679c\u8fdb\u7a0bA\u901a\u77e5\u8fdb\u7a0bB\u5b83\u5df2\u66f4\u65b0\u4e86\u4e00\u4e2a\u6570\u636e\u9879\uff0c\u90a3\u4e48\u8fdb\u7a0bB\u7684\u540e\u7eed\u8bbf\u95ee\u5c06\u8fd4\u56de\u66f4\u65b0\u540e\u7684\u503c\uff0c\u4e14\u4e00\u6b21\u5199\u5165\u5c06\u4fdd\u8bc1\u53d6\u4ee3\u524d\u4e00\u6b21\u5199\u5165\u3002\u4e0e\u8fdb\u7a0bA\u65e0\u56e0\u679c\u5173\u7cfb\u7684\u8fdb\u7a0bC\u7684\u8bbf\u95ee\uff0c\u9075\u5b88\u4e00\u822c\u7684\u6700\u7ec8\u4e00\u81f4\u6027\u89c4\u5219\u3002 2\u3001\u201c\u8bfb\u5df1\u4e4b\u6240\u5199\uff08read-your-writes\uff09\u201d\u4e00\u81f4\u6027\u3002\u5f53\u8fdb\u7a0bA\u81ea\u5df1\u66f4\u65b0\u4e00\u4e2a\u6570\u636e\u9879\u4e4b\u540e\uff0c\u5b83\u603b\u662f\u8bbf\u95ee\u5230\u66f4\u65b0\u8fc7\u7684\u503c\uff0c\u7edd\u4e0d\u4f1a\u770b\u5230\u65e7\u503c\u3002\u8fd9\u662f\u56e0\u679c\u4e00\u81f4\u6027\u6a21\u578b\u7684\u4e00\u4e2a\u7279\u4f8b\u3002 3\u3001\u4f1a\u8bdd\uff08Session\uff09\u4e00\u81f4\u6027\u3002\u8fd9\u662f\u4e0a\u4e00\u4e2a\u6a21\u578b\u7684\u5b9e\u7528\u7248\u672c\uff0c\u5b83\u628a\u8bbf\u95ee\u5b58\u50a8\u7cfb\u7edf\u7684\u8fdb\u7a0b\u653e\u5230\u4f1a\u8bdd\u7684\u4e0a\u4e0b\u6587\u4e2d\u3002\u53ea\u8981\u4f1a\u8bdd\u8fd8\u5b58\u5728\uff0c\u7cfb\u7edf\u5c31\u4fdd\u8bc1\u201c\u8bfb\u5df1\u4e4b\u6240\u5199\u201d\u4e00\u81f4\u6027\u3002\u5982\u679c\u7531\u4e8e\u67d0\u4e9b\u5931\u8d25\u60c5\u5f62\u4ee4\u4f1a\u8bdd\u7ec8\u6b62\uff0c\u5c31\u8981\u5efa\u7acb\u65b0\u7684\u4f1a\u8bdd\uff0c\u800c\u4e14\u7cfb\u7edf\u7684\u4fdd\u8bc1\u4e0d\u4f1a\u5ef6\u7eed\u5230\u65b0\u7684\u4f1a\u8bdd\u3002 4\u3001\u5355\u8c03\uff08Monotonic\uff09\u8bfb\u4e00\u81f4\u6027\u3002\u5982\u679c\u8fdb\u7a0b\u5df2\u7ecf\u770b\u5230\u8fc7\u6570\u636e\u5bf9\u8c61\u7684\u67d0\u4e2a\u503c\uff0c\u90a3\u4e48\u4efb\u4f55\u540e\u7eed\u8bbf\u95ee\u90fd\u4e0d\u4f1a\u8fd4\u56de\u5728\u90a3\u4e2a\u503c\u4e4b\u524d\u7684\u503c\u3002 5\u3001\u5355\u8c03\u5199\u4e00\u81f4\u6027\u3002\u7cfb\u7edf\u4fdd\u8bc1\u6765\u81ea\u540c\u4e00\u4e2a\u8fdb\u7a0b\u7684\u5199\u64cd\u4f5c\u987a\u5e8f\u6267\u884c\u3002\u8981\u662f\u7cfb\u7edf\u4e0d\u80fd\u4fdd\u8bc1\u8fd9\u79cd\u7a0b\u5ea6\u7684\u4e00\u81f4\u6027\uff0c\u5c31\u975e\u5e38\u96be\u4ee5\u7f16\u7a0b\u4e86\u3002 \u5171\u8bc6\uff08Consensus) \u5171\u8bc6\u95ee\u9898\u4e2d\u6240\u6709\u7684\u8282\u70b9\u8981\u6700\u7ec8\u8fbe\u6210\u5171\u8bc6\uff0c\u7531\u4e8e\u6700\u7ec8\u76ee\u6807\u662f\u6240\u6709\u8282\u70b9\u90fd\u8981\u8fbe\u6210\u4e00\u81f4\uff0c\u6240\u4ee5\u6839\u672c\u4e0d\u5b58\u5728\u4e00\u81f4\u6027\u5f3a\u5f31\u4e4b\u5206\u3002 \u4f8b\u5982\uff0cPaxos\u662f\u5171\u8bc6\uff08Consensus\uff09\u7b97\u6cd5\u800c\u4e0d\u662f\u5f3a\u4e00\u81f4\u6027\uff08Consistency\uff09\u534f\u8bae\u3002\u5171\u8bc6\u7b97\u6cd5\u6ca1\u6709\u4e00\u81f4\u6027\u7ea7\u522b\u7684\u533a\u5206\u3002 csdn \u5206\u5e03\u5f0f\u4e00\u81f4\u6027\u534f\u8bae \u4e00\u81f4\u6027\uff08Consistency\uff09\u662f\u6307\u591a\u526f\u672c\uff08Replications\uff09\u95ee\u9898\u4e2d\u7684\u6570\u636e\u4e00\u81f4\u6027\u3002\u5173\u4e8e\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u4e00\u81f4\u6027\u6a21\u578b\u6709\u4ee5\u4e0b\u51e0\u79cd\uff1a \u5f3a\u4e00\u81f4\u6027 \u5f53\u66f4\u65b0\u64cd\u4f5c\u5b8c\u6210\u4e4b\u540e\uff0c\u4efb\u4f55\u591a\u4e2a**\u540e\u7eed**\u8fdb\u7a0b\u6216\u8005\u7ebf\u7a0b\u7684\u8bbf\u95ee\u90fd\u4f1a**\u8fd4\u56de\u6700\u65b0\u7684\u66f4\u65b0\u8fc7\u7684\u503c**\uff0c\u76f4\u5230\u8fd9\u4e2a\u6570\u636e\u88ab\u5176\u4ed6\u6570\u636e\u66f4\u65b0\u4e3a\u6b62\u3002 \u4f46\u662f\u8fd9\u79cd\u5b9e\u73b0\u5bf9\u6027\u80fd\u5f71\u54cd\u8f83\u5927\uff0c\u56e0\u4e3a\u8fd9\u610f\u5473\u7740\uff0c\u53ea\u8981\u4e0a\u6b21\u7684\u64cd\u4f5c\u6ca1\u6709\u5904\u7406\u5b8c\uff0c\u5c31\u4e0d\u80fd\u8ba9\u7528\u6237\u8bfb\u53d6\u6570\u636e\u3002 \u5f31\u4e00\u81f4\u6027 \u7cfb\u7edf\u5e76\u4e0d\u4fdd\u8bc1\u8fdb\u7a0b\u6216\u8005\u7ebf\u7a0b\u7684\u8bbf\u95ee\u90fd\u4f1a\u8fd4\u56de\u6700\u65b0\u66f4\u65b0\u8fc7\u7684\u503c\u3002\u7cfb\u7edf\u5728\u6570\u636e\u5199\u5165\u6210\u529f\u4e4b\u540e\uff0c \u4e0d\u627f\u8bfa\u7acb\u5373\u53ef\u4ee5\u8bfb\u5230\u6700\u65b0\u5199\u5165\u7684\u503c \uff0c\u4e5f\u4e0d\u4f1a\u5177\u4f53\u7684\u627f\u8bfa\u591a\u4e45\u4e4b\u540e\u53ef\u4ee5\u8bfb\u5230\u3002\u751a\u81f3\u4e0d\u80fd\u4fdd\u8bc1\u53ef\u4ee5\u8bbf\u95ee\u5230\u3002 \u6700\u7ec8\u4e00\u81f4\u6027 \u6700\u7ec8\u4e00\u81f4\u6027\u4e5f\u662f\u5f31\u4e00\u81f4\u6027\u7684\u4e00\u79cd \uff0c\u5b83\u65e0\u6cd5\u4fdd\u8bc1\u6570\u636e\u66f4\u65b0\u540e\uff0c\u6240\u6709\u540e\u7eed\u7684\u8bbf\u95ee\u90fd\u80fd\u770b\u5230\u6700\u65b0\u6570\u503c\uff0c\u800c\u662f\u9700\u8981\u4e00\u4e2a\u65f6\u95f4\uff0c\u5728\u8fd9\u4e2a\u65f6\u95f4\u4e4b\u540e\u53ef\u4ee5\u4fdd\u8bc1\u8fd9\u4e00\u70b9\uff08 \u5c31\u662f\u5728\u4e00\u6bb5\u65f6\u95f4\u540e\uff0c\u8282\u70b9\u95f4\u7684\u6570\u636e\u4f1a\u6700\u7ec8\u8fbe\u5230\u4e00\u81f4\u72b6\u6001 \uff09\uff0c\u800c\u5728\u8fd9\u4e2a\u65f6\u95f4\u5185\uff0c\u6570\u636e\u4e5f\u8bb8\u662f\u4e0d\u4e00\u81f4\u7684\uff0c\u8fd9\u4e2a\u7cfb\u7edf\u65e0\u6cd5\u4fdd\u8bc1\u5f3a\u4e00\u81f4\u6027\u7684\u65f6\u95f4\u7247\u6bb5\u88ab\u79f0\u4e3a\u300c\u4e0d\u4e00\u81f4\u7a97\u53e3\u300d\u3002\u4e0d\u4e00\u81f4\u7a97\u53e3\u7684\u65f6\u95f4\u957f\u77ed\u53d6\u51b3\u4e8e\u5f88\u591a\u56e0\u7d20\uff0c\u6bd4\u5982\u5907\u4efd\u6570\u636e\u7684\u4e2a\u6570\u3001\u7f51\u7edc\u4f20\u8f93\u5ef6\u8fdf\u901f\u5ea6\u3001\u7cfb\u7edf\u8d1f\u8f7d\u7b49\u3002 \u6700\u7ec8\u4e00\u81f4\u6027\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53c8\u6709\u591a\u79cd\u53d8\u79cd\uff1a \u7c7b\u578b \u8bf4\u660e \u56e0\u679c\u4e00\u81f4\u6027 \u5982\u679c A \u8fdb\u7a0b\u5728\u66f4\u65b0\u4e4b\u540e\u5411 B \u8fdb\u7a0b\u901a\u77e5\u66f4\u65b0\u7684\u5b8c\u6210\uff0c\u90a3\u4e48 B \u7684\u8bbf\u95ee\u64cd\u4f5c\u5c06\u4f1a\u8fd4\u56de\u66f4\u65b0\u7684\u503c\u3002\u800c\u6ca1\u6709\u56e0\u679c\u5173\u7cfb\u7684 C \u8fdb\u7a0b\u5c06\u4f1a\u9075\u5faa\u6700\u7ec8\u4e00\u81f4\u6027\u7684\u89c4\u5219\uff08C \u5728\u4e0d\u4e00\u81f4\u7a97\u53e3\u5185\u8fd8\u662f\u770b\u5230\u662f\u65e7\u503c\uff09\u3002 \u8bfb\u4f60\u6240\u5199\u4e00\u81f4\u6027 \u56e0\u679c\u4e00\u81f4\u6027\u7684\u7279\u5b9a\u5f62\u5f0f\u3002\u4e00\u4e2a\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u66f4\u65b0\u540e\uff0c\u4f1a\u7ed9\u81ea\u5df1\u53d1\u9001\u4e00\u6761\u901a\u77e5\uff0c\u8be5\u8fdb\u7a0b\u540e\u7eed\u7684\u64cd\u4f5c\u90fd\u4f1a\u4ee5\u6700\u65b0\u503c\u4f5c\u4e3a\u57fa\u7840\uff0c\u800c\u5176\u4ed6\u7684\u8fdb\u7a0b\u8fd8\u662f\u53ea\u80fd\u5728\u4e0d\u4e00\u81f4\u7a97\u53e3\u4e4b\u540e\u624d\u80fd\u770b\u5230\u6700\u65b0\u503c\u3002 \u4f1a\u8bdd\u4e00\u81f4\u6027 \u8bfb\u4f60\u6240\u5199\u4e00\u81f4\u6027\u7684\u7279\u5b9a\u5f62\u5f0f\u3002\u8fdb\u7a0b\u5728\u8bbf\u95ee\u5b58\u50a8\u7cfb\u7edf\u540c\u4e00\u4e2a\u4f1a\u8bdd\u5185\uff0c\u7cfb\u7edf\u4fdd\u8bc1\u8be5\u8fdb\u7a0b\u53ef\u4ee5\u8bfb\u53d6\u5230\u6700\u65b0\u4e4b\uff0c\u4f46\u5982\u679c\u4f1a\u8bdd\u7ec8\u6b62\uff0c\u91cd\u65b0\u8fde\u63a5\u540e\uff0c\u5982\u679c\u6b64\u65f6\u8fd8\u5728\u4e0d\u4e00\u81f4\u7a97\u53e3\u5185\uff0c\u8fd8\u662f\u53ef\u5ae9\u8bfb\u53d6\u5230\u65e7\u503c\u3002 \u5355\u8c03\u8bfb\u4e00\u81f4\u6027 \u5982\u679c\u4e00\u4e2a\u8fdb\u7a0b\u5df2\u7ecf\u8bfb\u53d6\u5230\u4e00\u4e2a\u7279\u5b9a\u503c\uff0c\u90a3\u4e48\u8be5\u8fdb\u7a0b\u4e0d\u4f1a\u8bfb\u53d6\u5230\u8be5\u503c\u4ee5\u524d\u7684\u4efb\u4f55\u503c\u3002 \u5355\u8c03\u5199\u4e00\u81f4\u6027 \u7cfb\u7edf\u4fdd\u8bc1\u5bf9\u540c\u4e00\u4e2a\u8fdb\u7a0b\u7684\u5199\u64cd\u4f5c\u4e32\u884c\u5316\u3002 \u4e00\u81f4\u6027\u6a21\u578b cnblogs \u5f3a\u4e00\u81f4\u6027\u3001\u5f31\u4e00\u81f4\u6027\u3001\u6700\u7ec8\u4e00\u81f4\u6027 \u5f3a\u4e00\u81f4\u6027\uff1a\u7cfb\u7edf\u4e2d\u7684\u67d0\u4e2a\u6570\u636e\u88ab\u6210\u529f\u66f4\u65b0\u540e\uff0c\u540e\u7eed\u4efb\u4f55\u5bf9\u8be5\u6570\u636e\u7684\u8bfb\u53d6\u64cd\u4f5c\u90fd\u5c06\u5f97\u5230\u66f4\u65b0\u540e\u7684\u503c\uff1b \u5f31\u4e00\u81f4\u6027\uff1a\u7cfb\u7edf\u4e2d\u7684\u67d0\u4e2a\u6570\u636e\u88ab\u66f4\u65b0\u540e\uff0c\u540e\u7eed\u5bf9\u8be5\u6570\u636e\u7684\u8bfb\u53d6\u64cd\u4f5c\u53ef\u80fd\u5f97\u5230\u66f4\u65b0\u540e\u7684\u503c\uff0c\u4e5f\u53ef\u80fd\u662f\u66f4\u6539\u524d\u7684\u503c\u3002\u4f46\u7ecf\u8fc7\u201c\u4e0d\u4e00\u81f4\u65f6\u95f4\u7a97\u53e3\u201d\u8fd9\u6bb5\u65f6\u95f4\u540e\uff0c\u540e\u7eed\u5bf9\u8be5\u6570\u636e\u7684\u8bfb\u53d6\u90fd\u662f\u66f4\u65b0\u540e\u7684\u503c\uff1b \u6700\u7ec8\u4e00\u81f4\u6027\uff1a\u662f\u5f31\u4e00\u81f4\u6027\u7684\u7279\u6b8a\u5f62\u5f0f\uff0c\u5b58\u50a8\u7cfb\u7edf\u4fdd\u8bc1\u5728\u6ca1\u6709\u65b0\u7684\u66f4\u65b0\u7684\u6761\u4ef6\u4e0b\uff0c\u6700\u7ec8\u6240\u6709\u7684\u8bbf\u95ee\u90fd\u662f\u6700\u540e\u66f4\u65b0\u7684\u503c\u3002","title":"Introduction"},{"location":"Distributed-computing/Theory/Consistency/Primer/#primer","text":"","title":"Primer"},{"location":"Distributed-computing/Theory/Consistency/Primer/#consistency#model","text":"","title":"\u5982\u4f55\u638c\u63e1consistency model?"},{"location":"Distributed-computing/Theory/Consistency/Primer/#consistency#modeldatamemorymodel","text":"\u5bf9data/memory\u7684operation\u5305\u62ec 1\u3001read 2\u3001write","title":"Consistency model\u662f\u5173\u4e8edata/memory\u7684model"},{"location":"Distributed-computing/Theory/Consistency/Primer/#consistency#modelmodel","text":"\u5b83\u80fd\u591f\u5bf9multiple model\u4e2d\u7684shared data\u8fdb\u884c\u63cf\u8ff0\uff0cmultiple model\u80fd\u591f\u63cf\u8ff0\u975e\u5e38\u591a\u7684\uff0c\u4e0b\u9762\u53ef\u4ee5\u770b\u5230:","title":"Consistency model\u662f\u4e00\u4e2a\u975e\u5e38\u5f3a\u5927\u7684\u3001\u9002\u7528\u9762\u975e\u5e38\u5e7f\u6cdb\u7684model"},{"location":"Distributed-computing/Theory/Consistency/Primer/#multiple#model","text":"Multiple model\u4e2d\u7684entity\u53ef\u4ee5\u4e3a: 1\u3001multicore: processor core 2\u3001multithread: thread 3\u3001multiprocess: process 4\u3001distributed computing: node \u56e0\u6b64\u4e0b\u9762\u7684\u63cf\u8ff0\u7684\u5185\u5bb9\uff0c\u6709\u7684\u65f6\u5019\u5e76\u4e0d\u9488\u5bf9\u4e0a\u8ff0\u60c5\u51b5\u4e2d\u7684\u4e00\u79cd\uff0c\u800c\u662f\u63cf\u8ff0\u7684\u4e00\u79cd\u901a\u7528\u7684\u539f\u7406\u3002","title":"Multiple model"},{"location":"Distributed-computing/Theory/Consistency/Primer/#_1","text":"\u73b0\u5b9e : \u5404\u4e2aentity\u4e4b\u95f4\u5b58\u5728\u7740**\u901a\u4fe1\u5ef6\u65f6**( delay )\uff0c\u4e00\u65e6entity\u5bf9\u81ea\u5df1\u7684**copy of shared data**\u505a\u51fa\u4e86write\uff0c\u5728\u5b8c\u6210\u548c\u5176\u4ed6\u7684entity\u7684**synchronization**\u4e4b\u524d( \u540c\u6b65\u5ef6\u65f6 )\uff0c\u5404\u4e2aentity\u7684**copy of shared data**\u662f\u5b58\u5728\u7740\u5dee\u5f02\u7684\uff0c\u5373\u5404\u4e2aentity\u6240\u770b\u5230\u7684**shared data**\u662f\u4e0d\u540c\u7684\u3002 NOTE: replication lag \u7406\u60f3 : \u5404\u4e2aentity\u4e4b\u95f4\u7684**\u901a\u4fe1\u5ef6\u65f6**\u4e3a0\uff0c\u6216\u8005\u8bf4\uff0c\u5b83\u4eec\u4e4b\u95f4\u4e0d\u5b58\u5728\u5ef6\u65f6\uff0c\u8fd9\u6837\u5c31\u4e0d\u5b58\u5728**\u540c\u6b65\u5ef6\u65f6**\uff0c\u8fd9\u6837\u4efb\u4f55\u4e00\u4e2aentity\u5bf9\u81ea\u5df1\u7684copy of shared data\u7684\u4fee\u6539\u80fd\u591f\u7acb\u5373\u540c\u6b65\u5230\u5176\u4ed6\u7684entity\u4e2d\uff0c\u5404\u4e2aentity\u7684**copy of shared data**\u662f\u4e0d\u5b58\u5728\u5dee\u5f02\u7684\uff0c\u5373\u5404\u4e2aentity\u6240\u770b\u5230\u7684**shared data**\u662f\u76f8\u540c\u7684\u3002\u5176\u5b9e\u8fd9\u5c31\u662f**strict consistency**\u3002","title":"\u7406\u60f3 \u4e0e \u73b0\u5b9e"},{"location":"Distributed-computing/Theory/Consistency/Primer/#consistency#model#abstract#machine","text":"\u4e3a\u4e86\u4fbf\u4e8e\u7406\u89e3\u5404\u79cdconsistency model\uff0c\u672c\u8282\u63d0\u51fa\u4e00\u4e2aconsistency model abstract machine\uff0c\u5b83\u5176\u5b9e\u5c31\u662fprogrammer\u7ad9\u5728\u5168\u5c40\u7684\u89c6\u89d2\u6765\u770b\u5f85\u5e76\u8fd0\u884c\u6574\u4e2amultiple model\uff0c\u4ece\u800c\u5f97\u5230\u8f93\u51fa\uff0c\u5b83\u7684\u7b80\u5355\u5b9a\u4e49\u5982\u4e0b: 1\u3001\u6240\u6709\u7684entity\u90fd\u72ec\u7acb\u5730\u8fd0\u884c\u5728\u8fd9\u53f0abstract machine\u4e0a NOTE: \u53ef\u4ee5\u8ba4\u4e3a\u6bcf\u4e2aentity\u6709\u4e00\u4e2a\u81ea\u5df1\u7684processor 2\u3001shared data NOTE: \u53ef\u4ee5\u8ba4\u4e3a\u662fmemory 3\u3001\u6bcf\u4e2aentity\u90fd\u6709\u81ea\u5df1\u7684copy of shared data NOTE: \u53ef\u4ee5\u8ba4\u4e3a\u6bcf\u4e2aentity\u90fd\u5168\u91cf\u7684cache shared data 4\u3001\u6bcf\u4e2aentity\u9700\u8981\u5c06\u81ea\u5df1\u5bf9shared data\u7684\u66f4\u6539\u540c\u6b65\u5230shared data\u4e2d 5\u3001 \u5168\u5c40\u65f6\u949f \u3001abstract machine\u7684\u65f6\u949f\uff0c\u5176\u5b9e\u5c31\u662f\u5899\u4e0a\u65f6\u949f\uff0cprogrammer\u4ee5\u8fd9\u4e2a\u65f6\u949f\u4e3a\u53c2\u8003\uff0c\u6765\u770babstract machine\u4e0a\u7684\u5404\u4e2aentity\uff0c\u5176\u5b9e\u5c31\u662f\u4eceprogrammer\u7684\u89d2\u5ea6\u6765\u770b\u5404\u4e2aentity\u7684\u6d3b\u52a8\u3002\u8fd9\u4f4dprogrammer\u63d0\u4f9b\u4e86\u89c2\u5bdf\u6574\u4e2a\u7cfb\u7edf\u7684\u89c6\u89d2\u3002","title":"Consistency model abstract machine"},{"location":"Distributed-computing/Theory/Consistency/Primer/#strict#consistency#model","text":"**Strict consistency model**\u662f\u6700\u6700\u7406\u60f3\u7684\u60c5\u51b5\uff0c\u540c\u65f6\u5b83\u4e5f\u662f\u975e\u5e38\u5bb9\u6613\u7406\u89e3\u7684\u3002 \u5728strict consistency\u4e2d\uff0c\u7531\u4e8e: 1\u3001 \u6240\u6709\u7684write\u90fd\u7acb\u5373\u751f\u6548 (\u5728\u5b9e\u9645\u4e2d\uff0c\u53ef\u80fd\u65e0\u6cd5\u8fbe\u6210) 2\u3001entity\u7684copy of shared data\u80fd\u591f\u4e00\u76f4\u4fdd\u6301\u76f8\u540c \u56e0\u6b64**Strict consistency model**\u53ef\u4ee5\u8fd9\u6837\u6a21\u62df: \u6240\u6709\u7684**entity**\u90fd\u5728\u540c\u4e00\u4e2aprocessor\u4e0a\u6267\u884c\uff0c\u4f7f\u7528\u540c\u4e00\u4e2ashared data\uff0c\u8fd9\u4e2aprocessor\u6ca1\u6709cache\uff0c\u6240\u6709\u7684write\u7acb\u5373\u751f\u6548\u3002","title":"\u6a21\u62dfstrict consistency model"},{"location":"Distributed-computing/Theory/Consistency/Primer/#consistency#modelcontract","text":"1\u3001\u5b83\u662f\u4e00\u79cdcontract\uff0c\u9075\u5faa\u4e00\u5b9a\u7684contract\uff0c\u5219\u53ef\u4ee5\u5b9e\u73b0\u9884\u671f\u7684\u6548\u679c\uff1b","title":"Consistency model\u662fcontract"},{"location":"Distributed-computing/Theory/Consistency/Primer/#level#and#tradeoff","text":"NOTE: \u6709\u54ea\u4e9bconstrain\u3001\u76f8\u8f83\u4e8e\u4e0a\u4e00\u5c42\uff0c\u653e\u677e\u4e86\u54ea\u4e9bconstrain\u3002 2\u3001\u6bcf\u79cdconsistency model\u90fd\u6709constrain\uff0c\u4ece\u800c\u51b3\u5b9a\u4e86consistency model\u7684strong\u3001weak 1\u3001strict consistency\u662f\u6700\u6700\u7406\u60f3\u7684\u3001\u5bb9\u6613\u7406\u89e3\u60c5\u51b5\uff0c\u53ef\u4ee5\u4ee5\u5b83\u6765\u4f5c\u4e3a\u5bf9\u6bd4 2\u3001\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\uff0cstrict consistency\u7684\u5b9e\u73b0\u662f\u6bd4\u8f83\u56f0\u96be\u7684\uff0c\u5e76\u4e14\u6709\u7684application\u4e0d\u9700\u8981strict consistency\uff0c\u56e0\u6b64\u53ef\u4ee5\u653e\u5bbdconstrain\uff0c\u4f7f\u7528weak consistency model 3\u3001programmer\u9700\u8981\u8fdb\u884ctradeoff","title":"Level and tradeoff"},{"location":"Distributed-computing/Theory/Consistency/Primer/#csdn","text":"NOTE: \u6bd4\u8f83\u96be\u4ee5\u7406\u89e3\u7684: \"\u7cfb\u7edf\u4e2d\u8fdb\u7a0b\u770b\u5230\u7684\u987a\u5e8f\"\u548c \"\u5168\u5c40\u65f6\u949f\u4e0b\u7684\u987a\u5e8f\" \u56e0\u4e3a\u7cfb\u7edf\u4e2d\u7684\u6bcf\u4e2anode\u90fd\u6709\u4e00\u4efdcopy\uff0c\u56e0\u6b64\u5b83\u4eec\u5404\u81ea\u5bf9\u81ea\u5df1\u7684copy\u90fd\u8fdb\u884c\u4e86read\u3001write\uff0c\u56e0\u6b64\u6bcf\u4e2anode\u90fd\u6709\u81ea\u5df1\u7684\u5b9e\u9645\u64cd\u4f5c\u7684\u987a\u5e8f\uff0c\u8fd9\u5c31\u662f\u6240\u8c13\"\u7cfb\u7edf\u4e2d\u8fdb\u7a0b\u770b\u5230\u7684\u987a\u5e8f\"\uff1b \"\u5168\u5c40\u65f6\u949f\u4e0b\u7684\u987a\u5e8f\"\u5219\u662f\u6307\u7406\u60f3\u7684\u72b6\u6001(\u987a\u5e8f)\uff0c\u8fd9\u79cd\u7406\u60f3\u72b6\u6001\u53ef\u4ee5\u8fd9\u6837\u6765\u6a21\u62df: \u7531\u4e8e\u6240\u6709\u7684node share data\uff0c\u6240\u6709\u7684node\u90fd\u662f\u5728\u5bf9shared data\u8fdb\u884coperation(read\u3001write)\uff0c\u56e0\u6b64\u53ef\u4ee5\u8ba4\u4e3a\u6240\u6709\u7684node\u90fd\u5728\u540c\u4e00\u4e2aprocessor\u4e0a\u6267\u884c\uff0c\u663e\u7136\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5b83\u4eec\u5c31\u6709\u4e00\u4e2a\u5168\u5c40\u65f6\u949f\u4e86\uff0c\u5728\u8fd9\u4e2a\u7406\u60f3\u7684\u72b6\u6001(\u987a\u5e8f)\u4e0b\uff0c\u6240\u6709\u7684write\u90fd\u7acb\u5373\u751f\u6548(\u5728\u5b9e\u9645\u4e2d\uff0c\u53ef\u80fd\u65e0\u6cd5\u8fbe\u6210)\uff0c\u5c31\u597d\u6bd4\u5728\u540c\u4e00\u4e2aprocessor\uff0c\u8fd9\u5c31\u662f\"\u5f3a\u4e00\u81f4\u6027\uff08Strong Consistency\uff09\"\u3002 \u7531\u4e8e\u7cfb\u7edf\u4e2d\u7684\u6bcf\u4e2anode\u90fd\u6709\u4e00\u4efdcopy\uff0cchange\u5728\u5b83\u4eec\u4e4b\u95f4\u540c\u6b65\u662f\u9700\u8981\u65f6\u95f4\u7684\u3002 \u4e0d\u662f\u6240\u6709\u7684consistency model\u90fd\u53c2\u8003\u4e86**\u5168\u5c40\u65f6\u949f**\u7684\uff0c\"Sequential Consistency\"\u5c31\u6ca1\u6709\u53c2\u8003\"\u5168\u5c40\u65f6\u949f\"\uff0c\u8c8c\u4f3c\u53ea\u6709strong consistency\u624d\u53c2\u8003\u4e86\"\u5168\u5c40\u65f6\u949f\"\u3002Sequential Consistency\u8c8c\u4f3c\u4ec5\u4ec5\u5f3a\u8c03sequence\uff1b","title":"csdn \u5f3a\u4e00\u81f4\u6027\u3001\u987a\u5e8f\u4e00\u81f4\u6027\u3001\u5f31\u4e00\u81f4\u6027\u548c\u5171\u8bc6"},{"location":"Distributed-computing/Theory/Consistency/Primer/#1#consistency","text":"\u4e00\u81f4\u6027\uff08Consistency\uff09\u662f\u6307\u591a\u526f\u672c\uff08Replications\uff09\u95ee\u9898\u4e2d\u7684\u6570\u636e\u4e00\u81f4\u6027\u3002\u53ef\u4ee5\u5206\u4e3a\u5f3a\u4e00\u81f4\u6027\u3001\u987a\u5e8f\u4e00\u81f4\u6027\u4e0e\u5f31\u4e00\u81f4\u6027\u3002","title":"1. \u4e00\u81f4\u6027\uff08Consistency\uff09"},{"location":"Distributed-computing/Theory/Consistency/Primer/#11#strong#consistency","text":"\u4e5f\u79f0\u4e3a\uff1a \u539f\u5b50\u4e00\u81f4\u6027\uff08Atomic Consistency\uff09 \u7ebf\u6027\u4e00\u81f4\u6027\uff08Linearizable Consistency\uff09 \u4e24\u4e2a\u8981\u6c42\uff1a \u4efb\u4f55\u4e00\u6b21\u8bfb\u90fd\u80fd\u8bfb\u5230\u67d0\u4e2a\u6570\u636e\u7684\u6700\u8fd1\u4e00\u6b21\u5199\u7684\u6570\u636e\u3002 \u7cfb\u7edf\u4e2d\u7684\u6240\u6709\u8fdb\u7a0b\uff0c\u770b\u5230\u7684\u64cd\u4f5c\u987a\u5e8f\uff0c\u90fd\u548c**\u5168\u5c40\u65f6\u949f\u4e0b\u7684\u987a\u5e8f\u4e00\u81f4**\u3002 \u7b80\u8a00\u4e4b\uff0c\u5728\u4efb\u610f\u65f6\u523b\uff0c\u6240\u6709\u8282\u70b9\u4e2d\u7684\u6570\u636e\u662f\u4e00\u6837\u7684\u3002 \u4f8b\u5982\uff0c\u5bf9\u4e8e\u5173\u7cfb\u578b\u6570\u636e\u5e93\uff0c\u8981\u6c42\u66f4\u65b0\u8fc7\u7684\u6570\u636e\u80fd\u88ab\u540e\u7eed\u7684\u8bbf\u95ee\u90fd\u80fd\u770b\u5230\uff0c\u8fd9\u662f\u5f3a\u4e00\u81f4\u6027\u3002 NOTE:","title":"1.1 \u5f3a\u4e00\u81f4\u6027\uff08Strong Consistency\uff09"},{"location":"Distributed-computing/Theory/Consistency/Primer/#12#sequential#consistency","text":"the result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program. - - Lamport \u4e24\u4e2a\u8981\u6c42\uff1a \u4efb\u4f55\u4e00\u6b21\u8bfb\u90fd\u80fd\u8bfb\u5230\u67d0\u4e2a\u6570\u636e\u7684\u6700\u8fd1\u4e00\u6b21\u5199\u7684\u6570\u636e\u3002 \u7cfb\u7edf\u7684\u6240\u6709\u8fdb\u7a0b\u7684\u987a\u5e8f\u4e00\u81f4\uff0c\u800c\u4e14\u662f\u5408\u7406\u7684\u3002\u5373\u4e0d\u9700\u8981\u548c\u5168\u5c40\u65f6\u949f\u4e0b\u7684\u987a\u5e8f\u4e00\u81f4\uff0c\u9519\u7684\u8bdd\u4e00\u8d77\u9519\uff0c\u5bf9\u7684\u8bdd\u4e00\u8d77\u5bf9\u3002 \u4e3e\u4e2a\u6817\u5b50\uff1a NOTE: \u4e0a\u56fe\u5176\u5b9e\u662f\u4e00\u4e2aspace-time diagram\uff0c\u6a2a\u8f74\u8868\u793a\u7684\u5168\u5c40\u65f6\u949f; \u4e0b\u9762\u662f\u6a21\u62df\u7a0b\u5e8f: Global int x = 0 , y = 0 ; Process 1 Process 2 x = 4 ; y = 2 cout << y ; cout << x ; Write(x, 4)\uff1a\u5199\u5165x=4 Read(x, 0)\uff1a\u8bfb\u51fax=0 1\uff09\u56fea\u662f\u6ee1\u8db3**\u987a\u5e8f\u4e00\u81f4\u6027**\uff0c\u4f46\u662f\u4e0d\u6ee1\u8db3**\u5f3a\u4e00\u81f4\u6027**\u7684\u3002\u539f\u56e0\u5728\u4e8e\uff0c\u4ece**\u5168\u5c40\u65f6\u949f**\u7684\u89c2\u70b9\u6765\u770b\uff0cP2\u8fdb\u7a0b\u5bf9\u53d8\u91cf X \u7684\u8bfb\u64cd\u4f5c\u5728P1\u8fdb\u7a0b\u5bf9\u53d8\u91cfX\u7684\u5199\u64cd\u4f5c\u4e4b\u540e\uff0c\u7136\u800c\u8bfb\u51fa\u6765\u7684\u5374\u662f\u65e7\u7684\u6570\u636e\u3002\u4f46\u662f\u8fd9\u4e2a\u56fe\u5374\u662f\u6ee1\u8db3**\u987a\u5e8f\u4e00\u81f4\u6027**\u7684\uff0c\u56e0\u4e3a\u4e24\u4e2a\u8fdb\u7a0bP1\uff0cP2\u7684\u4e00\u81f4\u6027\u5e76\u6ca1\u6709\u51b2\u7a81\u3002\u4ece\u8fd9\u4e24\u4e2a\u8fdb\u7a0b\u7684\u89d2\u5ea6\u6765\u770b\uff0c**\u987a\u5e8f**\u5e94\u8be5\u662f\u8fd9\u6837\u7684\uff1aWrite(y,2) , Read(x,0) , Write(x,4), Read(y,2)\uff0c\u6bcf\u4e2a\u8fdb\u7a0b\u5185\u90e8\u7684\u8bfb\u5199\u987a\u5e8f\u90fd\u662f\u5408\u7406\u7684\uff0c\u4f46\u662f\u8fd9\u4e2a\u987a\u5e8f\u4e0e**\u5168\u5c40\u65f6\u949f**\u4e0b\u770b\u5230\u7684\u987a\u5e8f\u5e76\u4e0d\u4e00\u6837\u3002 NOTE: \u8f93\u51fa\u4e3a 02 ; \"\u4ece**\u5168\u5c40\u65f6\u949f**\u7684\u89c2\u70b9\u6765\u770b\uff0cP2\u8fdb\u7a0b\u5bf9\u53d8\u91cf X \u7684\u8bfb\u64cd\u4f5c\u5728P1\u8fdb\u7a0b\u5bf9\u53d8\u91cfX\u7684\u5199\u64cd\u4f5c\u4e4b\u540e\uff0c\u7136\u800c\u8bfb\u51fa\u6765\u7684\u5374\u662f\u65e7\u7684\u6570\u636e\"\u7684\u53ef\u80fd\u65b9\u5f0f\u7684: Write(x, 4) \u7531\u4e8e\u7f51\u7edc\u5ef6\u65f6\uff0c\u5bfc\u81f4\u66f4\u65b0\u7684\u6570\u636e\u6ca1\u6709\u53ca\u65f6\u540c\u6b65\u5230P2\u4e2d\uff0c\u56e0\u6b64P2\u8bfb\u51fa\u7684\u662f\u65e7\u503c \u6bcf\u4e2aP\u4e2d\uff0cload\u548cstore\u662f\u7ef4\u6301\u7684\uff1b \u4e0a\u8ff0 \"\u4ece\u8fd9\u4e24\u4e2a\u8fdb\u7a0b\u7684\u89d2\u5ea6\u6765\u770b\uff0c**\u987a\u5e8f**\u5e94\u8be5\u662f\u8fd9\u6837\u7684\uff1aWrite(y,2) , Read(x,0) , Write(x,4), Read(y,2)\"\uff0c\u5176\u4e2d\u5f97\u51fa\u7684\u987a\u5e8f\u5176\u5b9e\u4e5f\u662f\u5c06\u4e24\u4e2aprocess\u6a21\u62df\u5230\u540c\u4e00\u4e2aprocessor\u4e0a\u5f97\u51fa\u7684\uff0c\u51c6\u786e\u6765\u8bf4\uff0c\u4f7f\u7528C++11 std::atomic \u7684 sequential consistency\u6765\u8fdb\u884c\u6a21\u62df\uff0c\u53c2\u89c1 \"introduction\" \u7ae0\u8282\uff1b 2\uff09\u56feb\u6ee1\u8db3**\u5f3a\u4e00\u81f4\u6027**\uff0c\u56e0\u4e3a\u6bcf\u4e2a\u8bfb\u64cd\u4f5c\u90fd\u8bfb\u5230\u4e86\u8be5\u53d8\u91cf\u7684\u6700\u65b0\u5199\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u4e24\u4e2a\u8fdb\u7a0b\u770b\u5230\u7684\u64cd\u4f5c\u987a\u5e8f\u4e0e\u5168\u5c40\u65f6\u949f\u7684\u987a\u5e8f\u4e00\u6837\uff0c\u90fd\u662fWrite(y,2) , Read(x,4) , Write(x,4), Read(y,2)\u3002 NOTE: \"Write(y,2) , Read(x,4) , Write(x,4), Read(y,2)\"\u662f\u6709\u8bef\u7684 \uff0c\u5e94\u8be5\u662f\"Write(y,2) , Write(x,4), Read(x,4) , Read(y,2)\" 3\uff09\u56fec\u4e0d\u6ee1\u8db3\u987a\u5e8f\u4e00\u81f4\u6027\uff0c\u5f53\u7136\u4e5f\u5c31\u4e0d\u6ee1\u8db3\u5f3a\u4e00\u81f4\u6027\u4e86\u3002\u56e0\u4e3a\u4ece\u8fdb\u7a0bP1\u7684\u89d2\u5ea6\u770b\uff0c\u5b83\u5bf9\u53d8\u91cfY\u7684\u8bfb\u64cd\u4f5c\u8fd4\u56de\u4e86\u7ed3\u679c0\u3002\u90a3\u4e48\u5c31\u662f\u8bf4\uff0cP1\u8fdb\u7a0b\u7684\u5bf9\u53d8\u91cfY\u7684\u8bfb\u64cd\u4f5c\u5728P2\u8fdb\u7a0b\u5bf9\u53d8\u91cfY\u7684\u5199\u64cd\u4f5c\u4e4b\u524d\uff0c\u8fd9\u610f\u5473\u7740\u5b83\u8ba4\u4e3a\u7684\u987a\u5e8f\u662f\u8fd9\u6837\u7684\uff1awrite(x,4) , Read(y,0) , Write(y,2), Read(x,0)\uff0c\u663e\u7136\u8fd9\u4e2a\u987a\u5e8f\u53c8\u662f\u4e0d\u80fd\u88ab\u6ee1\u8db3\u7684\uff0c\u56e0\u4e3a\u6700\u540e\u4e00\u4e2a\u5bf9\u53d8\u91cfx\u7684\u8bfb\u64cd\u4f5c\u8bfb\u51fa\u6765\u4e5f\u662f\u65e7\u7684\u6570\u636e\u3002\u56e0\u6b64\u8fd9\u4e2a\u987a\u5e8f\u662f\u6709\u51b2\u7a81\u7684\uff0c\u4e0d\u6ee1\u8db3\u987a\u5e8f\u4e00\u81f4\u6027\u3002 NOTE: \u8f93\u51fa\u4e3a 00 ; \u53ef\u80fd\u65b9\u5f0f: Write(x, 4) \u7531\u4e8e\u7f51\u7edc\u5ef6\u65f6\uff0c\u5bfc\u81f4\u66f4\u65b0\u7684\u6570\u636e\u6ca1\u6709\u53ca\u65f6\u540c\u6b65\u5230P2\u4e2d\uff0c\u56e0\u6b64P2\u8bfb\u51fa\u7684\u662f\u65e7\u503c Write(y, 2) \u7531\u4e8e\u7f51\u7edc\u5ef6\u65f6\uff0c\u5bfc\u81f4\u66f4\u65b0\u7684\u6570\u636e\u6ca1\u6709\u53ca\u65f6\u540c\u6b65\u5230P2\u4e2d\uff0c\u56e0\u6b64P1\u8bfb\u51fa\u7684\u662f\u65e7\u503c \u6b64\u65f6\u6bcf\u4e2aP\u4e2d\uff0cload\u548cstore\u7684order\u88abreorder\u4e86\uff1b","title":"1.2 \u987a\u5e8f\u4e00\u81f4\u6027\uff08Sequential Consistency\uff09"},{"location":"Distributed-computing/Theory/Consistency/Primer/#13","text":"\u6570\u636e\u66f4\u65b0\u540e\uff0c\u5982\u679c\u80fd\u5bb9\u5fcd\u540e\u7eed\u7684\u8bbf\u95ee\u53ea\u80fd\u8bbf\u95ee\u5230\u90e8\u5206\u6216\u8005\u5168\u90e8\u8bbf\u95ee\u4e0d\u5230\uff0c\u5219\u662f**\u5f31\u4e00\u81f4\u6027**\u3002 \u6700\u7ec8\u4e00\u81f4\u6027**\u5c31\u5c5e\u4e8e**\u5f31\u4e00\u81f4\u6027 \u3002","title":"1.3 \u5f31\u4e00\u81f4\u6027"},{"location":"Distributed-computing/Theory/Consistency/Primer/#_2","text":"\u4e0d\u4fdd\u8bc1\u5728\u4efb\u610f\u65f6\u523b\u4efb\u610f\u8282\u70b9\u4e0a\u7684\u540c\u4e00\u4efd\u6570\u636e\u90fd\u662f\u76f8\u540c\u7684\uff0c\u4f46\u662f\u968f\u7740\u65f6\u95f4\u7684\u8fc1\u79fb\uff0c\u4e0d\u540c\u8282\u70b9\u4e0a\u7684\u540c\u4e00\u4efd\u6570\u636e\u603b\u662f\u5728\u5411\u8d8b\u540c\u7684\u65b9\u5411\u53d8\u5316\u3002 \u7b80\u5355\u8bf4\uff0c\u5c31\u662f\u5728\u4e00\u6bb5\u65f6\u95f4\u540e\uff0c\u8282\u70b9\u95f4\u7684\u6570\u636e\u4f1a\u6700\u7ec8\u8fbe\u5230\u4e00\u81f4\u72b6\u6001\u3002 **\u6700\u7ec8\u4e00\u81f4\u6027**\u6839\u636e\u66f4\u65b0\u6570\u636e\u540e\u5404\u8fdb\u7a0b\u8bbf\u95ee\u5230\u6570\u636e\u7684\u65f6\u95f4\u548c\u65b9\u5f0f\u7684\u4e0d\u540c\uff0c\u53c8\u53ef\u4ee5\u533a\u5206\u4e3a\uff1a 1\u3001\u56e0\u679c\u4e00\u81f4\u6027\uff08Casual Consistency\uff09\u3002\u5982\u679c\u8fdb\u7a0bA\u901a\u77e5\u8fdb\u7a0bB\u5b83\u5df2\u66f4\u65b0\u4e86\u4e00\u4e2a\u6570\u636e\u9879\uff0c\u90a3\u4e48\u8fdb\u7a0bB\u7684\u540e\u7eed\u8bbf\u95ee\u5c06\u8fd4\u56de\u66f4\u65b0\u540e\u7684\u503c\uff0c\u4e14\u4e00\u6b21\u5199\u5165\u5c06\u4fdd\u8bc1\u53d6\u4ee3\u524d\u4e00\u6b21\u5199\u5165\u3002\u4e0e\u8fdb\u7a0bA\u65e0\u56e0\u679c\u5173\u7cfb\u7684\u8fdb\u7a0bC\u7684\u8bbf\u95ee\uff0c\u9075\u5b88\u4e00\u822c\u7684\u6700\u7ec8\u4e00\u81f4\u6027\u89c4\u5219\u3002 2\u3001\u201c\u8bfb\u5df1\u4e4b\u6240\u5199\uff08read-your-writes\uff09\u201d\u4e00\u81f4\u6027\u3002\u5f53\u8fdb\u7a0bA\u81ea\u5df1\u66f4\u65b0\u4e00\u4e2a\u6570\u636e\u9879\u4e4b\u540e\uff0c\u5b83\u603b\u662f\u8bbf\u95ee\u5230\u66f4\u65b0\u8fc7\u7684\u503c\uff0c\u7edd\u4e0d\u4f1a\u770b\u5230\u65e7\u503c\u3002\u8fd9\u662f\u56e0\u679c\u4e00\u81f4\u6027\u6a21\u578b\u7684\u4e00\u4e2a\u7279\u4f8b\u3002 3\u3001\u4f1a\u8bdd\uff08Session\uff09\u4e00\u81f4\u6027\u3002\u8fd9\u662f\u4e0a\u4e00\u4e2a\u6a21\u578b\u7684\u5b9e\u7528\u7248\u672c\uff0c\u5b83\u628a\u8bbf\u95ee\u5b58\u50a8\u7cfb\u7edf\u7684\u8fdb\u7a0b\u653e\u5230\u4f1a\u8bdd\u7684\u4e0a\u4e0b\u6587\u4e2d\u3002\u53ea\u8981\u4f1a\u8bdd\u8fd8\u5b58\u5728\uff0c\u7cfb\u7edf\u5c31\u4fdd\u8bc1\u201c\u8bfb\u5df1\u4e4b\u6240\u5199\u201d\u4e00\u81f4\u6027\u3002\u5982\u679c\u7531\u4e8e\u67d0\u4e9b\u5931\u8d25\u60c5\u5f62\u4ee4\u4f1a\u8bdd\u7ec8\u6b62\uff0c\u5c31\u8981\u5efa\u7acb\u65b0\u7684\u4f1a\u8bdd\uff0c\u800c\u4e14\u7cfb\u7edf\u7684\u4fdd\u8bc1\u4e0d\u4f1a\u5ef6\u7eed\u5230\u65b0\u7684\u4f1a\u8bdd\u3002 4\u3001\u5355\u8c03\uff08Monotonic\uff09\u8bfb\u4e00\u81f4\u6027\u3002\u5982\u679c\u8fdb\u7a0b\u5df2\u7ecf\u770b\u5230\u8fc7\u6570\u636e\u5bf9\u8c61\u7684\u67d0\u4e2a\u503c\uff0c\u90a3\u4e48\u4efb\u4f55\u540e\u7eed\u8bbf\u95ee\u90fd\u4e0d\u4f1a\u8fd4\u56de\u5728\u90a3\u4e2a\u503c\u4e4b\u524d\u7684\u503c\u3002 5\u3001\u5355\u8c03\u5199\u4e00\u81f4\u6027\u3002\u7cfb\u7edf\u4fdd\u8bc1\u6765\u81ea\u540c\u4e00\u4e2a\u8fdb\u7a0b\u7684\u5199\u64cd\u4f5c\u987a\u5e8f\u6267\u884c\u3002\u8981\u662f\u7cfb\u7edf\u4e0d\u80fd\u4fdd\u8bc1\u8fd9\u79cd\u7a0b\u5ea6\u7684\u4e00\u81f4\u6027\uff0c\u5c31\u975e\u5e38\u96be\u4ee5\u7f16\u7a0b\u4e86\u3002","title":"\u6700\u7ec8\u4e00\u81f4\u6027"},{"location":"Distributed-computing/Theory/Consistency/Primer/#consensus","text":"\u5171\u8bc6\u95ee\u9898\u4e2d\u6240\u6709\u7684\u8282\u70b9\u8981\u6700\u7ec8\u8fbe\u6210\u5171\u8bc6\uff0c\u7531\u4e8e\u6700\u7ec8\u76ee\u6807\u662f\u6240\u6709\u8282\u70b9\u90fd\u8981\u8fbe\u6210\u4e00\u81f4\uff0c\u6240\u4ee5\u6839\u672c\u4e0d\u5b58\u5728\u4e00\u81f4\u6027\u5f3a\u5f31\u4e4b\u5206\u3002 \u4f8b\u5982\uff0cPaxos\u662f\u5171\u8bc6\uff08Consensus\uff09\u7b97\u6cd5\u800c\u4e0d\u662f\u5f3a\u4e00\u81f4\u6027\uff08Consistency\uff09\u534f\u8bae\u3002\u5171\u8bc6\u7b97\u6cd5\u6ca1\u6709\u4e00\u81f4\u6027\u7ea7\u522b\u7684\u533a\u5206\u3002","title":"\u5171\u8bc6\uff08Consensus)"},{"location":"Distributed-computing/Theory/Consistency/Primer/#csdn_1","text":"\u4e00\u81f4\u6027\uff08Consistency\uff09\u662f\u6307\u591a\u526f\u672c\uff08Replications\uff09\u95ee\u9898\u4e2d\u7684\u6570\u636e\u4e00\u81f4\u6027\u3002\u5173\u4e8e\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u4e00\u81f4\u6027\u6a21\u578b\u6709\u4ee5\u4e0b\u51e0\u79cd\uff1a","title":"csdn \u5206\u5e03\u5f0f\u4e00\u81f4\u6027\u534f\u8bae"},{"location":"Distributed-computing/Theory/Consistency/Primer/#_3","text":"\u5f53\u66f4\u65b0\u64cd\u4f5c\u5b8c\u6210\u4e4b\u540e\uff0c\u4efb\u4f55\u591a\u4e2a**\u540e\u7eed**\u8fdb\u7a0b\u6216\u8005\u7ebf\u7a0b\u7684\u8bbf\u95ee\u90fd\u4f1a**\u8fd4\u56de\u6700\u65b0\u7684\u66f4\u65b0\u8fc7\u7684\u503c**\uff0c\u76f4\u5230\u8fd9\u4e2a\u6570\u636e\u88ab\u5176\u4ed6\u6570\u636e\u66f4\u65b0\u4e3a\u6b62\u3002 \u4f46\u662f\u8fd9\u79cd\u5b9e\u73b0\u5bf9\u6027\u80fd\u5f71\u54cd\u8f83\u5927\uff0c\u56e0\u4e3a\u8fd9\u610f\u5473\u7740\uff0c\u53ea\u8981\u4e0a\u6b21\u7684\u64cd\u4f5c\u6ca1\u6709\u5904\u7406\u5b8c\uff0c\u5c31\u4e0d\u80fd\u8ba9\u7528\u6237\u8bfb\u53d6\u6570\u636e\u3002","title":"\u5f3a\u4e00\u81f4\u6027"},{"location":"Distributed-computing/Theory/Consistency/Primer/#_4","text":"\u7cfb\u7edf\u5e76\u4e0d\u4fdd\u8bc1\u8fdb\u7a0b\u6216\u8005\u7ebf\u7a0b\u7684\u8bbf\u95ee\u90fd\u4f1a\u8fd4\u56de\u6700\u65b0\u66f4\u65b0\u8fc7\u7684\u503c\u3002\u7cfb\u7edf\u5728\u6570\u636e\u5199\u5165\u6210\u529f\u4e4b\u540e\uff0c \u4e0d\u627f\u8bfa\u7acb\u5373\u53ef\u4ee5\u8bfb\u5230\u6700\u65b0\u5199\u5165\u7684\u503c \uff0c\u4e5f\u4e0d\u4f1a\u5177\u4f53\u7684\u627f\u8bfa\u591a\u4e45\u4e4b\u540e\u53ef\u4ee5\u8bfb\u5230\u3002\u751a\u81f3\u4e0d\u80fd\u4fdd\u8bc1\u53ef\u4ee5\u8bbf\u95ee\u5230\u3002","title":"\u5f31\u4e00\u81f4\u6027"},{"location":"Distributed-computing/Theory/Consistency/Primer/#_5","text":"\u6700\u7ec8\u4e00\u81f4\u6027\u4e5f\u662f\u5f31\u4e00\u81f4\u6027\u7684\u4e00\u79cd \uff0c\u5b83\u65e0\u6cd5\u4fdd\u8bc1\u6570\u636e\u66f4\u65b0\u540e\uff0c\u6240\u6709\u540e\u7eed\u7684\u8bbf\u95ee\u90fd\u80fd\u770b\u5230\u6700\u65b0\u6570\u503c\uff0c\u800c\u662f\u9700\u8981\u4e00\u4e2a\u65f6\u95f4\uff0c\u5728\u8fd9\u4e2a\u65f6\u95f4\u4e4b\u540e\u53ef\u4ee5\u4fdd\u8bc1\u8fd9\u4e00\u70b9\uff08 \u5c31\u662f\u5728\u4e00\u6bb5\u65f6\u95f4\u540e\uff0c\u8282\u70b9\u95f4\u7684\u6570\u636e\u4f1a\u6700\u7ec8\u8fbe\u5230\u4e00\u81f4\u72b6\u6001 \uff09\uff0c\u800c\u5728\u8fd9\u4e2a\u65f6\u95f4\u5185\uff0c\u6570\u636e\u4e5f\u8bb8\u662f\u4e0d\u4e00\u81f4\u7684\uff0c\u8fd9\u4e2a\u7cfb\u7edf\u65e0\u6cd5\u4fdd\u8bc1\u5f3a\u4e00\u81f4\u6027\u7684\u65f6\u95f4\u7247\u6bb5\u88ab\u79f0\u4e3a\u300c\u4e0d\u4e00\u81f4\u7a97\u53e3\u300d\u3002\u4e0d\u4e00\u81f4\u7a97\u53e3\u7684\u65f6\u95f4\u957f\u77ed\u53d6\u51b3\u4e8e\u5f88\u591a\u56e0\u7d20\uff0c\u6bd4\u5982\u5907\u4efd\u6570\u636e\u7684\u4e2a\u6570\u3001\u7f51\u7edc\u4f20\u8f93\u5ef6\u8fdf\u901f\u5ea6\u3001\u7cfb\u7edf\u8d1f\u8f7d\u7b49\u3002 \u6700\u7ec8\u4e00\u81f4\u6027\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53c8\u6709\u591a\u79cd\u53d8\u79cd\uff1a \u7c7b\u578b \u8bf4\u660e \u56e0\u679c\u4e00\u81f4\u6027 \u5982\u679c A \u8fdb\u7a0b\u5728\u66f4\u65b0\u4e4b\u540e\u5411 B \u8fdb\u7a0b\u901a\u77e5\u66f4\u65b0\u7684\u5b8c\u6210\uff0c\u90a3\u4e48 B \u7684\u8bbf\u95ee\u64cd\u4f5c\u5c06\u4f1a\u8fd4\u56de\u66f4\u65b0\u7684\u503c\u3002\u800c\u6ca1\u6709\u56e0\u679c\u5173\u7cfb\u7684 C \u8fdb\u7a0b\u5c06\u4f1a\u9075\u5faa\u6700\u7ec8\u4e00\u81f4\u6027\u7684\u89c4\u5219\uff08C \u5728\u4e0d\u4e00\u81f4\u7a97\u53e3\u5185\u8fd8\u662f\u770b\u5230\u662f\u65e7\u503c\uff09\u3002 \u8bfb\u4f60\u6240\u5199\u4e00\u81f4\u6027 \u56e0\u679c\u4e00\u81f4\u6027\u7684\u7279\u5b9a\u5f62\u5f0f\u3002\u4e00\u4e2a\u8fdb\u7a0b\u8fdb\u884c\u6570\u636e\u66f4\u65b0\u540e\uff0c\u4f1a\u7ed9\u81ea\u5df1\u53d1\u9001\u4e00\u6761\u901a\u77e5\uff0c\u8be5\u8fdb\u7a0b\u540e\u7eed\u7684\u64cd\u4f5c\u90fd\u4f1a\u4ee5\u6700\u65b0\u503c\u4f5c\u4e3a\u57fa\u7840\uff0c\u800c\u5176\u4ed6\u7684\u8fdb\u7a0b\u8fd8\u662f\u53ea\u80fd\u5728\u4e0d\u4e00\u81f4\u7a97\u53e3\u4e4b\u540e\u624d\u80fd\u770b\u5230\u6700\u65b0\u503c\u3002 \u4f1a\u8bdd\u4e00\u81f4\u6027 \u8bfb\u4f60\u6240\u5199\u4e00\u81f4\u6027\u7684\u7279\u5b9a\u5f62\u5f0f\u3002\u8fdb\u7a0b\u5728\u8bbf\u95ee\u5b58\u50a8\u7cfb\u7edf\u540c\u4e00\u4e2a\u4f1a\u8bdd\u5185\uff0c\u7cfb\u7edf\u4fdd\u8bc1\u8be5\u8fdb\u7a0b\u53ef\u4ee5\u8bfb\u53d6\u5230\u6700\u65b0\u4e4b\uff0c\u4f46\u5982\u679c\u4f1a\u8bdd\u7ec8\u6b62\uff0c\u91cd\u65b0\u8fde\u63a5\u540e\uff0c\u5982\u679c\u6b64\u65f6\u8fd8\u5728\u4e0d\u4e00\u81f4\u7a97\u53e3\u5185\uff0c\u8fd8\u662f\u53ef\u5ae9\u8bfb\u53d6\u5230\u65e7\u503c\u3002 \u5355\u8c03\u8bfb\u4e00\u81f4\u6027 \u5982\u679c\u4e00\u4e2a\u8fdb\u7a0b\u5df2\u7ecf\u8bfb\u53d6\u5230\u4e00\u4e2a\u7279\u5b9a\u503c\uff0c\u90a3\u4e48\u8be5\u8fdb\u7a0b\u4e0d\u4f1a\u8bfb\u53d6\u5230\u8be5\u503c\u4ee5\u524d\u7684\u4efb\u4f55\u503c\u3002 \u5355\u8c03\u5199\u4e00\u81f4\u6027 \u7cfb\u7edf\u4fdd\u8bc1\u5bf9\u540c\u4e00\u4e2a\u8fdb\u7a0b\u7684\u5199\u64cd\u4f5c\u4e32\u884c\u5316\u3002 \u4e00\u81f4\u6027\u6a21\u578b","title":"\u6700\u7ec8\u4e00\u81f4\u6027"},{"location":"Distributed-computing/Theory/Consistency/Primer/#cnblogs","text":"\u5f3a\u4e00\u81f4\u6027\uff1a\u7cfb\u7edf\u4e2d\u7684\u67d0\u4e2a\u6570\u636e\u88ab\u6210\u529f\u66f4\u65b0\u540e\uff0c\u540e\u7eed\u4efb\u4f55\u5bf9\u8be5\u6570\u636e\u7684\u8bfb\u53d6\u64cd\u4f5c\u90fd\u5c06\u5f97\u5230\u66f4\u65b0\u540e\u7684\u503c\uff1b \u5f31\u4e00\u81f4\u6027\uff1a\u7cfb\u7edf\u4e2d\u7684\u67d0\u4e2a\u6570\u636e\u88ab\u66f4\u65b0\u540e\uff0c\u540e\u7eed\u5bf9\u8be5\u6570\u636e\u7684\u8bfb\u53d6\u64cd\u4f5c\u53ef\u80fd\u5f97\u5230\u66f4\u65b0\u540e\u7684\u503c\uff0c\u4e5f\u53ef\u80fd\u662f\u66f4\u6539\u524d\u7684\u503c\u3002\u4f46\u7ecf\u8fc7\u201c\u4e0d\u4e00\u81f4\u65f6\u95f4\u7a97\u53e3\u201d\u8fd9\u6bb5\u65f6\u95f4\u540e\uff0c\u540e\u7eed\u5bf9\u8be5\u6570\u636e\u7684\u8bfb\u53d6\u90fd\u662f\u66f4\u65b0\u540e\u7684\u503c\uff1b \u6700\u7ec8\u4e00\u81f4\u6027\uff1a\u662f\u5f31\u4e00\u81f4\u6027\u7684\u7279\u6b8a\u5f62\u5f0f\uff0c\u5b58\u50a8\u7cfb\u7edf\u4fdd\u8bc1\u5728\u6ca1\u6709\u65b0\u7684\u66f4\u65b0\u7684\u6761\u4ef6\u4e0b\uff0c\u6700\u7ec8\u6240\u6709\u7684\u8bbf\u95ee\u90fd\u662f\u6700\u540e\u66f4\u65b0\u7684\u503c\u3002","title":"cnblogs \u5f3a\u4e00\u81f4\u6027\u3001\u5f31\u4e00\u81f4\u6027\u3001\u6700\u7ec8\u4e00\u81f4\u6027"},{"location":"Distributed-computing/Theory/Consistency/wikipedia-Consistency-model/","text":"wikipedia Consistency model In computer science , consistency models are used in distributed systems like distributed shared memory systems or distributed data stores (such as a filesystems , databases , optimistic replication systems or web caching ). The system is said to support a given model if operations on memory follow specific rules . The data consistency model specifies a contract between programmer and system, wherein the system guarantees that if the programmer follows the rules, memory will be consistent and the results of reading, writing, or updating memory will be predictable . NOTE: \u4e0a\u8ff0**predictable**\uff0c\u8ba9\u6211\u60f3\u8d77\u4e86\"make it computational\"\u3002 **predictable**\u8bf4\u767d\u4e86\u5176\u5b9e\u5c31\u662f\u6570\u636e\u4e0d\u4f1a\u4e22\u5931\uff0c\u4e0d\u4f1a\u51fa\u73b0\u9519\u8bef\u3002 This is different from coherence , which occurs in systems that are cached or cache-less, and is consistency of data with respect to all processors. Coherence deals with maintaining a global order in which writes to a single location or single variable are seen by all processors. Consistency deals with the ordering of operations to multiple locations with respect to all processors. NOTE: coherence\uff0c\u5728\u5de5\u7a0bhardware\u4e2d\u8fdb\u884c\u4e86\u63cf\u8ff0\uff1b \u4e0a\u8ff0consistency\uff0c\u53ef\u4ee5\u4f7f\u7528consistency model abstract machine\u6765\u63cf\u8ff0\uff1b High level languages , such as C++ and Java , partially maintain the contract by translating memory operations into low-level operations in a way that preserves memory semantics . To hold to the contract, compilers may reorder some memory instructions, and library calls such as pthread_mutex_lock() encapsulate required synchronization.[ 1] NOTE: programming language\u7684memory model\u3002 Verifying sequential consistency through model checking is undecidable in general, even for finite-state cache coherence protocols.[ 2] Consistency models define rules for the apparent order and visibility of updates, and are on a continuum with tradeoffs.[ 3] Example NOTE: \u539f\u6587\u8fd9\u4e00\u6bb5\u7ed9\u51fa\u7684\u4f8b\u5b50\u662f\u5bb9\u6613\u7406\u89e3\u7684 Types There are two methods to define and categorize consistency models ; issue and view. 1\u3001Issue Issue method describes the restrictions(\u9650\u5236) that define how a process can issue operations. NOTE: \u5b9a\u4e49\u8fdb\u7a0b\u5982\u4f55\u53d1\u51fa\u64cd\u4f5c\u7684\u9650\u5236 2\u3001View View method which defines the order of operations visible to processes. NOTE: \u5b9a\u4e49\u8fdb\u7a0b\u53ef\u89c1\u7684\u64cd\u4f5c\u987a\u5e8f\uff0c\u8fd9\u662f\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u6982\u5ff5\uff0c\u5982\u4f55\u6765\u7406\u89e3\"the order of operations visible to processes\"\uff1f For example, a consistency model can define that a process is not allowed to issue an operation until all previously issued operations are completed. Different consistency models enforce different conditions . One consistency model can be considered stronger than another if it requires all conditions of that model and more. In other words, a model with fewer constraints is considered a weaker consistency model. NOTE: level and tradeoff These models define how the hardware needs to be laid out and at high-level, how the programmer must code. The chosen model also affects how the compiler can re-order instructions. Generally, if control dependencies between instructions and if writes to same location are ordered , then the compiler can reorder as required. However, with the models described below, some may allow writes before loads to be reordered while some may not. Strict consistency Strict consistency is the strongest consistency model. Under this model, a write to a variable by any processor needs to be seen instantaneously by all processors. NOTE: \u4e0a\u9762\u63cf\u8ff0\u4e86strict consistency\u7684constrain The strict model diagram and non-strict model diagrams describe the time constraint \u2013 instantaneous . It can be better understood as though a global clock is present in which every write should be reflected in all processor caches by the end of that clock period . The next operation must happen only in the next clock period. This is the most rigid model and is impossible to implement (with current technology). NOTE: \u9700\u8981\u6ce8\u610f: \u5f53\u524d\u7684\u6280\u672f\u6c34\u5e73\uff0cstrict consistency\u662f\u65e0\u6cd5\u5b9e\u73b0\u7684\uff0c\u4e3a\u4ec0\u4e48? \u5728\u4e0b\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u56de\u7b54\u4e86\u8fd9\u4e2a\u95ee\u9898: the speed of light . \u90a3\u4eba\u7c7b\u7684\u7a81\u7834\u5b83\u7684\u76ee\u6807\u5728\u54ea\u5462\uff1f\u5728\u4e0b\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u56de\u7b54\u4e86\u8fd9\u4e2a\u95ee\u9898: quantum entanglement and quantum computing In this model, the programmer\u2019s expected result will be received every time. It is deterministic. A distributed system with many nodes will take some time to copy information written to one node to all the other nodes responsible for replicating that information. That time can't be zero (without quantum entanglement and quantum computing \u91cf\u5b50\u7ea0\u7f20\u548c\u91cf\u5b50\u8ba1\u7b97) because it takes time for information to propagate through space , and there is a limit to how fast information can travel through space: the speed of light . Therefore, strict consistency is impossible (with current production level technology). The best one can do is design a system where the time-to-replicate approaches the theoretical minimum. Sequential consistency The sequential consistency model was proposed by Lamport(1979). It is a weaker memory model than strict consistency model. A write to a variable does not have to be seen instantaneously, however, writes to variables by different processors have to be seen in the same order by all processors . As defined by Lamport(1979),[ 4] sequential consistency is met if \"the result of any execution is the same as if the operations of all the processors were executed in some sequential order , and the operations of each individual processor appear in this sequence in the order specified by its program.\" NOTE: \u5982\u4f55\u6765\u7406\u89e3Lamport(1979) \u5173\u4e8e sequential order\u7684\u5b9a\u4e49\u5462\uff1f\u53c2\u89c1 \"introduction\" \u7ae0\u8282\uff1b \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7684\"writes to variables by different processors have to be seen in the same order by all processors\"\u8981\u5982\u4f55\u8fdb\u884c\u7406\u89e3\u5462\uff1f\u5728 csdn \u5f3a\u4e00\u81f4\u6027\u3001\u987a\u5e8f\u4e00\u81f4\u6027\u3001\u5f31\u4e00\u81f4\u6027\u548c\u5171\u8bc6 \u4e2d\u7ed9\u51fa\u4e86\u4f8b\u5b50\uff0c\u53ef\u4ee5\u7ed3\u5408\u5176\u4e2d\u7684\u4f8b\u5b50\u6765\u8fdb\u884c\u7406\u89e3\u3002 Program order within each processor and sequential ordering of operations between processors should be maintained. In order to preserve sequential order of execution between processors , all operations must appear to execute instantaneously or atomically with respect to every other processor( \u4e3a\u4e86\u4fdd\u6301\u5904\u7406\u5668\u4e4b\u95f4\u7684\u8fde\u7eed\u6267\u884c\u987a\u5e8f\uff0c\u6240\u6709\u64cd\u4f5c\u5fc5\u987b\u770b\u8d77\u6765\u76f8\u5bf9\u4e8e\u6bcf\u4e2a\u5176\u4ed6\u5904\u7406\u5668\u5373\u65f6\u6216\u539f\u5b50\u5730\u6267\u884c). These operations need only \"appear\" to be completed because it is physically impossible to send information instantaneously. For instance, once a bus line is posted with information, it is guaranteed that all processors will see the information at the same instant. Thus, passing the information to the bus line completes the execution with respect to all processors and has appeared to have been executed. Cache-less architectures or cached architectures with interconnect networks that are not instantaneous can contain a slow path between processors and memories. These slow paths can result in sequential inconsistency, because some memories receive the broadcast data faster than others. Sequential consistency can produce non-deterministic results. This is because the sequence of sequential operations between processors can be different during different runs of the program. All memory operations need to happen in the program order. NOTE: \u4e0a\u9762\u7684\u8fd9\u4e9b\u5185\u5bb9\uff0c\u4f7f\u7528 \"\u80fd\u591f\u4f7f\u7528C++11 std::atomic \u7684 sequential consistency\u6765\u8fdb\u884c\u6a21\u62df\" \u540e\uff0c\u662f\u975e\u5e38\u5bb9\u6613\u7406\u89e3\u7684\u3002 Linearizability (also known as atomic consistency ) can be defined as sequential consistency with the real-time constraint. Causal consistency NOTE: \u56e0\u679c\u4e00\u81f4\u6027 Causal consistency is a weakening model of sequential consistency by categorizing events into those causally related and those that are not. It defines that only write operations that are causally related, need to be seen in the same order by all processes. This model relaxes sequential consistency on concurrent writes by a processor and on writes that are not causally related. Two writes can become causally related if one write to a variable is dependent on a previous write to any variable if the processor doing the second write has just read the first write. The two writes could have been done by the same processor or by different processors. As in sequential consistency, reads do not need to reflect changes instantaneously, however, they need to reflect all changes to a variable sequentially. Sequence P1 P2 1 W1( x )3 2 W2(x)5 3 R1( x )3 W1 is not causally related to W2. R1 would be sequentially inconsistent but is causally consistent .[ clarification needed ][ 5] Sequence P1 P2 P3 P4 1 W(x)1 R(x)1 R(x)1 R(x)1 2 W(x)3 3 W(x)2 R(x)3 R(x)2 4 R(x)2 R(x)3 W(x)1 and W(x)2 are causally related due to the read made by P2 to x before W(x)2.[ 5] Consistency and replication NOTE:\u8fd9\u5c31\u662f\u5bfb\u5e38\u6240\u8bf4\u7684 \"\u4e3b\u5907\u7684\u4e00\u81f4\u6027\"\uff0c\u8fd9\u662f\u5f88\u591a\u7684data system\u91c7\u7528\u7684\u65b9\u6cd5\uff0credis\u5c31\u662f\u91c7\u7528\u7684\u8fd9\u79cd\u65b9\u5f0f\u3002","title":"Introduction"},{"location":"Distributed-computing/Theory/Consistency/wikipedia-Consistency-model/#wikipedia#consistency#model","text":"In computer science , consistency models are used in distributed systems like distributed shared memory systems or distributed data stores (such as a filesystems , databases , optimistic replication systems or web caching ). The system is said to support a given model if operations on memory follow specific rules . The data consistency model specifies a contract between programmer and system, wherein the system guarantees that if the programmer follows the rules, memory will be consistent and the results of reading, writing, or updating memory will be predictable . NOTE: \u4e0a\u8ff0**predictable**\uff0c\u8ba9\u6211\u60f3\u8d77\u4e86\"make it computational\"\u3002 **predictable**\u8bf4\u767d\u4e86\u5176\u5b9e\u5c31\u662f\u6570\u636e\u4e0d\u4f1a\u4e22\u5931\uff0c\u4e0d\u4f1a\u51fa\u73b0\u9519\u8bef\u3002 This is different from coherence , which occurs in systems that are cached or cache-less, and is consistency of data with respect to all processors. Coherence deals with maintaining a global order in which writes to a single location or single variable are seen by all processors. Consistency deals with the ordering of operations to multiple locations with respect to all processors. NOTE: coherence\uff0c\u5728\u5de5\u7a0bhardware\u4e2d\u8fdb\u884c\u4e86\u63cf\u8ff0\uff1b \u4e0a\u8ff0consistency\uff0c\u53ef\u4ee5\u4f7f\u7528consistency model abstract machine\u6765\u63cf\u8ff0\uff1b High level languages , such as C++ and Java , partially maintain the contract by translating memory operations into low-level operations in a way that preserves memory semantics . To hold to the contract, compilers may reorder some memory instructions, and library calls such as pthread_mutex_lock() encapsulate required synchronization.[ 1] NOTE: programming language\u7684memory model\u3002 Verifying sequential consistency through model checking is undecidable in general, even for finite-state cache coherence protocols.[ 2] Consistency models define rules for the apparent order and visibility of updates, and are on a continuum with tradeoffs.[ 3]","title":"wikipedia Consistency model"},{"location":"Distributed-computing/Theory/Consistency/wikipedia-Consistency-model/#example","text":"NOTE: \u539f\u6587\u8fd9\u4e00\u6bb5\u7ed9\u51fa\u7684\u4f8b\u5b50\u662f\u5bb9\u6613\u7406\u89e3\u7684","title":"Example"},{"location":"Distributed-computing/Theory/Consistency/wikipedia-Consistency-model/#types","text":"There are two methods to define and categorize consistency models ; issue and view. 1\u3001Issue Issue method describes the restrictions(\u9650\u5236) that define how a process can issue operations. NOTE: \u5b9a\u4e49\u8fdb\u7a0b\u5982\u4f55\u53d1\u51fa\u64cd\u4f5c\u7684\u9650\u5236 2\u3001View View method which defines the order of operations visible to processes. NOTE: \u5b9a\u4e49\u8fdb\u7a0b\u53ef\u89c1\u7684\u64cd\u4f5c\u987a\u5e8f\uff0c\u8fd9\u662f\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u6982\u5ff5\uff0c\u5982\u4f55\u6765\u7406\u89e3\"the order of operations visible to processes\"\uff1f For example, a consistency model can define that a process is not allowed to issue an operation until all previously issued operations are completed. Different consistency models enforce different conditions . One consistency model can be considered stronger than another if it requires all conditions of that model and more. In other words, a model with fewer constraints is considered a weaker consistency model. NOTE: level and tradeoff These models define how the hardware needs to be laid out and at high-level, how the programmer must code. The chosen model also affects how the compiler can re-order instructions. Generally, if control dependencies between instructions and if writes to same location are ordered , then the compiler can reorder as required. However, with the models described below, some may allow writes before loads to be reordered while some may not.","title":"Types"},{"location":"Distributed-computing/Theory/Consistency/wikipedia-Consistency-model/#strict#consistency","text":"Strict consistency is the strongest consistency model. Under this model, a write to a variable by any processor needs to be seen instantaneously by all processors. NOTE: \u4e0a\u9762\u63cf\u8ff0\u4e86strict consistency\u7684constrain The strict model diagram and non-strict model diagrams describe the time constraint \u2013 instantaneous . It can be better understood as though a global clock is present in which every write should be reflected in all processor caches by the end of that clock period . The next operation must happen only in the next clock period. This is the most rigid model and is impossible to implement (with current technology). NOTE: \u9700\u8981\u6ce8\u610f: \u5f53\u524d\u7684\u6280\u672f\u6c34\u5e73\uff0cstrict consistency\u662f\u65e0\u6cd5\u5b9e\u73b0\u7684\uff0c\u4e3a\u4ec0\u4e48? \u5728\u4e0b\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u56de\u7b54\u4e86\u8fd9\u4e2a\u95ee\u9898: the speed of light . \u90a3\u4eba\u7c7b\u7684\u7a81\u7834\u5b83\u7684\u76ee\u6807\u5728\u54ea\u5462\uff1f\u5728\u4e0b\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u56de\u7b54\u4e86\u8fd9\u4e2a\u95ee\u9898: quantum entanglement and quantum computing In this model, the programmer\u2019s expected result will be received every time. It is deterministic. A distributed system with many nodes will take some time to copy information written to one node to all the other nodes responsible for replicating that information. That time can't be zero (without quantum entanglement and quantum computing \u91cf\u5b50\u7ea0\u7f20\u548c\u91cf\u5b50\u8ba1\u7b97) because it takes time for information to propagate through space , and there is a limit to how fast information can travel through space: the speed of light . Therefore, strict consistency is impossible (with current production level technology). The best one can do is design a system where the time-to-replicate approaches the theoretical minimum.","title":"Strict consistency"},{"location":"Distributed-computing/Theory/Consistency/wikipedia-Consistency-model/#sequential#consistency","text":"The sequential consistency model was proposed by Lamport(1979). It is a weaker memory model than strict consistency model. A write to a variable does not have to be seen instantaneously, however, writes to variables by different processors have to be seen in the same order by all processors . As defined by Lamport(1979),[ 4] sequential consistency is met if \"the result of any execution is the same as if the operations of all the processors were executed in some sequential order , and the operations of each individual processor appear in this sequence in the order specified by its program.\" NOTE: \u5982\u4f55\u6765\u7406\u89e3Lamport(1979) \u5173\u4e8e sequential order\u7684\u5b9a\u4e49\u5462\uff1f\u53c2\u89c1 \"introduction\" \u7ae0\u8282\uff1b \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7684\"writes to variables by different processors have to be seen in the same order by all processors\"\u8981\u5982\u4f55\u8fdb\u884c\u7406\u89e3\u5462\uff1f\u5728 csdn \u5f3a\u4e00\u81f4\u6027\u3001\u987a\u5e8f\u4e00\u81f4\u6027\u3001\u5f31\u4e00\u81f4\u6027\u548c\u5171\u8bc6 \u4e2d\u7ed9\u51fa\u4e86\u4f8b\u5b50\uff0c\u53ef\u4ee5\u7ed3\u5408\u5176\u4e2d\u7684\u4f8b\u5b50\u6765\u8fdb\u884c\u7406\u89e3\u3002 Program order within each processor and sequential ordering of operations between processors should be maintained. In order to preserve sequential order of execution between processors , all operations must appear to execute instantaneously or atomically with respect to every other processor( \u4e3a\u4e86\u4fdd\u6301\u5904\u7406\u5668\u4e4b\u95f4\u7684\u8fde\u7eed\u6267\u884c\u987a\u5e8f\uff0c\u6240\u6709\u64cd\u4f5c\u5fc5\u987b\u770b\u8d77\u6765\u76f8\u5bf9\u4e8e\u6bcf\u4e2a\u5176\u4ed6\u5904\u7406\u5668\u5373\u65f6\u6216\u539f\u5b50\u5730\u6267\u884c). These operations need only \"appear\" to be completed because it is physically impossible to send information instantaneously. For instance, once a bus line is posted with information, it is guaranteed that all processors will see the information at the same instant. Thus, passing the information to the bus line completes the execution with respect to all processors and has appeared to have been executed. Cache-less architectures or cached architectures with interconnect networks that are not instantaneous can contain a slow path between processors and memories. These slow paths can result in sequential inconsistency, because some memories receive the broadcast data faster than others. Sequential consistency can produce non-deterministic results. This is because the sequence of sequential operations between processors can be different during different runs of the program. All memory operations need to happen in the program order. NOTE: \u4e0a\u9762\u7684\u8fd9\u4e9b\u5185\u5bb9\uff0c\u4f7f\u7528 \"\u80fd\u591f\u4f7f\u7528C++11 std::atomic \u7684 sequential consistency\u6765\u8fdb\u884c\u6a21\u62df\" \u540e\uff0c\u662f\u975e\u5e38\u5bb9\u6613\u7406\u89e3\u7684\u3002 Linearizability (also known as atomic consistency ) can be defined as sequential consistency with the real-time constraint.","title":"Sequential consistency"},{"location":"Distributed-computing/Theory/Consistency/wikipedia-Consistency-model/#causal#consistency","text":"NOTE: \u56e0\u679c\u4e00\u81f4\u6027 Causal consistency is a weakening model of sequential consistency by categorizing events into those causally related and those that are not. It defines that only write operations that are causally related, need to be seen in the same order by all processes. This model relaxes sequential consistency on concurrent writes by a processor and on writes that are not causally related. Two writes can become causally related if one write to a variable is dependent on a previous write to any variable if the processor doing the second write has just read the first write. The two writes could have been done by the same processor or by different processors. As in sequential consistency, reads do not need to reflect changes instantaneously, however, they need to reflect all changes to a variable sequentially. Sequence P1 P2 1 W1( x )3 2 W2(x)5 3 R1( x )3 W1 is not causally related to W2. R1 would be sequentially inconsistent but is causally consistent .[ clarification needed ][ 5] Sequence P1 P2 P3 P4 1 W(x)1 R(x)1 R(x)1 R(x)1 2 W(x)3 3 W(x)2 R(x)3 R(x)2 4 R(x)2 R(x)3 W(x)1 and W(x)2 are causally related due to the read made by P2 to x before W(x)2.[ 5]","title":"Causal consistency"},{"location":"Distributed-computing/Theory/Consistency/wikipedia-Consistency-model/#consistency#and#replication","text":"NOTE:\u8fd9\u5c31\u662f\u5bfb\u5e38\u6240\u8bf4\u7684 \"\u4e3b\u5907\u7684\u4e00\u81f4\u6027\"\uff0c\u8fd9\u662f\u5f88\u591a\u7684data system\u91c7\u7528\u7684\u65b9\u6cd5\uff0credis\u5c31\u662f\u91c7\u7528\u7684\u8fd9\u79cd\u65b9\u5f0f\u3002","title":"Consistency and replication"},{"location":"Distributed-computing/Theory/Fault-tolerance/","text":"Fault tolerance wikipedia Fault tolerance Fault tolerance is the property that enables a system to continue operating properly in the event of the failure of (or one or more faults within) some of its components. If its operating quality decreases at all, the decrease is proportional to the severity of the failure, as compared to a naively designed system, in which even a small failure can cause total breakdown. Fault tolerance is particularly sought after in high-availability or life-critical systems . The ability of maintaining functionality when portions of a system break down is referred to as graceful degradation .[ 1] NOTE: \u5bb9\u9519\u662f\u4f7f\u7cfb\u7edf\u5728\u67d0\u4e9b\u7ec4\u4ef6\u53d1\u751f\u6545\u969c\uff08\u6216\u5176\u4e2d\u7684\u4e00\u4e2a\u6216\u591a\u4e2a\u6545\u969c\uff09\u65f6\u80fd\u591f\u7ee7\u7eed\u6b63\u5e38\u8fd0\u884c\u7684\u5c5e\u6027\u3002 \u5982\u679c\u7cfb\u7edf\u7684\u8fd0\u884c\u8d28\u91cf\u5b8c\u5168\u4e0b\u964d\uff0c\u4e0b\u964d\u7684\u7a0b\u5ea6\u4e0e\u6545\u969c\u7684\u4e25\u91cd\u7a0b\u5ea6\u6210\u6bd4\u4f8b\uff0c\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5728\u5929\u771f\u8bbe\u8ba1\u7684\u7cfb\u7edf\u4e2d\uff0c\u5373\u4f7f\u662f\u5f88\u5c0f\u7684\u6545\u969c\u4e5f\u53ef\u80fd\u5bfc\u81f4\u6574\u4f53\u6545\u969c\u3002 \u5728\u9ad8\u53ef\u7528\u6027\u6216\u5bf9\u751f\u547d\u81f3\u5173\u91cd\u8981\u7684\u7cfb\u7edf\u4e2d\uff0c\u5c24\u5176\u9700\u8981\u5bb9\u9519\u529f\u80fd\u3002 \u5f53\u7cfb\u7edf\u7684\u4e00\u90e8\u5206\u53d1\u751f\u6545\u969c\u65f6\u4fdd\u6301\u529f\u80fd\u7684\u80fd\u529b\u79f0\u4e3a\u6b63\u5e38\u964d\u7ea7\u3002 A fault-tolerant design enables a system to continue its intended operation, possibly at a reduced level, rather than failing completely, when some part of the system fails .[ 2] The term is most commonly used to describe computer systems designed to continue more or less fully operational with, perhaps, a reduction in throughput or an increase in response time in the event of some partial failure. That is, the system as a whole is not stopped due to problems either in the hardware or the software . An example in another field is a motor vehicle designed so it will continue to be drivable if one of the tires\uff08\u8f6e\u80ce\uff09 is punctured\uff08\u523a\u7834\uff09, or a structure that is able to retain its integrity in the presence of damage due to causes such as fatigue , corrosion , manufacturing flaws, or impact. NOTE: \u5bb9\u9519\u8bbe\u8ba1\u4f7f\u7cfb\u7edf\u53ef\u4ee5\u5728\u7cfb\u7edf\u7684\u67d0\u4e9b\u90e8\u5206\u53d1\u751f\u6545\u969c\u65f6\u7ee7\u7eed\u4ee5\u9884\u671f\u7684\u65b9\u5f0f\u8fd0\u884c\uff08\u53ef\u80fd\u4f1a\u964d\u4f4e\u7ea7\u522b\uff09\uff0c\u800c\u4e0d\u662f\u5b8c\u5168\u6545\u969c\u3002[2] \u8be5\u672f\u8bed\u6700\u5e38\u7528\u4e8e\u63cf\u8ff0\u8bbe\u8ba1\u4e3a\u6216\u591a\u6216\u5c11\u5730\u5b8c\u5168\u6b63\u5e38\u8fd0\u884c\u7684\u8ba1\u7b97\u673a\u7cfb\u7edf\uff0c\u5728\u67d0\u4e9b\u5c40\u90e8\u6545\u969c\u7684\u60c5\u51b5\u4e0b\uff0c\u541e\u5410\u91cf\u53ef\u80fd\u4f1a\u964d\u4f4e\u6216\u54cd\u5e94\u65f6\u95f4\u4f1a\u589e\u52a0\u3002 \u5373\uff0c\u7531\u4e8e\u786c\u4ef6\u6216\u8f6f\u4ef6\u7684\u95ee\u9898\uff0c\u6574\u4e2a\u7cfb\u7edf\u4e0d\u4f1a\u505c\u6b62\u3002 \u53e6\u4e00\u4e2a\u9886\u57df\u7684\u4f8b\u5b50\u662f\u8bbe\u8ba1\u6210\u5728\u8f6e\u80ce\u4e2d\u7684\u4e00\u4e2a\u88ab\u523a\u7834\u7684\u60c5\u51b5\u4e0b\u4ecd\u53ef\u7ee7\u7eed\u9a7e\u9a76\u7684\u673a\u52a8\u8f66\u8f86\uff0c\u6216\u5728\u7531\u4e8e\u75b2\u52b3\uff0c\u8150\u8680\uff0c\u5236\u9020\u7b49\u539f\u56e0\u800c\u9020\u6210\u635f\u574f\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u4fdd\u6301\u5176\u5b8c\u6574\u6027\u7684\u7ed3\u6784\u3002 \u7f3a\u9677\u6216\u5f71\u54cd\u3002 Within the scope of an individual system, fault tolerance can be achieved by anticipating exceptional conditions and building the system to cope with them, and, in general, aiming for self-stabilization so that the system converges towards an error-free state. However, if the consequences of a system failure are catastrophic, or the cost of making it sufficiently reliable is very high, a better solution may be to use some form of duplication. In any case, if the consequence of a system failure is so catastrophic, the system must be able to use reversion to fall back to a safe mode. This is similar to roll-back recovery but can be a human action if humans are present in the loop. NOTE: \u5728\u5355\u4e2a\u7cfb\u7edf\u7684\u8303\u56f4\u5185\uff0c\u53ef\u4ee5\u901a\u8fc7\u9884\u6599\u5230\u5f02\u5e38\u60c5\u51b5\u5e76\u6784\u5efa\u7cfb\u7edf\u4ee5\u5e94\u5bf9\u8fd9\u4e9b\u60c5\u51b5\u6765\u5b9e\u73b0\u5bb9\u9519\u80fd\u529b\uff0c\u5e76\u4e14\u4e00\u822c\u800c\u8a00\uff0c\u5176\u76ee\u6807\u662f\u5b9e\u73b0\u81ea\u6211\u7a33\u5b9a\uff0c\u4ece\u800c\u4f7f\u7cfb\u7edf\u6536\u655b\u4e8e\u65e0\u9519\u8bef\u72b6\u6001\u3002 \u4f46\u662f\uff0c\u5982\u679c\u7cfb\u7edf\u6545\u969c\u7684\u540e\u679c\u662f\u707e\u96be\u6027\u7684\uff0c\u6216\u8005\u4f7f\u5176\u8db3\u591f\u53ef\u9760\u7684\u6210\u672c\u975e\u5e38\u9ad8\uff0c\u5219\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u53ef\u80fd\u662f\u4f7f\u7528\u67d0\u79cd\u5f62\u5f0f\u7684\u590d\u5236\u3002 \u65e0\u8bba\u5982\u4f55\uff0c\u5982\u679c\u7cfb\u7edf\u6545\u969c\u7684\u540e\u679c\u662f\u707e\u96be\u6027\u7684\uff0c\u5219\u7cfb\u7edf\u5fc5\u987b\u80fd\u591f\u4f7f\u7528\u8fd8\u539f\u6765\u9000\u56de\u5230\u5b89\u5168\u6a21\u5f0f\u3002 \u8fd9\u7c7b\u4f3c\u4e8e\u56de\u6eda\u6062\u590d\uff0c\u4f46\u5982\u679c\u5faa\u73af\u4e2d\u5b58\u5728\u4eba\u5458\uff0c\u5219\u53ef\u80fd\u662f\u4eba\u4e3a\u7684\u884c\u4e3a\u3002 Example Distributed locks with Redis Redis Cluster Specification Redis: replication, part 1 \u2013 an overview. Replication vs Sharding. Sentinel vs Cluster. Redis topology. TODO wikipedia Fault-tolerant computer system searchdisasterrecovery fault-tolerant","title":"Introduction"},{"location":"Distributed-computing/Theory/Fault-tolerance/#fault#tolerance","text":"","title":"Fault tolerance"},{"location":"Distributed-computing/Theory/Fault-tolerance/#wikipedia#fault#tolerance","text":"Fault tolerance is the property that enables a system to continue operating properly in the event of the failure of (or one or more faults within) some of its components. If its operating quality decreases at all, the decrease is proportional to the severity of the failure, as compared to a naively designed system, in which even a small failure can cause total breakdown. Fault tolerance is particularly sought after in high-availability or life-critical systems . The ability of maintaining functionality when portions of a system break down is referred to as graceful degradation .[ 1] NOTE: \u5bb9\u9519\u662f\u4f7f\u7cfb\u7edf\u5728\u67d0\u4e9b\u7ec4\u4ef6\u53d1\u751f\u6545\u969c\uff08\u6216\u5176\u4e2d\u7684\u4e00\u4e2a\u6216\u591a\u4e2a\u6545\u969c\uff09\u65f6\u80fd\u591f\u7ee7\u7eed\u6b63\u5e38\u8fd0\u884c\u7684\u5c5e\u6027\u3002 \u5982\u679c\u7cfb\u7edf\u7684\u8fd0\u884c\u8d28\u91cf\u5b8c\u5168\u4e0b\u964d\uff0c\u4e0b\u964d\u7684\u7a0b\u5ea6\u4e0e\u6545\u969c\u7684\u4e25\u91cd\u7a0b\u5ea6\u6210\u6bd4\u4f8b\uff0c\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5728\u5929\u771f\u8bbe\u8ba1\u7684\u7cfb\u7edf\u4e2d\uff0c\u5373\u4f7f\u662f\u5f88\u5c0f\u7684\u6545\u969c\u4e5f\u53ef\u80fd\u5bfc\u81f4\u6574\u4f53\u6545\u969c\u3002 \u5728\u9ad8\u53ef\u7528\u6027\u6216\u5bf9\u751f\u547d\u81f3\u5173\u91cd\u8981\u7684\u7cfb\u7edf\u4e2d\uff0c\u5c24\u5176\u9700\u8981\u5bb9\u9519\u529f\u80fd\u3002 \u5f53\u7cfb\u7edf\u7684\u4e00\u90e8\u5206\u53d1\u751f\u6545\u969c\u65f6\u4fdd\u6301\u529f\u80fd\u7684\u80fd\u529b\u79f0\u4e3a\u6b63\u5e38\u964d\u7ea7\u3002 A fault-tolerant design enables a system to continue its intended operation, possibly at a reduced level, rather than failing completely, when some part of the system fails .[ 2] The term is most commonly used to describe computer systems designed to continue more or less fully operational with, perhaps, a reduction in throughput or an increase in response time in the event of some partial failure. That is, the system as a whole is not stopped due to problems either in the hardware or the software . An example in another field is a motor vehicle designed so it will continue to be drivable if one of the tires\uff08\u8f6e\u80ce\uff09 is punctured\uff08\u523a\u7834\uff09, or a structure that is able to retain its integrity in the presence of damage due to causes such as fatigue , corrosion , manufacturing flaws, or impact. NOTE: \u5bb9\u9519\u8bbe\u8ba1\u4f7f\u7cfb\u7edf\u53ef\u4ee5\u5728\u7cfb\u7edf\u7684\u67d0\u4e9b\u90e8\u5206\u53d1\u751f\u6545\u969c\u65f6\u7ee7\u7eed\u4ee5\u9884\u671f\u7684\u65b9\u5f0f\u8fd0\u884c\uff08\u53ef\u80fd\u4f1a\u964d\u4f4e\u7ea7\u522b\uff09\uff0c\u800c\u4e0d\u662f\u5b8c\u5168\u6545\u969c\u3002[2] \u8be5\u672f\u8bed\u6700\u5e38\u7528\u4e8e\u63cf\u8ff0\u8bbe\u8ba1\u4e3a\u6216\u591a\u6216\u5c11\u5730\u5b8c\u5168\u6b63\u5e38\u8fd0\u884c\u7684\u8ba1\u7b97\u673a\u7cfb\u7edf\uff0c\u5728\u67d0\u4e9b\u5c40\u90e8\u6545\u969c\u7684\u60c5\u51b5\u4e0b\uff0c\u541e\u5410\u91cf\u53ef\u80fd\u4f1a\u964d\u4f4e\u6216\u54cd\u5e94\u65f6\u95f4\u4f1a\u589e\u52a0\u3002 \u5373\uff0c\u7531\u4e8e\u786c\u4ef6\u6216\u8f6f\u4ef6\u7684\u95ee\u9898\uff0c\u6574\u4e2a\u7cfb\u7edf\u4e0d\u4f1a\u505c\u6b62\u3002 \u53e6\u4e00\u4e2a\u9886\u57df\u7684\u4f8b\u5b50\u662f\u8bbe\u8ba1\u6210\u5728\u8f6e\u80ce\u4e2d\u7684\u4e00\u4e2a\u88ab\u523a\u7834\u7684\u60c5\u51b5\u4e0b\u4ecd\u53ef\u7ee7\u7eed\u9a7e\u9a76\u7684\u673a\u52a8\u8f66\u8f86\uff0c\u6216\u5728\u7531\u4e8e\u75b2\u52b3\uff0c\u8150\u8680\uff0c\u5236\u9020\u7b49\u539f\u56e0\u800c\u9020\u6210\u635f\u574f\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u4fdd\u6301\u5176\u5b8c\u6574\u6027\u7684\u7ed3\u6784\u3002 \u7f3a\u9677\u6216\u5f71\u54cd\u3002 Within the scope of an individual system, fault tolerance can be achieved by anticipating exceptional conditions and building the system to cope with them, and, in general, aiming for self-stabilization so that the system converges towards an error-free state. However, if the consequences of a system failure are catastrophic, or the cost of making it sufficiently reliable is very high, a better solution may be to use some form of duplication. In any case, if the consequence of a system failure is so catastrophic, the system must be able to use reversion to fall back to a safe mode. This is similar to roll-back recovery but can be a human action if humans are present in the loop. NOTE: \u5728\u5355\u4e2a\u7cfb\u7edf\u7684\u8303\u56f4\u5185\uff0c\u53ef\u4ee5\u901a\u8fc7\u9884\u6599\u5230\u5f02\u5e38\u60c5\u51b5\u5e76\u6784\u5efa\u7cfb\u7edf\u4ee5\u5e94\u5bf9\u8fd9\u4e9b\u60c5\u51b5\u6765\u5b9e\u73b0\u5bb9\u9519\u80fd\u529b\uff0c\u5e76\u4e14\u4e00\u822c\u800c\u8a00\uff0c\u5176\u76ee\u6807\u662f\u5b9e\u73b0\u81ea\u6211\u7a33\u5b9a\uff0c\u4ece\u800c\u4f7f\u7cfb\u7edf\u6536\u655b\u4e8e\u65e0\u9519\u8bef\u72b6\u6001\u3002 \u4f46\u662f\uff0c\u5982\u679c\u7cfb\u7edf\u6545\u969c\u7684\u540e\u679c\u662f\u707e\u96be\u6027\u7684\uff0c\u6216\u8005\u4f7f\u5176\u8db3\u591f\u53ef\u9760\u7684\u6210\u672c\u975e\u5e38\u9ad8\uff0c\u5219\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u53ef\u80fd\u662f\u4f7f\u7528\u67d0\u79cd\u5f62\u5f0f\u7684\u590d\u5236\u3002 \u65e0\u8bba\u5982\u4f55\uff0c\u5982\u679c\u7cfb\u7edf\u6545\u969c\u7684\u540e\u679c\u662f\u707e\u96be\u6027\u7684\uff0c\u5219\u7cfb\u7edf\u5fc5\u987b\u80fd\u591f\u4f7f\u7528\u8fd8\u539f\u6765\u9000\u56de\u5230\u5b89\u5168\u6a21\u5f0f\u3002 \u8fd9\u7c7b\u4f3c\u4e8e\u56de\u6eda\u6062\u590d\uff0c\u4f46\u5982\u679c\u5faa\u73af\u4e2d\u5b58\u5728\u4eba\u5458\uff0c\u5219\u53ef\u80fd\u662f\u4eba\u4e3a\u7684\u884c\u4e3a\u3002","title":"wikipedia Fault tolerance"},{"location":"Distributed-computing/Theory/Fault-tolerance/#example","text":"Distributed locks with Redis Redis Cluster Specification Redis: replication, part 1 \u2013 an overview. Replication vs Sharding. Sentinel vs Cluster. Redis topology.","title":"Example"},{"location":"Distributed-computing/Theory/Fault-tolerance/#todo","text":"wikipedia Fault-tolerant computer system searchdisasterrecovery fault-tolerant","title":"TODO"},{"location":"Distributed-computing/Theory/Fault-tolerance/Byzantine-Fault-Tolerance/","text":"Byzantine fault Byzantine fault\u662f\u7406\u89e3\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u6311\u6218\u3001\u7279\u6027\u7684\u4e00\u4e2a\u975e\u5e38\u597d\u7684\u4f8b\u5b50\u3002 \u53c2\u8003\u6587\u7ae0: 1\u3001wikipedia Byzantine fault 2\u3001csdn \u7528\u4e09\u56fd\u6740\u8bb2\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u8212\u9002\u4e86\u5427\uff1f \u8bb2\u89e3\u5f97\u975e\u5e38\u597d\uff0c\u7ed3\u5408\u4e09\u56fd\u7684\u6545\u4e8b\u6765\u5c06\u7684\uff0c\u5efa\u8bae \u4f18\u5148\u9605\u8bfb\u8fd9\u7bc7\u6587\u7ae0\uff0c\u6211\u662f\u901a\u8fc7\u5b83\u800c\u7406\u89e3\u7684\u3002 wikipedia Byzantine fault NOTE: wanweibaike Byzantine fault","title":"Introduction"},{"location":"Distributed-computing/Theory/Fault-tolerance/Byzantine-Fault-Tolerance/#byzantine#fault","text":"Byzantine fault\u662f\u7406\u89e3\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u6311\u6218\u3001\u7279\u6027\u7684\u4e00\u4e2a\u975e\u5e38\u597d\u7684\u4f8b\u5b50\u3002","title":"Byzantine fault"},{"location":"Distributed-computing/Theory/Fault-tolerance/Byzantine-Fault-Tolerance/#_1","text":"1\u3001wikipedia Byzantine fault 2\u3001csdn \u7528\u4e09\u56fd\u6740\u8bb2\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u8212\u9002\u4e86\u5427\uff1f \u8bb2\u89e3\u5f97\u975e\u5e38\u597d\uff0c\u7ed3\u5408\u4e09\u56fd\u7684\u6545\u4e8b\u6765\u5c06\u7684\uff0c\u5efa\u8bae \u4f18\u5148\u9605\u8bfb\u8fd9\u7bc7\u6587\u7ae0\uff0c\u6211\u662f\u901a\u8fc7\u5b83\u800c\u7406\u89e3\u7684\u3002","title":"\u53c2\u8003\u6587\u7ae0:"},{"location":"Distributed-computing/Theory/Fault-tolerance/Byzantine-Fault-Tolerance/#wikipedia#byzantine#fault","text":"NOTE: wanweibaike Byzantine fault","title":"wikipedia Byzantine fault"},{"location":"Distributed-computing/Theory/High-availability/","text":"High availability \u201chigh availability\u201d\u5373\u201c\u9ad8\u53ef\u7528\u201d\uff0c\u5e38\u88ab\u7b80\u5199\u4e3a\u201cHA\"\u3002 wikipedia High availability \u5b9e\u73b0\u65b9\u5f0f Redundancy\u3001replication \u53c2\u89c1 Replication \u7ae0\u8282\u3002 \u591a\u6d3b \u53c2\u89c1 \u591a\u6d3b-\u53cc\u6d3b \u7ae0\u8282\u3002","title":"Introduction"},{"location":"Distributed-computing/Theory/High-availability/#high#availability","text":"\u201chigh availability\u201d\u5373\u201c\u9ad8\u53ef\u7528\u201d\uff0c\u5e38\u88ab\u7b80\u5199\u4e3a\u201cHA\"\u3002","title":"High availability"},{"location":"Distributed-computing/Theory/High-availability/#wikipedia#high#availability","text":"","title":"wikipedia High availability"},{"location":"Distributed-computing/Theory/High-availability/#_1","text":"","title":"\u5b9e\u73b0\u65b9\u5f0f"},{"location":"Distributed-computing/Theory/High-availability/#redundancyreplication","text":"\u53c2\u89c1 Replication \u7ae0\u8282\u3002","title":"Redundancy\u3001replication"},{"location":"Distributed-computing/Theory/High-availability/#_2","text":"\u53c2\u89c1 \u591a\u6d3b-\u53cc\u6d3b \u7ae0\u8282\u3002","title":"\u591a\u6d3b"},{"location":"Distributed-computing/Theory/High-availability/High-availability-cluster/","text":"wikipedia High-availability cluster High-availability clusters (also known as HA clusters or fail-over clusters ) are groups of computers that support server applications that can be reliably utilized with a minimum amount of down-time \uff08\u9ad8\u53ef\u7528\u6027\u7fa4\u96c6\uff08\u4e5f\u79f0\u4e3aHA\u7fa4\u96c6\u6216\u6545\u969c\u8f6c\u79fb\u7fa4\u96c6\uff09\u662f\u652f\u6301\u670d\u52a1\u5668\u5e94\u7528\u7a0b\u5e8f\u7684\u8ba1\u7b97\u673a\u7ec4\uff0c\u53ef\u4ee5\u5728\u6700\u5c11\u7684\u505c\u673a\u65f6\u95f4\u5185\u53ef\u9760\u5730\u5229\u7528\u670d\u52a1\u5668\u5e94\u7528\u7a0b\u5e8f\uff09. They operate by using high availability software to harness\uff08\u5229\u7528\uff09 redundant computers in groups or clusters that provide continued service when system components fail. Without clustering, if a server running a particular application crashes, the application will be unavailable until the crashed server is fixed. HA clustering remedies\uff08\u89e3\u51b3\uff09 this situation by detecting hardware/software faults, and immediately restarting the application on another system without requiring administrative intervention, a process known as failover . As part of this process, clustering software may configure the node before starting the application on it. For example, appropriate file systems may need to be imported and mounted, network hardware may have to be configured, and some supporting applications may need to be running as well.[ 1] HA clusters are often used for critical databases , file sharing on a network, business applications, and customer services such as electronic commerce websites . HA cluster implementations attempt to build redundancy into a cluster to eliminate single points of failure, including multiple network connections and data storage which is redundantly connected via storage area networks . HA clusters usually use a heartbeat private network connection which is used to monitor the health and status of each node in the cluster. One subtle but serious condition all clustering software must be able to handle is split-brain , which occurs when all of the private links go down simultaneously, but the cluster nodes are still running. If that happens, each node in the cluster may mistakenly decide that every other node has gone down and attempt to start services that other nodes are still running. Having duplicate instances of services may cause data corruption on the shared storage. NOTE: HA\u7fa4\u96c6\u901a\u5e38\u4f7f\u7528\u5fc3\u8df3\u4e13\u7528\u7f51\u7edc\u8fde\u63a5\uff0c\u8be5\u8fde\u63a5\u7528\u4e8e\u76d1\u89c6\u7fa4\u96c6\u4e2d\u6bcf\u4e2a\u8282\u70b9\u7684\u8fd0\u884c\u72b6\u51b5\u548c\u72b6\u6001\u3002 \u6240\u6709\u7fa4\u96c6\u8f6f\u4ef6\u5fc5\u987b\u80fd\u591f\u5904\u7406\u7684\u4e00\u4e2a\u5fae\u5999\u4f46\u4e25\u5cfb\u7684\u60c5\u51b5\u662f\u88c2\u8111\uff0c\u8fd9\u662f\u5728\u6240\u6709\u4e13\u7528\u94fe\u63a5\u540c\u65f6\u4e2d\u65ad\u4f46\u7fa4\u96c6\u8282\u70b9\u4ecd\u5728\u8fd0\u884c\u65f6\u53d1\u751f\u7684\u3002 \u5982\u679c\u53d1\u751f\u8fd9\u79cd\u60c5\u51b5\uff0c\u7fa4\u96c6\u4e2d\u7684\u6bcf\u4e2a\u8282\u70b9\u53ef\u80fd\u4f1a\u9519\u8bef\u5730\u786e\u5b9a\u6bcf\u4e2a\u5176\u4ed6\u8282\u70b9\u90fd\u5df2\u5173\u95ed\uff0c\u5e76\u5c1d\u8bd5\u542f\u52a8\u5176\u4ed6\u8282\u70b9\u4ecd\u5728\u8fd0\u884c\u7684\u670d\u52a1\u3002 \u5177\u6709\u91cd\u590d\u7684\u670d\u52a1\u5b9e\u4f8b\u53ef\u80fd\u4f1a\u5bfc\u81f4\u5171\u4eab\u5b58\u50a8\u4e0a\u7684\u6570\u636e\u635f\u574f\u3002 HA clusters often also use quorum witness storage (local or cloud) to avoid this scenario. A witness device cannot be shared between two halves of a split cluster, so in the event that all cluster members cannot communicate with each other (e.g., failed heartbeat), if a member cannot access the witness, it cannot become active. NOTE: \u9ad8\u53ef\u7528\u6027\u7fa4\u96c6\u901a\u5e38\u8fd8\u4f7f\u7528\u4ef2\u88c1\u89c1\u8bc1\u5b58\u50a8\uff08\u672c\u5730\u6216\u4e91\uff09\u6765\u907f\u514d\u8fd9\u79cd\u60c5\u51b5\u3002 \u89c1\u8bc1\u8bbe\u5907\u65e0\u6cd5\u5728\u62c6\u5206\u540e\u7684\u96c6\u7fa4\u7684\u4e24\u534a\u4e4b\u95f4\u5171\u4eab\uff0c\u56e0\u6b64\uff0c\u5982\u679c\u6240\u6709\u96c6\u7fa4\u6210\u5458\u90fd\u65e0\u6cd5\u76f8\u4e92\u901a\u4fe1\uff08\u4f8b\u5982\uff0c\u5fc3\u8df3\u5931\u8d25\uff09\uff0c\u5219\u5982\u679c\u6210\u5458\u65e0\u6cd5\u8bbf\u95ee\u89c1\u8bc1\uff0c\u5219\u5b83\u4e5f\u4e0d\u80fd\u5904\u4e8e\u6d3b\u52a8\u72b6\u6001\u3002 NOTE: \u6240\u8c13\u7684HA\uff0c\u5176\u5b9e\u662f\u6307\u7cfb\u7edf\u80fd\u591fmonitor\u96c6\u7fa4\u4e2d\u8282\u70b9\u7684\u72b6\u6001\uff0c\u5e76\u4e14\u5728\u53d1\u73b0\u8282\u70b9\u5f02\u5e38\u7684\u60c5\u51b5\u4e0b\uff0c\u7cfb\u7edf\u80fd\u591f\u81ea\u52a8\u8fdb\u884c failover \uff0c\u800c\u65e0\u9700\u4eba\u4e3a\u7684\u4ecb\u5165\uff1b\u6b63\u5982\u5728 High availability versus fault tolerance \u4e2d\u6240\u4ecb\u7ecd\u7684\uff1ahighly available environment has a minimal service interruption Application design requirements Not every application can run in a high-availability cluster environment, and the necessary design decisions need to be made early in the software design phase. In order to run in a high-availability cluster environment, an application must satisfy at least the following technical requirements, the last two of which are critical to its reliable function in a cluster, and are the most difficult to satisfy fully: 1\u3001There must be a relatively easy way to start, stop, force-stop, and check the status of the application. In practical terms, this means the application must have a command line interface or scripts to control the application, including support for multiple instances of the application. 2\u3001The application must be able to use shared storage ( NAS / SAN ). 3\u3001Most importantly, the application must store as much of its state on non-volatile shared storage as possible. Equally important is the ability to restart on another node at the last state before failure using the saved state from the shared storage. 4\u3001The application must not corrupt data if it crashes, or restarts from the saved state. 5\u3001A number of these constraints can be minimized through the use of virtual server environments, wherein the hypervisor itself is cluster-aware and provides seamless migration of virtual machines (including running memory state) between physical hosts -- see Microsoft Server 2012 and 2016 Failover Clusters. A key difference between this approach and running cluster-aware applications is that the latter can deal with server application crashes and support live \"rolling\" software upgrades while maintaining client access to the service (e.g. database), by having one instance provide service while another is being upgraded or repaired. This requires the cluster instances to communicate, flush caches and coordinate file access during hand-off. Node configurations The most common size for an HA cluster is a two-node cluster, since that is the minimum required to provide redundancy, but many clusters consist of many more, sometimes dozens of nodes. The attached diagram is a good overview of a classic HA cluster, with the caveat that it does not make any mention of quorum/witness functionality (see above). Such configurations can sometimes be categorized into one of the following models: Active/active \u2014 Traffic intended for the failed node is either passed onto an existing node or load balanced across the remaining nodes. This is usually only possible when the nodes use a homogeneous software configuration. Active/passive \u2014 Provides a fully redundant instance of each node, which is only brought online when its associated primary node fails.[ 2] This configuration typically requires the most extra hardware. N+1 \u2014 Provides a single extra node that is brought online to take over the role of the node that has failed. In the case of heterogeneous software configuration on each primary node, the extra node must be universally capable of assuming any of the roles of the primary nodes it is responsible for. This normally refers to clusters that have multiple services running simultaneously; in the single service case, this degenerates to active/passive. N+M \u2014 In cases where a single cluster is managing many services, having only one dedicated failover node might not offer sufficient redundancy. In such cases, more than one (M) standby servers are included and available. The number of standby servers is a tradeoff between cost and reliability requirements. N-to-1 \u2014 Allows the failover standby node to become the active one temporarily, until the original node can be restored or brought back online, at which point the services or instances must be failed-back to it in order to restore high availability. N-to-N \u2014 A combination of active/active and N+M clusters, N to N clusters redistribute the services, instances or connections from the failed node among the remaining active nodes, thus eliminating (as with active/active) the need for a 'standby' node, but introducing a need for extra capacity on all active nodes. The terms logical host or cluster logical host is used to describe the network address that is used to access services provided by the cluster. This logical host identity is not tied to a single cluster node. It is actually a network address/hostname that is linked with the service(s) provided by the cluster. If a cluster node with a running database goes down, the database will be restarted on another cluster node. Node reliability HA clusters usually use all available techniques to make the individual systems and shared infrastructure as reliable as possible. These include: Disk mirroring (or Redundant Arrays of Independent Disks --RAID) so that failure of internal disks does not result in system crashes. The Distributed Replicated Block Device is one example. Redundant network connections so that single cable, switch, or network interface failures do not result in network outages. Redundant storage area network (SAN) connections so that single cable, switch, or interface failures do not lead to loss of connectivity to the storage (this would violate shared nothing architecture ). Redundant electrical power inputs on different circuits, usually both or all protected by uninterruptible power supply units, and redundant power supply units, so that single power feed, cable, UPS, or power supply failures do not lead to loss of power to the system. These features help minimize the chances that the clustering failover between systems will be required. In such a failover, the service provided is unavailable for at least a little while, so measures to avoid failover are preferred. Failover strategies Systems that handle failures in distributed computing have different strategies to cure a failure. For instance, the Apache Cassandra API Hector defines three ways to configure a failover: Fail Fast , scripted as \"FAIL_FAST\", means that the attempt to cure the failure fails if the first node cannot be reached. On Fail, Try One - Next Available , scripted as \"ON_FAIL_TRY_ONE_NEXT_AVAILABLE\", means that the system tries one host, the most accessible or available, before giving up. On Fail, Try All , scripted as \"ON_FAIL_TRY_ALL_AVAILABLE\", means that the system tries all existing, available nodes before giving up. Case study Case: Linux-HA https://en.wanweibaike.com/wiki-Heartbeat%20(program )","title":"Introduction"},{"location":"Distributed-computing/Theory/High-availability/High-availability-cluster/#wikipedia#high-availability#cluster","text":"High-availability clusters (also known as HA clusters or fail-over clusters ) are groups of computers that support server applications that can be reliably utilized with a minimum amount of down-time \uff08\u9ad8\u53ef\u7528\u6027\u7fa4\u96c6\uff08\u4e5f\u79f0\u4e3aHA\u7fa4\u96c6\u6216\u6545\u969c\u8f6c\u79fb\u7fa4\u96c6\uff09\u662f\u652f\u6301\u670d\u52a1\u5668\u5e94\u7528\u7a0b\u5e8f\u7684\u8ba1\u7b97\u673a\u7ec4\uff0c\u53ef\u4ee5\u5728\u6700\u5c11\u7684\u505c\u673a\u65f6\u95f4\u5185\u53ef\u9760\u5730\u5229\u7528\u670d\u52a1\u5668\u5e94\u7528\u7a0b\u5e8f\uff09. They operate by using high availability software to harness\uff08\u5229\u7528\uff09 redundant computers in groups or clusters that provide continued service when system components fail. Without clustering, if a server running a particular application crashes, the application will be unavailable until the crashed server is fixed. HA clustering remedies\uff08\u89e3\u51b3\uff09 this situation by detecting hardware/software faults, and immediately restarting the application on another system without requiring administrative intervention, a process known as failover . As part of this process, clustering software may configure the node before starting the application on it. For example, appropriate file systems may need to be imported and mounted, network hardware may have to be configured, and some supporting applications may need to be running as well.[ 1] HA clusters are often used for critical databases , file sharing on a network, business applications, and customer services such as electronic commerce websites . HA cluster implementations attempt to build redundancy into a cluster to eliminate single points of failure, including multiple network connections and data storage which is redundantly connected via storage area networks . HA clusters usually use a heartbeat private network connection which is used to monitor the health and status of each node in the cluster. One subtle but serious condition all clustering software must be able to handle is split-brain , which occurs when all of the private links go down simultaneously, but the cluster nodes are still running. If that happens, each node in the cluster may mistakenly decide that every other node has gone down and attempt to start services that other nodes are still running. Having duplicate instances of services may cause data corruption on the shared storage. NOTE: HA\u7fa4\u96c6\u901a\u5e38\u4f7f\u7528\u5fc3\u8df3\u4e13\u7528\u7f51\u7edc\u8fde\u63a5\uff0c\u8be5\u8fde\u63a5\u7528\u4e8e\u76d1\u89c6\u7fa4\u96c6\u4e2d\u6bcf\u4e2a\u8282\u70b9\u7684\u8fd0\u884c\u72b6\u51b5\u548c\u72b6\u6001\u3002 \u6240\u6709\u7fa4\u96c6\u8f6f\u4ef6\u5fc5\u987b\u80fd\u591f\u5904\u7406\u7684\u4e00\u4e2a\u5fae\u5999\u4f46\u4e25\u5cfb\u7684\u60c5\u51b5\u662f\u88c2\u8111\uff0c\u8fd9\u662f\u5728\u6240\u6709\u4e13\u7528\u94fe\u63a5\u540c\u65f6\u4e2d\u65ad\u4f46\u7fa4\u96c6\u8282\u70b9\u4ecd\u5728\u8fd0\u884c\u65f6\u53d1\u751f\u7684\u3002 \u5982\u679c\u53d1\u751f\u8fd9\u79cd\u60c5\u51b5\uff0c\u7fa4\u96c6\u4e2d\u7684\u6bcf\u4e2a\u8282\u70b9\u53ef\u80fd\u4f1a\u9519\u8bef\u5730\u786e\u5b9a\u6bcf\u4e2a\u5176\u4ed6\u8282\u70b9\u90fd\u5df2\u5173\u95ed\uff0c\u5e76\u5c1d\u8bd5\u542f\u52a8\u5176\u4ed6\u8282\u70b9\u4ecd\u5728\u8fd0\u884c\u7684\u670d\u52a1\u3002 \u5177\u6709\u91cd\u590d\u7684\u670d\u52a1\u5b9e\u4f8b\u53ef\u80fd\u4f1a\u5bfc\u81f4\u5171\u4eab\u5b58\u50a8\u4e0a\u7684\u6570\u636e\u635f\u574f\u3002 HA clusters often also use quorum witness storage (local or cloud) to avoid this scenario. A witness device cannot be shared between two halves of a split cluster, so in the event that all cluster members cannot communicate with each other (e.g., failed heartbeat), if a member cannot access the witness, it cannot become active. NOTE: \u9ad8\u53ef\u7528\u6027\u7fa4\u96c6\u901a\u5e38\u8fd8\u4f7f\u7528\u4ef2\u88c1\u89c1\u8bc1\u5b58\u50a8\uff08\u672c\u5730\u6216\u4e91\uff09\u6765\u907f\u514d\u8fd9\u79cd\u60c5\u51b5\u3002 \u89c1\u8bc1\u8bbe\u5907\u65e0\u6cd5\u5728\u62c6\u5206\u540e\u7684\u96c6\u7fa4\u7684\u4e24\u534a\u4e4b\u95f4\u5171\u4eab\uff0c\u56e0\u6b64\uff0c\u5982\u679c\u6240\u6709\u96c6\u7fa4\u6210\u5458\u90fd\u65e0\u6cd5\u76f8\u4e92\u901a\u4fe1\uff08\u4f8b\u5982\uff0c\u5fc3\u8df3\u5931\u8d25\uff09\uff0c\u5219\u5982\u679c\u6210\u5458\u65e0\u6cd5\u8bbf\u95ee\u89c1\u8bc1\uff0c\u5219\u5b83\u4e5f\u4e0d\u80fd\u5904\u4e8e\u6d3b\u52a8\u72b6\u6001\u3002 NOTE: \u6240\u8c13\u7684HA\uff0c\u5176\u5b9e\u662f\u6307\u7cfb\u7edf\u80fd\u591fmonitor\u96c6\u7fa4\u4e2d\u8282\u70b9\u7684\u72b6\u6001\uff0c\u5e76\u4e14\u5728\u53d1\u73b0\u8282\u70b9\u5f02\u5e38\u7684\u60c5\u51b5\u4e0b\uff0c\u7cfb\u7edf\u80fd\u591f\u81ea\u52a8\u8fdb\u884c failover \uff0c\u800c\u65e0\u9700\u4eba\u4e3a\u7684\u4ecb\u5165\uff1b\u6b63\u5982\u5728 High availability versus fault tolerance \u4e2d\u6240\u4ecb\u7ecd\u7684\uff1ahighly available environment has a minimal service interruption","title":"wikipedia High-availability cluster"},{"location":"Distributed-computing/Theory/High-availability/High-availability-cluster/#application#design#requirements","text":"Not every application can run in a high-availability cluster environment, and the necessary design decisions need to be made early in the software design phase. In order to run in a high-availability cluster environment, an application must satisfy at least the following technical requirements, the last two of which are critical to its reliable function in a cluster, and are the most difficult to satisfy fully: 1\u3001There must be a relatively easy way to start, stop, force-stop, and check the status of the application. In practical terms, this means the application must have a command line interface or scripts to control the application, including support for multiple instances of the application. 2\u3001The application must be able to use shared storage ( NAS / SAN ). 3\u3001Most importantly, the application must store as much of its state on non-volatile shared storage as possible. Equally important is the ability to restart on another node at the last state before failure using the saved state from the shared storage. 4\u3001The application must not corrupt data if it crashes, or restarts from the saved state. 5\u3001A number of these constraints can be minimized through the use of virtual server environments, wherein the hypervisor itself is cluster-aware and provides seamless migration of virtual machines (including running memory state) between physical hosts -- see Microsoft Server 2012 and 2016 Failover Clusters. A key difference between this approach and running cluster-aware applications is that the latter can deal with server application crashes and support live \"rolling\" software upgrades while maintaining client access to the service (e.g. database), by having one instance provide service while another is being upgraded or repaired. This requires the cluster instances to communicate, flush caches and coordinate file access during hand-off.","title":"Application design requirements"},{"location":"Distributed-computing/Theory/High-availability/High-availability-cluster/#node#configurations","text":"The most common size for an HA cluster is a two-node cluster, since that is the minimum required to provide redundancy, but many clusters consist of many more, sometimes dozens of nodes. The attached diagram is a good overview of a classic HA cluster, with the caveat that it does not make any mention of quorum/witness functionality (see above). Such configurations can sometimes be categorized into one of the following models: Active/active \u2014 Traffic intended for the failed node is either passed onto an existing node or load balanced across the remaining nodes. This is usually only possible when the nodes use a homogeneous software configuration. Active/passive \u2014 Provides a fully redundant instance of each node, which is only brought online when its associated primary node fails.[ 2] This configuration typically requires the most extra hardware. N+1 \u2014 Provides a single extra node that is brought online to take over the role of the node that has failed. In the case of heterogeneous software configuration on each primary node, the extra node must be universally capable of assuming any of the roles of the primary nodes it is responsible for. This normally refers to clusters that have multiple services running simultaneously; in the single service case, this degenerates to active/passive. N+M \u2014 In cases where a single cluster is managing many services, having only one dedicated failover node might not offer sufficient redundancy. In such cases, more than one (M) standby servers are included and available. The number of standby servers is a tradeoff between cost and reliability requirements. N-to-1 \u2014 Allows the failover standby node to become the active one temporarily, until the original node can be restored or brought back online, at which point the services or instances must be failed-back to it in order to restore high availability. N-to-N \u2014 A combination of active/active and N+M clusters, N to N clusters redistribute the services, instances or connections from the failed node among the remaining active nodes, thus eliminating (as with active/active) the need for a 'standby' node, but introducing a need for extra capacity on all active nodes. The terms logical host or cluster logical host is used to describe the network address that is used to access services provided by the cluster. This logical host identity is not tied to a single cluster node. It is actually a network address/hostname that is linked with the service(s) provided by the cluster. If a cluster node with a running database goes down, the database will be restarted on another cluster node.","title":"Node configurations"},{"location":"Distributed-computing/Theory/High-availability/High-availability-cluster/#node#reliability","text":"HA clusters usually use all available techniques to make the individual systems and shared infrastructure as reliable as possible. These include: Disk mirroring (or Redundant Arrays of Independent Disks --RAID) so that failure of internal disks does not result in system crashes. The Distributed Replicated Block Device is one example. Redundant network connections so that single cable, switch, or network interface failures do not result in network outages. Redundant storage area network (SAN) connections so that single cable, switch, or interface failures do not lead to loss of connectivity to the storage (this would violate shared nothing architecture ). Redundant electrical power inputs on different circuits, usually both or all protected by uninterruptible power supply units, and redundant power supply units, so that single power feed, cable, UPS, or power supply failures do not lead to loss of power to the system. These features help minimize the chances that the clustering failover between systems will be required. In such a failover, the service provided is unavailable for at least a little while, so measures to avoid failover are preferred.","title":"Node reliability"},{"location":"Distributed-computing/Theory/High-availability/High-availability-cluster/#failover#strategies","text":"Systems that handle failures in distributed computing have different strategies to cure a failure. For instance, the Apache Cassandra API Hector defines three ways to configure a failover: Fail Fast , scripted as \"FAIL_FAST\", means that the attempt to cure the failure fails if the first node cannot be reached. On Fail, Try One - Next Available , scripted as \"ON_FAIL_TRY_ONE_NEXT_AVAILABLE\", means that the system tries one host, the most accessible or available, before giving up. On Fail, Try All , scripted as \"ON_FAIL_TRY_ALL_AVAILABLE\", means that the system tries all existing, available nodes before giving up.","title":"Failover strategies"},{"location":"Distributed-computing/Theory/High-availability/High-availability-cluster/#case#study","text":"","title":"Case study"},{"location":"Distributed-computing/Theory/High-availability/High-availability-cluster/#case#linux-ha","text":"https://en.wanweibaike.com/wiki-Heartbeat%20(program )","title":"Case: Linux-HA"},{"location":"Distributed-computing/Theory/High-availability/Replication/","text":"Replication \u6709\u5907\u65e0\u60a3\u3002 wikipedia Replication (computing) Replication in computing involves sharing information so as to ensure consistency between redundant resources, such as software or hardware components, to improve reliability, fault-tolerance , or accessibility. wikipedia Redundancy (engineering) \u4e24\u79cd\u65b9\u5f0f master slave \u8fd9\u79cd\u65b9\u5f0f\u6bd4\u8f83\u5e7f\u6cdb\u7684\uff0c\u6bd4\u5982: 1\u3001Redis sentinel HA\u5c31\u662f\u91c7\u7528\u7684\u8fd9\u79cd\u65b9\u5f0f \u6210\u7ec4 \u5176\u5b9e\u8fd9\u4e5f\u662f\u4e00\u79cdreplication\uff0c\u5728\u4e0b\u9762\u7684\u6587\u7ae0\u4e2d\uff0c\u4ecb\u7ecd\u4e86\u8fd9\u79cd\u7528\u6cd5: 1\u3001Redis doc Partitioning: how to split data among multiple Redis instances It is not a single point of failure since you can start multiple proxies and instruct your clients to connect to the first that accepts the connection. \u663e\u7136\uff0c\u662f\u901a\u8fc7\u6210\u7ec4\u6765\u907f\u514d**single point of failure** \u3002","title":"Introduction"},{"location":"Distributed-computing/Theory/High-availability/Replication/#replication","text":"\u6709\u5907\u65e0\u60a3\u3002","title":"Replication"},{"location":"Distributed-computing/Theory/High-availability/Replication/#wikipedia#replication#computing","text":"Replication in computing involves sharing information so as to ensure consistency between redundant resources, such as software or hardware components, to improve reliability, fault-tolerance , or accessibility.","title":"wikipedia Replication (computing)"},{"location":"Distributed-computing/Theory/High-availability/Replication/#wikipedia#redundancy#engineering","text":"","title":"wikipedia Redundancy (engineering)"},{"location":"Distributed-computing/Theory/High-availability/Replication/#_1","text":"","title":"\u4e24\u79cd\u65b9\u5f0f"},{"location":"Distributed-computing/Theory/High-availability/Replication/#master#slave","text":"\u8fd9\u79cd\u65b9\u5f0f\u6bd4\u8f83\u5e7f\u6cdb\u7684\uff0c\u6bd4\u5982: 1\u3001Redis sentinel HA\u5c31\u662f\u91c7\u7528\u7684\u8fd9\u79cd\u65b9\u5f0f","title":"master slave"},{"location":"Distributed-computing/Theory/High-availability/Replication/#_2","text":"\u5176\u5b9e\u8fd9\u4e5f\u662f\u4e00\u79cdreplication\uff0c\u5728\u4e0b\u9762\u7684\u6587\u7ae0\u4e2d\uff0c\u4ecb\u7ecd\u4e86\u8fd9\u79cd\u7528\u6cd5: 1\u3001Redis doc Partitioning: how to split data among multiple Redis instances It is not a single point of failure since you can start multiple proxies and instruct your clients to connect to the first that accepts the connection. \u663e\u7136\uff0c\u662f\u901a\u8fc7\u6210\u7ec4\u6765\u907f\u514d**single point of failure** \u3002","title":"\u6210\u7ec4"},{"location":"Distributed-computing/Theory/High-availability/Replication/Conflict-free-replicated-data-type/","text":"Conflict-free replicated data type wikipedia Conflict-free replicated data type stackoverflow What is CRDT in Distributed Systems?","title":"Introduction"},{"location":"Distributed-computing/Theory/High-availability/Replication/Conflict-free-replicated-data-type/#conflict-free#replicated#data#type","text":"","title":"Conflict-free replicated data type"},{"location":"Distributed-computing/Theory/High-availability/Replication/Conflict-free-replicated-data-type/#wikipedia#conflict-free#replicated#data#type","text":"","title":"wikipedia Conflict-free replicated data type"},{"location":"Distributed-computing/Theory/High-availability/Replication/Conflict-free-replicated-data-type/#stackoverflow#what#is#crdt#in#distributed#systems","text":"","title":"stackoverflow What is CRDT in Distributed Systems?"},{"location":"Distributed-computing/Theory/High-availability/Replication/State-machine-replication/","text":"State machine replication \u5728raft\u7684paper usenix In Search of an Understandable Consensus Algorithm \u4e2d\uff0c\u7ed9\u51fa\u4e86Replicated state machine architecture: wikipedia State machine replication NOTE: \u4e00\u3001\u72b6\u6001\u673a\u590d\u5236 \u4e8c\u3001\u4f7f\u7528state machine\u8fd9\u4e2a\u62bd\u8c61\u6a21\u578b\uff0c\u80fd\u591f\u7b80\u5316\u5bf9distributed computing\u4e2dreplication\u7684\u5206\u6790\u3001\u7406\u89e3\u3002 In computer science , state machine replication or state machine approach is a general method for implementing a fault-tolerant service by replicating servers and coordinating client interactions with server replicas. The approach also provides a framework for understanding and designing replication management protocols.[ 1] NOTE: \u4f7f\u7528replicated state machine\u7684\u76ee\u7684\u662f\u6784\u5efafault-tolerant service\uff0c\u5982\u679c\u4ee5redis\u6765\u770b\u7684\u8bdd\uff0cfault-tolerant\u7684\u610f\u601d\u662f\u5373\u4f7fcluster\u4e2d\u6709\u4e00\u4e2anode\u51fa\u73b0\u4e86\u95ee\u9898\uff0c\u7cfb\u7edf\u80fd\u591f\u8fdb\u884cfailover\u3002\u6062\u590d\u7684\u6b63\u5e38\u7684\u72b6\u6001\u3002 \u8981\u60f3\u5b8c\u6574\u5730\u7406\u89e3state machine replication\u7684\u542b\u4e49\uff0c\u5176\u5b9e\u53ef\u4ee5\u9605\u8bfb\u4e00\u4e0b\u5982\u4e0b\u6587\u7ae0\uff1a 1\u3001wikipedia Raft (computer science) 2\u3001github The Raft Consensus Algorithm Problem definition Distributed services Distributed software is often structured in terms of clients and services. Each service comprises one or more servers and exports operations that clients invoke by making requests. Although using a single, centralized server is the simplest way to implement a service, the resulting service can only be as fault tolerant as the processor executing that server. If this level of fault tolerance is unacceptable, then multiple servers that fail independently must be used. Usually, replicas of a single server are executed on separate processors of a distributed system, and protocols are used to coordinate client interactions with these replicas. The physical and electrical isolation of processors in a distributed system ensures that server failures are independent, as required. State machine Main article: Finite-state machine For the subsequent discussion a State Machine will be defined as the following tuple of values [ 2] (See also Mealy machine and Moore Machine ): A set of States A set of Inputs A set of Outputs A transition function (Input \u00d7 State \u2192 State) An output function (Input \u00d7 State \u2192 Output) A distinguished State called Start. A State Machine begins at the State labeled Start. Each Input received is passed through the transition and output function to produce a new State and an Output. The State is held stable until a new Input is received, while the Output is communicated to the appropriate receiver. This discussion requires a State Machine to be deterministic : multiple copies of the same State Machine begin in the Start state, and receiving the same Inputs in the same order will arrive at the same State having generated the same Outputs. State Machines can implement any algorithm when driven by an appropriate Input stream, including Turing-complete algorithms (see Turing machine ). Typically, systems based on State Machine Replication voluntarily restrict their implementations to use finite-state machines to simplify error recovery. Fault Tolerance Determinism is an ideal characteristic for providing fault-tolerance. Intuitively\uff08\u76f4\u89c2\u5730\uff09, if multiple copies of a system exist, a fault in one would be noticeable as a difference in the State or Output from the others. A little deduction shows the minimum number of copies needed for fault-tolerance is three; one which has a fault, and two others to whom we compare State and Output. Two copies are not enough as there is no way to tell which copy is the faulty one. Further deduction shows a three-copy system can support at most one failure (after which it must repair or replace the faulty copy). If more than one of the copies were to fail, all three States and Outputs might differ, and there would be no way to choose which is the correct one. In general, a system which supports F failures must have 2F+1 copies (also called replicas).[ 3] The extra copies are used as evidence to decide which of the copies are correct and which are faulty. Special cases can improve these bounds.[ 4] All of this deduction pre-supposes that replicas are experiencing only random independent faults such as memory errors or hard-drive crash. Failures caused by replicas which attempt to lie, deceive, or collude can also be handled by the State Machine Approach, with isolated changes. NOTE: \u6240\u6709\u8fd9\u4e9b\u63a8\u65ad\u90fd\u5047\u8bbe\u526f\u672c\u53ea\u7ecf\u5386\u968f\u673a\u7684\u72ec\u7acb\u6545\u969c\uff0c\u6bd4\u5982\u5185\u5b58\u9519\u8bef\u6216\u786c\u76d8\u5d29\u6e83\u3002\u7531\u8bd5\u56fe\u8bf4\u8c0e\u3001\u6b3a\u9a97\u6216\u4e32\u901a\u7684\u526f\u672c\u5f15\u8d77\u7684\u5931\u8d25\u4e5f\u53ef\u4ee5\u7531\u72b6\u6001\u673a\u65b9\u6cd5\u5904\u7406\uff0c\u5e76\u8fdb\u884c\u72ec\u7acb\u7684\u66f4\u6539\u3002 Failed replicas are not required to stop; they may continue operating, including generating spurious or incorrect Outputs. Special Case: Fail-Stop Theoretically, if a failed replica is guaranteed to stop without generating outputs, only F+1 replicas are required, and clients may accept the first output generated by the system. No existing systems achieve this limit, but it is often used when analyzing systems built on top of a fault-tolerant layer (Since the fault-tolerant layer provides fail-stop semantics to all layers above it). Special Case: Byzantine Failure Faults where a replica sends different values in different directions (for instance, the correct Output to some of its fellow replicas and incorrect Outputs to others) are called Byzantine Failures .[ 5] Byzantine failures may be random, spurious faults, or malicious, intelligent attacks. 2F+1 replicas, with non-cryptographic hashes suffices to survive all non-malicious Byzantine failures (with high probability). Malicious attacks require cryptographic primitives to achieve 2F+1 (using message signatures), or non-cryptographic techniques can be applied but the number of replicas must be increased to 3F+1.[ 5] The State Machine Approach The preceding intuitive discussion implies a simple technique for implementing a fault-tolerant service in terms of a State Machine: 1\u3001Place copies of the State Machine on multiple, independent servers. 2\u3001Receive client requests, interpreted as Inputs to the State Machine. 3\u3001Choose an ordering for the Inputs. 4\u3001Execute Inputs in the chosen order on each server. 5\u3001Respond to clients with the Output from the State Machine. 6\u3001Monitor replicas for differences in State or Output. The remainder of this article develops the details of this technique. Step 1 and 2 are outside the scope of this article. Step 3 is the critical operation, see Ordering Inputs . Step 4 is covered by the State Machine Definition . Step 5, see Ordering Outputs . Step 6, see Auditing and Failure Detection . The appendix contains discussion on typical extensions used in real-world systems such as Logging , Checkpoints , Reconfiguration , and State Transfer .","title":"Introduction"},{"location":"Distributed-computing/Theory/High-availability/Replication/State-machine-replication/#state#machine#replication","text":"\u5728raft\u7684paper usenix In Search of an Understandable Consensus Algorithm \u4e2d\uff0c\u7ed9\u51fa\u4e86Replicated state machine architecture:","title":"State machine replication"},{"location":"Distributed-computing/Theory/High-availability/Replication/State-machine-replication/#wikipedia#state#machine#replication","text":"NOTE: \u4e00\u3001\u72b6\u6001\u673a\u590d\u5236 \u4e8c\u3001\u4f7f\u7528state machine\u8fd9\u4e2a\u62bd\u8c61\u6a21\u578b\uff0c\u80fd\u591f\u7b80\u5316\u5bf9distributed computing\u4e2dreplication\u7684\u5206\u6790\u3001\u7406\u89e3\u3002 In computer science , state machine replication or state machine approach is a general method for implementing a fault-tolerant service by replicating servers and coordinating client interactions with server replicas. The approach also provides a framework for understanding and designing replication management protocols.[ 1] NOTE: \u4f7f\u7528replicated state machine\u7684\u76ee\u7684\u662f\u6784\u5efafault-tolerant service\uff0c\u5982\u679c\u4ee5redis\u6765\u770b\u7684\u8bdd\uff0cfault-tolerant\u7684\u610f\u601d\u662f\u5373\u4f7fcluster\u4e2d\u6709\u4e00\u4e2anode\u51fa\u73b0\u4e86\u95ee\u9898\uff0c\u7cfb\u7edf\u80fd\u591f\u8fdb\u884cfailover\u3002\u6062\u590d\u7684\u6b63\u5e38\u7684\u72b6\u6001\u3002 \u8981\u60f3\u5b8c\u6574\u5730\u7406\u89e3state machine replication\u7684\u542b\u4e49\uff0c\u5176\u5b9e\u53ef\u4ee5\u9605\u8bfb\u4e00\u4e0b\u5982\u4e0b\u6587\u7ae0\uff1a 1\u3001wikipedia Raft (computer science) 2\u3001github The Raft Consensus Algorithm","title":"wikipedia State machine replication"},{"location":"Distributed-computing/Theory/High-availability/Replication/State-machine-replication/#problem#definition","text":"","title":"Problem definition"},{"location":"Distributed-computing/Theory/High-availability/Replication/State-machine-replication/#distributed#services","text":"Distributed software is often structured in terms of clients and services. Each service comprises one or more servers and exports operations that clients invoke by making requests. Although using a single, centralized server is the simplest way to implement a service, the resulting service can only be as fault tolerant as the processor executing that server. If this level of fault tolerance is unacceptable, then multiple servers that fail independently must be used. Usually, replicas of a single server are executed on separate processors of a distributed system, and protocols are used to coordinate client interactions with these replicas. The physical and electrical isolation of processors in a distributed system ensures that server failures are independent, as required.","title":"Distributed services"},{"location":"Distributed-computing/Theory/High-availability/Replication/State-machine-replication/#state#machine","text":"Main article: Finite-state machine For the subsequent discussion a State Machine will be defined as the following tuple of values [ 2] (See also Mealy machine and Moore Machine ): A set of States A set of Inputs A set of Outputs A transition function (Input \u00d7 State \u2192 State) An output function (Input \u00d7 State \u2192 Output) A distinguished State called Start. A State Machine begins at the State labeled Start. Each Input received is passed through the transition and output function to produce a new State and an Output. The State is held stable until a new Input is received, while the Output is communicated to the appropriate receiver. This discussion requires a State Machine to be deterministic : multiple copies of the same State Machine begin in the Start state, and receiving the same Inputs in the same order will arrive at the same State having generated the same Outputs. State Machines can implement any algorithm when driven by an appropriate Input stream, including Turing-complete algorithms (see Turing machine ). Typically, systems based on State Machine Replication voluntarily restrict their implementations to use finite-state machines to simplify error recovery.","title":"State machine"},{"location":"Distributed-computing/Theory/High-availability/Replication/State-machine-replication/#fault#tolerance","text":"Determinism is an ideal characteristic for providing fault-tolerance. Intuitively\uff08\u76f4\u89c2\u5730\uff09, if multiple copies of a system exist, a fault in one would be noticeable as a difference in the State or Output from the others. A little deduction shows the minimum number of copies needed for fault-tolerance is three; one which has a fault, and two others to whom we compare State and Output. Two copies are not enough as there is no way to tell which copy is the faulty one. Further deduction shows a three-copy system can support at most one failure (after which it must repair or replace the faulty copy). If more than one of the copies were to fail, all three States and Outputs might differ, and there would be no way to choose which is the correct one. In general, a system which supports F failures must have 2F+1 copies (also called replicas).[ 3] The extra copies are used as evidence to decide which of the copies are correct and which are faulty. Special cases can improve these bounds.[ 4] All of this deduction pre-supposes that replicas are experiencing only random independent faults such as memory errors or hard-drive crash. Failures caused by replicas which attempt to lie, deceive, or collude can also be handled by the State Machine Approach, with isolated changes. NOTE: \u6240\u6709\u8fd9\u4e9b\u63a8\u65ad\u90fd\u5047\u8bbe\u526f\u672c\u53ea\u7ecf\u5386\u968f\u673a\u7684\u72ec\u7acb\u6545\u969c\uff0c\u6bd4\u5982\u5185\u5b58\u9519\u8bef\u6216\u786c\u76d8\u5d29\u6e83\u3002\u7531\u8bd5\u56fe\u8bf4\u8c0e\u3001\u6b3a\u9a97\u6216\u4e32\u901a\u7684\u526f\u672c\u5f15\u8d77\u7684\u5931\u8d25\u4e5f\u53ef\u4ee5\u7531\u72b6\u6001\u673a\u65b9\u6cd5\u5904\u7406\uff0c\u5e76\u8fdb\u884c\u72ec\u7acb\u7684\u66f4\u6539\u3002 Failed replicas are not required to stop; they may continue operating, including generating spurious or incorrect Outputs.","title":"Fault Tolerance"},{"location":"Distributed-computing/Theory/High-availability/Replication/State-machine-replication/#special#case#fail-stop","text":"Theoretically, if a failed replica is guaranteed to stop without generating outputs, only F+1 replicas are required, and clients may accept the first output generated by the system. No existing systems achieve this limit, but it is often used when analyzing systems built on top of a fault-tolerant layer (Since the fault-tolerant layer provides fail-stop semantics to all layers above it).","title":"Special Case: Fail-Stop"},{"location":"Distributed-computing/Theory/High-availability/Replication/State-machine-replication/#special#case#byzantine#failure","text":"Faults where a replica sends different values in different directions (for instance, the correct Output to some of its fellow replicas and incorrect Outputs to others) are called Byzantine Failures .[ 5] Byzantine failures may be random, spurious faults, or malicious, intelligent attacks. 2F+1 replicas, with non-cryptographic hashes suffices to survive all non-malicious Byzantine failures (with high probability). Malicious attacks require cryptographic primitives to achieve 2F+1 (using message signatures), or non-cryptographic techniques can be applied but the number of replicas must be increased to 3F+1.[ 5]","title":"Special Case: Byzantine Failure"},{"location":"Distributed-computing/Theory/High-availability/Replication/State-machine-replication/#the#state#machine#approach","text":"The preceding intuitive discussion implies a simple technique for implementing a fault-tolerant service in terms of a State Machine: 1\u3001Place copies of the State Machine on multiple, independent servers. 2\u3001Receive client requests, interpreted as Inputs to the State Machine. 3\u3001Choose an ordering for the Inputs. 4\u3001Execute Inputs in the chosen order on each server. 5\u3001Respond to clients with the Output from the State Machine. 6\u3001Monitor replicas for differences in State or Output. The remainder of this article develops the details of this technique. Step 1 and 2 are outside the scope of this article. Step 3 is the critical operation, see Ordering Inputs . Step 4 is covered by the State Machine Definition . Step 5, see Ordering Outputs . Step 6, see Auditing and Failure Detection . The appendix contains discussion on typical extensions used in real-world systems such as Logging , Checkpoints , Reconfiguration , and State Transfer .","title":"The State Machine Approach"},{"location":"Distributed-computing/Theory/High-availability/Single-point-of-failure/","text":"Single point of failure \u201csingle point of failure\u201d\u5373\u201c\u5355\u70b9\u5931\u8d25\u201d\u3002 \u7ef4\u57fa\u767e\u79d1 Single point of failure","title":"Introduction"},{"location":"Distributed-computing/Theory/High-availability/Single-point-of-failure/#single#point#of#failure","text":"\u201csingle point of failure\u201d\u5373\u201c\u5355\u70b9\u5931\u8d25\u201d\u3002","title":"Single point of failure"},{"location":"Distributed-computing/Theory/High-availability/Single-point-of-failure/#single#point#of#failure_1","text":"","title":"\u7ef4\u57fa\u767e\u79d1 Single point of failure"},{"location":"Distributed-computing/Theory/High-availability/%E5%A4%9A%E6%B4%BB-%E5%8F%8C%E6%B4%BB/","text":"\u591a\u6d3b\u3001\u53cc\u6d3b \u53cc\u6d3b cnblogs \u4ec0\u4e48\u662f\"\u53cc\u6d3b\" \u5f02\u5730\u591a\u6d3b https://baike.baidu.com/item/%E5%BC%82%E5%9C%B0%E5%A4%9A%E6%B4%BB/23734068?fr=aladdin https://zhuanlan.zhihu.com/p/97622624 https://www.sohu.com/a/312437796_534345","title":"Introduction"},{"location":"Distributed-computing/Theory/High-availability/%E5%A4%9A%E6%B4%BB-%E5%8F%8C%E6%B4%BB/#_1","text":"","title":"\u591a\u6d3b\u3001\u53cc\u6d3b"},{"location":"Distributed-computing/Theory/High-availability/%E5%A4%9A%E6%B4%BB-%E5%8F%8C%E6%B4%BB/#_2","text":"cnblogs \u4ec0\u4e48\u662f\"\u53cc\u6d3b\"","title":"\u53cc\u6d3b"},{"location":"Distributed-computing/Theory/High-availability/%E5%A4%9A%E6%B4%BB-%E5%8F%8C%E6%B4%BB/#_3","text":"https://baike.baidu.com/item/%E5%BC%82%E5%9C%B0%E5%A4%9A%E6%B4%BB/23734068?fr=aladdin https://zhuanlan.zhihu.com/p/97622624 https://www.sohu.com/a/312437796_534345","title":"\u5f02\u5730\u591a\u6d3b"},{"location":"Distributed-computing/Theory/Network-partition/","text":"Network partition wikipedia Network partition A network partition refers to network decomposition\uff08\u5206\u89e3\uff09 into relatively independent subnets for their separate optimization as well as network split due to the failure of network devices . In both cases the partition-tolerant behavior of subnets is expected. This means that even after the network is partitioned into multiple sub-systems, it still works correctly. NOTE: \u7f51\u7edc\u5212\u5206\u662f\u6307\u5c06\u7f51\u7edc\u5206\u89e3\u4e3a\u76f8\u5bf9\u72ec\u7acb\u7684\u5b50\u7f51\uff0c\u8fdb\u884c\u5404\u81ea\u7684\u4f18\u5316\uff0c\u4ee5\u53ca\u7531\u4e8e\u7f51\u7edc\u8bbe\u5907\u6545\u969c\u5bfc\u81f4\u7684\u7f51\u7edc\u5206\u5272\u3002\u5728\u8fd9\u4e24\u79cd\u60c5\u51b5\u4e0b\uff0c\u5b50\u7f51\u7684\u5206\u533a\u5bb9\u5fcd\u884c\u4e3a\u90fd\u662f\u9884\u671f\u7684\u3002\u8fd9\u610f\u5473\u7740\u5373\u4f7f\u5c06\u7f51\u7edc\u5212\u5206\u4e3a\u591a\u4e2a\u5b50\u7cfb\u7edf\uff0c\u5b83\u4ecd\u7136\u53ef\u4ee5\u6b63\u5e38\u5de5\u4f5c\u3002 For example, in a network with multiple subnets where nodes A and B are located in one subnet and nodes C and D are in another, a partition\uff08\u5206\u88c2\uff09 occurs if the network switch device between the two subnets fails. In that case nodes A and B can no longer communicate with nodes C and D, but all nodes A-D work the same as before. Network Partition for Optimization To decompose an NP-hard network optimization task into subtasks, the network can be decomposed into relatively independent subnets. In order to partition the network, it is useful to visualize it as a weighted complete graph, where each vertex corresponds to a network element, and each edge has a weight equal to the rank of the correlation between each pair of corresponding elements. Then the most irrelevant interactions between elements of network are discarded. Based on the remaining connections, the network is then further split into relatively independent subnets.[ 1] Wherein different allocations of optimized elements predispose alternative splits of the network (Fig. 1). In the case of a large network, the optimization of each subnet can then be performed independently on different computer clusters. As a CAP trade-off NOTE: \u5f53network partition\u53d1\u751f\u7684\u65f6\u5019\uff0c\u9009\u62e9consistency\u8fd8\u662favailability\u3002 The CAP Theorem is based on three trade-offs: Consistency , Availability , and Partition tolerance . Partition tolerance, in this context, means the ability of a data processing system to continue processing data even if a network partition causes communication errors between subsystems.[ 2]","title":"Introduction"},{"location":"Distributed-computing/Theory/Network-partition/#network#partition","text":"","title":"Network partition"},{"location":"Distributed-computing/Theory/Network-partition/#wikipedia#network#partition","text":"A network partition refers to network decomposition\uff08\u5206\u89e3\uff09 into relatively independent subnets for their separate optimization as well as network split due to the failure of network devices . In both cases the partition-tolerant behavior of subnets is expected. This means that even after the network is partitioned into multiple sub-systems, it still works correctly. NOTE: \u7f51\u7edc\u5212\u5206\u662f\u6307\u5c06\u7f51\u7edc\u5206\u89e3\u4e3a\u76f8\u5bf9\u72ec\u7acb\u7684\u5b50\u7f51\uff0c\u8fdb\u884c\u5404\u81ea\u7684\u4f18\u5316\uff0c\u4ee5\u53ca\u7531\u4e8e\u7f51\u7edc\u8bbe\u5907\u6545\u969c\u5bfc\u81f4\u7684\u7f51\u7edc\u5206\u5272\u3002\u5728\u8fd9\u4e24\u79cd\u60c5\u51b5\u4e0b\uff0c\u5b50\u7f51\u7684\u5206\u533a\u5bb9\u5fcd\u884c\u4e3a\u90fd\u662f\u9884\u671f\u7684\u3002\u8fd9\u610f\u5473\u7740\u5373\u4f7f\u5c06\u7f51\u7edc\u5212\u5206\u4e3a\u591a\u4e2a\u5b50\u7cfb\u7edf\uff0c\u5b83\u4ecd\u7136\u53ef\u4ee5\u6b63\u5e38\u5de5\u4f5c\u3002 For example, in a network with multiple subnets where nodes A and B are located in one subnet and nodes C and D are in another, a partition\uff08\u5206\u88c2\uff09 occurs if the network switch device between the two subnets fails. In that case nodes A and B can no longer communicate with nodes C and D, but all nodes A-D work the same as before.","title":"wikipedia Network partition"},{"location":"Distributed-computing/Theory/Network-partition/#network#partition#for#optimization","text":"To decompose an NP-hard network optimization task into subtasks, the network can be decomposed into relatively independent subnets. In order to partition the network, it is useful to visualize it as a weighted complete graph, where each vertex corresponds to a network element, and each edge has a weight equal to the rank of the correlation between each pair of corresponding elements. Then the most irrelevant interactions between elements of network are discarded. Based on the remaining connections, the network is then further split into relatively independent subnets.[ 1] Wherein different allocations of optimized elements predispose alternative splits of the network (Fig. 1). In the case of a large network, the optimization of each subnet can then be performed independently on different computer clusters.","title":"Network Partition for Optimization"},{"location":"Distributed-computing/Theory/Network-partition/#as#a#cap#trade-off","text":"NOTE: \u5f53network partition\u53d1\u751f\u7684\u65f6\u5019\uff0c\u9009\u62e9consistency\u8fd8\u662favailability\u3002 The CAP Theorem is based on three trade-offs: Consistency , Availability , and Partition tolerance . Partition tolerance, in this context, means the ability of a data processing system to continue processing data even if a network partition causes communication errors between subsystems.[ 2]","title":"As a CAP trade-off"},{"location":"Distributed-computing/Theory/Protocol/2PC/","text":"Two-phase commit protocol wikipedia Two-phase commit protocol csdn \u4e24\u9636\u6bb5\u63d0\u4ea4\u534f\u8bae\uff08two phase commit protocol\uff0c2PC\uff09 Example sqlite \u7531\u4e8e\u8981\u57fa\u4e8esqlite\u5f00\u53d1\u4e00\u4e2a\u57fa\u4e8evirtual table\u7684extension\uff0c\u5728\u9605\u8bfbsqlite The Virtual Table Mechanism Of SQLite#2.16. The xSync Method \u65f6\uff0c\u5176\u4e2d\u6709\u8fd9\u6837\u7684\u63cf\u8ff0: This method signals the start of a two-phase commit on a virtual table. \u663e\u7136\uff0c\u8fd9\u662f\u4e3a\u4e86\u652f\u6301transaction\u7684\uff0c\u8fd9\u8ba9\u6211\u60f3\u8d77\u6765\u4e4b\u524d\u5728\u5b66\u4e60Redis\u7684\u65f6\u5019\uff0c\u5176\u4e2d\u4e5f\u6709\u63cf\u8ff0two-phase commit protocol\uff0c\u6240\u4ee5\u6709\u5fc5\u8981\u5bf9Two-phase commit \u8fdb\u884c\u603b\u7ed3\u3002 sqlite implementation of Two-phase commit \u5982\u4e0b\u91cd\u8981\u65b9\u6cd5: xBegin xSync xCommit xRollback \u53c2\u89c1: sqlite The Virtual Table Mechanism Of SQLite#2.16. The xSync Method C++ Generic: Change the Way You Write Exception-Safe Code \u2014 Forever AddFriend now has two distinct parts: the activity phase, in which the operations occur, and the commitment phase, which doesn't throw \u2014 it only stops the undo from happening.","title":"Introduction"},{"location":"Distributed-computing/Theory/Protocol/2PC/#two-phase#commit#protocol","text":"","title":"Two-phase commit protocol"},{"location":"Distributed-computing/Theory/Protocol/2PC/#wikipedia#two-phase#commit#protocol","text":"","title":"wikipedia Two-phase commit protocol"},{"location":"Distributed-computing/Theory/Protocol/2PC/#csdn#two#phase#commit#protocol2pc","text":"","title":"csdn \u4e24\u9636\u6bb5\u63d0\u4ea4\u534f\u8bae\uff08two phase commit protocol\uff0c2PC\uff09"},{"location":"Distributed-computing/Theory/Protocol/2PC/#example","text":"","title":"Example"},{"location":"Distributed-computing/Theory/Protocol/2PC/#sqlite","text":"\u7531\u4e8e\u8981\u57fa\u4e8esqlite\u5f00\u53d1\u4e00\u4e2a\u57fa\u4e8evirtual table\u7684extension\uff0c\u5728\u9605\u8bfbsqlite The Virtual Table Mechanism Of SQLite#2.16. The xSync Method \u65f6\uff0c\u5176\u4e2d\u6709\u8fd9\u6837\u7684\u63cf\u8ff0: This method signals the start of a two-phase commit on a virtual table. \u663e\u7136\uff0c\u8fd9\u662f\u4e3a\u4e86\u652f\u6301transaction\u7684\uff0c\u8fd9\u8ba9\u6211\u60f3\u8d77\u6765\u4e4b\u524d\u5728\u5b66\u4e60Redis\u7684\u65f6\u5019\uff0c\u5176\u4e2d\u4e5f\u6709\u63cf\u8ff0two-phase commit protocol\uff0c\u6240\u4ee5\u6709\u5fc5\u8981\u5bf9Two-phase commit \u8fdb\u884c\u603b\u7ed3\u3002","title":"sqlite"},{"location":"Distributed-computing/Theory/Protocol/2PC/#sqlite#implementation#of#two-phase#commit","text":"\u5982\u4e0b\u91cd\u8981\u65b9\u6cd5: xBegin xSync xCommit xRollback \u53c2\u89c1: sqlite The Virtual Table Mechanism Of SQLite#2.16. The xSync Method","title":"sqlite implementation of Two-phase commit"},{"location":"Distributed-computing/Theory/Protocol/2PC/#c","text":"","title":"C++"},{"location":"Distributed-computing/Theory/Protocol/2PC/#generic#change#the#way#you#write#exception-safe#code#forever","text":"AddFriend now has two distinct parts: the activity phase, in which the operations occur, and the commitment phase, which doesn't throw \u2014 it only stops the undo from happening.","title":"Generic: Change the Way You Write Exception-Safe Code \u2014 Forever"},{"location":"Distributed-computing/Theory/Protocol/3PC/","text":"Three-phase commit protocol wikipedia Three-phase commit protocol","title":"Introduction"},{"location":"Distributed-computing/Theory/Protocol/3PC/#three-phase#commit#protocol","text":"","title":"Three-phase commit protocol"},{"location":"Distributed-computing/Theory/Protocol/3PC/#wikipedia#three-phase#commit#protocol","text":"","title":"wikipedia Three-phase commit protocol"},{"location":"Distributed-computing/Theory/Protocol/Gossip/","text":"Gossip \u662f\u5728\u5b66\u4e60Redis cluster\u7684\u65f6\u5019\uff0c\u53d1\u73b0\u7684gossip protocol\u3002 \u4e0b\u9762\u662f\u4e00\u4e9b\u597d\u7684\u6587\u7ae0: highscalability Using Gossip Protocols For Failure Detection, Monitoring, Messaging And Other Good Things flopezluis What are Gossip Protocols? Gossip eventual consistency 1\u3001 Redis\u6e90\u7801\u89e3\u6790\uff1a25\u96c6\u7fa4(\u4e00)\u63e1\u624b\u3001\u5fc3\u8df3\u6d88\u606f\u4ee5\u53ca\u4e0b\u7ebf\u68c0\u6d4b \u4e2d\u63d0\u53caGossip\u662f\u4e00\u4e2a \u6700\u7ec8\u4e00\u81f4\u6027\u7b97\u6cd5 \u3002","title":"Introduction"},{"location":"Distributed-computing/Theory/Protocol/Gossip/#gossip","text":"\u662f\u5728\u5b66\u4e60Redis cluster\u7684\u65f6\u5019\uff0c\u53d1\u73b0\u7684gossip protocol\u3002 \u4e0b\u9762\u662f\u4e00\u4e9b\u597d\u7684\u6587\u7ae0: highscalability Using Gossip Protocols For Failure Detection, Monitoring, Messaging And Other Good Things flopezluis What are Gossip Protocols?","title":"Gossip"},{"location":"Distributed-computing/Theory/Protocol/Gossip/#gossip#eventual#consistency","text":"1\u3001 Redis\u6e90\u7801\u89e3\u6790\uff1a25\u96c6\u7fa4(\u4e00)\u63e1\u624b\u3001\u5fc3\u8df3\u6d88\u606f\u4ee5\u53ca\u4e0b\u7ebf\u68c0\u6d4b \u4e2d\u63d0\u53caGossip\u662f\u4e00\u4e2a \u6700\u7ec8\u4e00\u81f4\u6027\u7b97\u6cd5 \u3002","title":"Gossip eventual consistency"},{"location":"Distributed-computing/Theory/Protocol/Gossip/wikipedia-Gossip-protocol/","text":"wikipedia Gossip protocol NOTE: gossip\u7684\u672c\u610f\u662f\u95f2\u8bdd\uff0c\u516b\u5366 A gossip protocol [ 1] is a procedure or process of computer peer-to-peer communication that is based on the way epidemics (\u4f20\u67d3\u75c5)spread. Some distributed systems use peer-to-peer gossip to ensure that data is routed to all members of an ad-hoc network. Some ad-hoc networks have no central registry and the only way to spread common data is to rely on each member to pass it along to their neighbors . The term epidemic protocol is sometimes used as a synonym for a gossip protocol, as gossip spreads information in a manner similar to the spread of a virus in a biological community. Gossip communication The concept of gossip communication can be illustrated by the analogy of office workers spreading rumors. Let's say each hour the office workers congregate around the water cooler. Each employee pairs off with another, chosen at random, and shares the latest gossip. At the start of the day, Alice starts a new rumor: she comments to Bob that she believes that Charlie dyes his mustache. At the next meeting, Bob tells Dave, while Alice repeats the idea to Eve. After each water cooler rendezvous, the number of individuals who have heard the rumor roughly doubles (though this doesn't account for gossiping twice to the same person; perhaps Alice tries to tell the story to Frank, only to find that Frank already heard it from Dave). Computer systems typically implement this type of protocol with a form of random \"peer selection\": with a given frequency, each machine picks another machine at random and shares any hot rumors. NOTE: \u53ef\u4ee5\u901a\u8fc7\u529e\u516c\u5ba4\u5de5\u4f5c\u4eba\u5458\u4f20\u64ad\u8c23\u8a00\u7684\u7c7b\u6bd4\u6765\u8bf4\u660e*gossip communication*\u7684\u6982\u5ff5\u3002\u8ba9\u6211\u4eec\u8bf4\u529e\u516c\u5ba4\u5de5\u4f5c\u4eba\u5458\u6bcf\u5c0f\u65f6\u805a\u96c6\u5728\u6c34\u51b7\u5374\u5668\u5468\u56f4\u3002\u6bcf\u4e2a\u5458\u5de5\u4e0e\u53e6\u4e00\u4e2a\u5458\u5de5\u914d\u5bf9\uff0c\u968f\u673a\u9009\u62e9\uff0c\u5e76\u5206\u4eab\u6700\u65b0\u7684\u516b\u5366\u3002\u5728\u4e00\u5929\u5f00\u59cb\u65f6\uff0c\u7231\u4e3d\u4e1d\u5f00\u59cb\u4e86\u4e00\u4e2a\u65b0\u7684\u8c23\u8a00\uff1a\u5979\u5bf9\u9c8d\u52c3\u8bf4\u5979\u8ba4\u4e3a\u67e5\u7406\u67d3\u4e86\u4ed6\u7684\u80e1\u5b50\u3002\u5728\u4e0b\u6b21\u4f1a\u8bae\u4e0a\uff0c\u9c8d\u52c3\u544a\u8bc9\u6234\u592b\uff0c\u800c\u7231\u4e3d\u4e1d\u5219\u91cd\u590d\u4e86\u590f\u5a03\u7684\u60f3\u6cd5\u3002\u5728\u6bcf\u4e2a\u6c34\u51b7\u5374\u5668\u4f1a\u5408\u4e4b\u540e\uff0c\u542c\u5230\u8c23\u8a00\u7684\u4eba\u6570\u5927\u81f4\u7ffb\u4e86\u4e00\u756a\uff08\u867d\u7136\u8fd9\u5e76\u6ca1\u6709\u8bf4\u660e\u4e24\u6b21\u5bf9\u540c\u4e00\u4e2a\u4eba\u8bf4\u95f2\u8bdd;\u4e5f\u8bb8\u7231\u4e3d\u4e1d\u8bd5\u56fe\u628a\u8fd9\u4e2a\u6545\u4e8b\u8bb2\u7ed9\u5f17\u5170\u514b\uff0c\u4f46\u5374\u53d1\u73b0\u5f17\u5170\u514b\u5df2\u7ecf\u542c\u8fc7\u4e86\u6765\u81ea\u6234\u592b\uff09\u3002\u8ba1\u7b97\u673a\u7cfb\u7edf\u901a\u5e38\u4ee5\u968f\u673a\u201c\u5bf9\u7b49\u9009\u62e9\u201d\u7684\u5f62\u5f0f\u5b9e\u73b0\u8fd9\u79cd\u7c7b\u578b\u7684\u534f\u8bae\uff1a\u5728\u7ed9\u5b9a\u9891\u7387\u4e0b\uff0c\u6bcf\u53f0\u673a\u5668\u968f\u673a\u9009\u62e9\u53e6\u4e00\u53f0\u673a\u5668\u5e76\u5206\u4eab\u4efb\u4f55\u70ed\u95e8\u8c23\u8a00\u3002 The weakness of gossip is that quality of service, i.e. complete and timely dissemination, is predicated on the requirement that each member does not discriminate and ensures prompt and dependable transmission of the data to every member of their own peer network. In a real office gossip scenario, not everyone is privy to the gossip that is being spread. Gossip, versus broadcast, is discriminatory and often participants are left out of vital or important communications. As such, the comparison to 'office gossip' is not as good as the comparison to the spread of an epidemic. Nevertheless, the technique of peer-to-peer communication is sometimes referred to as 'gossip'. gossip \u7684\u5f31\u70b9\u5728\u4e8e\u670d\u52a1\u8d28\u91cf\uff0c\u5373\u5b8c\u6574\u548c\u53ca\u65f6\u7684\u4f20\u64ad\uff0c\u662f\u57fa\u4e8e\u6bcf\u4e2a\u6210\u5458\u4e0d\u533a\u5206\u5e76\u786e\u4fdd\u5c06\u6570\u636e\u8fc5\u901f\u548c\u53ef\u9760\u5730\u4f20\u8f93\u5230\u4ed6\u4eec\u81ea\u5df1\u7684\u5bf9\u7b49\u7f51\u7edc\u7684\u6bcf\u4e2a\u6210\u5458\u7684\u8981\u6c42\u3002\u5728\u4e00\u4e2a\u771f\u6b63\u7684\u529e\u516c\u5ba4\u516b\u5366\u573a\u666f\u4e2d\uff0c\u5e76\u975e\u6bcf\u4e2a\u4eba\u90fd\u77e5\u9053\u6b63\u5728\u4f20\u64ad\u7684\u516b\u5366\u3002\u516b\u5366\u4e0e\u5e7f\u64ad\u76f8\u6bd4\u5177\u6709\u6b67\u89c6\u6027\uff0c\u901a\u5e38\u53c2\u4e0e\u8005\u88ab\u6392\u9664\u5728\u91cd\u8981\u6216\u91cd\u8981\u7684\u901a\u4fe1\u4e4b\u5916\u3002\u56e0\u6b64\uff0c\u4e0e\u201c\u529e\u516c\u5ba4\u516b\u5366\u201d\u7684\u6bd4\u8f83\u5e76\u4e0d\u50cf\u4e0e\u6d41\u884c\u75c5\u4f20\u64ad\u7684\u6bd4\u8f83\u90a3\u4e48\u597d\u3002\u7136\u800c\uff0c\u70b9\u5bf9\u70b9\u901a\u4fe1\u6280\u672f\u6709\u65f6\u88ab\u79f0\u4e3a\u201cgossip\u201d\u3002 Many variants and styles There are probably hundreds of variants of specific Gossip-like protocols because each use-scenario is likely to be customized to the organization's specific needs. For example, a gossip protocol might employ some of these ideas: The core of the protocol involves periodic, pairwise, inter-process interactions. The information exchanged during these interactions is of bounded size. When agents interact, the state of at least one agent changes to reflect the state of the other. Reliable communication is not assumed. The frequency of the interactions is low compared to typical message latencies so that the protocol costs are negligible. There is some form of randomness in the peer selection. Peers might be selected from the full set of nodes or from a smaller set of neighbors . Due to the replication there is an implicit redundancy of the delivered information.","title":"Introduction"},{"location":"Distributed-computing/Theory/Protocol/Gossip/wikipedia-Gossip-protocol/#wikipedia#gossip#protocol","text":"NOTE: gossip\u7684\u672c\u610f\u662f\u95f2\u8bdd\uff0c\u516b\u5366 A gossip protocol [ 1] is a procedure or process of computer peer-to-peer communication that is based on the way epidemics (\u4f20\u67d3\u75c5)spread. Some distributed systems use peer-to-peer gossip to ensure that data is routed to all members of an ad-hoc network. Some ad-hoc networks have no central registry and the only way to spread common data is to rely on each member to pass it along to their neighbors . The term epidemic protocol is sometimes used as a synonym for a gossip protocol, as gossip spreads information in a manner similar to the spread of a virus in a biological community.","title":"wikipedia Gossip protocol"},{"location":"Distributed-computing/Theory/Protocol/Gossip/wikipedia-Gossip-protocol/#gossip#communication","text":"The concept of gossip communication can be illustrated by the analogy of office workers spreading rumors. Let's say each hour the office workers congregate around the water cooler. Each employee pairs off with another, chosen at random, and shares the latest gossip. At the start of the day, Alice starts a new rumor: she comments to Bob that she believes that Charlie dyes his mustache. At the next meeting, Bob tells Dave, while Alice repeats the idea to Eve. After each water cooler rendezvous, the number of individuals who have heard the rumor roughly doubles (though this doesn't account for gossiping twice to the same person; perhaps Alice tries to tell the story to Frank, only to find that Frank already heard it from Dave). Computer systems typically implement this type of protocol with a form of random \"peer selection\": with a given frequency, each machine picks another machine at random and shares any hot rumors. NOTE: \u53ef\u4ee5\u901a\u8fc7\u529e\u516c\u5ba4\u5de5\u4f5c\u4eba\u5458\u4f20\u64ad\u8c23\u8a00\u7684\u7c7b\u6bd4\u6765\u8bf4\u660e*gossip communication*\u7684\u6982\u5ff5\u3002\u8ba9\u6211\u4eec\u8bf4\u529e\u516c\u5ba4\u5de5\u4f5c\u4eba\u5458\u6bcf\u5c0f\u65f6\u805a\u96c6\u5728\u6c34\u51b7\u5374\u5668\u5468\u56f4\u3002\u6bcf\u4e2a\u5458\u5de5\u4e0e\u53e6\u4e00\u4e2a\u5458\u5de5\u914d\u5bf9\uff0c\u968f\u673a\u9009\u62e9\uff0c\u5e76\u5206\u4eab\u6700\u65b0\u7684\u516b\u5366\u3002\u5728\u4e00\u5929\u5f00\u59cb\u65f6\uff0c\u7231\u4e3d\u4e1d\u5f00\u59cb\u4e86\u4e00\u4e2a\u65b0\u7684\u8c23\u8a00\uff1a\u5979\u5bf9\u9c8d\u52c3\u8bf4\u5979\u8ba4\u4e3a\u67e5\u7406\u67d3\u4e86\u4ed6\u7684\u80e1\u5b50\u3002\u5728\u4e0b\u6b21\u4f1a\u8bae\u4e0a\uff0c\u9c8d\u52c3\u544a\u8bc9\u6234\u592b\uff0c\u800c\u7231\u4e3d\u4e1d\u5219\u91cd\u590d\u4e86\u590f\u5a03\u7684\u60f3\u6cd5\u3002\u5728\u6bcf\u4e2a\u6c34\u51b7\u5374\u5668\u4f1a\u5408\u4e4b\u540e\uff0c\u542c\u5230\u8c23\u8a00\u7684\u4eba\u6570\u5927\u81f4\u7ffb\u4e86\u4e00\u756a\uff08\u867d\u7136\u8fd9\u5e76\u6ca1\u6709\u8bf4\u660e\u4e24\u6b21\u5bf9\u540c\u4e00\u4e2a\u4eba\u8bf4\u95f2\u8bdd;\u4e5f\u8bb8\u7231\u4e3d\u4e1d\u8bd5\u56fe\u628a\u8fd9\u4e2a\u6545\u4e8b\u8bb2\u7ed9\u5f17\u5170\u514b\uff0c\u4f46\u5374\u53d1\u73b0\u5f17\u5170\u514b\u5df2\u7ecf\u542c\u8fc7\u4e86\u6765\u81ea\u6234\u592b\uff09\u3002\u8ba1\u7b97\u673a\u7cfb\u7edf\u901a\u5e38\u4ee5\u968f\u673a\u201c\u5bf9\u7b49\u9009\u62e9\u201d\u7684\u5f62\u5f0f\u5b9e\u73b0\u8fd9\u79cd\u7c7b\u578b\u7684\u534f\u8bae\uff1a\u5728\u7ed9\u5b9a\u9891\u7387\u4e0b\uff0c\u6bcf\u53f0\u673a\u5668\u968f\u673a\u9009\u62e9\u53e6\u4e00\u53f0\u673a\u5668\u5e76\u5206\u4eab\u4efb\u4f55\u70ed\u95e8\u8c23\u8a00\u3002 The weakness of gossip is that quality of service, i.e. complete and timely dissemination, is predicated on the requirement that each member does not discriminate and ensures prompt and dependable transmission of the data to every member of their own peer network. In a real office gossip scenario, not everyone is privy to the gossip that is being spread. Gossip, versus broadcast, is discriminatory and often participants are left out of vital or important communications. As such, the comparison to 'office gossip' is not as good as the comparison to the spread of an epidemic. Nevertheless, the technique of peer-to-peer communication is sometimes referred to as 'gossip'. gossip \u7684\u5f31\u70b9\u5728\u4e8e\u670d\u52a1\u8d28\u91cf\uff0c\u5373\u5b8c\u6574\u548c\u53ca\u65f6\u7684\u4f20\u64ad\uff0c\u662f\u57fa\u4e8e\u6bcf\u4e2a\u6210\u5458\u4e0d\u533a\u5206\u5e76\u786e\u4fdd\u5c06\u6570\u636e\u8fc5\u901f\u548c\u53ef\u9760\u5730\u4f20\u8f93\u5230\u4ed6\u4eec\u81ea\u5df1\u7684\u5bf9\u7b49\u7f51\u7edc\u7684\u6bcf\u4e2a\u6210\u5458\u7684\u8981\u6c42\u3002\u5728\u4e00\u4e2a\u771f\u6b63\u7684\u529e\u516c\u5ba4\u516b\u5366\u573a\u666f\u4e2d\uff0c\u5e76\u975e\u6bcf\u4e2a\u4eba\u90fd\u77e5\u9053\u6b63\u5728\u4f20\u64ad\u7684\u516b\u5366\u3002\u516b\u5366\u4e0e\u5e7f\u64ad\u76f8\u6bd4\u5177\u6709\u6b67\u89c6\u6027\uff0c\u901a\u5e38\u53c2\u4e0e\u8005\u88ab\u6392\u9664\u5728\u91cd\u8981\u6216\u91cd\u8981\u7684\u901a\u4fe1\u4e4b\u5916\u3002\u56e0\u6b64\uff0c\u4e0e\u201c\u529e\u516c\u5ba4\u516b\u5366\u201d\u7684\u6bd4\u8f83\u5e76\u4e0d\u50cf\u4e0e\u6d41\u884c\u75c5\u4f20\u64ad\u7684\u6bd4\u8f83\u90a3\u4e48\u597d\u3002\u7136\u800c\uff0c\u70b9\u5bf9\u70b9\u901a\u4fe1\u6280\u672f\u6709\u65f6\u88ab\u79f0\u4e3a\u201cgossip\u201d\u3002","title":"Gossip communication"},{"location":"Distributed-computing/Theory/Protocol/Gossip/wikipedia-Gossip-protocol/#many#variants#and#styles","text":"There are probably hundreds of variants of specific Gossip-like protocols because each use-scenario is likely to be customized to the organization's specific needs. For example, a gossip protocol might employ some of these ideas: The core of the protocol involves periodic, pairwise, inter-process interactions. The information exchanged during these interactions is of bounded size. When agents interact, the state of at least one agent changes to reflect the state of the other. Reliable communication is not assumed. The frequency of the interactions is low compared to typical message latencies so that the protocol costs are negligible. There is some form of randomness in the peer selection. Peers might be selected from the full set of nodes or from a smaller set of neighbors . Due to the replication there is an implicit redundancy of the delivered information.","title":"Many variants and styles"},{"location":"Distributed-computing/Theory/Protocol/Guide/","text":"\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u534f\u8bae \u4e00\u3001\u672c\u7ae0\u5bf9\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684consistency\u3001consensus\u3001transaction\u3001gossip\u7b49\u534f\u8bae\u8fdb\u884c\u6c47\u603b\u7edf\u4e00\u8bf4\u660e\u3002 \u4e8c\u3001\u6211\u4eec\u77e5\u9053\uff0c\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u8fd0\u884c\u662f\u4f9d\u8d56\u4e8e\u8282\u70b9\u4e4b\u95f4\u76f8\u4e92passing message\u800c\u8fd0\u4f5c\u7684\uff0c\u56e0\u6b64\uff0c\u5728\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\uff0cprotocol\u5360\u636e\u7740\u975e\u5e38\u91cd\u8981\u7684\u4f4d\u7f6e\u3002 \u53c2\u8003\u6587\u7ae0 1\u3001cdmana Distributed consistency protocols and algorithms 2\u3001csdn \u5206\u5e03\u5f0f\u4e00\u81f4\u6027\u534f\u8bae - CAP\u3001BASE\u3001NWR 3\u3001csdn \u4e00\u81f4\u6027\u534f\u8bae\u7b97\u6cd5-2PC\u30013PC\u3001Paxos\u3001Raft\u3001ZAB\u3001NWR\u8d85\u8be6\u7ec6\u89e3\u6790 \u53c2\u89c1 \u4e00\u81f4\u6027\u534f\u8bae\u7b97\u6cd5-2PC-3PC-Paxos-Raft-ZAB-NWR \u7ae0\u8282 4\u3001csdn \u5206\u5e03\u5f0f\u4e00\u81f4\u6027\u534f\u8bae \u53c2\u89c1 csdn-\u5206\u5e03\u5f0f\u4e00\u81f4\u6027\u534f\u8bae \u7ae0\u8282 \u53d1\u5c55\u5386\u7a0b \u4e00\u30012PC\u30013PC\u3001 Paxos \u3001Raft\u3001ZAB \u4e8c\u3001 Paxos \u662f\u4e00\u4e2a\u5212\u65f6\u4ee3\u7684\u7b97\u6cd5\uff0c\u5b83\u5bf92PC\u30013PC\u8fdb\u884c\u4e86\u6539\u8fdb\uff0cRaft\u3001ZAB\u90fd\u662f\u57fa\u4e8e Paxos 2PC\u30013PC zhihu \u6f2b\u8bdd\u5206\u5e03\u5f0f\u7cfb\u7edf\u5171\u8bc6\u534f\u8bae: 2PC/3PC\u7bc7 \u8bb2\u5f97\u6bd4\u8f83\u597d csdn \u7528\u592a\u6781\u62f3\u8bb2\u5206\u5e03\u5f0f\u7406\u8bba\uff0c\u771f\u8212\u670d\uff01 \u8bb2\u5f97\u4e00\u822c Paxos csdn \u8bf8\u845b\u4eae VS \u5e9e\u7edf\uff0c\u62ff\u4e0b Paxos \u5171\u8bc6\u7b97\u6cd5 \u8bb2\u5f97\u6bd4\u8f83\u597d zhihu \u6f2b\u8bdd\u5206\u5e03\u5f0f\u7cfb\u7edf\u5171\u8bc6\u534f\u8bae: Paxos\u7bc7 \u8bb2\u5f97\u4e00\u822c \u6a2a\u5411\u5bf9\u6bd4\u4e0a\u8ff0algorithm 2PC 3PC Paxos Raft ZAB \u89d2\u8272 \u9636\u6bb5 voting","title":"Introduction"},{"location":"Distributed-computing/Theory/Protocol/Guide/#_1","text":"\u4e00\u3001\u672c\u7ae0\u5bf9\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684consistency\u3001consensus\u3001transaction\u3001gossip\u7b49\u534f\u8bae\u8fdb\u884c\u6c47\u603b\u7edf\u4e00\u8bf4\u660e\u3002 \u4e8c\u3001\u6211\u4eec\u77e5\u9053\uff0c\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u8fd0\u884c\u662f\u4f9d\u8d56\u4e8e\u8282\u70b9\u4e4b\u95f4\u76f8\u4e92passing message\u800c\u8fd0\u4f5c\u7684\uff0c\u56e0\u6b64\uff0c\u5728\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\uff0cprotocol\u5360\u636e\u7740\u975e\u5e38\u91cd\u8981\u7684\u4f4d\u7f6e\u3002","title":"\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u534f\u8bae"},{"location":"Distributed-computing/Theory/Protocol/Guide/#_2","text":"1\u3001cdmana Distributed consistency protocols and algorithms 2\u3001csdn \u5206\u5e03\u5f0f\u4e00\u81f4\u6027\u534f\u8bae - CAP\u3001BASE\u3001NWR 3\u3001csdn \u4e00\u81f4\u6027\u534f\u8bae\u7b97\u6cd5-2PC\u30013PC\u3001Paxos\u3001Raft\u3001ZAB\u3001NWR\u8d85\u8be6\u7ec6\u89e3\u6790 \u53c2\u89c1 \u4e00\u81f4\u6027\u534f\u8bae\u7b97\u6cd5-2PC-3PC-Paxos-Raft-ZAB-NWR \u7ae0\u8282 4\u3001csdn \u5206\u5e03\u5f0f\u4e00\u81f4\u6027\u534f\u8bae \u53c2\u89c1 csdn-\u5206\u5e03\u5f0f\u4e00\u81f4\u6027\u534f\u8bae \u7ae0\u8282","title":"\u53c2\u8003\u6587\u7ae0"},{"location":"Distributed-computing/Theory/Protocol/Guide/#_3","text":"\u4e00\u30012PC\u30013PC\u3001 Paxos \u3001Raft\u3001ZAB \u4e8c\u3001 Paxos \u662f\u4e00\u4e2a\u5212\u65f6\u4ee3\u7684\u7b97\u6cd5\uff0c\u5b83\u5bf92PC\u30013PC\u8fdb\u884c\u4e86\u6539\u8fdb\uff0cRaft\u3001ZAB\u90fd\u662f\u57fa\u4e8e Paxos","title":"\u53d1\u5c55\u5386\u7a0b"},{"location":"Distributed-computing/Theory/Protocol/Guide/#2pc3pc","text":"zhihu \u6f2b\u8bdd\u5206\u5e03\u5f0f\u7cfb\u7edf\u5171\u8bc6\u534f\u8bae: 2PC/3PC\u7bc7 \u8bb2\u5f97\u6bd4\u8f83\u597d csdn \u7528\u592a\u6781\u62f3\u8bb2\u5206\u5e03\u5f0f\u7406\u8bba\uff0c\u771f\u8212\u670d\uff01 \u8bb2\u5f97\u4e00\u822c","title":"2PC\u30013PC"},{"location":"Distributed-computing/Theory/Protocol/Guide/#paxos","text":"csdn \u8bf8\u845b\u4eae VS \u5e9e\u7edf\uff0c\u62ff\u4e0b Paxos \u5171\u8bc6\u7b97\u6cd5 \u8bb2\u5f97\u6bd4\u8f83\u597d zhihu \u6f2b\u8bdd\u5206\u5e03\u5f0f\u7cfb\u7edf\u5171\u8bc6\u534f\u8bae: Paxos\u7bc7 \u8bb2\u5f97\u4e00\u822c","title":"Paxos"},{"location":"Distributed-computing/Theory/Protocol/Guide/#algorithm","text":"2PC 3PC Paxos Raft ZAB \u89d2\u8272 \u9636\u6bb5 voting","title":"\u6a2a\u5411\u5bf9\u6bd4\u4e0a\u8ff0algorithm"},{"location":"Distributed-computing/Theory/Protocol/Guide/CSDN-%E5%88%86%E5%B8%83%E5%BC%8F%E5%9F%BA%E7%A1%80%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AEpaxos-totem-gossip/","text":"csdn \u5206\u5e03\u5f0f\u57fa\u7840\u901a\u4fe1\u534f\u8bae:paxos,totem\u548cgossip \u80cc\u666f\uff1a \u5728\u5206\u5e03\u5f0f\u4e2d\uff0c\u6700\u96be\u89e3\u51b3\u7684\u4e00\u4e2a\u95ee\u9898\u5c31\u662f\u591a\u4e2a\u8282\u70b9\u95f4\u6570\u636e\u540c\u6b65\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u6837\u7684\u95ee\u9898\uff0c\u6d8c\u73b0\u51fa\u4e86\u5404\u79cd\u5947\u601d\u5999\u60f3\u3002\u53ea\u6709\u5728\u89e3\u51b3\u4e86\u5982\u4f55\u8fdb\u884c\u4fe1\u606f\u540c\u6b65\u7684\u57fa\u7840\u4e4b\u4e0a\u624d\u884d\u751f\u51fa\u5f62\u5f62\u8272\u8272\u7684\u5e94\u7528\u3002\u8fd9\u91cc\u5f00\u59cb\u4ecb\u7ecd\u51e0\u79cd\u5206\u5e03\u5f0f\u901a\u4fe1\u534f\u8bae\u3002 \u57fa\u7840\u534f\u8bae\u7684\u5bf9\u6bd4\uff1a \u57fa\u7840\u534f\u8bae paxos totem gossip \u6570\u636e\u540c\u6b65 \u6570\u636e\u4e00\u81f4\u6027 \u5f3a\u4e00\u81f4\u6027 \u5f3a\u4e00\u81f4\u6027 \u6700\u7ec8\u4e00\u81f4\u6027 \u76f8\u5173\u5e94\u7528 zookeeper corosync Cassandra \u4f18\u70b9 \u53ef\u4ee5\u5f88\u597d\u7684\u89e3\u51b3\u901a\u4fe1\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5728\u96c6\u7fa4\u89c4\u6a21\u4e0a\u6bd4corosync\u8981\u7565\u5927\u4e00\u4e9b \u7b80\u5355\u65b9\u4fbf\uff0c\u6309\u7167\u534f\u8bae\u5b9e\u73b0\u540e\u5c31\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 \u534f\u8bae\u672c\u8eab\u7b80\u5355\uff0c\u7ec4\u7f51\u89c4\u6a21\u51e0\u4e4e\u4e0d\u53d7\u9650\u5236\uff0c\u901a\u4fe1\u6027\u80fd\u597d \u7f3a\u70b9 \u7406\u8bba\u6027\u592a\u5f3a\uff0c\u5982\u679c\u8981\u5b9e\u9645\u4f7f\u7528\uff0c\u8fd8\u662f\u9700\u8981\u8fdb\u884c\u4f18\u5316 \u4f7f\u7528\u4e86\u5e7f\u64ad\u5305\uff0c\u5bf9\u4e8e\u8de8\u57df\u4f20\u9001\u6709\u5f71\u54cd\uff0c\u800c\u4e14\u4ee4\u724c\u73af\u672c\u8eab\u5e26\u6765\u7684\u95ee\u9898\u4f7f\u5f97\u7ec4\u7f51\u89c4\u6a21\u4e0d\u5927 \u4e0d\u80fd\u63d0\u4f9b\u4f20\u7edf\u7684\u6570\u636e\u4e00\u81f4\u6027\u670d\u52a1\uff0c\u5728\u4f20\u8f93\u4e2d\u5360\u7528\u8f83\u591a\u7684\u7f51\u7edc\u6d41\u91cf","title":"Introduction"},{"location":"Distributed-computing/Theory/Protocol/Guide/CSDN-%E5%88%86%E5%B8%83%E5%BC%8F%E5%9F%BA%E7%A1%80%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AEpaxos-totem-gossip/#csdn#paxostotemgossip","text":"","title":"csdn \u5206\u5e03\u5f0f\u57fa\u7840\u901a\u4fe1\u534f\u8bae:paxos,totem\u548cgossip"},{"location":"Distributed-computing/Theory/Protocol/Guide/CSDN-%E5%88%86%E5%B8%83%E5%BC%8F%E5%9F%BA%E7%A1%80%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AEpaxos-totem-gossip/#_1","text":"\u5728\u5206\u5e03\u5f0f\u4e2d\uff0c\u6700\u96be\u89e3\u51b3\u7684\u4e00\u4e2a\u95ee\u9898\u5c31\u662f\u591a\u4e2a\u8282\u70b9\u95f4\u6570\u636e\u540c\u6b65\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u6837\u7684\u95ee\u9898\uff0c\u6d8c\u73b0\u51fa\u4e86\u5404\u79cd\u5947\u601d\u5999\u60f3\u3002\u53ea\u6709\u5728\u89e3\u51b3\u4e86\u5982\u4f55\u8fdb\u884c\u4fe1\u606f\u540c\u6b65\u7684\u57fa\u7840\u4e4b\u4e0a\u624d\u884d\u751f\u51fa\u5f62\u5f62\u8272\u8272\u7684\u5e94\u7528\u3002\u8fd9\u91cc\u5f00\u59cb\u4ecb\u7ecd\u51e0\u79cd\u5206\u5e03\u5f0f\u901a\u4fe1\u534f\u8bae\u3002","title":"\u80cc\u666f\uff1a"},{"location":"Distributed-computing/Theory/Protocol/Guide/CSDN-%E5%88%86%E5%B8%83%E5%BC%8F%E5%9F%BA%E7%A1%80%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AEpaxos-totem-gossip/#_2","text":"\u57fa\u7840\u534f\u8bae paxos totem gossip \u6570\u636e\u540c\u6b65 \u6570\u636e\u4e00\u81f4\u6027 \u5f3a\u4e00\u81f4\u6027 \u5f3a\u4e00\u81f4\u6027 \u6700\u7ec8\u4e00\u81f4\u6027 \u76f8\u5173\u5e94\u7528 zookeeper corosync Cassandra \u4f18\u70b9 \u53ef\u4ee5\u5f88\u597d\u7684\u89e3\u51b3\u901a\u4fe1\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5728\u96c6\u7fa4\u89c4\u6a21\u4e0a\u6bd4corosync\u8981\u7565\u5927\u4e00\u4e9b \u7b80\u5355\u65b9\u4fbf\uff0c\u6309\u7167\u534f\u8bae\u5b9e\u73b0\u540e\u5c31\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 \u534f\u8bae\u672c\u8eab\u7b80\u5355\uff0c\u7ec4\u7f51\u89c4\u6a21\u51e0\u4e4e\u4e0d\u53d7\u9650\u5236\uff0c\u901a\u4fe1\u6027\u80fd\u597d \u7f3a\u70b9 \u7406\u8bba\u6027\u592a\u5f3a\uff0c\u5982\u679c\u8981\u5b9e\u9645\u4f7f\u7528\uff0c\u8fd8\u662f\u9700\u8981\u8fdb\u884c\u4f18\u5316 \u4f7f\u7528\u4e86\u5e7f\u64ad\u5305\uff0c\u5bf9\u4e8e\u8de8\u57df\u4f20\u9001\u6709\u5f71\u54cd\uff0c\u800c\u4e14\u4ee4\u724c\u73af\u672c\u8eab\u5e26\u6765\u7684\u95ee\u9898\u4f7f\u5f97\u7ec4\u7f51\u89c4\u6a21\u4e0d\u5927 \u4e0d\u80fd\u63d0\u4f9b\u4f20\u7edf\u7684\u6570\u636e\u4e00\u81f4\u6027\u670d\u52a1\uff0c\u5728\u4f20\u8f93\u4e2d\u5360\u7528\u8f83\u591a\u7684\u7f51\u7edc\u6d41\u91cf","title":"\u57fa\u7840\u534f\u8bae\u7684\u5bf9\u6bd4\uff1a"},{"location":"Distributed-computing/Theory/Protocol/Guide/Expert-%E8%8C%83%E6%96%8C/","text":"\u8303\u658c zhihu \u8303\u658c \u4e13\u680f: \u5b8c\u5907\u7a7a\u95f4 \u5176\u4e2d\u5bf9\u5206\u5e03\u5f0f\u534f\u8bae\u8fdb\u884c\u4e86\u5305\u542b\u597d\u7684\u5206\u6790 zhihu \u6f2b\u8bdd\u5206\u5e03\u5f0f\u7cfb\u7edf\u5171\u8bc6\u534f\u8bae: 2PC/3PC\u7bc7 zhihu \u6f2b\u8bdd\u5206\u5e03\u5f0f\u7cfb\u7edf\u5171\u8bc6\u534f\u8bae: Paxos\u7bc7 zhihu \u6f2b\u8bdd\u5206\u5e03\u5f0f\u7cfb\u7edf\u5171\u8bc6\u534f\u8bae: PoW\u7bc7 cs.cmu Bin Fan (\u8303\u658c in Chinese) Cuckoo Filter VS Bloom filter 1\u3001**Cuckoo Filter: Practically Better Than Bloom**Bin Fan, David G. Andersen, Michael Kaminsky, and Michael D. Mitzenmacher*In Proc. of ACM CoNEXT 2014* [ pdf] [ slides] [ source code] 2\u3001**Cuckoo Filter: Better Than Bloom**Bin Fan, Dave Andersen and Michael Kaminsky*USENIX ;login:, August 2013* [ pdf]","title":"Introduction"},{"location":"Distributed-computing/Theory/Protocol/Guide/Expert-%E8%8C%83%E6%96%8C/#_1","text":"","title":"\u8303\u658c"},{"location":"Distributed-computing/Theory/Protocol/Guide/Expert-%E8%8C%83%E6%96%8C/#zhihu","text":"","title":"zhihu \u8303\u658c"},{"location":"Distributed-computing/Theory/Protocol/Guide/Expert-%E8%8C%83%E6%96%8C/#_2","text":"\u5176\u4e2d\u5bf9\u5206\u5e03\u5f0f\u534f\u8bae\u8fdb\u884c\u4e86\u5305\u542b\u597d\u7684\u5206\u6790 zhihu \u6f2b\u8bdd\u5206\u5e03\u5f0f\u7cfb\u7edf\u5171\u8bc6\u534f\u8bae: 2PC/3PC\u7bc7 zhihu \u6f2b\u8bdd\u5206\u5e03\u5f0f\u7cfb\u7edf\u5171\u8bc6\u534f\u8bae: Paxos\u7bc7 zhihu \u6f2b\u8bdd\u5206\u5e03\u5f0f\u7cfb\u7edf\u5171\u8bc6\u534f\u8bae: PoW\u7bc7","title":"\u4e13\u680f: \u5b8c\u5907\u7a7a\u95f4"},{"location":"Distributed-computing/Theory/Protocol/Guide/Expert-%E8%8C%83%E6%96%8C/#cscmu#bin#fan#in#chinese","text":"","title":"cs.cmu Bin Fan (\u8303\u658c in Chinese)"},{"location":"Distributed-computing/Theory/Protocol/Guide/Expert-%E8%8C%83%E6%96%8C/#cuckoo#filter#vs#bloom#filter","text":"1\u3001**Cuckoo Filter: Practically Better Than Bloom**Bin Fan, David G. Andersen, Michael Kaminsky, and Michael D. Mitzenmacher*In Proc. of ACM CoNEXT 2014* [ pdf] [ slides] [ source code] 2\u3001**Cuckoo Filter: Better Than Bloom**Bin Fan, Dave Andersen and Michael Kaminsky*USENIX ;login:, August 2013* [ pdf]","title":"Cuckoo Filter VS Bloom filter"},{"location":"Distributed-computing/Theory/Protocol/Guide/Expert-%E8%8C%83%E6%96%8C/%E6%BC%AB%E8%AF%9D%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%85%B1%E8%AF%86%E5%8D%8F%E8%AE%AE2PC-3PC%E7%AF%87/","text":"zhihu \u6f2b\u8bdd\u5206\u5e03\u5f0f\u7cfb\u7edf\u5171\u8bc6\u534f\u8bae: 2PC/3PC\u7bc7 NOTE: \u4f5c\u8005\u5c062PC\u30013PC\u89c6\u4e3a\u5171\u8bc6\u534f\u8bae 2PC (\u4e24\u9636\u6bb5\u63d0\u4ea4)\u534f\u8bae 2PC\u7684\u539f\u7406 \u987e\u540d\u601d\u4e49, 2PC\u534f\u8bae\u6709\u4e24\u4e2a\u9636\u6bb5:Propose\u548cCommit. **\u5728\u65e0failure\u60c5\u51b5\u4e0b**\u76842PC\u534f\u8bae\u6d41\u7a0b\u7684\u753b\u98ce\u662f\u8fd9\u6837\u7684: \u56fe1: 2PC, coordinator\u63d0\u8bae\u901a\u8fc7, voter{1,2,3}\u8fbe\u6210\u65b0\u7684\u5171\u8bc6 \u5982\u679c\u6709\u81f3\u5c11\u4e00\u4e2avoter (\u6bd4\u5982voter3)\u5728Propose\u9636\u6bb5\u6295\u4e86\u53cd\u5bf9\u7968, \u90a3\u4e48propose\u901a\u8fc7\u5931\u8d25. coordinator\u5c31\u4f1a\u5728Commit(or abort)\u9636\u6bb5\u8ddf\u6240\u6709voter\u8bf4, \u653e\u5f03\u8fd9\u4e2apropose. \u56fe2: 2PC, coordinator\u63d0\u8bae\u6ca1\u6709\u901a\u8fc7, voter{1,2,3}\u4fdd\u6301\u65e7\u6709\u7684\u5171\u8bc6 2PC\u7684\u7f3a\u9677 2PC\u7684\u7f3a\u70b9\u5728\u4e8e\u4e0d\u80fd\u5904\u7406fail-stop\u5f62\u5f0f\u7684\u8282\u70b9failure. \u6bd4\u5982\u4e0b\u56fe\u8fd9\u79cd\u60c5\u51b5. \u5047\u8bbecoordinator\u548cvoter3\u90fd\u5728Commit\u8fd9\u4e2a\u9636\u6bb5crash\u4e86, \u800cvoter1\u548cvoter2\u6ca1\u6709\u6536\u5230commit\u6d88\u606f. \u8fd9\u65f6\u5019voter1\u548cvoter2\u5c31\u9677\u5165\u4e86\u4e00\u4e2a\u56f0\u5883. \u56e0\u4e3a\u4ed6\u4eec\u5e76\u4e0d\u80fd\u5224\u65ad\u73b0\u5728\u662f\u4e24\u4e2a\u573a\u666f\u4e2d\u7684\u54ea\u4e00\u79cd: (1)\u4e0a\u8f6e\u5168\u7968\u901a\u8fc7\u7136\u540evoter3\u7b2c\u4e00\u4e2a\u6536\u5230\u4e86commit\u7684\u6d88\u606f\u5e76\u5728commit\u64cd\u4f5c\u4e4b\u540ecrash\u4e86, (2)\u4e0a\u8f6evoter3\u53cd\u5bf9\u6240\u4ee5\u5e72\u8106\u6ca1\u6709\u901a\u8fc7. \u56fe3: 2PC, coordinator\u548cvoter3 crash, voter{1,2}\u65e0\u6cd5\u5224\u65ad\u5f53\u524d\u72b6\u6001\u800c\u5361\u6b7b 2PC\u5728\u8fd9\u79cdfail-stop\u60c5\u51b5\u4e0b\u4f1a\u5931\u8d25\u662f\u56e0\u4e3avoter\u5728\u5f97\u77e5Propose Phase\u7ed3\u679c\u540e\u5c31\u76f4\u63a5commit\u4e86, \u800c\u5e76\u6ca1\u6709\u5728commit\u4e4b\u524d\u544a\u77e5\u5176\u4ed6voter\u81ea\u5df1\u5df2\u6536\u5230Propose Phase\u7684\u7ed3\u679c. \u4ece\u800c\u5bfc\u81f4\u5728coordinator\u548c\u4e00\u4e2avoter\u53cc\u53cc\u6389\u7ebf\u7684\u60c5\u51b5\u4e0b, \u5176\u4f59voter\u4e0d\u4f46\u65e0\u6cd5\u590d\u539fPropose Phase\u7684\u7ed3\u679c, \u4e5f\u65e0\u6cd5\u77e5\u9053\u6389\u7ebf\u7684voter\u662f\u5426\u6253\u7b97\u751a\u81f3\u5df2\u7ecfcommit. \u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898, 3PC\u4e86\u89e3\u4e00\u4e0b. 3PC (\u4e09\u9636\u6bb5\u63d0\u4ea4)\u534f\u8bae 3PC\u7684\u539f\u7406 \u7b80\u5355\u7684\u8bf4\u6765, 3PC\u5c31\u662f\u628a2PC\u7684Commit\u9636\u6bb5\u62c6\u6210\u4e86PreCommit\u548cCommit\u4e24\u4e2a\u9636\u6bb5. \u901a\u8fc7\u8fdb\u5165\u589e\u52a0\u7684\u8fd9\u4e00\u4e2aPreCommit\u9636\u6bb5, voter\u53ef\u4ee5\u5f97\u5230Propose\u9636\u6bb5\u7684\u6295\u7968\u7ed3\u679c, \u4f46\u4e0d\u4f1acommit; \u800c\u901a\u8fc7\u8fdb\u5165Commit\u9636\u6bb5, voter\u53ef\u4ee5\u76d8\u51fa\u5176\u4ed6\u6bcf\u4e2avoter\u4e5f\u90fd\u6253\u7b97commit\u4e86, \u4ece\u800c\u53ef\u4ee5\u653e\u5fc3\u7684commit. \u6362\u8a00\u4e4b, 3PC\u57282PC\u7684Commit\u9636\u6bb5\u91cc\u589e\u52a0\u4e86\u4e00\u4e2abarrier (\u5373\u76f8\u5f53\u4e8e\u544a\u8bc9\u5176\u4ed6\u6240\u6709voter, \u6211\u6536\u5230\u4e86Propose\u7684\u7ed3\u679c\u5566). \u5728\u8fd9\u4e2abarrier\u4e4b\u524dcoordinator\u6389\u7ebf\u7684\u8bdd, \u5176\u4ed6voter\u53ef\u4ee5\u5f97\u51fa\u7ed3\u8bba\u4e0d\u662f\u6bcf\u4e2avoter\u90fd\u6536\u5230Propose Phase\u7684\u7ed3\u679c, \u4ece\u800c\u653e\u5f03\u6216\u9009\u51fa\u65b0\u7684coordinator; \u5728\u8fd9\u4e2abarrier\u4e4b\u540ecoordinator\u6389\u7ebf\u7684\u8bdd, \u6bcf\u4e2avoter\u4f1a\u653e\u5fc3\u7684commit, \u56e0\u4e3a\u4ed6\u4eec\u77e5\u9053\u5176\u4ed6voter\u4e5f\u90fd\u505a\u540c\u6837\u7684\u8ba1\u5212. \u56fe4: 3PC, coordinator\u63d0\u8bae\u901a\u8fc7, voter{1,2,3}\u8fbe\u6210\u65b0\u7684\u5171\u8bc6 3PC\u7684\u7f3a\u9677 3PC\u53ef\u4ee5\u6709\u6548\u7684\u5904\u7406fail-stop\u7684\u6a21\u5f0f, \u4f46\u4e0d\u80fd\u5904\u7406\u7f51\u7edc\u5212\u5206(network partition)\u7684\u60c5\u51b5---\u8282\u70b9\u4e92\u76f8\u4e0d\u80fd\u901a\u4fe1. \u5047\u8bbe\u5728PreCommit\u9636\u6bb5\u6240\u6709\u8282\u70b9\u88ab\u4e00\u5206\u4e3a\u4e8c, \u6536\u5230preCommit\u6d88\u606f\u7684voter\u5728\u4e00\u8fb9, \u800c\u6ca1\u6709\u6536\u5230\u8fd9\u4e2a\u6d88\u606f\u7684\u5728\u53e6\u5916\u4e00\u8fb9. \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b, \u4e24\u8fb9\u5c31\u53ef\u80fd\u4f1a\u9009\u51fa\u65b0\u7684coordinator\u800c\u505a\u51fa\u4e0d\u540c\u7684\u51b3\u5b9a. \u56fe5: 3PC, network partition, voter{1,2,3}\u5931\u53bb\u5171\u8bc6 \u9664\u4e86\u7f51\u7edc\u5212\u5206\u4ee5\u5916, 3PC\u4e5f\u4e0d\u80fd\u5904\u7406fail-recover\u7684\u9519\u8bef\u60c5\u51b5. \u7b80\u5355\u8bf4\u6765\u5f53coordinator\u6536\u5230preCommit\u7684\u786e\u8ba4\u524dcrash, \u4e8e\u662f\u5176\u4ed6\u67d0\u4e00\u4e2avoter\u63a5\u66ff\u4e86\u539fcoordinator\u7684\u4efb\u52a1\u800c\u5f00\u59cb\u7ec4\u7ec7\u6240\u6709voter commit. \u800c\u4e0e\u6b64\u540c\u65f6\u539fcoordinator\u91cd\u542f\u540e\u53c8\u56de\u5230\u4e86\u7f51\u7edc\u4e2d, \u5f00\u59cb\u7ee7\u7eed\u4e4b\u524d\u7684\u56de\u5408---\u53d1\u9001abort\u7ed9\u5404\u4f4dvoter\u56e0\u4e3a\u5b83\u5e76\u6ca1\u6709\u6536\u5230preCommit. \u6b64\u65f6\u6709\u53ef\u80fd\u4f1a\u51fa\u73b0\u539fcoordinator\u548c\u7ee7\u4efb\u7684coordinator\u7ed9\u4e0d\u540c\u8282\u70b9\u53d1\u9001\u76f8\u77db\u76fe\u7684commit\u548cabort\u6307\u4ee4, \u4ece\u800c\u51fa\u73b0\u4e2a\u8282\u70b9\u7684\u72b6\u6001\u5206\u6b67. \u8fd9\u79cd\u60c5\u51b5\u7b49\u4ef7\u4e8e\u4e00\u4e2a\u66f4\u771f\u5b9e\u6216\u8005\u66f4\u8d1f\u8d23\u7684\u7f51\u7edc\u73af\u5883\u5047\u8bbe: \u5f02\u6b65\u7f51\u7edc. \u5728\u8fd9\u79cd\u5047\u8bbe\u4e0b, \u7f51\u7edc\u4f20\u8f93\u65f6\u95f4\u53ef\u80fd\u4efb\u610f\u957f. \u4e3a\u4e86\u89e3\u51b3\u8fd9\u79cd\u60c5\u51b5, \u90a3\u5c31\u5f97\u8bf7\u51fa\u4e0b\u4e00\u7bc7\u7684\u4e3b\u89d2: Paxos \u603b\u7ed3 1\u30012PC\u4f7f\u7528\u4e24\u4e2aroundtrip\u6765\u8fbe\u6210\u65b0\u7684\u5171\u8bc6\u6216\u7ef4\u6301\u65e7\u6709\u7684\u5171\u8bc6. \u5176\u5c40\u9650\u6027\u5728\u4e8e\u4e0d\u80fd\u4fdd\u8bc1\u6709\u8282\u70b9\u6c38\u4e45\u6027\u5d29\u6e83(fail-stop)\u7684\u60c5\u51b5\u4e0b\u7b97\u6cd5\u80fd\u5411\u524d\u63a8\u8fdb; 2\u30013PC\u6269\u5c55\u4e862PC, \u4f7f\u7528\u4e09\u4e2aroundtrip\u8fbe\u6210\u5171\u8bc6. \u5176\u5c40\u9650\u6027\u5728\u4e8e\u4e0d\u80fd\u4fdd\u8bc1\u5728\u8282\u70b9\u6682\u65f6\u6027\u5d29\u6e83(fail-recover), \u6216\u662f\u6709\u7f51\u7edc\u5212\u5206\u7684\u60c5\u51b5\u4e0b, \u5171\u8bc6\u4f9d\u65e7\u6210\u7acb. \u63a8\u8350\u9605\u8bfb Consensus Protocols: Two-Phase Commit - Paper Trail Consensus Protocols: Three-phase Commit - Paper Trail In distributed systems, what is a simple explanation of the Paxos algorithm?","title":"Introduction"},{"location":"Distributed-computing/Theory/Protocol/Guide/Expert-%E8%8C%83%E6%96%8C/%E6%BC%AB%E8%AF%9D%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%85%B1%E8%AF%86%E5%8D%8F%E8%AE%AE2PC-3PC%E7%AF%87/#zhihu#2pc3pc","text":"NOTE: \u4f5c\u8005\u5c062PC\u30013PC\u89c6\u4e3a\u5171\u8bc6\u534f\u8bae","title":"zhihu \u6f2b\u8bdd\u5206\u5e03\u5f0f\u7cfb\u7edf\u5171\u8bc6\u534f\u8bae: 2PC/3PC\u7bc7"},{"location":"Distributed-computing/Theory/Protocol/Guide/Expert-%E8%8C%83%E6%96%8C/%E6%BC%AB%E8%AF%9D%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%85%B1%E8%AF%86%E5%8D%8F%E8%AE%AE2PC-3PC%E7%AF%87/#2pc","text":"","title":"2PC (\u4e24\u9636\u6bb5\u63d0\u4ea4)\u534f\u8bae"},{"location":"Distributed-computing/Theory/Protocol/Guide/Expert-%E8%8C%83%E6%96%8C/%E6%BC%AB%E8%AF%9D%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%85%B1%E8%AF%86%E5%8D%8F%E8%AE%AE2PC-3PC%E7%AF%87/#2pc_1","text":"\u987e\u540d\u601d\u4e49, 2PC\u534f\u8bae\u6709\u4e24\u4e2a\u9636\u6bb5:Propose\u548cCommit. **\u5728\u65e0failure\u60c5\u51b5\u4e0b**\u76842PC\u534f\u8bae\u6d41\u7a0b\u7684\u753b\u98ce\u662f\u8fd9\u6837\u7684: \u56fe1: 2PC, coordinator\u63d0\u8bae\u901a\u8fc7, voter{1,2,3}\u8fbe\u6210\u65b0\u7684\u5171\u8bc6 \u5982\u679c\u6709\u81f3\u5c11\u4e00\u4e2avoter (\u6bd4\u5982voter3)\u5728Propose\u9636\u6bb5\u6295\u4e86\u53cd\u5bf9\u7968, \u90a3\u4e48propose\u901a\u8fc7\u5931\u8d25. coordinator\u5c31\u4f1a\u5728Commit(or abort)\u9636\u6bb5\u8ddf\u6240\u6709voter\u8bf4, \u653e\u5f03\u8fd9\u4e2apropose. \u56fe2: 2PC, coordinator\u63d0\u8bae\u6ca1\u6709\u901a\u8fc7, voter{1,2,3}\u4fdd\u6301\u65e7\u6709\u7684\u5171\u8bc6","title":"2PC\u7684\u539f\u7406"},{"location":"Distributed-computing/Theory/Protocol/Guide/Expert-%E8%8C%83%E6%96%8C/%E6%BC%AB%E8%AF%9D%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%85%B1%E8%AF%86%E5%8D%8F%E8%AE%AE2PC-3PC%E7%AF%87/#2pc_2","text":"2PC\u7684\u7f3a\u70b9\u5728\u4e8e\u4e0d\u80fd\u5904\u7406fail-stop\u5f62\u5f0f\u7684\u8282\u70b9failure. \u6bd4\u5982\u4e0b\u56fe\u8fd9\u79cd\u60c5\u51b5. \u5047\u8bbecoordinator\u548cvoter3\u90fd\u5728Commit\u8fd9\u4e2a\u9636\u6bb5crash\u4e86, \u800cvoter1\u548cvoter2\u6ca1\u6709\u6536\u5230commit\u6d88\u606f. \u8fd9\u65f6\u5019voter1\u548cvoter2\u5c31\u9677\u5165\u4e86\u4e00\u4e2a\u56f0\u5883. \u56e0\u4e3a\u4ed6\u4eec\u5e76\u4e0d\u80fd\u5224\u65ad\u73b0\u5728\u662f\u4e24\u4e2a\u573a\u666f\u4e2d\u7684\u54ea\u4e00\u79cd: (1)\u4e0a\u8f6e\u5168\u7968\u901a\u8fc7\u7136\u540evoter3\u7b2c\u4e00\u4e2a\u6536\u5230\u4e86commit\u7684\u6d88\u606f\u5e76\u5728commit\u64cd\u4f5c\u4e4b\u540ecrash\u4e86, (2)\u4e0a\u8f6evoter3\u53cd\u5bf9\u6240\u4ee5\u5e72\u8106\u6ca1\u6709\u901a\u8fc7. \u56fe3: 2PC, coordinator\u548cvoter3 crash, voter{1,2}\u65e0\u6cd5\u5224\u65ad\u5f53\u524d\u72b6\u6001\u800c\u5361\u6b7b 2PC\u5728\u8fd9\u79cdfail-stop\u60c5\u51b5\u4e0b\u4f1a\u5931\u8d25\u662f\u56e0\u4e3avoter\u5728\u5f97\u77e5Propose Phase\u7ed3\u679c\u540e\u5c31\u76f4\u63a5commit\u4e86, \u800c\u5e76\u6ca1\u6709\u5728commit\u4e4b\u524d\u544a\u77e5\u5176\u4ed6voter\u81ea\u5df1\u5df2\u6536\u5230Propose Phase\u7684\u7ed3\u679c. \u4ece\u800c\u5bfc\u81f4\u5728coordinator\u548c\u4e00\u4e2avoter\u53cc\u53cc\u6389\u7ebf\u7684\u60c5\u51b5\u4e0b, \u5176\u4f59voter\u4e0d\u4f46\u65e0\u6cd5\u590d\u539fPropose Phase\u7684\u7ed3\u679c, \u4e5f\u65e0\u6cd5\u77e5\u9053\u6389\u7ebf\u7684voter\u662f\u5426\u6253\u7b97\u751a\u81f3\u5df2\u7ecfcommit. \u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898, 3PC\u4e86\u89e3\u4e00\u4e0b.","title":"2PC\u7684\u7f3a\u9677"},{"location":"Distributed-computing/Theory/Protocol/Guide/Expert-%E8%8C%83%E6%96%8C/%E6%BC%AB%E8%AF%9D%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%85%B1%E8%AF%86%E5%8D%8F%E8%AE%AE2PC-3PC%E7%AF%87/#3pc","text":"","title":"3PC (\u4e09\u9636\u6bb5\u63d0\u4ea4)\u534f\u8bae"},{"location":"Distributed-computing/Theory/Protocol/Guide/Expert-%E8%8C%83%E6%96%8C/%E6%BC%AB%E8%AF%9D%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%85%B1%E8%AF%86%E5%8D%8F%E8%AE%AE2PC-3PC%E7%AF%87/#3pc_1","text":"\u7b80\u5355\u7684\u8bf4\u6765, 3PC\u5c31\u662f\u628a2PC\u7684Commit\u9636\u6bb5\u62c6\u6210\u4e86PreCommit\u548cCommit\u4e24\u4e2a\u9636\u6bb5. \u901a\u8fc7\u8fdb\u5165\u589e\u52a0\u7684\u8fd9\u4e00\u4e2aPreCommit\u9636\u6bb5, voter\u53ef\u4ee5\u5f97\u5230Propose\u9636\u6bb5\u7684\u6295\u7968\u7ed3\u679c, \u4f46\u4e0d\u4f1acommit; \u800c\u901a\u8fc7\u8fdb\u5165Commit\u9636\u6bb5, voter\u53ef\u4ee5\u76d8\u51fa\u5176\u4ed6\u6bcf\u4e2avoter\u4e5f\u90fd\u6253\u7b97commit\u4e86, \u4ece\u800c\u53ef\u4ee5\u653e\u5fc3\u7684commit. \u6362\u8a00\u4e4b, 3PC\u57282PC\u7684Commit\u9636\u6bb5\u91cc\u589e\u52a0\u4e86\u4e00\u4e2abarrier (\u5373\u76f8\u5f53\u4e8e\u544a\u8bc9\u5176\u4ed6\u6240\u6709voter, \u6211\u6536\u5230\u4e86Propose\u7684\u7ed3\u679c\u5566). \u5728\u8fd9\u4e2abarrier\u4e4b\u524dcoordinator\u6389\u7ebf\u7684\u8bdd, \u5176\u4ed6voter\u53ef\u4ee5\u5f97\u51fa\u7ed3\u8bba\u4e0d\u662f\u6bcf\u4e2avoter\u90fd\u6536\u5230Propose Phase\u7684\u7ed3\u679c, \u4ece\u800c\u653e\u5f03\u6216\u9009\u51fa\u65b0\u7684coordinator; \u5728\u8fd9\u4e2abarrier\u4e4b\u540ecoordinator\u6389\u7ebf\u7684\u8bdd, \u6bcf\u4e2avoter\u4f1a\u653e\u5fc3\u7684commit, \u56e0\u4e3a\u4ed6\u4eec\u77e5\u9053\u5176\u4ed6voter\u4e5f\u90fd\u505a\u540c\u6837\u7684\u8ba1\u5212. \u56fe4: 3PC, coordinator\u63d0\u8bae\u901a\u8fc7, voter{1,2,3}\u8fbe\u6210\u65b0\u7684\u5171\u8bc6","title":"3PC\u7684\u539f\u7406"},{"location":"Distributed-computing/Theory/Protocol/Guide/Expert-%E8%8C%83%E6%96%8C/%E6%BC%AB%E8%AF%9D%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%85%B1%E8%AF%86%E5%8D%8F%E8%AE%AE2PC-3PC%E7%AF%87/#3pc_2","text":"3PC\u53ef\u4ee5\u6709\u6548\u7684\u5904\u7406fail-stop\u7684\u6a21\u5f0f, \u4f46\u4e0d\u80fd\u5904\u7406\u7f51\u7edc\u5212\u5206(network partition)\u7684\u60c5\u51b5---\u8282\u70b9\u4e92\u76f8\u4e0d\u80fd\u901a\u4fe1. \u5047\u8bbe\u5728PreCommit\u9636\u6bb5\u6240\u6709\u8282\u70b9\u88ab\u4e00\u5206\u4e3a\u4e8c, \u6536\u5230preCommit\u6d88\u606f\u7684voter\u5728\u4e00\u8fb9, \u800c\u6ca1\u6709\u6536\u5230\u8fd9\u4e2a\u6d88\u606f\u7684\u5728\u53e6\u5916\u4e00\u8fb9. \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b, \u4e24\u8fb9\u5c31\u53ef\u80fd\u4f1a\u9009\u51fa\u65b0\u7684coordinator\u800c\u505a\u51fa\u4e0d\u540c\u7684\u51b3\u5b9a. \u56fe5: 3PC, network partition, voter{1,2,3}\u5931\u53bb\u5171\u8bc6 \u9664\u4e86\u7f51\u7edc\u5212\u5206\u4ee5\u5916, 3PC\u4e5f\u4e0d\u80fd\u5904\u7406fail-recover\u7684\u9519\u8bef\u60c5\u51b5. \u7b80\u5355\u8bf4\u6765\u5f53coordinator\u6536\u5230preCommit\u7684\u786e\u8ba4\u524dcrash, \u4e8e\u662f\u5176\u4ed6\u67d0\u4e00\u4e2avoter\u63a5\u66ff\u4e86\u539fcoordinator\u7684\u4efb\u52a1\u800c\u5f00\u59cb\u7ec4\u7ec7\u6240\u6709voter commit. \u800c\u4e0e\u6b64\u540c\u65f6\u539fcoordinator\u91cd\u542f\u540e\u53c8\u56de\u5230\u4e86\u7f51\u7edc\u4e2d, \u5f00\u59cb\u7ee7\u7eed\u4e4b\u524d\u7684\u56de\u5408---\u53d1\u9001abort\u7ed9\u5404\u4f4dvoter\u56e0\u4e3a\u5b83\u5e76\u6ca1\u6709\u6536\u5230preCommit. \u6b64\u65f6\u6709\u53ef\u80fd\u4f1a\u51fa\u73b0\u539fcoordinator\u548c\u7ee7\u4efb\u7684coordinator\u7ed9\u4e0d\u540c\u8282\u70b9\u53d1\u9001\u76f8\u77db\u76fe\u7684commit\u548cabort\u6307\u4ee4, \u4ece\u800c\u51fa\u73b0\u4e2a\u8282\u70b9\u7684\u72b6\u6001\u5206\u6b67. \u8fd9\u79cd\u60c5\u51b5\u7b49\u4ef7\u4e8e\u4e00\u4e2a\u66f4\u771f\u5b9e\u6216\u8005\u66f4\u8d1f\u8d23\u7684\u7f51\u7edc\u73af\u5883\u5047\u8bbe: \u5f02\u6b65\u7f51\u7edc. \u5728\u8fd9\u79cd\u5047\u8bbe\u4e0b, \u7f51\u7edc\u4f20\u8f93\u65f6\u95f4\u53ef\u80fd\u4efb\u610f\u957f. \u4e3a\u4e86\u89e3\u51b3\u8fd9\u79cd\u60c5\u51b5, \u90a3\u5c31\u5f97\u8bf7\u51fa\u4e0b\u4e00\u7bc7\u7684\u4e3b\u89d2: Paxos","title":"3PC\u7684\u7f3a\u9677"},{"location":"Distributed-computing/Theory/Protocol/Guide/Expert-%E8%8C%83%E6%96%8C/%E6%BC%AB%E8%AF%9D%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%85%B1%E8%AF%86%E5%8D%8F%E8%AE%AE2PC-3PC%E7%AF%87/#_1","text":"1\u30012PC\u4f7f\u7528\u4e24\u4e2aroundtrip\u6765\u8fbe\u6210\u65b0\u7684\u5171\u8bc6\u6216\u7ef4\u6301\u65e7\u6709\u7684\u5171\u8bc6. \u5176\u5c40\u9650\u6027\u5728\u4e8e\u4e0d\u80fd\u4fdd\u8bc1\u6709\u8282\u70b9\u6c38\u4e45\u6027\u5d29\u6e83(fail-stop)\u7684\u60c5\u51b5\u4e0b\u7b97\u6cd5\u80fd\u5411\u524d\u63a8\u8fdb; 2\u30013PC\u6269\u5c55\u4e862PC, \u4f7f\u7528\u4e09\u4e2aroundtrip\u8fbe\u6210\u5171\u8bc6. \u5176\u5c40\u9650\u6027\u5728\u4e8e\u4e0d\u80fd\u4fdd\u8bc1\u5728\u8282\u70b9\u6682\u65f6\u6027\u5d29\u6e83(fail-recover), \u6216\u662f\u6709\u7f51\u7edc\u5212\u5206\u7684\u60c5\u51b5\u4e0b, \u5171\u8bc6\u4f9d\u65e7\u6210\u7acb.","title":"\u603b\u7ed3"},{"location":"Distributed-computing/Theory/Protocol/Guide/Expert-%E8%8C%83%E6%96%8C/%E6%BC%AB%E8%AF%9D%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%85%B1%E8%AF%86%E5%8D%8F%E8%AE%AE2PC-3PC%E7%AF%87/#_2","text":"Consensus Protocols: Two-Phase Commit - Paper Trail Consensus Protocols: Three-phase Commit - Paper Trail In distributed systems, what is a simple explanation of the Paxos algorithm?","title":"\u63a8\u8350\u9605\u8bfb"},{"location":"Distributed-computing/Theory/Protocol/Guide/Expert-%E8%8C%83%E6%96%8C/%E6%BC%AB%E8%AF%9D%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%85%B1%E8%AF%86%E5%8D%8F%E8%AE%AEPaxos%E7%AF%87/","text":"zhihu \u6f2b\u8bdd\u5206\u5e03\u5f0f\u7cfb\u7edf\u5171\u8bc6\u534f\u8bae: Paxos\u7bc7 \u53ef\u80fd\u548c\u5f88\u591a\u4eba\u7684\u5370\u8c61\u76f8\u53cd, Paxos\u5176\u5b9e\u662f\u4e00\u4e2a\u5f02\u5e38\u7b80\u6d01\u800c\u7cbe\u5de7\u7684\u7b97\u6cd5. \u89e3\u8bfb\u4e00\u904dPaxos\u7b97\u6cd5\u5176\u5b9e\u53ea\u9700\u89815\u5206\u949f. \u672c\u6587\u5c06\u96c6\u4e2d\u5728\u7ecf\u5178\u7684basic Paxos\u4e0a, \u800c\u4e0d\u4f1a\u6d89\u53ca\u5176\u5404\u79cd\u53d8\u79cd(\u5b9e\u5728\u4e5f\u592a\u591a\u4e86). \u524d\u8a00 \u672c\u6587\u662f\u201c\u6f2b\u8bdd\u5206\u5e03\u5f0f\u7cfb\u7edf\u5171\u8bc6\u534f\u8bae\u201d\u8fd9\u4e2a\u7cfb\u5217\u7684\u7b2c\u4e8c\u7bc7. \u524d\u4e00\u7bc7\" 2PC/3PC\u7bc7 \"\u4ecb\u7ecd\u4e86\u5206\u5e03\u5f0f\u5171\u8bc6\u7b97\u6cd5\u4e2d\u6700\u65e9\u76842PC\u548c3PC\u4e24\u4f4d\u8001\u524d\u8f88. \u518d\u6bd4\u56e0\u4e3aLamport\u7a33\u91cd\u5e26\u76ae\u7684\u64cd\u4f5c, \u5bfc\u81f4\u5927\u5bb6\u53e3\u53e3\u76f8\u4f20Paxos\u7406\u89e3\u548c\u5b9e\u73b0\u8d77\u6765\u6709\u591a\u56f0\u96be\u591a\u590d\u6742, \u5bfc\u81f4\u51fa\u73b0\u4e86Raft\u8fd9\u79cd\u6539\u826f\u7248\u7b49\u7b49. NOTE: paxos\u548craft\u4e4b\u95f4\u7684\u5173\u7cfb \u6572\u9ed1\u677f : Paxos\u5176\u5b9e\u662f\u4e00\u4e2a\u5f02\u5e38\u7b80\u6d01\u800c\u7cbe\u5de7\u7684\u7b97\u6cd5. \u89e3\u8bfb\u4e00\u904dPaxos\u7b97\u6cd5\u5176\u5b9e\u53ea\u9700\u89815\u5206\u949f. \u771f\u6b63\u590d\u6742\u7684\u5730\u65b9\u5728\u4e8e\u60f3\u6e05\u695aPaxos\u7b97\u6cd5\u5728\u5404\u79cdfailure\u60c5\u5f62\u4e0b\u5982\u4f55\u4f9d\u7136\"\u6b63\u786e\"\u7684\u5de5\u4f5c. \u53ea\u6709\u660e\u767d\u4e86\u8fd9\u4e00\u5c42, \u624d\u7b97\u7ec3\u6210\u4e86Paxos\u7684\u5fc3\u6cd5, \u624d\u80fd\u771f\u6b63\u6b23\u8d4fPaxos\u7b97\u6cd5\u7684\u7cbe\u5999\u8bbe\u8ba1, \u8d5e\u53f9Lamport\u7684\u5929\u624d\u601d\u7ef4. \u5728\u6211\u770b\u6765, Paxos\u7b97\u6cd5(\u8fde\u540cLamport\u7684\u5176\u4ed6\u5982BFT, Vector Clock\u7b49\u6210\u5c31)\u662f\u4e0a\u4e2a\u4e16\u7eaa\u516b\u5341/\u4e5d\u5341\u5e74\u4ee3\u7684\u7ecf\u5178\u5206\u5e03\u5f0f\u7cfb\u7edf\u7814\u7a76\u4e2d\u6700\u7eaf\u7cb9\u6700\u4f18\u7f8e, \u4e5f\u662f\u6574\u680b\u5927\u53a6\u5e95\u5ea7\u6700\u575a\u5b9e\u7684\u90a3\u4e00\u90e8\u5206. NOTE: \u4e00\u3001Leslie-Lamport \u53c2\u89c1 Expert-Leslie-Lamport \u7ae0\u8282 \u4e8c\u3001BFT\u5373Byzantine-Fault-Tolerance \u63d2\u70b9\u9898\u5916\u8bdd: \u6211\u7b2c\u4e00\u6b21\u8ba4\u771f\u63a5\u89e6\u548c\u5b66\u4e60Paxos\u662f\u5728CMU\u65f6TA\u5206\u5e03\u5f0f\u7cfb\u7edf( 15-440, Fall 2012: Distributed Systems ). Paxos\u7b97\u6cd5\u63cf\u8ff0 NOTE: \u4e00\u3001paper Paxos Made Simple We let the three roles in the consensus algorithm be performed by three classes of agents: proposers, acceptors, and learners. \u4e8c\u3001\u770b\u4e86\u4e00\u4e0b\uff0c\u4e0b\u9762\u7684\u5185\u5bb9\u662f\u4f7f\u7528 paper Paxos Made Simple \u4e2d\u7684\u672f\u8bed\u63cf\u8ff0\u7684 \u8003\u8651\u4e00\u4e2a\u7b80\u5316\u4e86\u7684Paxos\u7cfb\u7edf: \u53ea\u6709leader\u548cacceptor\u4e24\u79cd\u89d2\u8272. 1\u3001Prepare\u9636\u6bb5 NOTE: \u4e00\u3001paper Paxos Made Simple The algorithm chooses a leader, which plays the roles of the distinguished proposer and the distinguished learner. \u4e8c\u3001Prepare\u9636\u6bb5\u5c31\u662f\u4eceproposer\u4e2d\u9009\u62e9\u4e00\u4e2a\u4f5c\u4e3aleader\uff0c\u5373leader election (1a) leader\u7684\u8282\u70b9\u7ed9\u6240\u6709\u5176\u4ed6acceptor\u8282\u70b9\u53d1\u9001\u6d88\u606f\"proposal(n)\"---n\u662f\u8be5\u8282\u70b9\u4e3a\u8fd9\u4e2a\u63d0\u8bae\u9009\u62e9\u7684\u4e00\u4e2a\u6570\u5b57, \u59d1\u4e14\u7406\u89e3\u4e3a\u4e00\u4e2a\u65b9\u6848\u7f16\u53f7. \u5e76\u671f\u5f85\u8be5\u63d0\u8bae\u83b7\u5f97\u6240\u6709\u8282\u70b9\u4e2d\u7684\u7b80\u5355\u591a\u6570(Paxos\u7684Quorum)\u7684\u8bb8\u53ef. (1b) \u6bcf\u4e00\u4e2a\u63a5\u53d7\u5230proposal\u7684acceptor\u8282\u70b9: \u4e00\u3001\u5982\u679c\u8fd9\u662f\u5b83\u63a5\u53d7\u5230\u7684\u7b2c\u4e00\u4e2aproposal, \u56de\u7b54\"promise\". \u4ee3\u8868\u8be5\u8282\u70b9**\u8bb8\u8bfa**\u5c06\u4f1a\u4fdd\u6301\u627f\u8ba4\u8be5proposal\u53d1\u9001\u65b9\u4e3aleader, \u9664\u975e\u6536\u5230\u5176\u4ed6\u4f18\u5148\u7ea7\u66f4\u9ad8\u7684proposal; NOTE: \u56de\u7b54promise \u4e8c\u3001\u5982\u679c\u5df2\u7ecf\u6709\u63a5\u6536\u5230\u5e76accepted(\u6ce8: \u8fd9\u662f\u4e0b\u4e00\u9636\u6bb5\u53ef\u80fd\u4f1a\u53d1\u751f\u7684\u52a8\u4f5c)\u5176\u4ed6\u7684proposal(n',v')--n'\u662f\u8be5proposal\u7684\u65b9\u6848\u53f7\u800cv'\u662f\u63d0\u8bae\u7684\u5171\u8bc6: 1\u3001\u5982\u679c n < n', \u4e4b\u524daccept\u7684\u63d0\u8bae\u6709\u66f4\u9ad8\u4f18\u5148\u7ea7, \u5bf9\u65b0\u63a5\u53d7\u7684\u63d0\u8bae\u56de\u7b54\"reject\", \u4ee5\u5151\u73b0\u4e4b\u524d\u7684\u8bb8\u8bfa. NOTE: \u56de\u7b54reject 2\u3001\u5982\u679c n > n', \u56de\u7b54\"promise\"\u7684\u5e76\u540c\u65f6\u9644\u4e0a\u65e7\u7684\u63d0\u8bae,proposal(n', v'). \u8fd9\u6837\u5728\u8ba4\u53ef\u65b0\u7684leader\u8eab\u4efd\u7684\u540c\u65f6, \u4e5f\u544a\u8bc9\u4e86\u65b0\u7684leader\u8fc7\u53bb\u7684\u88ab\u7b80\u5355\u591a\u6570\u8ba4\u53ef\u8fc7\u7684\u63d0\u8bae NOTE: \u56de\u7b54promise NOTE: \u9700\u8981\u6ce8\u610f\uff0c\u4e0a\u9762\u4ec5\u4ec5\u5217\u4e3e\u4e86\u4e24\u79cd\u60c5\u51b5\uff0c\u5176\u5b9e\u4e0d\u6b62\u4e24\u79cd\u60c5\u51b5\uff0c\u8fd8\u6709\u7b2c\u4e09\u79cd\u60c5\u51b5\uff0c\u5728 csdn \u8bf8\u845b\u4eae VS \u5e9e\u7edf\uff0c\u62ff\u4e0b Paxos \u5171\u8bc6\u7b97\u6cd5 \u4e2d\uff0c\u5c31\u5217\u4e3e\u4e86\u8fd9\u79cd\u60c5\u51b5: \u4e09\u3001\u5982\u679c\u5df2\u7ecf\u6709\u63a5\u6536\u5230\u5e76promise\u5176\u4ed6\u7684proposal(n',v') 1\u3001\u5982\u679c n < n'\uff0c\u56de\u7b54reject 2\u3001\u5982\u679c n > n'\uff0c\u56de\u7b54promise (1c) \u4e00\u3001\u5982\u679cproposer\u7684\u63d0\u8bae\u53d7\u5230\u4e86\u7b80\u5355\u591a\u6570\u7684\"reject\", \u7ade\u4e89leader\u5ba3\u544a\u5931\u8d25, \u53ef\u4ee5\u653e\u5f03\u8fd9\u4e00\u63d0\u8bae; \u4e8c\u3001\u5982\u679c\u63a5\u53d7\u5230\u4e86\u7b80\u5355\u591a\u6570\u7684\"promise\", \u5219\u8be5proposer\u6210\u4e3aleader, \u5b83\u9700\u8981\u4ece\u6536\u5230\u7684promise\u91cc\u9644\u5e26\u7684\u4e4b\u524daccepted\u7684\u63d0\u8bae\u4e2d\u9009\u53d6\u65b9\u6848\u53f7(n\u503c)\u6700\u9ad8\u7684\u5bf9\u5e94\u7684\u5171\u8bc6; \u5982\u679c\u5386\u53f2\u4e0a\u6ca1\u6709\u88abaccept\u8fc7\u7684\u63d0\u8bae, leader\u53ef\u4ee5\u81ea\u5df1\u9009\u53d6\u4e00\u4e2a\u5171\u8bc6v. NOTE: \u9700\u8981\u6ce8\u610f\uff0c\u5b83\u9700\u8981\u9996\u5148\u5904\u7406\"\u4e4b\u524daccepted\u7684\u63d0\u8bae\u4e2d\u9009\u53d6\u65b9\u6848\u53f7(n\u503c)\u6700\u9ad8\u7684\u5bf9\u5e94\u7684\u5171\u8bc6\" 2\u3001Accept\u9636\u6bb5 (2a) leader\u4f1a\u5bf9\u6240\u6709acceptor\u53d1\u9001\"accept-request(n,v)\", \u8bf7\u6c42\u6240\u6709acceptor\u63a5\u53d7\u7f16\u53f7\u4e3an\u7684\u5171\u8bc6v\u7684\u63d0\u8bae (2b) \u6bcf\u4e00\u4e2a\u63a5\u6536\u5230\u8be5\u63d0\u8bae\u7684acceptor\u8282\u70b9: \u5982\u679c\u6ca1\u6709\u63a5\u53d7\u8fc7\u7f16\u53f7\u6bd4n\u66f4\u9ad8\u7684\u63d0\u8bae, \u5219\u8fd4\u56de\"accept\"\u8868\u793a\u63a5\u53d7\u8fd9\u4e00\u5171\u8bc6\u63d0\u8bae; \u5426\u5219\u8fd4\u56de\"reject\" NOTE: \u56de\u7b54accept \u56de\u7b54reject (2c) \u5982\u679c\u7b80\u5355\u591a\u6570\u7684acceptor\u8fd4\u56de\u4e86\"accept\", \u5219\u5171\u8bc6\u8fbe\u6210; \u5426\u5219\u5171\u8bc6\u5931\u8d25, \u91cd\u542fPaxos\u534f\u8bae. \u8bf7\u6ce8\u610f\u51e0\u70b9\u4ee5\u5e2e\u52a9\u7406\u89e3\u534f\u8bae: 1\u3001\u7b2c\u4e00\u9636\u6bb5\u7ade\u4e89\u7684\u5e76\u4e0d\u662f\u5171\u8bc6\u672c\u8eab, \u800c\u662f\u5728\u4e89\u53d6\u5750\u5b9eleader\u8eab\u4efd\u83b7\u5f97\u7b80\u5355\u591a\u6570\u7684\u8ba4\u53ef 2\u3001\u65b9\u6848\u7f16\u53f7n\u672c\u8eab\u5e76\u4e0d\u662f\u5171\u8bc6, \u800c\u662f\u63d0\u8bae\u7684\u4e00\u4e2a\u4f18\u5148\u7ea7, \u5728\u591a\u4e2a\u8282\u70b9\u7ade\u4e89leader\u8eab\u4efd\u65f6\u53ef\u4ee5\u533a\u5206\u4f18\u5148\u987a\u5e8f. \u5171\u8bc6\u672c\u8eab(v)\u4f1a\u5728\u4e0b\u4e00\u9636\u6bb5leader\u8eab\u4efd\u786e\u8ba4\u540e\u7531leader\u6dfb\u52a0\u8fdb\u63d0\u8bae; NOTE: \u663e\u7136\"\u65b9\u6848\u7f16\u53f7n\"\u662f\u4e3a\u4e86\u5904\u7406\u591a\u4e2a\u8282\u70b9\u540c\u65f6\u7ade\u4e89leader\u8eab\u4efd\u800c\u6dfb\u52a0\u7684 3\u3001\u867d\u7136\u8fd9\u4e00\u8f6e\u4e0a\u53ea\u4f1a\u6709\u4e00\u4e2aleader\u83b7\u5f97\u7b80\u5355\u591a\u6570\u7684\u8ba4\u53ef\u4ea7\u751f, \u4f46\u53ef\u80fd\u6709\u591a\u4e2a\"\u7cca\u6d82\"\u8282\u70b9\u8ba4\u4e3a\u81ea\u5df1\u5e94\u8be5\u505aleader, \u89c1\u540e\u9762\u7684\u5206\u6790; Paxos\u5bf92PC\u548c3PC\u7684\u6539\u8fdb \u5728\u6211\u770b\u6765, Paxos\u5bf92PC\u548c3PC\u6709\u51e0\u70b9\u91cd\u8981\u7684\u6539\u8fdb. \u7b2c\u4e00\uff0c\u5206\u79bb\u5171\u8bc6\u7684\u63d0\u8bae\u8005proposer\u4ee5\u53ca\u5e2e\u52a9\u63d0\u8bae\u6700\u7ec8\u901a\u8fc7\u7684leader\u8fd9\u4e24\u4e2a\u89d2\u8272. Paxos\u91cc, \u5373\u4f7f\u4e00\u4e2aleader\u8eab\u4efd\u88ab\u6279\u51c6, \u5b83\u4e5f\u9700\u8981\u5c0a\u91cd\u5386\u53f2\u4e0a\u5176\u4ed6\u88ab\u540c\u610f\u8fc7\u7684\u63d0\u8bae. \u6362\u8a00\u4e4bleader\u672c\u8eab\u53ea\u662f\u4e00\u4e2a\u670d\u52a1\u6027\u7684\u89d2\u8272, \u672a\u5fc5\u6709\u673a\u4f1a\u81ea\u5df1\u63d0\u51fa\u5171\u8bc6. NOTE: \u8fd9\u662f\u4e00\u4e2a\u975e\u5e38\u6df1\u523b\u7684\u8ba4\u8bc6 \u56de\u5fc6\u4e00\u4e0b\u4e0a\u4e00\u7bc7\u4ecb\u7ecd\u76842PC\u548c3PC\u8fd9\u4e24\u4e2a\u534f\u8bae\u5f53\u4e2d, coordinator\u4e0d\u4ec5\u8d1f\u8d23\u63d0\u51fa\u6700\u540e\u7684\u5171\u8bc6\u534f\u8bae, \u540c\u65f6\u4e5f\u8d1f\u8d23\u670d\u52a1\u6240\u6709\u8282\u70b9\u4fdd\u8bc1\u5b83\u7684\u5171\u8bc6\u88ab\u901a\u8fc7. \u800c\u6b63\u662f\u56e0\u4e3aPaxos\u4e2d\u628acoordinator\u7684\u804c\u8d23\u89e3\u8026\u5408\u6210\u4e86proposer\u548cleader, \u4f7f\u5f97\u6574\u4e2a\u7b97\u6cd5\u66f4\u52a0robust.\u5c31\u7b97\u524d\u4efbleader\u5b95\u673a\u4e86, \u540e\u9762\u65b0\u4ea7\u751fleader\u4e5f\u53ef\u4ee5\u7ee7\u627f\u524d\u4efb\u7684\"\u9057\u5fd7\"\u6765\u5b8c\u6210\u4e00\u4e2aPaxos\u534f\u8bae. \u7b2c\u4e8c\uff0c\u5bf9\u7b80\u5355\u591a\u6570\u7684\u5de7\u5999\u5e94\u7528. \u7b2c\u4e00\u9636\u6bb5\u91cc\u9009\u4e3eleader\u8981\u6c42\u7684\u7b80\u5355\u591a\u6570\u4fdd\u8bc1\u4e86\u9009\u4e3e\u51fa\u6765\u7684leader\u4e00\u5b9a\u4e0d\u4f1a\u9519\u8fc7\u4e4b\u524d\u88abaccept\u8fc7\u7684\u63d0\u8bae---\u6240\u4ee5\u5c31\u7b97\u90a3\u4e2a\u63d0\u8bae\u6700\u521d\u7684proposer\u6302\u4e86, \u4e5f\u4f1a\u81f3\u5c11\u88ab\u4e00\u4e2aacceptor\u53d1\u7ed9\u65b0\u7684leader\u6765\u7ee7\u627f. \u800c\u7b2c\u4e8c\u9636\u6bb5\u91cc\u8981\u6c42\u7684\u8fbe\u6210\u5171\u8bc6\u7684\u7b80\u5355\u591a\u6570\u4fdd\u8bc1\u4e86\u6709\u591a\u4e2a\"\u81ea\u4ee5\u4e3a\u662f\"\u7684leader\u51fa\u73b0\u65f6(\u6bd4\u5982\u4e00\u4e2aleader\u6389\u7ebf, \u65b0leader\u9009\u51fa, \u65e7leader\u91cd\u65b0\u4e0a\u7ebf), \u4e00\u5b9a\u53ea\u4f1a\u6709\u4e00\u4e2a\u6700\u540e\u901a\u8fc7. \u770b\u8fc7\u4e00\u4e2a\u7cbe\u5f69\u7684\u8bc4\u8bba, \u8bf4Paxos\u5176\u5b9e\u5c31\u662f\u8fde\u7eed\u8fd0\u7528\u4e24\u6b21\"\u62bd\u5c49\u539f\u7406\", \u5176\u5b9e\u975e\u5e38\u51c6\u786e. NOTE: \u4e00\u3001\u5173\u4e8e\"Paxos\u5176\u5b9e\u5c31\u662f\u8fde\u7eed\u8fd0\u7528\u4e24\u6b21\"\u62bd\u5c49\u539f\u7406\"\"\uff0c\u5728\u4e0b\u9762\u7684\u6587\u7ae0\u4e2d\u6709\u975e\u5e38\u597d\u7684\u4ecb\u7ecd: 1\u3001cnblogs \u5206\u5e03\u5f0f\u4e00\u81f4\u6027\u7b97\u6cd5\u2014\u2014Paxos\u539f\u7406\u5206\u6790 \u6211\u4eec\u79f0\u4e0a\u9762\u7684\u7b97\u6cd5\u4e3a\u201c\u591a\u6570\u540c\u610f\u7b97\u6cd5\u201d\u3002\u4e3a\u4ec0\u4e48\u8fd9\u4e2a\u7b97\u6cd5\u80fd\u591f\u786e\u4fdd\u8fbe\u6210\u5171\u8bc6\u2014\u2014\u6362\u4e2a\u8bf4\u6cd5\uff0c\u80fd\u591f\u4fdd\u8bc1\u6700\u591a\u53ea\u6709\u4e00\u4e2a\u8282\u70b9\u6709\u6743\u8fdb\u884c\u5e7f\u64ad\u5462\uff1f\u7528\u53cd\u8bc1\u6cd5\u5f88\u5bb9\u6613\u8bc1\u660e\u8fd9\u4e00\u70b9\u3002\u5047\u8bbe\u6709\u4e24\u4e2a\u63d0\u6848\u8005\u90fd\u6536\u5230\u4e86\u4e00\u534a\u4ee5\u4e0a\u7684\u63a5\u53d7\u56de\u590d\uff0c\u90a3\u4e48\u6839\u636e**\u62bd\u5c49\u539f\u7406**\uff0c\u56de\u590d\u8fd9\u4e24\u4e2a\u8282\u70b9\u7684\u63a5\u53d7\u8005\u91cc\u9762\uff0c\u5fc5\u5b9a\u4f1a\u81f3\u5c11\u6709\u4e00\u4e2a\u8282\u70b9\u540c\u65f6\u56de\u590d\u4e86\u8fd9\u4e24\u4e2a\u63d0\u6848\u8005\u3002\u8fd9\u5c31\u4ea7\u751f\u4e86\u660e\u663e\u7684\u77db\u76fe\u2014\u2014\u8bf4\u597d\u7684\u53ea\u56de\u590d\u4e00\u4e2a\u63d0\u6848\u8005\u5462\uff1f\u5230\u6b64\uff0c\u6211\u4eec\u5df2\u7ecf\u4fdd\u8bc1\u4e86\u8fd9\u4e2a\u7b97\u6cd5\u7684\u6b63\u786e\u6027\uff0c\u4efb\u52a1\u4f3c\u4e4e\u5df2\u7ecf\u5b8c\u6210\u4e86\u3002 Paxos\u4e0e2PC/3PC\u7684\u5173\u7cfb Paxos\u5982\u4f55\u514b\u670d2PC\u7684\u95ee\u9898 2PC\u7684\u95ee\u9898\u5728\u4e8e\u4e0d\u80fd\u5904\u7406\u6700\u7b80\u5355\u7684fail-stop\u9519\u8bef\u6a21\u5f0f. 1\u30012PC\u4e2dcoordinator\u662f\u552f\u4e00\u800c\u56fa\u5b9a\u7684, \u5982\u679ccoordinator\u5b95\u673a, \u90a3\u4e48\u5c31\u4f1a\u6709\u60c5\u5f62\u5bfc\u81f4coordinator\u4e4b\u524dpropose\u7684\u63d0\u8bae\u7684\u6295\u7968\u7ed3\u679c\u4e22\u5931. \u5c31\u7b97\u542f\u52a8\u65b0\u7684\u540e\u5907coordinator, \u6ca1\u6709\u673a\u5236\u53ef\u4ee5\u5b66\u4e60\u4ee5\u524d\u7684\u6295\u7968\u7ed3\u679c. 2\u3001Paxos\u56e0\u4e3a\u5206\u79bb\u4e86\u63d0\u8bae\u548cleader, \u4ece\u7b97\u6cd5\u4e0a\u4fdd\u8bc1\u603b\u53ef\u4ee5\u9009\u4e3e\u51fa\u540e\u5907leader\u5e76\u63a5\u66ff\u524d\u4efbleader\u7684\u5de5\u4f5c. Paxos\u5982\u4f55\u514b\u670d3PC\u7684\u95ee\u9898 3PC\u6539\u8fdb\u4e862PC\u7684fail-stop\u7684\u95ee\u9898, \u4f46\u662f\u4e0d\u80fd\u5904\u7406fail-recover\u7c7b\u578b\u7684\u9519\u8bef. 1\u30013PC\u53d1\u751f\u7684\u95ee\u9898\u5728\u4e8e\u5f53\u6709\u591a\u4e2a\"\u81ea\u8ba4\u7684leader\"\u51fa\u73b0\u65f6, \u5e76\u4e0d\u80fd\u6709\u6548\u7684\u89e3\u51b3coordinator\u4e4b\u95f4\u7684\u7ade\u4e89---\u8c01\u662f\u771f\u6b63\u7684coordinator. 2\u3001\u800cPaxos\u901a\u8fc7Quorum\u7684\u8fd0\u7528, \u4fdd\u8bc1\u4e86\u591a\u4e86\u4e2aleader\u4e4b\u95f4\u53ef\u4ee5\u4e92\u76f8\u53d1\u73b0. Paxos\u7684\u5c40\u9650\u6027 \u5c31\u50cf2PC\u4ee5\u53ca3PC\u4e00\u6837, Paxos\u4e5f\u6709\u5176\u5c40\u9650\u6027. 1 \u6d3b\u9501\u95ee\u9898. Paxos\u7406\u8bba\u4e0a\u5b58\u5728\u4e00\u4e2a\u4e0d\u80fd\u7ec8\u7ed3\u534f\u8bae\u7684\u6d3b\u9501\u7ade\u4e89\u95ee\u9898. \u6bd4\u5982\u4e00\u4e2aproposer\u63d0\u4ea4\u7684\u63d0\u8bae\u56e0\u4e3a\u7f16\u53f7\u8fc7\u4f4e\u88ab\u62d2\u7edd\u65f6, \u6b64proposer\u53ef\u80fd\u91cd\u542fPaxos\u800c\u63d0\u9ad8\u7f16\u53f7\u91cd\u65b0\u63d0\u4ea4. \u5982\u679c\u540c\u65f6\u6709\u4e24\u4e2aproposer\u90fd\u53d1\u73b0\u81ea\u5df1\u7684\u65b9\u6848\u7f16\u53f7\u8fc7\u4f4e, \u4ece\u800c\u8f6e\u6d41\u63d0\u51fa\u66f4\u9ad8\u7f16\u53f7\u7684proposal\u800c\u5bfc\u81f4\u5bf9\u65b9\u88ab\u62d2, \u53ef\u80fd\u4f1a\u5bfc\u81f4\u6b7b\u5faa\u73af(\u6216\u6d3b\u9501). NOTE: \u4e0d\u7406\u89e3 2 \u6076\u610f\u8282\u70b9. \u76ee\u524d\u4e3a\u6b622PC, 3PC, Paxos\u5747\u662f\u5047\u8bbe\u6240\u6709\u8282\u70b9\u90fd\u9075\u5b88\u534f\u8bae\u7684\u89c4\u5b9a. \u5f53\u5b58\u5728\u6076\u610f\u7684, \u53ef\u80fd\u53d1\u9001\u4efb\u4f55\u5bfc\u81f4\u534f\u8bae\u505c\u6b62\u6216\u8005\u51fa\u9519\u7684\u6d88\u606f\u7684\u8282\u70b9\u5b58\u5728\u65f6, \u5c31\u9700\u8981\u6709\u66f4\u5f3a\u7684\u5171\u8bc6\u7b97\u6cd5\u5728\"\u5b88\u6cd5\u8282\u70b9\"\u95f4\u8fbe\u6210\u5171\u8bc6. Lamport \u7684BFT(\u62dc\u5360\u5ead\u5c06\u519b\u95ee\u9898)\u4e86\u89e3\u4e00\u4e0b.","title":"Introduction"},{"location":"Distributed-computing/Theory/Protocol/Guide/Expert-%E8%8C%83%E6%96%8C/%E6%BC%AB%E8%AF%9D%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%85%B1%E8%AF%86%E5%8D%8F%E8%AE%AEPaxos%E7%AF%87/#zhihu#paxos","text":"\u53ef\u80fd\u548c\u5f88\u591a\u4eba\u7684\u5370\u8c61\u76f8\u53cd, Paxos\u5176\u5b9e\u662f\u4e00\u4e2a\u5f02\u5e38\u7b80\u6d01\u800c\u7cbe\u5de7\u7684\u7b97\u6cd5. \u89e3\u8bfb\u4e00\u904dPaxos\u7b97\u6cd5\u5176\u5b9e\u53ea\u9700\u89815\u5206\u949f. \u672c\u6587\u5c06\u96c6\u4e2d\u5728\u7ecf\u5178\u7684basic Paxos\u4e0a, \u800c\u4e0d\u4f1a\u6d89\u53ca\u5176\u5404\u79cd\u53d8\u79cd(\u5b9e\u5728\u4e5f\u592a\u591a\u4e86).","title":"zhihu \u6f2b\u8bdd\u5206\u5e03\u5f0f\u7cfb\u7edf\u5171\u8bc6\u534f\u8bae: Paxos\u7bc7"},{"location":"Distributed-computing/Theory/Protocol/Guide/Expert-%E8%8C%83%E6%96%8C/%E6%BC%AB%E8%AF%9D%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%85%B1%E8%AF%86%E5%8D%8F%E8%AE%AEPaxos%E7%AF%87/#_1","text":"\u672c\u6587\u662f\u201c\u6f2b\u8bdd\u5206\u5e03\u5f0f\u7cfb\u7edf\u5171\u8bc6\u534f\u8bae\u201d\u8fd9\u4e2a\u7cfb\u5217\u7684\u7b2c\u4e8c\u7bc7. \u524d\u4e00\u7bc7\" 2PC/3PC\u7bc7 \"\u4ecb\u7ecd\u4e86\u5206\u5e03\u5f0f\u5171\u8bc6\u7b97\u6cd5\u4e2d\u6700\u65e9\u76842PC\u548c3PC\u4e24\u4f4d\u8001\u524d\u8f88. \u518d\u6bd4\u56e0\u4e3aLamport\u7a33\u91cd\u5e26\u76ae\u7684\u64cd\u4f5c, \u5bfc\u81f4\u5927\u5bb6\u53e3\u53e3\u76f8\u4f20Paxos\u7406\u89e3\u548c\u5b9e\u73b0\u8d77\u6765\u6709\u591a\u56f0\u96be\u591a\u590d\u6742, \u5bfc\u81f4\u51fa\u73b0\u4e86Raft\u8fd9\u79cd\u6539\u826f\u7248\u7b49\u7b49. NOTE: paxos\u548craft\u4e4b\u95f4\u7684\u5173\u7cfb \u6572\u9ed1\u677f : Paxos\u5176\u5b9e\u662f\u4e00\u4e2a\u5f02\u5e38\u7b80\u6d01\u800c\u7cbe\u5de7\u7684\u7b97\u6cd5. \u89e3\u8bfb\u4e00\u904dPaxos\u7b97\u6cd5\u5176\u5b9e\u53ea\u9700\u89815\u5206\u949f. \u771f\u6b63\u590d\u6742\u7684\u5730\u65b9\u5728\u4e8e\u60f3\u6e05\u695aPaxos\u7b97\u6cd5\u5728\u5404\u79cdfailure\u60c5\u5f62\u4e0b\u5982\u4f55\u4f9d\u7136\"\u6b63\u786e\"\u7684\u5de5\u4f5c. \u53ea\u6709\u660e\u767d\u4e86\u8fd9\u4e00\u5c42, \u624d\u7b97\u7ec3\u6210\u4e86Paxos\u7684\u5fc3\u6cd5, \u624d\u80fd\u771f\u6b63\u6b23\u8d4fPaxos\u7b97\u6cd5\u7684\u7cbe\u5999\u8bbe\u8ba1, \u8d5e\u53f9Lamport\u7684\u5929\u624d\u601d\u7ef4. \u5728\u6211\u770b\u6765, Paxos\u7b97\u6cd5(\u8fde\u540cLamport\u7684\u5176\u4ed6\u5982BFT, Vector Clock\u7b49\u6210\u5c31)\u662f\u4e0a\u4e2a\u4e16\u7eaa\u516b\u5341/\u4e5d\u5341\u5e74\u4ee3\u7684\u7ecf\u5178\u5206\u5e03\u5f0f\u7cfb\u7edf\u7814\u7a76\u4e2d\u6700\u7eaf\u7cb9\u6700\u4f18\u7f8e, \u4e5f\u662f\u6574\u680b\u5927\u53a6\u5e95\u5ea7\u6700\u575a\u5b9e\u7684\u90a3\u4e00\u90e8\u5206. NOTE: \u4e00\u3001Leslie-Lamport \u53c2\u89c1 Expert-Leslie-Lamport \u7ae0\u8282 \u4e8c\u3001BFT\u5373Byzantine-Fault-Tolerance \u63d2\u70b9\u9898\u5916\u8bdd: \u6211\u7b2c\u4e00\u6b21\u8ba4\u771f\u63a5\u89e6\u548c\u5b66\u4e60Paxos\u662f\u5728CMU\u65f6TA\u5206\u5e03\u5f0f\u7cfb\u7edf( 15-440, Fall 2012: Distributed Systems ).","title":"\u524d\u8a00"},{"location":"Distributed-computing/Theory/Protocol/Guide/Expert-%E8%8C%83%E6%96%8C/%E6%BC%AB%E8%AF%9D%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%85%B1%E8%AF%86%E5%8D%8F%E8%AE%AEPaxos%E7%AF%87/#paxos","text":"NOTE: \u4e00\u3001paper Paxos Made Simple We let the three roles in the consensus algorithm be performed by three classes of agents: proposers, acceptors, and learners. \u4e8c\u3001\u770b\u4e86\u4e00\u4e0b\uff0c\u4e0b\u9762\u7684\u5185\u5bb9\u662f\u4f7f\u7528 paper Paxos Made Simple \u4e2d\u7684\u672f\u8bed\u63cf\u8ff0\u7684 \u8003\u8651\u4e00\u4e2a\u7b80\u5316\u4e86\u7684Paxos\u7cfb\u7edf: \u53ea\u6709leader\u548cacceptor\u4e24\u79cd\u89d2\u8272.","title":"Paxos\u7b97\u6cd5\u63cf\u8ff0"},{"location":"Distributed-computing/Theory/Protocol/Guide/Expert-%E8%8C%83%E6%96%8C/%E6%BC%AB%E8%AF%9D%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%85%B1%E8%AF%86%E5%8D%8F%E8%AE%AEPaxos%E7%AF%87/#1prepare","text":"NOTE: \u4e00\u3001paper Paxos Made Simple The algorithm chooses a leader, which plays the roles of the distinguished proposer and the distinguished learner. \u4e8c\u3001Prepare\u9636\u6bb5\u5c31\u662f\u4eceproposer\u4e2d\u9009\u62e9\u4e00\u4e2a\u4f5c\u4e3aleader\uff0c\u5373leader election","title":"1\u3001Prepare\u9636\u6bb5"},{"location":"Distributed-computing/Theory/Protocol/Guide/Expert-%E8%8C%83%E6%96%8C/%E6%BC%AB%E8%AF%9D%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%85%B1%E8%AF%86%E5%8D%8F%E8%AE%AEPaxos%E7%AF%87/#1a","text":"leader\u7684\u8282\u70b9\u7ed9\u6240\u6709\u5176\u4ed6acceptor\u8282\u70b9\u53d1\u9001\u6d88\u606f\"proposal(n)\"---n\u662f\u8be5\u8282\u70b9\u4e3a\u8fd9\u4e2a\u63d0\u8bae\u9009\u62e9\u7684\u4e00\u4e2a\u6570\u5b57, \u59d1\u4e14\u7406\u89e3\u4e3a\u4e00\u4e2a\u65b9\u6848\u7f16\u53f7. \u5e76\u671f\u5f85\u8be5\u63d0\u8bae\u83b7\u5f97\u6240\u6709\u8282\u70b9\u4e2d\u7684\u7b80\u5355\u591a\u6570(Paxos\u7684Quorum)\u7684\u8bb8\u53ef.","title":"(1a)"},{"location":"Distributed-computing/Theory/Protocol/Guide/Expert-%E8%8C%83%E6%96%8C/%E6%BC%AB%E8%AF%9D%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%85%B1%E8%AF%86%E5%8D%8F%E8%AE%AEPaxos%E7%AF%87/#1b","text":"\u6bcf\u4e00\u4e2a\u63a5\u53d7\u5230proposal\u7684acceptor\u8282\u70b9: \u4e00\u3001\u5982\u679c\u8fd9\u662f\u5b83\u63a5\u53d7\u5230\u7684\u7b2c\u4e00\u4e2aproposal, \u56de\u7b54\"promise\". \u4ee3\u8868\u8be5\u8282\u70b9**\u8bb8\u8bfa**\u5c06\u4f1a\u4fdd\u6301\u627f\u8ba4\u8be5proposal\u53d1\u9001\u65b9\u4e3aleader, \u9664\u975e\u6536\u5230\u5176\u4ed6\u4f18\u5148\u7ea7\u66f4\u9ad8\u7684proposal; NOTE: \u56de\u7b54promise \u4e8c\u3001\u5982\u679c\u5df2\u7ecf\u6709\u63a5\u6536\u5230\u5e76accepted(\u6ce8: \u8fd9\u662f\u4e0b\u4e00\u9636\u6bb5\u53ef\u80fd\u4f1a\u53d1\u751f\u7684\u52a8\u4f5c)\u5176\u4ed6\u7684proposal(n',v')--n'\u662f\u8be5proposal\u7684\u65b9\u6848\u53f7\u800cv'\u662f\u63d0\u8bae\u7684\u5171\u8bc6: 1\u3001\u5982\u679c n < n', \u4e4b\u524daccept\u7684\u63d0\u8bae\u6709\u66f4\u9ad8\u4f18\u5148\u7ea7, \u5bf9\u65b0\u63a5\u53d7\u7684\u63d0\u8bae\u56de\u7b54\"reject\", \u4ee5\u5151\u73b0\u4e4b\u524d\u7684\u8bb8\u8bfa. NOTE: \u56de\u7b54reject 2\u3001\u5982\u679c n > n', \u56de\u7b54\"promise\"\u7684\u5e76\u540c\u65f6\u9644\u4e0a\u65e7\u7684\u63d0\u8bae,proposal(n', v'). \u8fd9\u6837\u5728\u8ba4\u53ef\u65b0\u7684leader\u8eab\u4efd\u7684\u540c\u65f6, \u4e5f\u544a\u8bc9\u4e86\u65b0\u7684leader\u8fc7\u53bb\u7684\u88ab\u7b80\u5355\u591a\u6570\u8ba4\u53ef\u8fc7\u7684\u63d0\u8bae NOTE: \u56de\u7b54promise NOTE: \u9700\u8981\u6ce8\u610f\uff0c\u4e0a\u9762\u4ec5\u4ec5\u5217\u4e3e\u4e86\u4e24\u79cd\u60c5\u51b5\uff0c\u5176\u5b9e\u4e0d\u6b62\u4e24\u79cd\u60c5\u51b5\uff0c\u8fd8\u6709\u7b2c\u4e09\u79cd\u60c5\u51b5\uff0c\u5728 csdn \u8bf8\u845b\u4eae VS \u5e9e\u7edf\uff0c\u62ff\u4e0b Paxos \u5171\u8bc6\u7b97\u6cd5 \u4e2d\uff0c\u5c31\u5217\u4e3e\u4e86\u8fd9\u79cd\u60c5\u51b5: \u4e09\u3001\u5982\u679c\u5df2\u7ecf\u6709\u63a5\u6536\u5230\u5e76promise\u5176\u4ed6\u7684proposal(n',v') 1\u3001\u5982\u679c n < n'\uff0c\u56de\u7b54reject 2\u3001\u5982\u679c n > n'\uff0c\u56de\u7b54promise","title":"(1b)"},{"location":"Distributed-computing/Theory/Protocol/Guide/Expert-%E8%8C%83%E6%96%8C/%E6%BC%AB%E8%AF%9D%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%85%B1%E8%AF%86%E5%8D%8F%E8%AE%AEPaxos%E7%AF%87/#1c","text":"\u4e00\u3001\u5982\u679cproposer\u7684\u63d0\u8bae\u53d7\u5230\u4e86\u7b80\u5355\u591a\u6570\u7684\"reject\", \u7ade\u4e89leader\u5ba3\u544a\u5931\u8d25, \u53ef\u4ee5\u653e\u5f03\u8fd9\u4e00\u63d0\u8bae; \u4e8c\u3001\u5982\u679c\u63a5\u53d7\u5230\u4e86\u7b80\u5355\u591a\u6570\u7684\"promise\", \u5219\u8be5proposer\u6210\u4e3aleader, \u5b83\u9700\u8981\u4ece\u6536\u5230\u7684promise\u91cc\u9644\u5e26\u7684\u4e4b\u524daccepted\u7684\u63d0\u8bae\u4e2d\u9009\u53d6\u65b9\u6848\u53f7(n\u503c)\u6700\u9ad8\u7684\u5bf9\u5e94\u7684\u5171\u8bc6; \u5982\u679c\u5386\u53f2\u4e0a\u6ca1\u6709\u88abaccept\u8fc7\u7684\u63d0\u8bae, leader\u53ef\u4ee5\u81ea\u5df1\u9009\u53d6\u4e00\u4e2a\u5171\u8bc6v. NOTE: \u9700\u8981\u6ce8\u610f\uff0c\u5b83\u9700\u8981\u9996\u5148\u5904\u7406\"\u4e4b\u524daccepted\u7684\u63d0\u8bae\u4e2d\u9009\u53d6\u65b9\u6848\u53f7(n\u503c)\u6700\u9ad8\u7684\u5bf9\u5e94\u7684\u5171\u8bc6\"","title":"(1c)"},{"location":"Distributed-computing/Theory/Protocol/Guide/Expert-%E8%8C%83%E6%96%8C/%E6%BC%AB%E8%AF%9D%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%85%B1%E8%AF%86%E5%8D%8F%E8%AE%AEPaxos%E7%AF%87/#2accept","text":"","title":"2\u3001Accept\u9636\u6bb5"},{"location":"Distributed-computing/Theory/Protocol/Guide/Expert-%E8%8C%83%E6%96%8C/%E6%BC%AB%E8%AF%9D%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%85%B1%E8%AF%86%E5%8D%8F%E8%AE%AEPaxos%E7%AF%87/#2a","text":"leader\u4f1a\u5bf9\u6240\u6709acceptor\u53d1\u9001\"accept-request(n,v)\", \u8bf7\u6c42\u6240\u6709acceptor\u63a5\u53d7\u7f16\u53f7\u4e3an\u7684\u5171\u8bc6v\u7684\u63d0\u8bae","title":"(2a)"},{"location":"Distributed-computing/Theory/Protocol/Guide/Expert-%E8%8C%83%E6%96%8C/%E6%BC%AB%E8%AF%9D%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%85%B1%E8%AF%86%E5%8D%8F%E8%AE%AEPaxos%E7%AF%87/#2b","text":"\u6bcf\u4e00\u4e2a\u63a5\u6536\u5230\u8be5\u63d0\u8bae\u7684acceptor\u8282\u70b9: \u5982\u679c\u6ca1\u6709\u63a5\u53d7\u8fc7\u7f16\u53f7\u6bd4n\u66f4\u9ad8\u7684\u63d0\u8bae, \u5219\u8fd4\u56de\"accept\"\u8868\u793a\u63a5\u53d7\u8fd9\u4e00\u5171\u8bc6\u63d0\u8bae; \u5426\u5219\u8fd4\u56de\"reject\" NOTE: \u56de\u7b54accept \u56de\u7b54reject","title":"(2b)"},{"location":"Distributed-computing/Theory/Protocol/Guide/Expert-%E8%8C%83%E6%96%8C/%E6%BC%AB%E8%AF%9D%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%85%B1%E8%AF%86%E5%8D%8F%E8%AE%AEPaxos%E7%AF%87/#2c","text":"\u5982\u679c\u7b80\u5355\u591a\u6570\u7684acceptor\u8fd4\u56de\u4e86\"accept\", \u5219\u5171\u8bc6\u8fbe\u6210; \u5426\u5219\u5171\u8bc6\u5931\u8d25, \u91cd\u542fPaxos\u534f\u8bae. \u8bf7\u6ce8\u610f\u51e0\u70b9\u4ee5\u5e2e\u52a9\u7406\u89e3\u534f\u8bae: 1\u3001\u7b2c\u4e00\u9636\u6bb5\u7ade\u4e89\u7684\u5e76\u4e0d\u662f\u5171\u8bc6\u672c\u8eab, \u800c\u662f\u5728\u4e89\u53d6\u5750\u5b9eleader\u8eab\u4efd\u83b7\u5f97\u7b80\u5355\u591a\u6570\u7684\u8ba4\u53ef 2\u3001\u65b9\u6848\u7f16\u53f7n\u672c\u8eab\u5e76\u4e0d\u662f\u5171\u8bc6, \u800c\u662f\u63d0\u8bae\u7684\u4e00\u4e2a\u4f18\u5148\u7ea7, \u5728\u591a\u4e2a\u8282\u70b9\u7ade\u4e89leader\u8eab\u4efd\u65f6\u53ef\u4ee5\u533a\u5206\u4f18\u5148\u987a\u5e8f. \u5171\u8bc6\u672c\u8eab(v)\u4f1a\u5728\u4e0b\u4e00\u9636\u6bb5leader\u8eab\u4efd\u786e\u8ba4\u540e\u7531leader\u6dfb\u52a0\u8fdb\u63d0\u8bae; NOTE: \u663e\u7136\"\u65b9\u6848\u7f16\u53f7n\"\u662f\u4e3a\u4e86\u5904\u7406\u591a\u4e2a\u8282\u70b9\u540c\u65f6\u7ade\u4e89leader\u8eab\u4efd\u800c\u6dfb\u52a0\u7684 3\u3001\u867d\u7136\u8fd9\u4e00\u8f6e\u4e0a\u53ea\u4f1a\u6709\u4e00\u4e2aleader\u83b7\u5f97\u7b80\u5355\u591a\u6570\u7684\u8ba4\u53ef\u4ea7\u751f, \u4f46\u53ef\u80fd\u6709\u591a\u4e2a\"\u7cca\u6d82\"\u8282\u70b9\u8ba4\u4e3a\u81ea\u5df1\u5e94\u8be5\u505aleader, \u89c1\u540e\u9762\u7684\u5206\u6790;","title":"(2c)"},{"location":"Distributed-computing/Theory/Protocol/Guide/Expert-%E8%8C%83%E6%96%8C/%E6%BC%AB%E8%AF%9D%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%85%B1%E8%AF%86%E5%8D%8F%E8%AE%AEPaxos%E7%AF%87/#paxos2pc3pc","text":"\u5728\u6211\u770b\u6765, Paxos\u5bf92PC\u548c3PC\u6709\u51e0\u70b9\u91cd\u8981\u7684\u6539\u8fdb. \u7b2c\u4e00\uff0c\u5206\u79bb\u5171\u8bc6\u7684\u63d0\u8bae\u8005proposer\u4ee5\u53ca\u5e2e\u52a9\u63d0\u8bae\u6700\u7ec8\u901a\u8fc7\u7684leader\u8fd9\u4e24\u4e2a\u89d2\u8272. Paxos\u91cc, \u5373\u4f7f\u4e00\u4e2aleader\u8eab\u4efd\u88ab\u6279\u51c6, \u5b83\u4e5f\u9700\u8981\u5c0a\u91cd\u5386\u53f2\u4e0a\u5176\u4ed6\u88ab\u540c\u610f\u8fc7\u7684\u63d0\u8bae. \u6362\u8a00\u4e4bleader\u672c\u8eab\u53ea\u662f\u4e00\u4e2a\u670d\u52a1\u6027\u7684\u89d2\u8272, \u672a\u5fc5\u6709\u673a\u4f1a\u81ea\u5df1\u63d0\u51fa\u5171\u8bc6. NOTE: \u8fd9\u662f\u4e00\u4e2a\u975e\u5e38\u6df1\u523b\u7684\u8ba4\u8bc6 \u56de\u5fc6\u4e00\u4e0b\u4e0a\u4e00\u7bc7\u4ecb\u7ecd\u76842PC\u548c3PC\u8fd9\u4e24\u4e2a\u534f\u8bae\u5f53\u4e2d, coordinator\u4e0d\u4ec5\u8d1f\u8d23\u63d0\u51fa\u6700\u540e\u7684\u5171\u8bc6\u534f\u8bae, \u540c\u65f6\u4e5f\u8d1f\u8d23\u670d\u52a1\u6240\u6709\u8282\u70b9\u4fdd\u8bc1\u5b83\u7684\u5171\u8bc6\u88ab\u901a\u8fc7. \u800c\u6b63\u662f\u56e0\u4e3aPaxos\u4e2d\u628acoordinator\u7684\u804c\u8d23\u89e3\u8026\u5408\u6210\u4e86proposer\u548cleader, \u4f7f\u5f97\u6574\u4e2a\u7b97\u6cd5\u66f4\u52a0robust.\u5c31\u7b97\u524d\u4efbleader\u5b95\u673a\u4e86, \u540e\u9762\u65b0\u4ea7\u751fleader\u4e5f\u53ef\u4ee5\u7ee7\u627f\u524d\u4efb\u7684\"\u9057\u5fd7\"\u6765\u5b8c\u6210\u4e00\u4e2aPaxos\u534f\u8bae. \u7b2c\u4e8c\uff0c\u5bf9\u7b80\u5355\u591a\u6570\u7684\u5de7\u5999\u5e94\u7528. \u7b2c\u4e00\u9636\u6bb5\u91cc\u9009\u4e3eleader\u8981\u6c42\u7684\u7b80\u5355\u591a\u6570\u4fdd\u8bc1\u4e86\u9009\u4e3e\u51fa\u6765\u7684leader\u4e00\u5b9a\u4e0d\u4f1a\u9519\u8fc7\u4e4b\u524d\u88abaccept\u8fc7\u7684\u63d0\u8bae---\u6240\u4ee5\u5c31\u7b97\u90a3\u4e2a\u63d0\u8bae\u6700\u521d\u7684proposer\u6302\u4e86, \u4e5f\u4f1a\u81f3\u5c11\u88ab\u4e00\u4e2aacceptor\u53d1\u7ed9\u65b0\u7684leader\u6765\u7ee7\u627f. \u800c\u7b2c\u4e8c\u9636\u6bb5\u91cc\u8981\u6c42\u7684\u8fbe\u6210\u5171\u8bc6\u7684\u7b80\u5355\u591a\u6570\u4fdd\u8bc1\u4e86\u6709\u591a\u4e2a\"\u81ea\u4ee5\u4e3a\u662f\"\u7684leader\u51fa\u73b0\u65f6(\u6bd4\u5982\u4e00\u4e2aleader\u6389\u7ebf, \u65b0leader\u9009\u51fa, \u65e7leader\u91cd\u65b0\u4e0a\u7ebf), \u4e00\u5b9a\u53ea\u4f1a\u6709\u4e00\u4e2a\u6700\u540e\u901a\u8fc7. \u770b\u8fc7\u4e00\u4e2a\u7cbe\u5f69\u7684\u8bc4\u8bba, \u8bf4Paxos\u5176\u5b9e\u5c31\u662f\u8fde\u7eed\u8fd0\u7528\u4e24\u6b21\"\u62bd\u5c49\u539f\u7406\", \u5176\u5b9e\u975e\u5e38\u51c6\u786e. NOTE: \u4e00\u3001\u5173\u4e8e\"Paxos\u5176\u5b9e\u5c31\u662f\u8fde\u7eed\u8fd0\u7528\u4e24\u6b21\"\u62bd\u5c49\u539f\u7406\"\"\uff0c\u5728\u4e0b\u9762\u7684\u6587\u7ae0\u4e2d\u6709\u975e\u5e38\u597d\u7684\u4ecb\u7ecd: 1\u3001cnblogs \u5206\u5e03\u5f0f\u4e00\u81f4\u6027\u7b97\u6cd5\u2014\u2014Paxos\u539f\u7406\u5206\u6790 \u6211\u4eec\u79f0\u4e0a\u9762\u7684\u7b97\u6cd5\u4e3a\u201c\u591a\u6570\u540c\u610f\u7b97\u6cd5\u201d\u3002\u4e3a\u4ec0\u4e48\u8fd9\u4e2a\u7b97\u6cd5\u80fd\u591f\u786e\u4fdd\u8fbe\u6210\u5171\u8bc6\u2014\u2014\u6362\u4e2a\u8bf4\u6cd5\uff0c\u80fd\u591f\u4fdd\u8bc1\u6700\u591a\u53ea\u6709\u4e00\u4e2a\u8282\u70b9\u6709\u6743\u8fdb\u884c\u5e7f\u64ad\u5462\uff1f\u7528\u53cd\u8bc1\u6cd5\u5f88\u5bb9\u6613\u8bc1\u660e\u8fd9\u4e00\u70b9\u3002\u5047\u8bbe\u6709\u4e24\u4e2a\u63d0\u6848\u8005\u90fd\u6536\u5230\u4e86\u4e00\u534a\u4ee5\u4e0a\u7684\u63a5\u53d7\u56de\u590d\uff0c\u90a3\u4e48\u6839\u636e**\u62bd\u5c49\u539f\u7406**\uff0c\u56de\u590d\u8fd9\u4e24\u4e2a\u8282\u70b9\u7684\u63a5\u53d7\u8005\u91cc\u9762\uff0c\u5fc5\u5b9a\u4f1a\u81f3\u5c11\u6709\u4e00\u4e2a\u8282\u70b9\u540c\u65f6\u56de\u590d\u4e86\u8fd9\u4e24\u4e2a\u63d0\u6848\u8005\u3002\u8fd9\u5c31\u4ea7\u751f\u4e86\u660e\u663e\u7684\u77db\u76fe\u2014\u2014\u8bf4\u597d\u7684\u53ea\u56de\u590d\u4e00\u4e2a\u63d0\u6848\u8005\u5462\uff1f\u5230\u6b64\uff0c\u6211\u4eec\u5df2\u7ecf\u4fdd\u8bc1\u4e86\u8fd9\u4e2a\u7b97\u6cd5\u7684\u6b63\u786e\u6027\uff0c\u4efb\u52a1\u4f3c\u4e4e\u5df2\u7ecf\u5b8c\u6210\u4e86\u3002","title":"Paxos\u5bf92PC\u548c3PC\u7684\u6539\u8fdb"},{"location":"Distributed-computing/Theory/Protocol/Guide/Expert-%E8%8C%83%E6%96%8C/%E6%BC%AB%E8%AF%9D%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%85%B1%E8%AF%86%E5%8D%8F%E8%AE%AEPaxos%E7%AF%87/#paxos2pc3pc_1","text":"","title":"Paxos\u4e0e2PC/3PC\u7684\u5173\u7cfb"},{"location":"Distributed-computing/Theory/Protocol/Guide/Expert-%E8%8C%83%E6%96%8C/%E6%BC%AB%E8%AF%9D%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%85%B1%E8%AF%86%E5%8D%8F%E8%AE%AEPaxos%E7%AF%87/#paxos2pc","text":"2PC\u7684\u95ee\u9898\u5728\u4e8e\u4e0d\u80fd\u5904\u7406\u6700\u7b80\u5355\u7684fail-stop\u9519\u8bef\u6a21\u5f0f. 1\u30012PC\u4e2dcoordinator\u662f\u552f\u4e00\u800c\u56fa\u5b9a\u7684, \u5982\u679ccoordinator\u5b95\u673a, \u90a3\u4e48\u5c31\u4f1a\u6709\u60c5\u5f62\u5bfc\u81f4coordinator\u4e4b\u524dpropose\u7684\u63d0\u8bae\u7684\u6295\u7968\u7ed3\u679c\u4e22\u5931. \u5c31\u7b97\u542f\u52a8\u65b0\u7684\u540e\u5907coordinator, \u6ca1\u6709\u673a\u5236\u53ef\u4ee5\u5b66\u4e60\u4ee5\u524d\u7684\u6295\u7968\u7ed3\u679c. 2\u3001Paxos\u56e0\u4e3a\u5206\u79bb\u4e86\u63d0\u8bae\u548cleader, \u4ece\u7b97\u6cd5\u4e0a\u4fdd\u8bc1\u603b\u53ef\u4ee5\u9009\u4e3e\u51fa\u540e\u5907leader\u5e76\u63a5\u66ff\u524d\u4efbleader\u7684\u5de5\u4f5c.","title":"Paxos\u5982\u4f55\u514b\u670d2PC\u7684\u95ee\u9898"},{"location":"Distributed-computing/Theory/Protocol/Guide/Expert-%E8%8C%83%E6%96%8C/%E6%BC%AB%E8%AF%9D%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%85%B1%E8%AF%86%E5%8D%8F%E8%AE%AEPaxos%E7%AF%87/#paxos3pc","text":"3PC\u6539\u8fdb\u4e862PC\u7684fail-stop\u7684\u95ee\u9898, \u4f46\u662f\u4e0d\u80fd\u5904\u7406fail-recover\u7c7b\u578b\u7684\u9519\u8bef. 1\u30013PC\u53d1\u751f\u7684\u95ee\u9898\u5728\u4e8e\u5f53\u6709\u591a\u4e2a\"\u81ea\u8ba4\u7684leader\"\u51fa\u73b0\u65f6, \u5e76\u4e0d\u80fd\u6709\u6548\u7684\u89e3\u51b3coordinator\u4e4b\u95f4\u7684\u7ade\u4e89---\u8c01\u662f\u771f\u6b63\u7684coordinator. 2\u3001\u800cPaxos\u901a\u8fc7Quorum\u7684\u8fd0\u7528, \u4fdd\u8bc1\u4e86\u591a\u4e86\u4e2aleader\u4e4b\u95f4\u53ef\u4ee5\u4e92\u76f8\u53d1\u73b0.","title":"Paxos\u5982\u4f55\u514b\u670d3PC\u7684\u95ee\u9898"},{"location":"Distributed-computing/Theory/Protocol/Guide/Expert-%E8%8C%83%E6%96%8C/%E6%BC%AB%E8%AF%9D%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%85%B1%E8%AF%86%E5%8D%8F%E8%AE%AEPaxos%E7%AF%87/#paxos_1","text":"\u5c31\u50cf2PC\u4ee5\u53ca3PC\u4e00\u6837, Paxos\u4e5f\u6709\u5176\u5c40\u9650\u6027. 1 \u6d3b\u9501\u95ee\u9898. Paxos\u7406\u8bba\u4e0a\u5b58\u5728\u4e00\u4e2a\u4e0d\u80fd\u7ec8\u7ed3\u534f\u8bae\u7684\u6d3b\u9501\u7ade\u4e89\u95ee\u9898. \u6bd4\u5982\u4e00\u4e2aproposer\u63d0\u4ea4\u7684\u63d0\u8bae\u56e0\u4e3a\u7f16\u53f7\u8fc7\u4f4e\u88ab\u62d2\u7edd\u65f6, \u6b64proposer\u53ef\u80fd\u91cd\u542fPaxos\u800c\u63d0\u9ad8\u7f16\u53f7\u91cd\u65b0\u63d0\u4ea4. \u5982\u679c\u540c\u65f6\u6709\u4e24\u4e2aproposer\u90fd\u53d1\u73b0\u81ea\u5df1\u7684\u65b9\u6848\u7f16\u53f7\u8fc7\u4f4e, \u4ece\u800c\u8f6e\u6d41\u63d0\u51fa\u66f4\u9ad8\u7f16\u53f7\u7684proposal\u800c\u5bfc\u81f4\u5bf9\u65b9\u88ab\u62d2, \u53ef\u80fd\u4f1a\u5bfc\u81f4\u6b7b\u5faa\u73af(\u6216\u6d3b\u9501). NOTE: \u4e0d\u7406\u89e3 2 \u6076\u610f\u8282\u70b9. \u76ee\u524d\u4e3a\u6b622PC, 3PC, Paxos\u5747\u662f\u5047\u8bbe\u6240\u6709\u8282\u70b9\u90fd\u9075\u5b88\u534f\u8bae\u7684\u89c4\u5b9a. \u5f53\u5b58\u5728\u6076\u610f\u7684, \u53ef\u80fd\u53d1\u9001\u4efb\u4f55\u5bfc\u81f4\u534f\u8bae\u505c\u6b62\u6216\u8005\u51fa\u9519\u7684\u6d88\u606f\u7684\u8282\u70b9\u5b58\u5728\u65f6, \u5c31\u9700\u8981\u6709\u66f4\u5f3a\u7684\u5171\u8bc6\u7b97\u6cd5\u5728\"\u5b88\u6cd5\u8282\u70b9\"\u95f4\u8fbe\u6210\u5171\u8bc6. Lamport \u7684BFT(\u62dc\u5360\u5ead\u5c06\u519b\u95ee\u9898)\u4e86\u89e3\u4e00\u4e0b.","title":"Paxos\u7684\u5c40\u9650\u6027"},{"location":"Distributed-computing/Theory/Protocol/Guide/csdn-%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE/","text":"csdn \u5206\u5e03\u5f0f\u4e00\u81f4\u6027\u534f\u8bae \u4e3a\u4e86\u89e3\u51b3\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5728\u957f\u671f\u7684\u7814\u7a76\u63a2\u7d22\u8fc7\u7a0b\u4e2d\uff0c\u4e1a\u5185\u6d8c\u73b0\u51fa\u4e86\u4e00\u5927\u6279\u7ecf\u5178\u7684\u4e00\u81f4\u6027\u534f\u8bae\u548c\u7b97\u6cd5\uff0c\u5176\u4e2d\u6bd4\u8f83\u8457\u540d\u7684\u6709\u4e8c\u9636\u6bb5\u63d0\u4ea4\u534f\u8bae\uff082PC\uff09\uff0c\u4e09\u9636\u6bb5\u63d0\u4ea4\u534f\u8bae\uff083PC\uff09\u548c Paxos \u7b97\u6cd5\u3002 Google 2009\u5e74 \u5728 Transaction Across DataCenter \u7684\u5206\u4eab\u4e2d\uff0c\u5bf9\u4e00\u81f4\u6027\u534f\u8bae\u5728\u4e1a\u5185\u7684\u5b9e\u8df5\u505a\u4e86\u4e00\u7b80\u5355\u7684\u603b\u7ed3\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u8fd9\u662f CAP \u7406\u8bba\u5728\u5de5\u4e1a\u754c\u5e94\u7528\u7684\u5b9e\u8df5\u7ecf\u9a8c\u3002 \u5176\u4e2d\uff0c\u7b2c\u4e00\u884c\u8868\u5934\u4ee3\u8868\u4e86\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u901a\u7528\u7684\u4e00\u81f4\u6027\u65b9\u6848\uff0c\u5305\u62ec\u51b7\u5907\u3001Master/Slave\u3001Master/Master\u3001\u4e24\u9636\u6bb5\u63d0\u4ea4\u4ee5\u53ca\u57fa\u4e8e Paxos \u7b97\u6cd5\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7b2c\u4e00\u5217\u8868\u5934\u4ee3\u8868\u4e86\u5206\u5e03\u5f0f\u7cfb\u7edf\u5927\u5bb6\u6240\u5173\u5fc3\u7684\u5404\u9879\u6307\u6807\uff0c\u5305\u62ec\u4e00\u81f4\u6027\u3001\u4e8b\u52a1\u652f\u6301\u7a0b\u5ea6\u3001\u6570\u636e\u5ef6\u8fdf\u3001\u7cfb\u7edf\u541e\u5410\u91cf\u3001\u6570\u636e\u4e22\u5931\u53ef\u80fd\u6027\u3001\u6545\u969c\u81ea\u52a8\u6062\u590d\u65b9\u5f0f\u3002","title":"Introduction"},{"location":"Distributed-computing/Theory/Protocol/Guide/csdn-%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE/#csdn","text":"\u4e3a\u4e86\u89e3\u51b3\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5728\u957f\u671f\u7684\u7814\u7a76\u63a2\u7d22\u8fc7\u7a0b\u4e2d\uff0c\u4e1a\u5185\u6d8c\u73b0\u51fa\u4e86\u4e00\u5927\u6279\u7ecf\u5178\u7684\u4e00\u81f4\u6027\u534f\u8bae\u548c\u7b97\u6cd5\uff0c\u5176\u4e2d\u6bd4\u8f83\u8457\u540d\u7684\u6709\u4e8c\u9636\u6bb5\u63d0\u4ea4\u534f\u8bae\uff082PC\uff09\uff0c\u4e09\u9636\u6bb5\u63d0\u4ea4\u534f\u8bae\uff083PC\uff09\u548c Paxos \u7b97\u6cd5\u3002 Google 2009\u5e74 \u5728 Transaction Across DataCenter \u7684\u5206\u4eab\u4e2d\uff0c\u5bf9\u4e00\u81f4\u6027\u534f\u8bae\u5728\u4e1a\u5185\u7684\u5b9e\u8df5\u505a\u4e86\u4e00\u7b80\u5355\u7684\u603b\u7ed3\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u8fd9\u662f CAP \u7406\u8bba\u5728\u5de5\u4e1a\u754c\u5e94\u7528\u7684\u5b9e\u8df5\u7ecf\u9a8c\u3002 \u5176\u4e2d\uff0c\u7b2c\u4e00\u884c\u8868\u5934\u4ee3\u8868\u4e86\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u901a\u7528\u7684\u4e00\u81f4\u6027\u65b9\u6848\uff0c\u5305\u62ec\u51b7\u5907\u3001Master/Slave\u3001Master/Master\u3001\u4e24\u9636\u6bb5\u63d0\u4ea4\u4ee5\u53ca\u57fa\u4e8e Paxos \u7b97\u6cd5\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7b2c\u4e00\u5217\u8868\u5934\u4ee3\u8868\u4e86\u5206\u5e03\u5f0f\u7cfb\u7edf\u5927\u5bb6\u6240\u5173\u5fc3\u7684\u5404\u9879\u6307\u6807\uff0c\u5305\u62ec\u4e00\u81f4\u6027\u3001\u4e8b\u52a1\u652f\u6301\u7a0b\u5ea6\u3001\u6570\u636e\u5ef6\u8fdf\u3001\u7cfb\u7edf\u541e\u5410\u91cf\u3001\u6570\u636e\u4e22\u5931\u53ef\u80fd\u6027\u3001\u6545\u969c\u81ea\u52a8\u6062\u590d\u65b9\u5f0f\u3002","title":"csdn \u5206\u5e03\u5f0f\u4e00\u81f4\u6027\u534f\u8bae"},{"location":"Distributed-computing/Theory/Protocol/Guide/%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE%E7%AE%97%E6%B3%95-2PC-3PC-Paxos-Raft-ZAB-NWR/","text":"weixin \u4e00\u81f4\u6027\u534f\u8bae\u7b97\u6cd5-2PC\u30013PC\u3001Paxos\u3001Raft\u3001ZAB\u3001NWR\u8d85\u8be6\u7ec6\u89e3\u6790 NOTE: \u5728\u4e0b\u9762\u7684\u6587\u7ae0\u4e2d\uff0c\u8f6c\u8f7d\u4e86\u8fd9\u7bc7\u6587\u7ae0: 1\u3001csdn \u4e00\u81f4\u6027\u534f\u8bae\u7b97\u6cd5-2PC\u30013PC\u3001Paxos\u3001Raft\u3001ZAB\u3001NWR\u8d85\u8be6\u7ec6\u89e3\u6790 \u80cc\u666f CAP \u5b9a\u7406 Base \u7406\u8bba 2PC 3PC Paxos\u7b97\u6cd5 Raft\u4e00\u81f4\u6027\u7b97\u6cd5 \u4e00\u81f4\u6027\u534f\u8bae\u4e4b ZAB NWR\u6a21\u578b","title":"Introduction"},{"location":"Distributed-computing/Theory/Protocol/Guide/%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE%E7%AE%97%E6%B3%95-2PC-3PC-Paxos-Raft-ZAB-NWR/#weixin#-2pc3pcpaxosraftzabnwr","text":"NOTE: \u5728\u4e0b\u9762\u7684\u6587\u7ae0\u4e2d\uff0c\u8f6c\u8f7d\u4e86\u8fd9\u7bc7\u6587\u7ae0: 1\u3001csdn \u4e00\u81f4\u6027\u534f\u8bae\u7b97\u6cd5-2PC\u30013PC\u3001Paxos\u3001Raft\u3001ZAB\u3001NWR\u8d85\u8be6\u7ec6\u89e3\u6790","title":"weixin \u4e00\u81f4\u6027\u534f\u8bae\u7b97\u6cd5-2PC\u30013PC\u3001Paxos\u3001Raft\u3001ZAB\u3001NWR\u8d85\u8be6\u7ec6\u89e3\u6790"},{"location":"Distributed-computing/Theory/Protocol/Guide/%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE%E7%AE%97%E6%B3%95-2PC-3PC-Paxos-Raft-ZAB-NWR/#_1","text":"","title":"\u80cc\u666f"},{"location":"Distributed-computing/Theory/Protocol/Guide/%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE%E7%AE%97%E6%B3%95-2PC-3PC-Paxos-Raft-ZAB-NWR/#cap","text":"","title":"CAP \u5b9a\u7406"},{"location":"Distributed-computing/Theory/Protocol/Guide/%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE%E7%AE%97%E6%B3%95-2PC-3PC-Paxos-Raft-ZAB-NWR/#base","text":"","title":"Base \u7406\u8bba"},{"location":"Distributed-computing/Theory/Protocol/Guide/%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE%E7%AE%97%E6%B3%95-2PC-3PC-Paxos-Raft-ZAB-NWR/#2pc","text":"","title":"2PC"},{"location":"Distributed-computing/Theory/Protocol/Guide/%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE%E7%AE%97%E6%B3%95-2PC-3PC-Paxos-Raft-ZAB-NWR/#3pc","text":"","title":"3PC"},{"location":"Distributed-computing/Theory/Protocol/Guide/%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE%E7%AE%97%E6%B3%95-2PC-3PC-Paxos-Raft-ZAB-NWR/#paxos","text":"","title":"Paxos\u7b97\u6cd5"},{"location":"Distributed-computing/Theory/Protocol/Guide/%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE%E7%AE%97%E6%B3%95-2PC-3PC-Paxos-Raft-ZAB-NWR/#raft","text":"","title":"Raft\u4e00\u81f4\u6027\u7b97\u6cd5"},{"location":"Distributed-computing/Theory/Protocol/Guide/%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE%E7%AE%97%E6%B3%95-2PC-3PC-Paxos-Raft-ZAB-NWR/#zab","text":"","title":"\u4e00\u81f4\u6027\u534f\u8bae\u4e4b ZAB"},{"location":"Distributed-computing/Theory/Protocol/Guide/%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE%E7%AE%97%E6%B3%95-2PC-3PC-Paxos-Raft-ZAB-NWR/#nwr","text":"","title":"NWR\u6a21\u578b"},{"location":"Distributed-computing/Theory/Protocol/Paxos/","text":"Paxos Google Chubby\u7684\u4f5c\u8005Mike Burrows\u8bf4\u8fc7\u8fd9\u4e2a\u4e16\u754c\u4e0a\u53ea\u6709\u4e00\u79cd\u4e00\u81f4\u6027\u7b97\u6cd5\uff0c\u90a3\u5c31\u662fPaxos\uff0c\u5176\u5b83\u7684\u7b97\u6cd5\u90fd\u662f\u6b8b\u6b21\u54c1\u3002 \u5728\u5f88\u591a\u4ecb\u7ecdpaxos\u7684\u6587\u7ae0\u4e2d\uff0c\u90fd\u90a3\u8fd9\u6bb5\u8bdd\u5f00\u5934\uff0c\u8fd9\u8bf4\u660e\u4e86paxos\u7684\u5a01\u529b\u3002 Guide\u6587\u7ae0 1\u3001csdn \u8bf8\u845b\u4eae VS \u5e9e\u7edf\uff0c\u62ff\u4e0b Paxos \u5171\u8bc6\u7b97\u6cd5 \u8fd9\u7bc7\u6587\u7ae0\uff0c\u7ed3\u5408\u4e86\u5177\u4f53\u4f8b\u5b50\u6765\u8bb2 \u6536\u5f55\u4e86\uff1b 2\u3001zhihu \u6f2b\u8bdd\u5206\u5e03\u5f0f\u7cfb\u7edf\u5171\u8bc6\u534f\u8bae: Paxos\u7bc7 \u7eaf\u7cb9\u8bb2\u7b97\u6cd5\uff0c\u4f46\u662f\u9ad8\u5c4b\u5efa\u74f4\uff0c\u770b\u900f\u672c\u8d28 \u6536\u5f55\u4e86\uff1b 3\u3001csdn \u8bf4\u5230\u5206\u5e03\u5f0f\uff0c\u91cd\u8981\u7684Paxos\u7b97\u6cd5\u4f60\u770b\u900f\u4e86\u4e48\uff1f \u6536\u5f55\u4e86\uff1b 4\u3001weixin \u4e00\u81f4\u6027\u534f\u8bae\u7b97\u6cd5-2PC\u30013PC\u3001Paxos\u3001Raft\u3001ZAB\u3001NWR\u8d85\u8be6\u7ec6\u89e3\u6790 \u6536\u5f55\u4e86\uff0c\u53c2\u89c1 Protocol\\Guide\\\u4e00\u81f4\u6027\u534f\u8bae\u7b97\u6cd5-2PC-3PC-Paxos-Raft-ZAB-NWR 5\u3001developpaper Detailed Explanation of Paxos Consensus Algorithms \u5185\u5bb9\u5f88\u597d\u3002 paper Paxos Made Simple lamport Paxos Made Simple \u8fd9\u662f\u539f\u8bba\u6587\u3002 Paxos Playground paper Paxos Playground: a simulation to understand a replicated state machine implementation using Paxos implementation: github jivimberg / paxos-playground","title":"Introduction"},{"location":"Distributed-computing/Theory/Protocol/Paxos/#paxos","text":"Google Chubby\u7684\u4f5c\u8005Mike Burrows\u8bf4\u8fc7\u8fd9\u4e2a\u4e16\u754c\u4e0a\u53ea\u6709\u4e00\u79cd\u4e00\u81f4\u6027\u7b97\u6cd5\uff0c\u90a3\u5c31\u662fPaxos\uff0c\u5176\u5b83\u7684\u7b97\u6cd5\u90fd\u662f\u6b8b\u6b21\u54c1\u3002 \u5728\u5f88\u591a\u4ecb\u7ecdpaxos\u7684\u6587\u7ae0\u4e2d\uff0c\u90fd\u90a3\u8fd9\u6bb5\u8bdd\u5f00\u5934\uff0c\u8fd9\u8bf4\u660e\u4e86paxos\u7684\u5a01\u529b\u3002","title":"Paxos"},{"location":"Distributed-computing/Theory/Protocol/Paxos/#guide","text":"","title":"Guide\u6587\u7ae0"},{"location":"Distributed-computing/Theory/Protocol/Paxos/#1csdn#vs#paxos","text":"\u8fd9\u7bc7\u6587\u7ae0\uff0c\u7ed3\u5408\u4e86\u5177\u4f53\u4f8b\u5b50\u6765\u8bb2 \u6536\u5f55\u4e86\uff1b","title":"1\u3001csdn \u8bf8\u845b\u4eae VS \u5e9e\u7edf\uff0c\u62ff\u4e0b Paxos \u5171\u8bc6\u7b97\u6cd5"},{"location":"Distributed-computing/Theory/Protocol/Paxos/#2zhihu#paxos","text":"\u7eaf\u7cb9\u8bb2\u7b97\u6cd5\uff0c\u4f46\u662f\u9ad8\u5c4b\u5efa\u74f4\uff0c\u770b\u900f\u672c\u8d28 \u6536\u5f55\u4e86\uff1b","title":"2\u3001zhihu \u6f2b\u8bdd\u5206\u5e03\u5f0f\u7cfb\u7edf\u5171\u8bc6\u534f\u8bae: Paxos\u7bc7"},{"location":"Distributed-computing/Theory/Protocol/Paxos/#3csdn#paxos","text":"\u6536\u5f55\u4e86\uff1b","title":"3\u3001csdn \u8bf4\u5230\u5206\u5e03\u5f0f\uff0c\u91cd\u8981\u7684Paxos\u7b97\u6cd5\u4f60\u770b\u900f\u4e86\u4e48\uff1f"},{"location":"Distributed-computing/Theory/Protocol/Paxos/#4weixin#-2pc3pcpaxosraftzabnwr","text":"\u6536\u5f55\u4e86\uff0c\u53c2\u89c1 Protocol\\Guide\\\u4e00\u81f4\u6027\u534f\u8bae\u7b97\u6cd5-2PC-3PC-Paxos-Raft-ZAB-NWR","title":"4\u3001weixin \u4e00\u81f4\u6027\u534f\u8bae\u7b97\u6cd5-2PC\u30013PC\u3001Paxos\u3001Raft\u3001ZAB\u3001NWR\u8d85\u8be6\u7ec6\u89e3\u6790"},{"location":"Distributed-computing/Theory/Protocol/Paxos/#5developpaper#detailed#explanation#of#paxos#consensus#algorithms","text":"\u5185\u5bb9\u5f88\u597d\u3002","title":"5\u3001developpaper Detailed Explanation of Paxos Consensus Algorithms"},{"location":"Distributed-computing/Theory/Protocol/Paxos/#paper#paxos#made#simple","text":"lamport Paxos Made Simple \u8fd9\u662f\u539f\u8bba\u6587\u3002","title":"paper Paxos Made Simple"},{"location":"Distributed-computing/Theory/Protocol/Paxos/#paxos#playground","text":"paper Paxos Playground: a simulation to understand a replicated state machine implementation using Paxos implementation: github jivimberg / paxos-playground","title":"Paxos Playground"},{"location":"Distributed-computing/Theory/Protocol/Paxos/CSDN-%E9%87%8D%E8%A6%81%E7%9A%84Paxos%E7%AE%97%E6%B3%95%E4%BD%A0%E7%9C%8B%E9%80%8F%E4%BA%86%E4%B9%88/","text":"csdn \u8bf4\u5230\u5206\u5e03\u5f0f\uff0c\u91cd\u8981\u7684Paxos\u7b97\u6cd5\u4f60\u770b\u900f\u4e86\u4e48\uff1f Quorum \u673a\u5236 NOTE: \u8fd9\u4e00\u6bb5\u5176\u5b9e\u662f\u5728\u8bb2 \"NRW-quorum consistency\" \u5185\u5bb9\u4e00\u822c Paxos Paxos \u5e38\u89c1\u7684\u95ee\u9898 1\u3001Acceptor \u9700\u8981\u63a5\u53d7\u66f4\u5927\u7684 N\uff0c\u4e5f\u5c31\u662f ProposalID\uff0c\u8fd9\u6709\u4ec0\u4e48\u610f\u4e49\uff1f \u8fd9\u79cd\u673a\u5236\u53ef\u4ee5\u9632\u6b62\u5176\u4e2d\u4e00\u4e2a Proposer \u5d29\u6e83\u5b95\u673a\u4ea7\u751f\u963b\u585e\u95ee\u9898\uff0c\u5141\u8bb8\u5176\u4ed6 Proposer \u7528\u66f4\u5927 ProposalID \u6765\u62a2\u5360\u4e34\u65f6\u7684\u8bbf\u95ee\u6743\u3002 2\u3001\u5982\u4f55\u4ea7\u751f\u552f\u4e00\u7684\u7f16\u53f7\uff0c\u4e5f\u5c31\u662f ProposalID\uff1f \u5728\u300aPaxos made simple\u300b\u7684\u8bba\u6587\u4e2d\u63d0\u5230\uff0c\u552f\u4e00\u7f16\u53f7\u662f\u8ba9\u6240\u6709\u7684 Proposer \u90fd\u4ece\u4e0d\u76f8\u4ea4\u7684\u6570\u636e\u96c6\u5408\u4e2d\u8fdb\u884c\u9009\u62e9\uff0c\u9700\u8981\u4fdd\u8bc1\u5728\u4e0d\u540c Proposer \u4e4b\u95f4\u4e0d\u91cd\u590d\u3002 \u6bd4\u5982\u7cfb\u7edf\u6709 5 \u4e2a Proposer\uff0c\u5219\u53ef\u4e3a\u6bcf\u4e00\u4e2a Proposer \u5206\u914d\u4e00\u4e2a\u6807\u8bc6 j(0~4)\uff0c\u90a3\u4e48\u6bcf\u4e00\u4e2a Proposer \u6bcf\u6b21\u63d0\u51fa\u51b3\u8bae\u7684\u7f16\u53f7\u53ef\u4ee5\u4e3a 5*i+j\uff0ci \u53ef\u4ee5\u7528\u6765\u8868\u793a\u63d0\u51fa\u8bae\u6848\u7684\u6b21\u6570\u3002","title":"Introduction"},{"location":"Distributed-computing/Theory/Protocol/Paxos/CSDN-%E9%87%8D%E8%A6%81%E7%9A%84Paxos%E7%AE%97%E6%B3%95%E4%BD%A0%E7%9C%8B%E9%80%8F%E4%BA%86%E4%B9%88/#csdn#paxos","text":"","title":"csdn \u8bf4\u5230\u5206\u5e03\u5f0f\uff0c\u91cd\u8981\u7684Paxos\u7b97\u6cd5\u4f60\u770b\u900f\u4e86\u4e48\uff1f"},{"location":"Distributed-computing/Theory/Protocol/Paxos/CSDN-%E9%87%8D%E8%A6%81%E7%9A%84Paxos%E7%AE%97%E6%B3%95%E4%BD%A0%E7%9C%8B%E9%80%8F%E4%BA%86%E4%B9%88/#quorum","text":"NOTE: \u8fd9\u4e00\u6bb5\u5176\u5b9e\u662f\u5728\u8bb2 \"NRW-quorum consistency\" \u5185\u5bb9\u4e00\u822c","title":"Quorum \u673a\u5236"},{"location":"Distributed-computing/Theory/Protocol/Paxos/CSDN-%E9%87%8D%E8%A6%81%E7%9A%84Paxos%E7%AE%97%E6%B3%95%E4%BD%A0%E7%9C%8B%E9%80%8F%E4%BA%86%E4%B9%88/#paxos","text":"","title":"Paxos"},{"location":"Distributed-computing/Theory/Protocol/Paxos/CSDN-%E9%87%8D%E8%A6%81%E7%9A%84Paxos%E7%AE%97%E6%B3%95%E4%BD%A0%E7%9C%8B%E9%80%8F%E4%BA%86%E4%B9%88/#paxos_1","text":"1\u3001Acceptor \u9700\u8981\u63a5\u53d7\u66f4\u5927\u7684 N\uff0c\u4e5f\u5c31\u662f ProposalID\uff0c\u8fd9\u6709\u4ec0\u4e48\u610f\u4e49\uff1f \u8fd9\u79cd\u673a\u5236\u53ef\u4ee5\u9632\u6b62\u5176\u4e2d\u4e00\u4e2a Proposer \u5d29\u6e83\u5b95\u673a\u4ea7\u751f\u963b\u585e\u95ee\u9898\uff0c\u5141\u8bb8\u5176\u4ed6 Proposer \u7528\u66f4\u5927 ProposalID \u6765\u62a2\u5360\u4e34\u65f6\u7684\u8bbf\u95ee\u6743\u3002 2\u3001\u5982\u4f55\u4ea7\u751f\u552f\u4e00\u7684\u7f16\u53f7\uff0c\u4e5f\u5c31\u662f ProposalID\uff1f \u5728\u300aPaxos made simple\u300b\u7684\u8bba\u6587\u4e2d\u63d0\u5230\uff0c\u552f\u4e00\u7f16\u53f7\u662f\u8ba9\u6240\u6709\u7684 Proposer \u90fd\u4ece\u4e0d\u76f8\u4ea4\u7684\u6570\u636e\u96c6\u5408\u4e2d\u8fdb\u884c\u9009\u62e9\uff0c\u9700\u8981\u4fdd\u8bc1\u5728\u4e0d\u540c Proposer \u4e4b\u95f4\u4e0d\u91cd\u590d\u3002 \u6bd4\u5982\u7cfb\u7edf\u6709 5 \u4e2a Proposer\uff0c\u5219\u53ef\u4e3a\u6bcf\u4e00\u4e2a Proposer \u5206\u914d\u4e00\u4e2a\u6807\u8bc6 j(0~4)\uff0c\u90a3\u4e48\u6bcf\u4e00\u4e2a Proposer \u6bcf\u6b21\u63d0\u51fa\u51b3\u8bae\u7684\u7f16\u53f7\u53ef\u4ee5\u4e3a 5*i+j\uff0ci \u53ef\u4ee5\u7528\u6765\u8868\u793a\u63d0\u51fa\u8bae\u6848\u7684\u6b21\u6570\u3002","title":"Paxos \u5e38\u89c1\u7684\u95ee\u9898"},{"location":"Distributed-computing/Theory/Protocol/Paxos/Multi-paxos/","text":"Multiple paxos developpaper Detailed Explanation of Paxos Consensus Algorithms","title":"Introduction"},{"location":"Distributed-computing/Theory/Protocol/Paxos/Multi-paxos/#multiple#paxos","text":"developpaper Detailed Explanation of Paxos Consensus Algorithms","title":"Multiple paxos"},{"location":"Distributed-computing/Theory/Protocol/Paxos/Multi-paxos/cnblogs-%E4%BD%BF%E7%94%A8multi-paxos%E5%AE%9E%E7%8E%B0%E6%97%A5%E5%BF%97%E5%90%8C%E6%AD%A5%E5%BA%94%E7%94%A8/","text":"cnblogs \u4f7f\u7528multi-paxos\u5b9e\u73b0\u65e5\u5fd7\u540c\u6b65\u5e94\u7528","title":"Introduction"},{"location":"Distributed-computing/Theory/Protocol/Paxos/Multi-paxos/cnblogs-%E4%BD%BF%E7%94%A8multi-paxos%E5%AE%9E%E7%8E%B0%E6%97%A5%E5%BF%97%E5%90%8C%E6%AD%A5%E5%BA%94%E7%94%A8/#cnblogs#multi-paxos","text":"","title":"cnblogs \u4f7f\u7528multi-paxos\u5b9e\u73b0\u65e5\u5fd7\u540c\u6b65\u5e94\u7528"},{"location":"Distributed-computing/Theory/Protocol/Paxos/paper-Paxos-Made-Simple/","text":"lamport Paxos Made Simple Abstract The Paxos algorithm, when presented in plain English, is very simple. zhihu paxos made simple\u7684\u6700\u540e\u4e00\u8282\u5b9e\u73b0\u72b6\u6001\u673a\u5982\u4f55\u7406\u89e3\uff1f","title":"Introduction"},{"location":"Distributed-computing/Theory/Protocol/Paxos/paper-Paxos-Made-Simple/#lamport#paxos#made#simple","text":"","title":"lamport Paxos Made Simple"},{"location":"Distributed-computing/Theory/Protocol/Paxos/paper-Paxos-Made-Simple/#abstract","text":"The Paxos algorithm, when presented in plain English, is very simple.","title":"Abstract"},{"location":"Distributed-computing/Theory/Protocol/Paxos/paper-Paxos-Made-Simple/#zhihu#paxos#made#simple","text":"","title":"zhihu paxos made simple\u7684\u6700\u540e\u4e00\u8282\u5b9e\u73b0\u72b6\u6001\u673a\u5982\u4f55\u7406\u89e3\uff1f"},{"location":"Distributed-computing/Theory/Protocol/Paxos/wikipedia-Paxos/","text":"wikipedia Paxos (computer science) Paxos is a family of protocols for solving consensus in a network of unreliable processors (that is, processors that may fail). Consensus is the process of agreeing on one result among a group of participants. This problem becomes difficult when the participants or their communication medium may experience failures.[ 1] Consensus protocols are the basis for the state machine replication approach to distributed computing, as suggested by Leslie Lamport [ 2] and surveyed by Fred Schneider .[ 3] State machine replication is a technique for converting an algorithm into a fault-tolerant, distributed implementation. Ad-hoc techniques may leave important cases of failures unresolved. The principled approach proposed by Lamport et al. ensures all cases are handled safely. NOTE: \u6b63\u5982Leslie Lamport[2]\u548cFred Schneider\u6240\u5efa\u8bae\u7684\uff0c\u534f\u5546\u4e00\u81f4\u534f\u8bae\u662f\u5206\u5e03\u5f0f\u8ba1\u7b97\u7684\u72b6\u6001\u673a\u590d\u5236\u65b9\u6cd5\u7684\u57fa\u7840\u3002\u72b6\u6001\u673a\u590d\u5236\u662f\u4e00\u79cd\u5c06\u7b97\u6cd5\u8f6c\u6362\u4e3a\u5bb9\u9519\u5206\u5e03\u5f0f\u5b9e\u73b0\u7684\u6280\u672f\u3002\u7279\u522b\u7684\u6280\u672f\u53ef\u80fd\u4f1a\u4f7f\u91cd\u8981\u7684\u5931\u8d25\u6848\u4f8b\u5f97\u4e0d\u5230\u89e3\u51b3\u3002Lamport\u7b49\u4eba\u63d0\u51fa\u7684\u539f\u5219\u65b9\u6cd5\u786e\u4fdd\u4e86\u6240\u6709\u6848\u4ef6\u90fd\u5f97\u5230\u5b89\u5168\u5904\u7406\u3002 The Paxos protocol was first published in 1989 and named after a fictional legislative consensus system used on the Paxos island in Greece.[ 4] It was later published as a journal article in 1998.[ 5] The Paxos family of protocols includes a spectrum of trade-offs between the number of processors, number of message delays before learning the agreed value, the activity level of individual participants, number of messages sent, and types of failures. Although no deterministic fault-tolerant consensus protocol can guarantee progress in an asynchronous network (a result proved in a paper by Fischer , Lynch and Paterson [ 6] ), Paxos guarantees safety (consistency), and the conditions that could prevent it from making progress are difficult to provoke. NOTE: Paxos\u534f\u8bae\u5bb6\u65cf\u5305\u62ec\u5904\u7406\u5668\u6570\u91cf\u3001\u5b66\u4e60\u5546\u5b9a\u503c\u4e4b\u524d\u7684\u6d88\u606f\u5ef6\u8fdf\u6570\u91cf\u3001\u5355\u4e2a\u53c2\u4e0e\u8005\u7684\u6d3b\u52a8\u7ea7\u522b\u3001\u53d1\u9001\u7684\u6d88\u606f\u6570\u91cf\u548c\u6545\u969c\u7c7b\u578b\u4e4b\u95f4\u7684\u4e00\u7cfb\u5217\u6743\u8861\u3002 \u867d\u7136\u6ca1\u6709\u786e\u5b9a\u7684\u5bb9\u9519\u4e00\u81f4\u6027\u534f\u8bae\u53ef\u4ee5\u4fdd\u8bc1\u5f02\u6b65\u7f51\u7edc\u4e2d\u7684\u8fdb\u7a0b(Fischer\u3001Lynch\u548cPaterson\u5728\u4e00\u7bc7\u8bba\u6587\u4e2d\u8bc1\u660e\u4e86\u8fd9\u4e00\u7ed3\u679c)\uff0c\u4f46\u662fPaxos\u4fdd\u8bc1\u4e86\u5b89\u5168\u6027(\u4e00\u81f4\u6027)\uff0c\u5e76\u4e14\u5f88\u96be\u6fc0\u53d1\u80fd\u591f\u963b\u6b62\u5b83\u53d6\u5f97\u8fdb\u5c55\u7684\u6761\u4ef6\u3002 Paxos is usually used where durability is required (for example, to replicate a file or a database), in which the amount of durable state could be large. The protocol attempts to make progress even during periods when some bounded number of replicas are unresponsive. There is also a mechanism to drop a permanently failed replica or to add a new replica. NOTE: Paxos\u901a\u5e38\u7528\u4e8e\u9700\u8981\u6301\u4e45\u6027\u7684\u5730\u65b9(\u4f8b\u5982\uff0c\u590d\u5236\u6587\u4ef6\u6216\u6570\u636e\u5e93)\uff0c\u5176\u4e2d\u6301\u4e45\u6027\u72b6\u6001\u7684\u6570\u91cf\u53ef\u80fd\u5f88\u5927\u3002\u5373\u4f7f\u5728\u67d0\u4e9b\u6709\u9650\u6570\u91cf\u7684\u526f\u672c\u6ca1\u6709\u54cd\u5e94\u65f6\uff0c\u534f\u8bae\u4e5f\u8bd5\u56fe\u53d6\u5f97\u8fdb\u5c55\u3002\u8fd8\u6709\u4e00\u79cd\u673a\u5236\u53ef\u4ee5\u5220\u9664\u6c38\u4e45\u5931\u8d25\u7684\u526f\u672c\u6216\u6dfb\u52a0\u65b0\u526f\u672c\u3002","title":"Introduction"},{"location":"Distributed-computing/Theory/Protocol/Paxos/wikipedia-Paxos/#wikipedia#paxos#computer#science","text":"Paxos is a family of protocols for solving consensus in a network of unreliable processors (that is, processors that may fail). Consensus is the process of agreeing on one result among a group of participants. This problem becomes difficult when the participants or their communication medium may experience failures.[ 1] Consensus protocols are the basis for the state machine replication approach to distributed computing, as suggested by Leslie Lamport [ 2] and surveyed by Fred Schneider .[ 3] State machine replication is a technique for converting an algorithm into a fault-tolerant, distributed implementation. Ad-hoc techniques may leave important cases of failures unresolved. The principled approach proposed by Lamport et al. ensures all cases are handled safely. NOTE: \u6b63\u5982Leslie Lamport[2]\u548cFred Schneider\u6240\u5efa\u8bae\u7684\uff0c\u534f\u5546\u4e00\u81f4\u534f\u8bae\u662f\u5206\u5e03\u5f0f\u8ba1\u7b97\u7684\u72b6\u6001\u673a\u590d\u5236\u65b9\u6cd5\u7684\u57fa\u7840\u3002\u72b6\u6001\u673a\u590d\u5236\u662f\u4e00\u79cd\u5c06\u7b97\u6cd5\u8f6c\u6362\u4e3a\u5bb9\u9519\u5206\u5e03\u5f0f\u5b9e\u73b0\u7684\u6280\u672f\u3002\u7279\u522b\u7684\u6280\u672f\u53ef\u80fd\u4f1a\u4f7f\u91cd\u8981\u7684\u5931\u8d25\u6848\u4f8b\u5f97\u4e0d\u5230\u89e3\u51b3\u3002Lamport\u7b49\u4eba\u63d0\u51fa\u7684\u539f\u5219\u65b9\u6cd5\u786e\u4fdd\u4e86\u6240\u6709\u6848\u4ef6\u90fd\u5f97\u5230\u5b89\u5168\u5904\u7406\u3002 The Paxos protocol was first published in 1989 and named after a fictional legislative consensus system used on the Paxos island in Greece.[ 4] It was later published as a journal article in 1998.[ 5] The Paxos family of protocols includes a spectrum of trade-offs between the number of processors, number of message delays before learning the agreed value, the activity level of individual participants, number of messages sent, and types of failures. Although no deterministic fault-tolerant consensus protocol can guarantee progress in an asynchronous network (a result proved in a paper by Fischer , Lynch and Paterson [ 6] ), Paxos guarantees safety (consistency), and the conditions that could prevent it from making progress are difficult to provoke. NOTE: Paxos\u534f\u8bae\u5bb6\u65cf\u5305\u62ec\u5904\u7406\u5668\u6570\u91cf\u3001\u5b66\u4e60\u5546\u5b9a\u503c\u4e4b\u524d\u7684\u6d88\u606f\u5ef6\u8fdf\u6570\u91cf\u3001\u5355\u4e2a\u53c2\u4e0e\u8005\u7684\u6d3b\u52a8\u7ea7\u522b\u3001\u53d1\u9001\u7684\u6d88\u606f\u6570\u91cf\u548c\u6545\u969c\u7c7b\u578b\u4e4b\u95f4\u7684\u4e00\u7cfb\u5217\u6743\u8861\u3002 \u867d\u7136\u6ca1\u6709\u786e\u5b9a\u7684\u5bb9\u9519\u4e00\u81f4\u6027\u534f\u8bae\u53ef\u4ee5\u4fdd\u8bc1\u5f02\u6b65\u7f51\u7edc\u4e2d\u7684\u8fdb\u7a0b(Fischer\u3001Lynch\u548cPaterson\u5728\u4e00\u7bc7\u8bba\u6587\u4e2d\u8bc1\u660e\u4e86\u8fd9\u4e00\u7ed3\u679c)\uff0c\u4f46\u662fPaxos\u4fdd\u8bc1\u4e86\u5b89\u5168\u6027(\u4e00\u81f4\u6027)\uff0c\u5e76\u4e14\u5f88\u96be\u6fc0\u53d1\u80fd\u591f\u963b\u6b62\u5b83\u53d6\u5f97\u8fdb\u5c55\u7684\u6761\u4ef6\u3002 Paxos is usually used where durability is required (for example, to replicate a file or a database), in which the amount of durable state could be large. The protocol attempts to make progress even during periods when some bounded number of replicas are unresponsive. There is also a mechanism to drop a permanently failed replica or to add a new replica. NOTE: Paxos\u901a\u5e38\u7528\u4e8e\u9700\u8981\u6301\u4e45\u6027\u7684\u5730\u65b9(\u4f8b\u5982\uff0c\u590d\u5236\u6587\u4ef6\u6216\u6570\u636e\u5e93)\uff0c\u5176\u4e2d\u6301\u4e45\u6027\u72b6\u6001\u7684\u6570\u91cf\u53ef\u80fd\u5f88\u5927\u3002\u5373\u4f7f\u5728\u67d0\u4e9b\u6709\u9650\u6570\u91cf\u7684\u526f\u672c\u6ca1\u6709\u54cd\u5e94\u65f6\uff0c\u534f\u8bae\u4e5f\u8bd5\u56fe\u53d6\u5f97\u8fdb\u5c55\u3002\u8fd8\u6709\u4e00\u79cd\u673a\u5236\u53ef\u4ee5\u5220\u9664\u6c38\u4e45\u5931\u8d25\u7684\u526f\u672c\u6216\u6dfb\u52a0\u65b0\u526f\u672c\u3002","title":"wikipedia Paxos (computer science)"},{"location":"Distributed-computing/Theory/Protocol/Raft/","text":"Raft TODO https://engineering.cerner.com/2014/01/the-raft-protocol-a-better-paxos/ \u5728redis\u4e2d\uff0c\u501f\u9274\u4e86 raft \u7684Leader Election\u673a\u5236\uff0c\u90a3\u5728raft\u4e2d\uff0c\u5b83\u7684Leader Election\u6240\u91c7\u7528\u7684\u662f\u4ec0\u4e48\u601d\u60f3\u5462\uff1f","title":"Introduction"},{"location":"Distributed-computing/Theory/Protocol/Raft/#raft","text":"","title":"Raft"},{"location":"Distributed-computing/Theory/Protocol/Raft/#todo","text":"https://engineering.cerner.com/2014/01/the-raft-protocol-a-better-paxos/ \u5728redis\u4e2d\uff0c\u501f\u9274\u4e86 raft \u7684Leader Election\u673a\u5236\uff0c\u90a3\u5728raft\u4e2d\uff0c\u5b83\u7684Leader Election\u6240\u91c7\u7528\u7684\u662f\u4ec0\u4e48\u601d\u60f3\u5462\uff1f","title":"TODO"},{"location":"Distributed-computing/Theory/Protocol/Raft/github-The-Raft-Consensus-Algorithm/","text":"raft.github The Raft Consensus Algorithm What is Raft? Raft is a consensus algorithm that is designed to be easy to understand. It's equivalent to Paxos in fault-tolerance and performance. The difference is that it's decomposed into relatively independent subproblems, and it cleanly addresses all major pieces needed for practical systems. We hope Raft will make consensus available to a wider audience, and that this wider audience will be able to develop a variety of higher quality consensus-based systems than are available today. Hold on\u2014what is consensus? Consensus is a fundamental problem in fault-tolerant distributed systems. Consensus involves multiple servers agreeing on values. Once they reach a decision on a value, that decision is final. Typical consensus algorithms make progress when any majority of their servers is available; for example, a cluster of 5 servers can continue to operate even if 2 servers fail. If more servers fail, they stop making progress (but will never return an incorrect result). Consensus typically arises in the context of replicated state machines , a general approach to building fault-tolerant systems. Each server has a state machine and a log . The state machine is the component that we want to make fault-tolerant , such as a hash table. It will appear to clients that they are interacting with a single, reliable state machine, even if a minority of the servers in the cluster fail. Each state machine takes as input commands from its log . In our hash table example, the log would include commands like set x to 3 . A consensus algorithm is used to agree on the commands in the servers' logs. The consensus algorithm must ensure that if any state machine applies set x to 3 as the n*th command, no other **state machine* will ever apply a different n*th command. As a result, each **state machine* processes the same series of commands and thus produces the same series of results and arrives at the same series of states. NOTE : Raft Visualization Here's a Raft cluster running in your browser. You can interact with it to see Raft in action. Five servers are shown on the left, and their logs are shown on the right. We hope to create a screencast soon to explain what's going on. This visualization ( RaftScope ) is still pretty rough around the edges; pull requests would be very welcome. The Secret Lives of Data is a different visualization of Raft. It's more guided and less interactive, so it may be a gentler starting point. Publications This is \"the Raft paper\", which describes Raft in detail: In Search of an Understandable Consensus Algorithm (Extended Version) by Diego Ongaro and John Ousterhout . A slightly shorter version of this paper received a Best Paper Award at the 2014 USENIX Annual Technical Conference . Diego Ongaro's Ph.D. dissertation expands on the content of the paper in much more detail, and it includes a simpler cluster membership change algorithm.","title":"Introduction"},{"location":"Distributed-computing/Theory/Protocol/Raft/github-The-Raft-Consensus-Algorithm/#raftgithub#the#raft#consensus#algorithm","text":"","title":"raft.github The Raft Consensus Algorithm"},{"location":"Distributed-computing/Theory/Protocol/Raft/github-The-Raft-Consensus-Algorithm/#what#is#raft","text":"Raft is a consensus algorithm that is designed to be easy to understand. It's equivalent to Paxos in fault-tolerance and performance. The difference is that it's decomposed into relatively independent subproblems, and it cleanly addresses all major pieces needed for practical systems. We hope Raft will make consensus available to a wider audience, and that this wider audience will be able to develop a variety of higher quality consensus-based systems than are available today.","title":"What is Raft?"},{"location":"Distributed-computing/Theory/Protocol/Raft/github-The-Raft-Consensus-Algorithm/#hold#onwhat#is#consensus","text":"Consensus is a fundamental problem in fault-tolerant distributed systems. Consensus involves multiple servers agreeing on values. Once they reach a decision on a value, that decision is final. Typical consensus algorithms make progress when any majority of their servers is available; for example, a cluster of 5 servers can continue to operate even if 2 servers fail. If more servers fail, they stop making progress (but will never return an incorrect result). Consensus typically arises in the context of replicated state machines , a general approach to building fault-tolerant systems. Each server has a state machine and a log . The state machine is the component that we want to make fault-tolerant , such as a hash table. It will appear to clients that they are interacting with a single, reliable state machine, even if a minority of the servers in the cluster fail. Each state machine takes as input commands from its log . In our hash table example, the log would include commands like set x to 3 . A consensus algorithm is used to agree on the commands in the servers' logs. The consensus algorithm must ensure that if any state machine applies set x to 3 as the n*th command, no other **state machine* will ever apply a different n*th command. As a result, each **state machine* processes the same series of commands and thus produces the same series of results and arrives at the same series of states. NOTE :","title":"Hold on\u2014what is consensus?"},{"location":"Distributed-computing/Theory/Protocol/Raft/github-The-Raft-Consensus-Algorithm/#raft#visualization","text":"Here's a Raft cluster running in your browser. You can interact with it to see Raft in action. Five servers are shown on the left, and their logs are shown on the right. We hope to create a screencast soon to explain what's going on. This visualization ( RaftScope ) is still pretty rough around the edges; pull requests would be very welcome. The Secret Lives of Data is a different visualization of Raft. It's more guided and less interactive, so it may be a gentler starting point.","title":"Raft Visualization"},{"location":"Distributed-computing/Theory/Protocol/Raft/github-The-Raft-Consensus-Algorithm/#publications","text":"This is \"the Raft paper\", which describes Raft in detail: In Search of an Understandable Consensus Algorithm (Extended Version) by Diego Ongaro and John Ousterhout . A slightly shorter version of this paper received a Best Paper Award at the 2014 USENIX Annual Technical Conference . Diego Ongaro's Ph.D. dissertation expands on the content of the paper in much more detail, and it includes a simpler cluster membership change algorithm.","title":"Publications"},{"location":"Distributed-computing/Theory/Protocol/Raft/paper-In-Search-of-an-Understandable-Consensus-Algorithm/","text":"usenix In Search of an Understandable Consensus Algorithm Diego Ongaro and John Ousterhout Stanford University Abstract Raft is a consensus algorithm for managing a replicated log . It produces a result equivalent to (multi-)Paxos, and it is as efficient as Paxos, but its structure is different from Paxos; this makes Raft more understandable than Paxos and also provides a better foundation for building practical systems. In order to enhance understandability, Raft separates the key elements of consensus, such as leader election , log replication , and safety , and it enforces a stronger degree of coherency to reduce the number of states that must be considered. Results from a user study demonstrate that Raft is easier for students to learn than Paxos. Raft also includes a new mechanism for changing the cluster membership, which uses overlapping majorities to guarantee safety. 1 Introduction Consensus algorithms allow a collection of machines to work as a coherent group that can survive the failures of some of its members. Because of this, they play a key role in building reliable large-scale software systems. Paxos [13, 14] has dominated the discussion of consensus algorithms over the last decade: most implementations of consensus are based on Paxos or influenced by it, and Paxos has become the primary vehicle used to teach students about consensus . Unfortunately, Paxos is quite difficult to understand, in spite of numerous attempts to make it more approachable. Furthermore, its architecture requires complex changes to support practical systems. As a result, both system builders and students struggle with Paxos. After struggling with Paxos ourselves, we set out to find a new consensus algorithm that could provide a better foundation for system building and education. Our approach was unusual in that our primary goal was understandability: could we define a consensus algorithm for practical systems and describe it in a way that is significantly easier to learn than Paxos? Furthermore, we wanted the algorithm to facilitate the development of intuitions that are essential for system builders. It was important not just for the algorithm to work, but for it to be obvious why it works. The result of this work is a consensus algorithm called Raft . In designing Raft we applied specific techniques to improve understandability, including decomposition (Raft separates leader election , log replication , and safety ) and state space reduction (relative to Paxos, Raft reduces the degree of nondeterminism and the ways servers can be inconsistent with each other). A user study with 43 students at two universities shows that Raft is significantly easier to understand than Paxos: after learning both algorithms, 33 of these students were able to answer questions about Raft better than questions about Paxos. Raft is similar in many ways to existing consensus algorithms (most notably, Oki and Liskov\u2019s Viewstamped Replication [27, 20]), but it has several novel features: Strong leader : Raft uses a stronger form of leadership than other consensus algorithms . For example, log entries only flow from the leader to other servers. This simplifies the management of the replicated log and makes Raft easier to understand. Leader election : Raft uses randomized timers to elect leaders. This adds only a small amount of mechanism to the heartbeats already required for any consensus algorithm, while resolving conflicts simply and rapidly. Membership changes : Raft\u2019s mechanism for changing the set of servers in the cluster uses a new joint consensus approach where the majorities of two different configurations overlap during transitions. This allows the cluster to continue operating normally during configuration changes NOTE : \u8fd9\u662f\u4ec0\u4e48\u610f\u601d\uff1f\u662f\u6307\u5f53leader\u66f4\u6362\u7684\u65f6\u5019\uff1f We believe that Raft is superior to Paxos and other consensus algorithms , both for educational purposes and as a foundation for implementation. It is simpler and more understandable than other algorithms; it is described completely enough to meet the needs of a practical system; it has several open-source implementations and is used by several companies; its safety properties have been formally specified and proven; and its efficiency is comparable to other algorithms. The remainder of the paper introduces the replicated state machine problem (Section 2), discusses the strengths and weaknesses of Paxos (Section 3), describes our general approach to understandability (Section 4), presents the Raft consensus algorithm (Sections 5\u20137), evaluates Raft (Section 8), and discusses related work (Section 9). A few elements of the Raft algorithm have been omitted here because of space limitations, but they are available in an extended technical report [29]. The additional material describes how clients interact with the system, and how space in the Raft log can be reclaimed. 2 Replicated state machines NOTE: \u8fd9\u4e00\u6bb5\u5173\u4e8eReplicated state machine\u7684\u4ecb\u7ecd\u662f\u975e\u5e38\u597d\u7684\uff0c\u5bb9\u6613\u7406\u89e3\uff0c\u5e76\u4e14\u56fe\u793a\u4e5f\u975e\u5e38\u5f62\u8c61 Consensus algorithms typically arise in the context of replicated state machines [33]. In this approach, state machines on a collection of servers compute identical copies of the same state and can continue operating even if some of the servers are down. Replicated state machines are used to solve a variety of fault tolerance problems in distributed systems. For example, large-scale systems that have a single cluster leader , such as GFS [7], HDFS [34], and RAMCloud [30], typically use a separate replicated state machine to manage leader election and store configuration information that must survive leader crashes. Examples of replicated state machines include Chubby [2] and ZooKeeper [9]. NOTE : Chubby \uff0c ZooKeeper Figure 1: Replicated state machine architecture. The consensus algorithm manages a replicated log containing state machine commands from clients. The state machines process identical sequences of commands from the logs, so they produce the same outputs. Replicated state machines are typically implemented using a replicated log , as shown in Figure 1. Each server stores a log containing a series of commands, which its state machine executes in order. Each log contains the same commands in the same order, so each state machine processes the same sequence of commands. Since the state machines are deterministic, each computes the same state and the same sequence of outputs. Keeping the replicated log consistent is the job of the consensus algorithm . The consensus module on a server receives commands from clients and adds them to its log . It communicates with the consensus modules on other servers to ensure that every log eventually contains the same requests in the same order, even if some servers fail. Once commands are properly replicated, each server\u2019s state machine processes them in log order, and the outputs are returned to clients. As a result, the servers appear to form a single, highly reliable state machine. Consensus algorithms for practical systems typically have the following properties: They ensure safety (never returning an incorrect result) under all non-Byzantine conditions, including network delays, partitions, and packet loss, duplication, and reordering. They are fully functional (available) as long as any majority of the servers are operational and can communicate with each other and with clients. Thus, a typical cluster of five servers can tolerate the failure of any two servers. Servers are assumed to fail by stopping; they may later recover from state on stable storage and rejoin the cluster. They do not depend on timing to ensure the consistency of the logs: faulty clocks and extreme message delays can, at worst, cause availability problems. In the common case, a command can complete as soon as a majority of the cluster has responded to a single round of remote procedure calls; a minority of slow servers need not impact overall system performance 5 The Raft consensus algorithm Raft is an algorithm for managing a replicated log of the form described in Section 2. Figure 2 summarizes the algorithm in condensed\uff08\u6d53\u7f29\u7684\uff0c\u7b80\u660e\u627c\u8981\u7684\uff09 form for reference, and Figure 3 lists key properties of the algorithm;the elements of these figures are discussed piecewise over the rest of this section. NOTE : \u5728raft\u8fd0\u884c\u8fc7\u7a0b\u4e2d\uff0cleader\u627f\u62c5\u7740\u6838\u5fc3\u7684\u89d2\u8272\uff0c\u5b83\u662fraft\u7b97\u6cd5\u7684\u6838\u5fc3\u6240\u5728\uff0c\u611f\u89c9\u4ece\u8fd9\u4e2a\u89d2\u5ea6\u6765\u770b\uff0craft\u7b97\u6cd5\u662f\u4e2d\u5fc3\u5316\u7684\uff0c\u663e\u7136\u5728\u4e2d\u5fc3\u5316\u7684cluster\u4e2d\uff0c\u4e00\u4e2a\u663e\u8457\u7684\u7279\u70b9\u5c31\u662f\u8282\u70b9\u662f\u6709\u89d2\u8272\u5c5e\u6027\u7684\uff08\u6bd4\u5982\u5728raft\u4e2d\uff0c\u5c31\u6709leader\uff0cfollower\u7b49\u89d2\u8272\uff09\uff0c\u800c\u5728\u53bb\u4e2d\u5fc3\u5316\u7684cluster\u4e2d\uff0c\u8282\u70b9\u662f\u4e0d\u5177\u5907\u89d2\u8272\u5c5e\u6027\u7684\uff1b Raft implements consensus by first electing a distinguished leader, then giving the leader complete responsibility for managing the replicated log . The leader accepts log entries from clients, replicates them on other servers, and tells servers when it is safe to apply log entries to their state machines . Having a leader simplifies the management of the replicated log . For example,the leader can decide where to place new entries in the log without consulting other servers, and data flows in a simple fashion from the leader to other servers. A leader can fail or become disconnected from the other servers, in which case a new leader is elected. Given the leader approach, Raft decomposes the consensus problem into three relatively independent sub problems, which are discussed in the subsections that follow: Leader election: a new leader must be chosen when an existing leader fails (Section 5.2). Log replication: the leader must accept log entries from clients and replicate them across the cluster, forcing the other logs to agree with its own (Section 5.3). NOTE : \u5176\u5b9e\u5728redis\u4e2d\uff0creplication\u7684\u8fc7\u7a0b\u548c\u8fd9\u662f\u975e\u5e38\u7c7b\u4f3c\u7684\uff1b Safety: the key safety property for Raft is the State Machine Safety Property in Figure 3: if any server has applied a particular log entry to its state machine , then no other server may apply a different command for the same log index . Section 5.4 describes how Raft ensures this property; the solution involves an additional restriction on the election mechanism described in Section 5.2. After presenting the consensus algorithm ,this section discusses the issue of availability and the role of timing in the system. Figure2 State Persistent state on all servers: (Updated on stable storage before responding to RPCs) | | | | ------------- | ------------------------------------------------------------ | | currentTerm | latest term server has seen (initialized to 0 on first boot, increases monotonically\uff08\u5355\u8c03\u7684\uff09) | | votedFor | candidateId that received vote in current term (or null if none) | | log[] | log entries; each entry contains command for state machine , and term when entry was received by leader (first index is 1) | Volatile state on all servers: commitIndex index of highest log entry known to be committed (initialized to 0, increases monotonically) lastApplied index of highest log entry applied to state machine (initialized to 0, increases monotonically) Figure2: A condensed summary of the Raft consensus algorithm (excluding membership changes and log compaction). The server behavior in the upper-left box is described as a set of rules that trigger independently and repeatedly. Section numbers such as \u00a75.2 indicate where particular features are discussed. A formal specification [28] describes the algorithm more precisely. Figure 3 Election Safety : at most one leader can be elected in a given term. \u00a75.2 NOTE : \u5982\u4f55\u6765\u4fdd\u8bc1\uff1f\u5728distributed system\u4e2d\uff0c\u8fd9\u6837\u7684\u95ee\u9898\u662f\u975e\u5e38\u666e\u904d\u7684\uff1a\u5982\u4f55\u4fdd\u8bc1\u53ea\u6709\u4e00\u4e2anode\u88ab\u9009\u4e3aleader\uff1f\u5728redis\u4e2d\uff0c\u8fd9\u4e2a\u95ee\u9898\u5c31\u662f\u5982\u4f55\u4fdd\u8bc1\u53ea\u6709\u4e00\u4e2aslave\u88ab\u9009\u4e3amaster\uff1f Leader Append-Only : a leader never overwrites or deletes entries in its log; it only appends new entries. \u00a75.3 Log Matching : if two logs contain an entry with the same index and term , then the logs are identical in all entries up through the given index. \u00a75.3 Leader Completeness : if a log entry is committed in a given term , then that entry will be present in the logs of the leaders for all higher-numbered terms. \u00a75.4 State Machine Safety : if a server has applied a log entry at a given index to its state machine, no other server will ever apply a different log entry for the same index. \u00a75.4.3 Figure 3: Raft guarantees that each of these properties is true at all times. The section numbers indicate where each property is discussed. 5.1 Raft basics A Raft cluster contains several servers; five is a typical number, which allows the system to tolerate two failures. At any given time each server is in one of three states: leader , follower , or candidate . In normal operation there is exactly one leader and all of the other servers are followers . Followers are passive: they issue no requests on their own but simply respond to requests from leaders and candidates . The leader handles all client requests (if a client contacts a follower, the follower redirects it to the leader). The third state, candidate, is used to elect a new leader as described in Section 5.2. Figure 4 shows the states and their transitions; the transitions are discussed below. Figure 4 Figure 4: Server states. Followers only respond to requests from other servers. If a follower receives no communication, it becomes a candidate and initiates an election. A candidate that receives votes from a majority of the full cluster becomes the new leader. Leaders typically operate until they fail. Raft divides time into terms of arbitrary length, as shown in Figure 5. Terms are numbered with consecutive integers. Each term begins with an election , in which one or more candidates attempt to become leader as described in Section 5.2. If a candidate wins the election, then it serves as leader for the rest of the term. In some situations an election will result in a split vote . In this case the term will end with no leader; a new term (with a new election) will begin shortly. Raft ensures that there is at most one leader in a given term. Figure 5 Figure 5: Time is divided into terms , and each term begins with an election . After a successful election, a single leader manages the cluster until the end of the term. Some elections fail, in which case the term ends without choosing a leader. The transitions between terms may be observed at different times on different servers. Different servers may observe the transitions between terms at different times, and in some situations a server may not observe an election or even entire terms. Terms act as a logical clock [12] in Raft, and they allow servers to detect obsolete information \uff08\u9648\u65e7\u4fe1\u606f\uff09 such as stale leaders. Each server stores a current term number, which increases monotonically over time. Current terms are exchanged whenever servers communicate; if one server\u2019s current term is smaller than the other\u2019s, then it updates its current term to the larger value . If a candidate or leader discovers that its term is out of date, it immediately reverts to follower state . If a server receives a request with a stale term number, it rejects the request. NOTE : \u8981\u60f3\u51c6\u786e\u5730\u7406\u89e3term\u7684\u7528\u9014\uff0c\u5fc5\u987b\u8981\u9605\u8bfb\u8bba\u6587[12] \uff0c\u5176\u5b9e\u5728\u8fd9\u7bc7\u8bba\u6587\u4e2d\u5c31\u8bf4\u660e\u4e86\uff1a A distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events. \u5176\u5b9e\u5b83\u975e\u5e38\u4e86\u7c7b\u4f3c Lamport timestamps \uff0c\u663e\u7136\uff0craft\u4e2d\uff0c\u4f7f\u7528term\u6765\u4f5c\u4e3alogical clock\uff0c\u4f7f\u7528\u5b83\u6765order message\uff1b\u5728 Understanding Lamport Timestamps with Python\u2019s multiprocessing library \u4e2d\uff0c\u5bf9\u5b83\u7684\u89e3\u91ca\u662f\u975e\u5e38\u597d\u7684\uff0c\u9700\u8981\u53c2\u8003\u90a3\u7bc7\u6587\u7ae0\uff1b \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0craft\u4e2d\u7684term\u5176\u5b9e\u5e76\u4e0d\u662f\u5b8c\u5168\u7684\u4f7f\u7528\u7684 Lamport timestamps \uff0c\u800c\u662f\u501f\u9274\u4e8e\u5b83\uff0c\u5e76\u8fdb\u884c\u4e86\u8c03\u6574\uff0c\u5b83\u7684\u8c03\u6574\u662f\u4e3a\u4e86\u9002\u5e94raft\u4e2d\u5728leader election\u9636\u6bb5\u7684\u5404\u79cd\u95ee\u9898\u7684\uff1b NOTE : \u7b2c\u4e00\u53e5\u8bdd\u662f\u4ec0\u4e48\u610f\u601d\uff1f Raft servers communicate using remote procedure calls (RPCs), and the consensus algorithm requires only two types of RPCs. RequestVote RPCs are initiated by candidates during elections (Section 5.2), and AppendEntries RPCs are initiated by leaders to replicate log entries and to provide a form of heartbeat (Section 5.3). Servers retry RPCs if they do not receive a response in a timely manner, and they issue RPCs in parallel for best performance. 5.2 Leader election Raft uses a heartbeat mechanism to trigger leader election . When servers start up, they begin as followers. A server remains in follower state as long as it receives valid RPCs from a leader or candidate. Leaders send periodic heartbeats(AppendEntries RPCs that carry no logentries) to all followers in order to maintain their authority. If a follower receives no communication over a period of time called the election timeout , then it assumes there is no viable leader and begins an election to choose a new leader. To begin an election, a follower increments its current term and transitions to candidate state . It then votes for itself and issues RequestVote RPCs in parallel to each of the other servers in the cluster. A candidate continues in this state until one of three things happens: (a) it wins the election, (b) another server establishes itself as leader, or \u00a9 a period of time goes by with no winner. These outcomes are discussed separately in the paragraphs below. A candidate wins an election if it receives votes from a majority of the servers in the full cluster for the same term. Each server will vote for at most one candidate in a given term, on a first-come-first-served basis (note: Section 5.4 adds an additional restriction on votes). The majority rule ensures that at most one candidate can win the election for a particular term (the Election Safety Property in Figure 3). Once a candidate wins an election, it becomes leader. It then sends heartbeat messages to all of the other servers to establish its authority and prevent new elections. While waiting for votes, a candidate may receive an AppendEntries RPC from another server claiming to be leader. If the leader\u2019s term (includedin its RPC) is at least as large as the candidate\u2019s current term, then the candidate recognizes the leader as legitimate and returns to follower state. If the term in the RPC is smaller than the candidate\u2019s current term, then the candidate rejects the RPC and continues in candidate state. The third possible outcome is that a candidate neither wins nor loses the election: if many followers become candidates at the same time, votes could be split so that no candidate obtains a majority. When this happens, each candidate will time out and start a new election by incrementing its term and initiating another round of Request-Vote RPCs. However, without extra measures split votes could repeat indefinitely. Raft uses randomized election timeouts to ensure that split votes are rare and that they are resolved quickly. To prevent split votes in the first place, election timeouts are chosen randomly from a fixed interval (e.g., 150\u2013300ms). This spreads out the servers so that in most cases only a single server will time out; it wins the election and sends heartbeats before any other servers time out. The same mechanism is used to handle split votes. Each candidate restarts its randomized election timeout at the start of an election, and it waits for that timeout to elapse before starting the next election; this reduces the likelihood of another split vote in the new election. Section 8.3 shows that this approach elects a leader rapidly. Elections are an example of how understandability guided our choice between design alternatives. Initially we planned to use a ranking system: each candidate was assigned a unique rank, which was used to select between competing candidates. If a candidate discovered another candidate with higher rank, it would return to follower state so that the higher ranking candidate could more easily win the next election. We found that this approach created subtle issues around availability (a lower-ranked server might need to time out and become a candidate again if a higher-ranked server fails, but if it does so too soon, it can reset progress towards electing a leader). We made adjustments to the algorithm several times, but after each adjustment new corner cases appeared. Eventually we concluded that the randomized retry approach is more obvious and understandable. 5.3 Log replication Once a leader has been elected, it begins servicing client requests. Each client request contains a command to be executed by the replicated state machines . The leader appends the command to its log as a new entry, then issues AppendEntries RPCs in parallel to each of the other servers to replicate the entry. When the entry has been safely replicated (as described below), the leader applies the entry to its state machine and returns the result of that execution to the client. If followers crash or run slowly, or if network packets are lost, the leader retries Append-Entries RPCs indefinitely (even after it has responded to the client) until all followers eventually store all log entries. Logs are organized as shown in Figure 6. Each log entry stores a state machine command along with the term number when the entry was received by the leader. The term numbers in log entries are used to detect inconsistencies between logs and to ensure some of the properties in Figure 3. Each log entry also has an integer index identifying its position in the log. References [12] [12] L AMPORT , L. Time, clocks, and the ordering of events in a distributed system . Commununications of the ACM 21, 7 (July 1978), 558\u2013565.","title":"Introduction"},{"location":"Distributed-computing/Theory/Protocol/Raft/paper-In-Search-of-an-Understandable-Consensus-Algorithm/#usenix#in#search#of#an#understandable#consensus#algorithm","text":"Diego Ongaro and John Ousterhout Stanford University","title":"usenix In Search of an Understandable Consensus Algorithm"},{"location":"Distributed-computing/Theory/Protocol/Raft/paper-In-Search-of-an-Understandable-Consensus-Algorithm/#abstract","text":"Raft is a consensus algorithm for managing a replicated log . It produces a result equivalent to (multi-)Paxos, and it is as efficient as Paxos, but its structure is different from Paxos; this makes Raft more understandable than Paxos and also provides a better foundation for building practical systems. In order to enhance understandability, Raft separates the key elements of consensus, such as leader election , log replication , and safety , and it enforces a stronger degree of coherency to reduce the number of states that must be considered. Results from a user study demonstrate that Raft is easier for students to learn than Paxos. Raft also includes a new mechanism for changing the cluster membership, which uses overlapping majorities to guarantee safety.","title":"Abstract"},{"location":"Distributed-computing/Theory/Protocol/Raft/paper-In-Search-of-an-Understandable-Consensus-Algorithm/#1#introduction","text":"Consensus algorithms allow a collection of machines to work as a coherent group that can survive the failures of some of its members. Because of this, they play a key role in building reliable large-scale software systems. Paxos [13, 14] has dominated the discussion of consensus algorithms over the last decade: most implementations of consensus are based on Paxos or influenced by it, and Paxos has become the primary vehicle used to teach students about consensus . Unfortunately, Paxos is quite difficult to understand, in spite of numerous attempts to make it more approachable. Furthermore, its architecture requires complex changes to support practical systems. As a result, both system builders and students struggle with Paxos. After struggling with Paxos ourselves, we set out to find a new consensus algorithm that could provide a better foundation for system building and education. Our approach was unusual in that our primary goal was understandability: could we define a consensus algorithm for practical systems and describe it in a way that is significantly easier to learn than Paxos? Furthermore, we wanted the algorithm to facilitate the development of intuitions that are essential for system builders. It was important not just for the algorithm to work, but for it to be obvious why it works. The result of this work is a consensus algorithm called Raft . In designing Raft we applied specific techniques to improve understandability, including decomposition (Raft separates leader election , log replication , and safety ) and state space reduction (relative to Paxos, Raft reduces the degree of nondeterminism and the ways servers can be inconsistent with each other). A user study with 43 students at two universities shows that Raft is significantly easier to understand than Paxos: after learning both algorithms, 33 of these students were able to answer questions about Raft better than questions about Paxos. Raft is similar in many ways to existing consensus algorithms (most notably, Oki and Liskov\u2019s Viewstamped Replication [27, 20]), but it has several novel features: Strong leader : Raft uses a stronger form of leadership than other consensus algorithms . For example, log entries only flow from the leader to other servers. This simplifies the management of the replicated log and makes Raft easier to understand. Leader election : Raft uses randomized timers to elect leaders. This adds only a small amount of mechanism to the heartbeats already required for any consensus algorithm, while resolving conflicts simply and rapidly. Membership changes : Raft\u2019s mechanism for changing the set of servers in the cluster uses a new joint consensus approach where the majorities of two different configurations overlap during transitions. This allows the cluster to continue operating normally during configuration changes NOTE : \u8fd9\u662f\u4ec0\u4e48\u610f\u601d\uff1f\u662f\u6307\u5f53leader\u66f4\u6362\u7684\u65f6\u5019\uff1f We believe that Raft is superior to Paxos and other consensus algorithms , both for educational purposes and as a foundation for implementation. It is simpler and more understandable than other algorithms; it is described completely enough to meet the needs of a practical system; it has several open-source implementations and is used by several companies; its safety properties have been formally specified and proven; and its efficiency is comparable to other algorithms. The remainder of the paper introduces the replicated state machine problem (Section 2), discusses the strengths and weaknesses of Paxos (Section 3), describes our general approach to understandability (Section 4), presents the Raft consensus algorithm (Sections 5\u20137), evaluates Raft (Section 8), and discusses related work (Section 9). A few elements of the Raft algorithm have been omitted here because of space limitations, but they are available in an extended technical report [29]. The additional material describes how clients interact with the system, and how space in the Raft log can be reclaimed.","title":"1 Introduction"},{"location":"Distributed-computing/Theory/Protocol/Raft/paper-In-Search-of-an-Understandable-Consensus-Algorithm/#2#replicated#state#machines","text":"NOTE: \u8fd9\u4e00\u6bb5\u5173\u4e8eReplicated state machine\u7684\u4ecb\u7ecd\u662f\u975e\u5e38\u597d\u7684\uff0c\u5bb9\u6613\u7406\u89e3\uff0c\u5e76\u4e14\u56fe\u793a\u4e5f\u975e\u5e38\u5f62\u8c61 Consensus algorithms typically arise in the context of replicated state machines [33]. In this approach, state machines on a collection of servers compute identical copies of the same state and can continue operating even if some of the servers are down. Replicated state machines are used to solve a variety of fault tolerance problems in distributed systems. For example, large-scale systems that have a single cluster leader , such as GFS [7], HDFS [34], and RAMCloud [30], typically use a separate replicated state machine to manage leader election and store configuration information that must survive leader crashes. Examples of replicated state machines include Chubby [2] and ZooKeeper [9]. NOTE : Chubby \uff0c ZooKeeper Figure 1: Replicated state machine architecture. The consensus algorithm manages a replicated log containing state machine commands from clients. The state machines process identical sequences of commands from the logs, so they produce the same outputs. Replicated state machines are typically implemented using a replicated log , as shown in Figure 1. Each server stores a log containing a series of commands, which its state machine executes in order. Each log contains the same commands in the same order, so each state machine processes the same sequence of commands. Since the state machines are deterministic, each computes the same state and the same sequence of outputs. Keeping the replicated log consistent is the job of the consensus algorithm . The consensus module on a server receives commands from clients and adds them to its log . It communicates with the consensus modules on other servers to ensure that every log eventually contains the same requests in the same order, even if some servers fail. Once commands are properly replicated, each server\u2019s state machine processes them in log order, and the outputs are returned to clients. As a result, the servers appear to form a single, highly reliable state machine. Consensus algorithms for practical systems typically have the following properties: They ensure safety (never returning an incorrect result) under all non-Byzantine conditions, including network delays, partitions, and packet loss, duplication, and reordering. They are fully functional (available) as long as any majority of the servers are operational and can communicate with each other and with clients. Thus, a typical cluster of five servers can tolerate the failure of any two servers. Servers are assumed to fail by stopping; they may later recover from state on stable storage and rejoin the cluster. They do not depend on timing to ensure the consistency of the logs: faulty clocks and extreme message delays can, at worst, cause availability problems. In the common case, a command can complete as soon as a majority of the cluster has responded to a single round of remote procedure calls; a minority of slow servers need not impact overall system performance","title":"2 Replicated state machines"},{"location":"Distributed-computing/Theory/Protocol/Raft/paper-In-Search-of-an-Understandable-Consensus-Algorithm/#5#the#raft#consensus#algorithm","text":"Raft is an algorithm for managing a replicated log of the form described in Section 2. Figure 2 summarizes the algorithm in condensed\uff08\u6d53\u7f29\u7684\uff0c\u7b80\u660e\u627c\u8981\u7684\uff09 form for reference, and Figure 3 lists key properties of the algorithm;the elements of these figures are discussed piecewise over the rest of this section. NOTE : \u5728raft\u8fd0\u884c\u8fc7\u7a0b\u4e2d\uff0cleader\u627f\u62c5\u7740\u6838\u5fc3\u7684\u89d2\u8272\uff0c\u5b83\u662fraft\u7b97\u6cd5\u7684\u6838\u5fc3\u6240\u5728\uff0c\u611f\u89c9\u4ece\u8fd9\u4e2a\u89d2\u5ea6\u6765\u770b\uff0craft\u7b97\u6cd5\u662f\u4e2d\u5fc3\u5316\u7684\uff0c\u663e\u7136\u5728\u4e2d\u5fc3\u5316\u7684cluster\u4e2d\uff0c\u4e00\u4e2a\u663e\u8457\u7684\u7279\u70b9\u5c31\u662f\u8282\u70b9\u662f\u6709\u89d2\u8272\u5c5e\u6027\u7684\uff08\u6bd4\u5982\u5728raft\u4e2d\uff0c\u5c31\u6709leader\uff0cfollower\u7b49\u89d2\u8272\uff09\uff0c\u800c\u5728\u53bb\u4e2d\u5fc3\u5316\u7684cluster\u4e2d\uff0c\u8282\u70b9\u662f\u4e0d\u5177\u5907\u89d2\u8272\u5c5e\u6027\u7684\uff1b Raft implements consensus by first electing a distinguished leader, then giving the leader complete responsibility for managing the replicated log . The leader accepts log entries from clients, replicates them on other servers, and tells servers when it is safe to apply log entries to their state machines . Having a leader simplifies the management of the replicated log . For example,the leader can decide where to place new entries in the log without consulting other servers, and data flows in a simple fashion from the leader to other servers. A leader can fail or become disconnected from the other servers, in which case a new leader is elected. Given the leader approach, Raft decomposes the consensus problem into three relatively independent sub problems, which are discussed in the subsections that follow: Leader election: a new leader must be chosen when an existing leader fails (Section 5.2). Log replication: the leader must accept log entries from clients and replicate them across the cluster, forcing the other logs to agree with its own (Section 5.3). NOTE : \u5176\u5b9e\u5728redis\u4e2d\uff0creplication\u7684\u8fc7\u7a0b\u548c\u8fd9\u662f\u975e\u5e38\u7c7b\u4f3c\u7684\uff1b Safety: the key safety property for Raft is the State Machine Safety Property in Figure 3: if any server has applied a particular log entry to its state machine , then no other server may apply a different command for the same log index . Section 5.4 describes how Raft ensures this property; the solution involves an additional restriction on the election mechanism described in Section 5.2. After presenting the consensus algorithm ,this section discusses the issue of availability and the role of timing in the system.","title":"5 The Raft consensus algorithm"},{"location":"Distributed-computing/Theory/Protocol/Raft/paper-In-Search-of-an-Understandable-Consensus-Algorithm/#figure2","text":"","title":"Figure2"},{"location":"Distributed-computing/Theory/Protocol/Raft/paper-In-Search-of-an-Understandable-Consensus-Algorithm/#state","text":"","title":"State"},{"location":"Distributed-computing/Theory/Protocol/Raft/paper-In-Search-of-an-Understandable-Consensus-Algorithm/#persistent#state#on#all#servers","text":"(Updated on stable storage before responding to RPCs) | | | | ------------- | ------------------------------------------------------------ | | currentTerm | latest term server has seen (initialized to 0 on first boot, increases monotonically\uff08\u5355\u8c03\u7684\uff09) | | votedFor | candidateId that received vote in current term (or null if none) | | log[] | log entries; each entry contains command for state machine , and term when entry was received by leader (first index is 1) |","title":"Persistent state on all servers:"},{"location":"Distributed-computing/Theory/Protocol/Raft/paper-In-Search-of-an-Understandable-Consensus-Algorithm/#volatile#state#on#all#servers","text":"commitIndex index of highest log entry known to be committed (initialized to 0, increases monotonically) lastApplied index of highest log entry applied to state machine (initialized to 0, increases monotonically) Figure2: A condensed summary of the Raft consensus algorithm (excluding membership changes and log compaction). The server behavior in the upper-left box is described as a set of rules that trigger independently and repeatedly. Section numbers such as \u00a75.2 indicate where particular features are discussed. A formal specification [28] describes the algorithm more precisely.","title":"Volatile state on all servers:"},{"location":"Distributed-computing/Theory/Protocol/Raft/paper-In-Search-of-an-Understandable-Consensus-Algorithm/#figure#3","text":"Election Safety : at most one leader can be elected in a given term. \u00a75.2 NOTE : \u5982\u4f55\u6765\u4fdd\u8bc1\uff1f\u5728distributed system\u4e2d\uff0c\u8fd9\u6837\u7684\u95ee\u9898\u662f\u975e\u5e38\u666e\u904d\u7684\uff1a\u5982\u4f55\u4fdd\u8bc1\u53ea\u6709\u4e00\u4e2anode\u88ab\u9009\u4e3aleader\uff1f\u5728redis\u4e2d\uff0c\u8fd9\u4e2a\u95ee\u9898\u5c31\u662f\u5982\u4f55\u4fdd\u8bc1\u53ea\u6709\u4e00\u4e2aslave\u88ab\u9009\u4e3amaster\uff1f Leader Append-Only : a leader never overwrites or deletes entries in its log; it only appends new entries. \u00a75.3 Log Matching : if two logs contain an entry with the same index and term , then the logs are identical in all entries up through the given index. \u00a75.3 Leader Completeness : if a log entry is committed in a given term , then that entry will be present in the logs of the leaders for all higher-numbered terms. \u00a75.4 State Machine Safety : if a server has applied a log entry at a given index to its state machine, no other server will ever apply a different log entry for the same index. \u00a75.4.3 Figure 3: Raft guarantees that each of these properties is true at all times. The section numbers indicate where each property is discussed.","title":"Figure 3"},{"location":"Distributed-computing/Theory/Protocol/Raft/paper-In-Search-of-an-Understandable-Consensus-Algorithm/#51#raft#basics","text":"A Raft cluster contains several servers; five is a typical number, which allows the system to tolerate two failures. At any given time each server is in one of three states: leader , follower , or candidate . In normal operation there is exactly one leader and all of the other servers are followers . Followers are passive: they issue no requests on their own but simply respond to requests from leaders and candidates . The leader handles all client requests (if a client contacts a follower, the follower redirects it to the leader). The third state, candidate, is used to elect a new leader as described in Section 5.2. Figure 4 shows the states and their transitions; the transitions are discussed below.","title":"5.1 Raft basics"},{"location":"Distributed-computing/Theory/Protocol/Raft/paper-In-Search-of-an-Understandable-Consensus-Algorithm/#figure#4","text":"Figure 4: Server states. Followers only respond to requests from other servers. If a follower receives no communication, it becomes a candidate and initiates an election. A candidate that receives votes from a majority of the full cluster becomes the new leader. Leaders typically operate until they fail. Raft divides time into terms of arbitrary length, as shown in Figure 5. Terms are numbered with consecutive integers. Each term begins with an election , in which one or more candidates attempt to become leader as described in Section 5.2. If a candidate wins the election, then it serves as leader for the rest of the term. In some situations an election will result in a split vote . In this case the term will end with no leader; a new term (with a new election) will begin shortly. Raft ensures that there is at most one leader in a given term.","title":"Figure 4"},{"location":"Distributed-computing/Theory/Protocol/Raft/paper-In-Search-of-an-Understandable-Consensus-Algorithm/#figure#5","text":"Figure 5: Time is divided into terms , and each term begins with an election . After a successful election, a single leader manages the cluster until the end of the term. Some elections fail, in which case the term ends without choosing a leader. The transitions between terms may be observed at different times on different servers. Different servers may observe the transitions between terms at different times, and in some situations a server may not observe an election or even entire terms. Terms act as a logical clock [12] in Raft, and they allow servers to detect obsolete information \uff08\u9648\u65e7\u4fe1\u606f\uff09 such as stale leaders. Each server stores a current term number, which increases monotonically over time. Current terms are exchanged whenever servers communicate; if one server\u2019s current term is smaller than the other\u2019s, then it updates its current term to the larger value . If a candidate or leader discovers that its term is out of date, it immediately reverts to follower state . If a server receives a request with a stale term number, it rejects the request. NOTE : \u8981\u60f3\u51c6\u786e\u5730\u7406\u89e3term\u7684\u7528\u9014\uff0c\u5fc5\u987b\u8981\u9605\u8bfb\u8bba\u6587[12] \uff0c\u5176\u5b9e\u5728\u8fd9\u7bc7\u8bba\u6587\u4e2d\u5c31\u8bf4\u660e\u4e86\uff1a A distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events. \u5176\u5b9e\u5b83\u975e\u5e38\u4e86\u7c7b\u4f3c Lamport timestamps \uff0c\u663e\u7136\uff0craft\u4e2d\uff0c\u4f7f\u7528term\u6765\u4f5c\u4e3alogical clock\uff0c\u4f7f\u7528\u5b83\u6765order message\uff1b\u5728 Understanding Lamport Timestamps with Python\u2019s multiprocessing library \u4e2d\uff0c\u5bf9\u5b83\u7684\u89e3\u91ca\u662f\u975e\u5e38\u597d\u7684\uff0c\u9700\u8981\u53c2\u8003\u90a3\u7bc7\u6587\u7ae0\uff1b \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0craft\u4e2d\u7684term\u5176\u5b9e\u5e76\u4e0d\u662f\u5b8c\u5168\u7684\u4f7f\u7528\u7684 Lamport timestamps \uff0c\u800c\u662f\u501f\u9274\u4e8e\u5b83\uff0c\u5e76\u8fdb\u884c\u4e86\u8c03\u6574\uff0c\u5b83\u7684\u8c03\u6574\u662f\u4e3a\u4e86\u9002\u5e94raft\u4e2d\u5728leader election\u9636\u6bb5\u7684\u5404\u79cd\u95ee\u9898\u7684\uff1b NOTE : \u7b2c\u4e00\u53e5\u8bdd\u662f\u4ec0\u4e48\u610f\u601d\uff1f Raft servers communicate using remote procedure calls (RPCs), and the consensus algorithm requires only two types of RPCs. RequestVote RPCs are initiated by candidates during elections (Section 5.2), and AppendEntries RPCs are initiated by leaders to replicate log entries and to provide a form of heartbeat (Section 5.3). Servers retry RPCs if they do not receive a response in a timely manner, and they issue RPCs in parallel for best performance.","title":"Figure 5"},{"location":"Distributed-computing/Theory/Protocol/Raft/paper-In-Search-of-an-Understandable-Consensus-Algorithm/#52#leader#election","text":"Raft uses a heartbeat mechanism to trigger leader election . When servers start up, they begin as followers. A server remains in follower state as long as it receives valid RPCs from a leader or candidate. Leaders send periodic heartbeats(AppendEntries RPCs that carry no logentries) to all followers in order to maintain their authority. If a follower receives no communication over a period of time called the election timeout , then it assumes there is no viable leader and begins an election to choose a new leader. To begin an election, a follower increments its current term and transitions to candidate state . It then votes for itself and issues RequestVote RPCs in parallel to each of the other servers in the cluster. A candidate continues in this state until one of three things happens: (a) it wins the election, (b) another server establishes itself as leader, or \u00a9 a period of time goes by with no winner. These outcomes are discussed separately in the paragraphs below. A candidate wins an election if it receives votes from a majority of the servers in the full cluster for the same term. Each server will vote for at most one candidate in a given term, on a first-come-first-served basis (note: Section 5.4 adds an additional restriction on votes). The majority rule ensures that at most one candidate can win the election for a particular term (the Election Safety Property in Figure 3). Once a candidate wins an election, it becomes leader. It then sends heartbeat messages to all of the other servers to establish its authority and prevent new elections. While waiting for votes, a candidate may receive an AppendEntries RPC from another server claiming to be leader. If the leader\u2019s term (includedin its RPC) is at least as large as the candidate\u2019s current term, then the candidate recognizes the leader as legitimate and returns to follower state. If the term in the RPC is smaller than the candidate\u2019s current term, then the candidate rejects the RPC and continues in candidate state. The third possible outcome is that a candidate neither wins nor loses the election: if many followers become candidates at the same time, votes could be split so that no candidate obtains a majority. When this happens, each candidate will time out and start a new election by incrementing its term and initiating another round of Request-Vote RPCs. However, without extra measures split votes could repeat indefinitely. Raft uses randomized election timeouts to ensure that split votes are rare and that they are resolved quickly. To prevent split votes in the first place, election timeouts are chosen randomly from a fixed interval (e.g., 150\u2013300ms). This spreads out the servers so that in most cases only a single server will time out; it wins the election and sends heartbeats before any other servers time out. The same mechanism is used to handle split votes. Each candidate restarts its randomized election timeout at the start of an election, and it waits for that timeout to elapse before starting the next election; this reduces the likelihood of another split vote in the new election. Section 8.3 shows that this approach elects a leader rapidly. Elections are an example of how understandability guided our choice between design alternatives. Initially we planned to use a ranking system: each candidate was assigned a unique rank, which was used to select between competing candidates. If a candidate discovered another candidate with higher rank, it would return to follower state so that the higher ranking candidate could more easily win the next election. We found that this approach created subtle issues around availability (a lower-ranked server might need to time out and become a candidate again if a higher-ranked server fails, but if it does so too soon, it can reset progress towards electing a leader). We made adjustments to the algorithm several times, but after each adjustment new corner cases appeared. Eventually we concluded that the randomized retry approach is more obvious and understandable.","title":"5.2 Leader election"},{"location":"Distributed-computing/Theory/Protocol/Raft/paper-In-Search-of-an-Understandable-Consensus-Algorithm/#53#log#replication","text":"Once a leader has been elected, it begins servicing client requests. Each client request contains a command to be executed by the replicated state machines . The leader appends the command to its log as a new entry, then issues AppendEntries RPCs in parallel to each of the other servers to replicate the entry. When the entry has been safely replicated (as described below), the leader applies the entry to its state machine and returns the result of that execution to the client. If followers crash or run slowly, or if network packets are lost, the leader retries Append-Entries RPCs indefinitely (even after it has responded to the client) until all followers eventually store all log entries. Logs are organized as shown in Figure 6. Each log entry stores a state machine command along with the term number when the entry was received by the leader. The term numbers in log entries are used to detect inconsistencies between logs and to ensure some of the properties in Figure 3. Each log entry also has an integer index identifying its position in the log.","title":"5.3 Log replication"},{"location":"Distributed-computing/Theory/Protocol/Raft/paper-In-Search-of-an-Understandable-Consensus-Algorithm/#references","text":"","title":"References"},{"location":"Distributed-computing/Theory/Protocol/Raft/paper-In-Search-of-an-Understandable-Consensus-Algorithm/#12","text":"[12] L AMPORT , L. Time, clocks, and the ordering of events in a distributed system . Commununications of the ACM 21, 7 (July 1978), 558\u2013565.","title":"[12]"},{"location":"Distributed-computing/Theory/Protocol/Raft/wikipedia-Raft/","text":"wikipedia Raft (computer science) Raft is a consensus algorithm designed as an alternative to Paxos . It was meant to be more understandable than Paxos by means of separation of logic, but it is also formally proven safe and offers some additional features.[ 1] Raft offers a generic way to distribute(\u5206\u6563) a state machine across a cluster of computing systems, ensuring that each node in the cluster agrees upon the same series of state transitions . It has a number of open-source reference implementations, with full-specification implementations in Go , C++ , Java , and Scala .[ 2] It is named after Reliable, Replicated, Redundant, And Fault-Tolerant.[ 3] Raft is not a Byzantine fault tolerant algorithm: the nodes trust the elected leader.[ 1] NOTE : \u53ef\u4ee5\u7ed3\u5408redis\u6765\u7406\u89e3Raft\uff1b NOTE : raft\u7b97\u6cd5\u7684\u8bbe\u8ba1\u8003\u8651\u5230\u4e86 consensus \u548cconsistency Basics Raft achieves consensus via an elected leader . A server in a raft cluster is either a leader or a follower , and can be a candidate in the precise case of an election (leader unavailable). The leader is responsible for log replication to the followers. It regularly informs the followers of its existence by sending a heartbeat message . Each follower has a timeout (typically between 150 and 300 ms) in which it expects the heartbeat from the leader. The timeout is reset on receiving the heartbeat. If no heartbeat is received the follower changes its status to candidate and starts a leader election .[ 1] [ 4] Approach of the consensus problem in Raft Raft implements consensus by a leader approach. The cluster has one and only one elected leader which is fully responsible for managing log replication on the other servers of the cluster. It means that the leader can decide on new entries placement and establishment of data flow between it and the other servers without consulting other servers. A leader leads until it fails or disconnects, in which case a new leader is elected. The consensus problem is decomposed in Raft into two relatively independent subproblems listed down below. Leader Election When the existing leader fails or when you start your algorithm, a new leader needs to be elected. In this case, a new term starts in the cluster. A term is an arbitrary period of time on the server during which a new leader needs to be elected. Each term starts with a leader election . If the election is completed successfully (i.e. a single leader is elected) the term keeps going with normal operations orchestrated by the new leader. If the election is a failure, a new term starts, with a new election. A leader election is started by a candidate server. A server becomes a candidate if it receives no communication by the leader over a period called the election timeout , so it assumes there is no acting leader anymore. It starts the election by increasing the term counter, voting for itself as new leader, and sending a message to all other servers requesting their vote. A server will vote only once per term , on a first-come-first-served basis. If a candidate receives a message from another server with a term number at least as large as the candidate's current term, then the candidate's election is defeated\uff08\u88ab\u6253\u8d25\u4e86\uff09 and the candidate changes into a follower and recognizes the leader as legitimate. If a candidate receives a majority of votes, then it becomes the new leader. If neither happens, e.g., because of a split vote , then a new term starts, and a new election begins.[ 1] Raft uses randomized election timeout to ensure that split votes problem are resolved quickly. This should reduce the chance of a split vote because servers won't become candidates at the same time: a single server will timeout, win the election, then become leader and sends heartbeat messages to other servers before any of the followers can become candidates.[ 1] NOTE : redis\u4e2d\uff0c\u9009\u4e3e\u662f\u7531slave\u53d1\u8d77\u7684\uff1bredis\u4e2d\u7684 currentEpoch \u548c configEpoch \u76f8\u5f53\u4e8eraft\u4e2d\u7684 term Log Replication The leader is responsible for the log replication . It accepts client requests. Each client request consists of a command to be executed by the replicated state machines in the cluster. After being appended to the leader's log as a new entry, each of the requests is forwarded to the followers as AppendEntries messages. In case of unavailability of the followers, the leader retries AppendEntries messages indefinitely\uff08\u4e0d\u5b9a\u7684\uff09, until the log entry is eventually stored by all of the followers. Once the leader receives confirmation from the majority of its followers that the entry has been replicated, the leader applies the entry to its local state machine , and the request is considered committed .[ 1] [ 4] This event also commits all previous entries in the leader's log. Once a follower learns that a log entry is committed, it applies the entry to its local state machine . This ensures consistency of the logs between all the servers through the cluster, ensuring that the safety rule of Log Matching is respected. In the case of leader crash, the logs can be left inconsistent, with some logs from the old leader not being fully replicated through the cluster. The new leader will then handle inconsistency by forcing the followers to duplicate its own log. To do so, for each of its followers, the leader will compare its log with the log from the follower, find the last entry where they agree, then delete all the entries coming after this critical entry in the follower log and replace it with its own log entries. This mechanism will restore log consistency in a cluster subject to failures. NOTE : \u53ea\u6709\u5b57leader crash\u7684\u60c5\u51b5\u4e0b\uff0c\u624d\u4f1a\u51fa\u73b0inconsistent\uff0cfellow\u7684crash\u5e76\u4e0d\u4f1a\u5bfc\u81f4inconsistent\uff1b NOTE : \u9700\u8981\u68b3\u7406log\u548cstate machine\u4e4b\u95f4\u7684\u5173\u7cfb\uff1alog\u8868\u793a\u7684\u662f\u9700\u8981\u88ab\u6267\u884c/\u53d1\u751f\u7684state transition\uff0c\u5b83\u53ea\u6709commit\u5230\u4e86node\u7684local state machine\u4e2d\u624d\u80fd\u591f\u751f\u6548\uff1b NOTE : \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0craft\u7b97\u6cd5\u4e5f\u5b9e\u73b0\u4e86 State machine replication \u3002redis\u4e2d\u4e5f\u5b9e\u73b0\u4e86 State machine replication \u3002 Safety Safety rules in Raft Raft guarantees each of these safety properties : Election safety: at most one leader can be elected in a given term. Leader Append-Only: a leader can only append new entries to its logs (it can neither overwrite nor delete entries). Log Matching: if two logs contain an entry with the same index and term, then the logs are identical in all entries up through the given index. Leader Completeness: if a log entry is committed in a given term then it will be present in the logs of the leaders since this term State Machine Safety: if a server has applied a particular log entry to its state machine, then no other server may apply a different command for the same log. The first four rules are guaranteed by the details of the algorithm described in the previous section. The State Machine Safety is guaranteed by a restriction on the election process.","title":"Introduction"},{"location":"Distributed-computing/Theory/Protocol/Raft/wikipedia-Raft/#wikipedia#raft#computer#science","text":"Raft is a consensus algorithm designed as an alternative to Paxos . It was meant to be more understandable than Paxos by means of separation of logic, but it is also formally proven safe and offers some additional features.[ 1] Raft offers a generic way to distribute(\u5206\u6563) a state machine across a cluster of computing systems, ensuring that each node in the cluster agrees upon the same series of state transitions . It has a number of open-source reference implementations, with full-specification implementations in Go , C++ , Java , and Scala .[ 2] It is named after Reliable, Replicated, Redundant, And Fault-Tolerant.[ 3] Raft is not a Byzantine fault tolerant algorithm: the nodes trust the elected leader.[ 1] NOTE : \u53ef\u4ee5\u7ed3\u5408redis\u6765\u7406\u89e3Raft\uff1b NOTE : raft\u7b97\u6cd5\u7684\u8bbe\u8ba1\u8003\u8651\u5230\u4e86 consensus \u548cconsistency","title":"wikipedia Raft (computer science)"},{"location":"Distributed-computing/Theory/Protocol/Raft/wikipedia-Raft/#basics","text":"Raft achieves consensus via an elected leader . A server in a raft cluster is either a leader or a follower , and can be a candidate in the precise case of an election (leader unavailable). The leader is responsible for log replication to the followers. It regularly informs the followers of its existence by sending a heartbeat message . Each follower has a timeout (typically between 150 and 300 ms) in which it expects the heartbeat from the leader. The timeout is reset on receiving the heartbeat. If no heartbeat is received the follower changes its status to candidate and starts a leader election .[ 1] [ 4]","title":"Basics"},{"location":"Distributed-computing/Theory/Protocol/Raft/wikipedia-Raft/#approach#of#the#consensus#problem#in#raft","text":"Raft implements consensus by a leader approach. The cluster has one and only one elected leader which is fully responsible for managing log replication on the other servers of the cluster. It means that the leader can decide on new entries placement and establishment of data flow between it and the other servers without consulting other servers. A leader leads until it fails or disconnects, in which case a new leader is elected. The consensus problem is decomposed in Raft into two relatively independent subproblems listed down below.","title":"Approach of the consensus problem in Raft"},{"location":"Distributed-computing/Theory/Protocol/Raft/wikipedia-Raft/#leader#election","text":"When the existing leader fails or when you start your algorithm, a new leader needs to be elected. In this case, a new term starts in the cluster. A term is an arbitrary period of time on the server during which a new leader needs to be elected. Each term starts with a leader election . If the election is completed successfully (i.e. a single leader is elected) the term keeps going with normal operations orchestrated by the new leader. If the election is a failure, a new term starts, with a new election. A leader election is started by a candidate server. A server becomes a candidate if it receives no communication by the leader over a period called the election timeout , so it assumes there is no acting leader anymore. It starts the election by increasing the term counter, voting for itself as new leader, and sending a message to all other servers requesting their vote. A server will vote only once per term , on a first-come-first-served basis. If a candidate receives a message from another server with a term number at least as large as the candidate's current term, then the candidate's election is defeated\uff08\u88ab\u6253\u8d25\u4e86\uff09 and the candidate changes into a follower and recognizes the leader as legitimate. If a candidate receives a majority of votes, then it becomes the new leader. If neither happens, e.g., because of a split vote , then a new term starts, and a new election begins.[ 1] Raft uses randomized election timeout to ensure that split votes problem are resolved quickly. This should reduce the chance of a split vote because servers won't become candidates at the same time: a single server will timeout, win the election, then become leader and sends heartbeat messages to other servers before any of the followers can become candidates.[ 1] NOTE : redis\u4e2d\uff0c\u9009\u4e3e\u662f\u7531slave\u53d1\u8d77\u7684\uff1bredis\u4e2d\u7684 currentEpoch \u548c configEpoch \u76f8\u5f53\u4e8eraft\u4e2d\u7684 term","title":"Leader Election"},{"location":"Distributed-computing/Theory/Protocol/Raft/wikipedia-Raft/#log#replication","text":"The leader is responsible for the log replication . It accepts client requests. Each client request consists of a command to be executed by the replicated state machines in the cluster. After being appended to the leader's log as a new entry, each of the requests is forwarded to the followers as AppendEntries messages. In case of unavailability of the followers, the leader retries AppendEntries messages indefinitely\uff08\u4e0d\u5b9a\u7684\uff09, until the log entry is eventually stored by all of the followers. Once the leader receives confirmation from the majority of its followers that the entry has been replicated, the leader applies the entry to its local state machine , and the request is considered committed .[ 1] [ 4] This event also commits all previous entries in the leader's log. Once a follower learns that a log entry is committed, it applies the entry to its local state machine . This ensures consistency of the logs between all the servers through the cluster, ensuring that the safety rule of Log Matching is respected. In the case of leader crash, the logs can be left inconsistent, with some logs from the old leader not being fully replicated through the cluster. The new leader will then handle inconsistency by forcing the followers to duplicate its own log. To do so, for each of its followers, the leader will compare its log with the log from the follower, find the last entry where they agree, then delete all the entries coming after this critical entry in the follower log and replace it with its own log entries. This mechanism will restore log consistency in a cluster subject to failures. NOTE : \u53ea\u6709\u5b57leader crash\u7684\u60c5\u51b5\u4e0b\uff0c\u624d\u4f1a\u51fa\u73b0inconsistent\uff0cfellow\u7684crash\u5e76\u4e0d\u4f1a\u5bfc\u81f4inconsistent\uff1b NOTE : \u9700\u8981\u68b3\u7406log\u548cstate machine\u4e4b\u95f4\u7684\u5173\u7cfb\uff1alog\u8868\u793a\u7684\u662f\u9700\u8981\u88ab\u6267\u884c/\u53d1\u751f\u7684state transition\uff0c\u5b83\u53ea\u6709commit\u5230\u4e86node\u7684local state machine\u4e2d\u624d\u80fd\u591f\u751f\u6548\uff1b NOTE : \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0craft\u7b97\u6cd5\u4e5f\u5b9e\u73b0\u4e86 State machine replication \u3002redis\u4e2d\u4e5f\u5b9e\u73b0\u4e86 State machine replication \u3002","title":"Log Replication"},{"location":"Distributed-computing/Theory/Protocol/Raft/wikipedia-Raft/#safety","text":"","title":"Safety"},{"location":"Distributed-computing/Theory/Protocol/Raft/wikipedia-Raft/#safety#rules#in#raft","text":"Raft guarantees each of these safety properties : Election safety: at most one leader can be elected in a given term. Leader Append-Only: a leader can only append new entries to its logs (it can neither overwrite nor delete entries). Log Matching: if two logs contain an entry with the same index and term, then the logs are identical in all entries up through the given index. Leader Completeness: if a log entry is committed in a given term then it will be present in the logs of the leaders since this term State Machine Safety: if a server has applied a particular log entry to its state machine, then no other server may apply a different command for the same log. The first four rules are guaranteed by the details of the algorithm described in the previous section. The State Machine Safety is guaranteed by a restriction on the election process.","title":"Safety rules in Raft"},{"location":"Distributed-computing/Theory/Split-brain/","text":"Split brain wanweibaike Split-brain (computing) csdn \u5206\u5e03\u5f0f\u8111\u88c2\u95ee\u9898\u5206\u6790 cnblogs \u8111\u88c2\u662f\u4ec0\u4e48\uff1fZookeeper\u662f\u5982\u4f55\u89e3\u51b3\u7684\uff1f NOTE: \u7b80\u800c\u8a00\u4e4b: quorum\uff0c\u8fc7\u534a","title":"Introduction"},{"location":"Distributed-computing/Theory/Split-brain/#split#brain","text":"","title":"Split brain"},{"location":"Distributed-computing/Theory/Split-brain/#wanweibaike#split-brain#computing","text":"","title":"wanweibaike Split-brain (computing)"},{"location":"Distributed-computing/Theory/Split-brain/#csdn","text":"","title":"csdn \u5206\u5e03\u5f0f\u8111\u88c2\u95ee\u9898\u5206\u6790"},{"location":"Distributed-computing/Theory/Split-brain/#cnblogs#zookeeper","text":"NOTE: \u7b80\u800c\u8a00\u4e4b: quorum\uff0c\u8fc7\u534a","title":"cnblogs \u8111\u88c2\u662f\u4ec0\u4e48\uff1fZookeeper\u662f\u5982\u4f55\u89e3\u51b3\u7684\uff1f"},{"location":"Distributed-computing/Theory/Time%26Ordering/","text":"Time and ordering in distributed system \u8fd9\u4e2a\u9886\u57df\u7684\u5f00\u62d3\u8005\u662f: Leslie Lamport : \"Time, Clocks, and the Ordering of Events in a Distributed System\",[ 6] which received the PODC Influential Paper Award in 2000,[ 12] Make it computational-ordering \u4e00\u3001\u5173\u4e8eorder\u548ccomputation\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u53c2\u89c1\u5de5\u7a0bdiscrete\u7684 Relation-structure-computation\\Make-it-computational \u7ae0\u8282\u3002 \u4e8c\u3001\u5728\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\uff0c\u5c24\u5176\u9700\u8981\u5904\u7406time\u3001ordering\u7684\u95ee\u9898 \u6bd4\u8f83\u597d\u7684\u6587\u7ae0 \u5728\u4e0b\u9762\u6587\u7ae0\uff0c\u5bf9Distributed Computing\u4e2d\uff0ctime\u3001ordering\u95ee\u9898\u8fdb\u884c\u4e86\u6bd4\u8f83\u597d\u7684\u63a2\u8ba8: 1\u3001cockroachlabs Living Without Atomic Clocks \u5176\u4e2d\u8ba8\u8bba\u4e86\"The Importance of Time in Distributed Systems\"\u3002 2\u3001uci Distributed Computing Concepts - Global Time and State in Distributed Systems Ordering and synchronization \u672c\u8282\u8ba8\u8bbaordering\u548csynchronization\u4e4b\u95f4\u7684\u5173\u7cfb: 1) synchronization\u7684\u76ee\u7684\u662f\u4fdd\u6301ordering \u4e0a\u8ff0\"Distributed system\u7684disordering\u672c\u8d28\"\u7ae0\u8282\u7684\u5185\u5bb9\u8bba\u8bc1\u4e86\u8fd9\u4e2a\u89c2\u70b9\u3002 2) ordering\u80fd\u591f\u7528\u4e8e\u5b9e\u73b0synchronization \u4e0a\u8ff0\"Ordering in zookeeper\"\u7ae0\u8282\u7684\u5185\u5bb9\u8bba\u8bc1\u4e86\u8fd9\u4e2a\u89c2\u70b9\u3002 Ordering and consistency \u9700\u8981\u8ba8\u8bbaordering \u548c consistency\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\u5bf9\u6b64\u8fdb\u884c\u4e86\u63cf\u8ff0: cornell Distributed Systems: Ordering and Consistency Distributed system\u7684disordering\u672c\u8d28 \u4e00\u3001Distributed system\u662f\u5178\u578bmultiple entity model\uff0c\u5373\u5b83\u662f\u7531\u591a\u4e2a\u4e0d\u540c\u7684\u3001\u72ec\u7acb\u7684entity\u6784\u6210\uff0c\u8fd9\u4e9bentity\u7684execution\u662f\u72ec\u7acb\u7684\u3001\u968f\u673a\u7684\uff0c\u8fd9\u5c31\u662f\"distributed system\u7684disordering\u672c\u8d28\"\uff1b\u4f46\u662f\u4ece\u7cfb\u7edf\u7684\u6574\u4f53\u800c\u8a00\uff0c\u5982\u679c\u8981\u5b9e\u73b0computation\u3001\u786e\u5b9a\u6027\uff0c\u5c31\u9700\u8981\u8bbe\u6cd5\u6d88\u9664\u8fd9\u79cddisordering\uff0c\u663e\u7136\u5c31\u9700\u8981\u52a0\u5165\u4e00\u5b9a\u7684control\u3002control\u673a\u5236\u7684\u76ee\u7684\u662f\u6d88\u9664disordering\uff0c\u5b9e\u73b0ordering\u3002 \u4e8c\u3001\u901a\u8fc7 wikipedia Distributed computing \u3001 Distributed-system-challenge \u4e2d\u7684\u5185\u5bb9\uff0c\u6211\u4eec\u5df2\u7ecf\u77e5\u9053\uff0c\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u4e00\u4e2a\u975e\u5e38\u5927\u7684\u8c03\u6574\u662f: lack of a global clock \u56e0\u6b64\uff0c\u5728\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u5c31\u65e0\u6cd5\u4f9d\u8d56\u4e8ephysical clock\u6765\u8fdb\u884cordering\uff1b \u5982\u4f55\u514b\u670d\u8fd9\u4e9b\u95ee\u9898 \u4f7f\u7528logical clock \u53c2\u89c1 Logical-clock \u7ae0\u8282\u3002 \u5b9e\u73b0global clock Google spanner: \"GPS\u548c\u539f\u5b50\u949f\"\uff0ccockroachlabs Living Without Atomic Clocks \u4e2d\uff0c\u603b\u7ed3\u5982\u4e0b: One of the most surprising and inspired facets of Spanner is its use of atomic clocks and GPS clocks to give participating nodes really accurate wall time synchronization. The designers of Spanner call this \u201cTrueTime\u201d , and it provides a tight bound on clock offset between any two nodes in the system. This lets them do pretty nifty things! \u53c2\u89c1 Google-spanner \u7ae0\u8282\u3002 Example Ordering in TensorFlow \u5728\u9605\u8bfbTensorFlow whitepaper\u7684\u65f6\u5019\uff0c\u5176\u4e2d\u6709\u8fd9\u6837\u7684\u63cf\u8ff0: Our implementation also sometimes inserts control dependencies to enforce orderings between otherwise independent operations as a way of, for example, controlling the peak memory usage. Ordering in zookeeper \u5728 ZooKeeper: A Distributed Coordination Service for Distributed Applications \u4e2d\u5f88\u591a\u5173\u4e8eordering\u7684\u63cf\u8ff0: The ZooKeeper implementation puts a premium(\u4fdd\u8bc1) on high performance, highly available, strictly ordered access. The strict ordering means that sophisticated synchronization primitives can be implemented at the client. ZooKeeper is ordered. ZooKeeper stamps(\u8d34\u4e0a\u65f6\u95f4\u6233) each update with a number that reflects the order of all ZooKeeper transactions. Subsequent operations can use the order to implement higher-level abstractions, such as synchronization primitives. \u4ece\u4e0a\u9762\u7684\u63cf\u8ff0\u53ef\u4ee5\u770b\u51fa\uff0c\"ordering\u80fd\u591f\u7528\u4e8e\u5b9e\u73b0synchronization\"\u3002 Sequential Consistency - Updates from a client will be applied in the order that they were sent.","title":"Introduction"},{"location":"Distributed-computing/Theory/Time%26Ordering/#time#and#ordering#in#distributed#system","text":"\u8fd9\u4e2a\u9886\u57df\u7684\u5f00\u62d3\u8005\u662f: Leslie Lamport : \"Time, Clocks, and the Ordering of Events in a Distributed System\",[ 6] which received the PODC Influential Paper Award in 2000,[ 12]","title":"Time and ordering in distributed system"},{"location":"Distributed-computing/Theory/Time%26Ordering/#make#it#computational-ordering","text":"\u4e00\u3001\u5173\u4e8eorder\u548ccomputation\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u53c2\u89c1\u5de5\u7a0bdiscrete\u7684 Relation-structure-computation\\Make-it-computational \u7ae0\u8282\u3002 \u4e8c\u3001\u5728\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\uff0c\u5c24\u5176\u9700\u8981\u5904\u7406time\u3001ordering\u7684\u95ee\u9898","title":"Make it computational-ordering"},{"location":"Distributed-computing/Theory/Time%26Ordering/#_1","text":"\u5728\u4e0b\u9762\u6587\u7ae0\uff0c\u5bf9Distributed Computing\u4e2d\uff0ctime\u3001ordering\u95ee\u9898\u8fdb\u884c\u4e86\u6bd4\u8f83\u597d\u7684\u63a2\u8ba8: 1\u3001cockroachlabs Living Without Atomic Clocks \u5176\u4e2d\u8ba8\u8bba\u4e86\"The Importance of Time in Distributed Systems\"\u3002 2\u3001uci Distributed Computing Concepts - Global Time and State in Distributed Systems","title":"\u6bd4\u8f83\u597d\u7684\u6587\u7ae0"},{"location":"Distributed-computing/Theory/Time%26Ordering/#ordering#and#synchronization","text":"\u672c\u8282\u8ba8\u8bbaordering\u548csynchronization\u4e4b\u95f4\u7684\u5173\u7cfb: 1) synchronization\u7684\u76ee\u7684\u662f\u4fdd\u6301ordering \u4e0a\u8ff0\"Distributed system\u7684disordering\u672c\u8d28\"\u7ae0\u8282\u7684\u5185\u5bb9\u8bba\u8bc1\u4e86\u8fd9\u4e2a\u89c2\u70b9\u3002 2) ordering\u80fd\u591f\u7528\u4e8e\u5b9e\u73b0synchronization \u4e0a\u8ff0\"Ordering in zookeeper\"\u7ae0\u8282\u7684\u5185\u5bb9\u8bba\u8bc1\u4e86\u8fd9\u4e2a\u89c2\u70b9\u3002","title":"Ordering and synchronization"},{"location":"Distributed-computing/Theory/Time%26Ordering/#ordering#and#consistency","text":"\u9700\u8981\u8ba8\u8bbaordering \u548c consistency\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\u5bf9\u6b64\u8fdb\u884c\u4e86\u63cf\u8ff0: cornell Distributed Systems: Ordering and Consistency","title":"Ordering and consistency"},{"location":"Distributed-computing/Theory/Time%26Ordering/#distributed#systemdisordering","text":"\u4e00\u3001Distributed system\u662f\u5178\u578bmultiple entity model\uff0c\u5373\u5b83\u662f\u7531\u591a\u4e2a\u4e0d\u540c\u7684\u3001\u72ec\u7acb\u7684entity\u6784\u6210\uff0c\u8fd9\u4e9bentity\u7684execution\u662f\u72ec\u7acb\u7684\u3001\u968f\u673a\u7684\uff0c\u8fd9\u5c31\u662f\"distributed system\u7684disordering\u672c\u8d28\"\uff1b\u4f46\u662f\u4ece\u7cfb\u7edf\u7684\u6574\u4f53\u800c\u8a00\uff0c\u5982\u679c\u8981\u5b9e\u73b0computation\u3001\u786e\u5b9a\u6027\uff0c\u5c31\u9700\u8981\u8bbe\u6cd5\u6d88\u9664\u8fd9\u79cddisordering\uff0c\u663e\u7136\u5c31\u9700\u8981\u52a0\u5165\u4e00\u5b9a\u7684control\u3002control\u673a\u5236\u7684\u76ee\u7684\u662f\u6d88\u9664disordering\uff0c\u5b9e\u73b0ordering\u3002 \u4e8c\u3001\u901a\u8fc7 wikipedia Distributed computing \u3001 Distributed-system-challenge \u4e2d\u7684\u5185\u5bb9\uff0c\u6211\u4eec\u5df2\u7ecf\u77e5\u9053\uff0c\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u4e00\u4e2a\u975e\u5e38\u5927\u7684\u8c03\u6574\u662f: lack of a global clock \u56e0\u6b64\uff0c\u5728\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u5c31\u65e0\u6cd5\u4f9d\u8d56\u4e8ephysical clock\u6765\u8fdb\u884cordering\uff1b","title":"Distributed system\u7684disordering\u672c\u8d28"},{"location":"Distributed-computing/Theory/Time%26Ordering/#_2","text":"","title":"\u5982\u4f55\u514b\u670d\u8fd9\u4e9b\u95ee\u9898"},{"location":"Distributed-computing/Theory/Time%26Ordering/#logical#clock","text":"\u53c2\u89c1 Logical-clock \u7ae0\u8282\u3002","title":"\u4f7f\u7528logical clock"},{"location":"Distributed-computing/Theory/Time%26Ordering/#global#clock","text":"Google spanner: \"GPS\u548c\u539f\u5b50\u949f\"\uff0ccockroachlabs Living Without Atomic Clocks \u4e2d\uff0c\u603b\u7ed3\u5982\u4e0b: One of the most surprising and inspired facets of Spanner is its use of atomic clocks and GPS clocks to give participating nodes really accurate wall time synchronization. The designers of Spanner call this \u201cTrueTime\u201d , and it provides a tight bound on clock offset between any two nodes in the system. This lets them do pretty nifty things! \u53c2\u89c1 Google-spanner \u7ae0\u8282\u3002","title":"\u5b9e\u73b0global clock"},{"location":"Distributed-computing/Theory/Time%26Ordering/#example","text":"","title":"Example"},{"location":"Distributed-computing/Theory/Time%26Ordering/#ordering#in#tensorflow","text":"\u5728\u9605\u8bfbTensorFlow whitepaper\u7684\u65f6\u5019\uff0c\u5176\u4e2d\u6709\u8fd9\u6837\u7684\u63cf\u8ff0: Our implementation also sometimes inserts control dependencies to enforce orderings between otherwise independent operations as a way of, for example, controlling the peak memory usage.","title":"Ordering in TensorFlow"},{"location":"Distributed-computing/Theory/Time%26Ordering/#ordering#in#zookeeper","text":"\u5728 ZooKeeper: A Distributed Coordination Service for Distributed Applications \u4e2d\u5f88\u591a\u5173\u4e8eordering\u7684\u63cf\u8ff0: The ZooKeeper implementation puts a premium(\u4fdd\u8bc1) on high performance, highly available, strictly ordered access. The strict ordering means that sophisticated synchronization primitives can be implemented at the client. ZooKeeper is ordered. ZooKeeper stamps(\u8d34\u4e0a\u65f6\u95f4\u6233) each update with a number that reflects the order of all ZooKeeper transactions. Subsequent operations can use the order to implement higher-level abstractions, such as synchronization primitives. \u4ece\u4e0a\u9762\u7684\u63cf\u8ff0\u53ef\u4ee5\u770b\u51fa\uff0c\"ordering\u80fd\u591f\u7528\u4e8e\u5b9e\u73b0synchronization\"\u3002 Sequential Consistency - Updates from a client will be applied in the order that they were sent.","title":"Ordering in zookeeper"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/","text":"\u5173\u4e8e\u672c\u7ae0 \u6b63\u5982\u5728 wikipedia Logical clock \u4e2d\u6240\u8a00: Distributed systems may have no physically synchronous global clock \u8fd9\u662f\u6211\u4eec\u5df2\u7ecf\u77e5\u9053\u7684distributed system\u7684\u6311\u6218\uff0c\u90a3\u5982\u4f55\u6765\u4e3a\u53d1\u751f\u4e8edistributed system\u4e2d\u7684message\u3001event\u6392\u5e8f\u5462\uff1f Logical clock \u662f\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u7684\u4e00\u79cd\u65b9\u5f0f\u3002","title":"Introduction"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/#_1","text":"\u6b63\u5982\u5728 wikipedia Logical clock \u4e2d\u6240\u8a00: Distributed systems may have no physically synchronous global clock \u8fd9\u662f\u6211\u4eec\u5df2\u7ecf\u77e5\u9053\u7684distributed system\u7684\u6311\u6218\uff0c\u90a3\u5982\u4f55\u6765\u4e3a\u53d1\u751f\u4e8edistributed system\u4e2d\u7684message\u3001event\u6392\u5e8f\u5462\uff1f Logical clock \u662f\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u7684\u4e00\u79cd\u65b9\u5f0f\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/Happened-before/","text":"wikipedia Happened-before For example, in some programming languages such as Java, C++ or Rust, a happens-before edge exists if memory written to by statement A is visible to statement B, that is, if statement A completes its write before statement B starts its read. NOTE: 1\u3001\u4e0a\u8ff0\u4e09\u79cdprogramming language\u7684memory model\u90fd\u662f\u57fa\u4e8ehappens-before relation\u800c\u5efa\u7acb\u7684 Like all strict partial orders, the happened-before relation is transitive , irreflexive and antisymmetric , i.e. The processes that make up a distributed system have no knowledge of the happened-before relation unless they use a logical clock , like a Lamport clock or a vector clock . This allows one to design algorithms for mutual exclusion , and tasks like debugging or optimising distributed systems. NOTE: 1\u3001\u5b83\u662f\u6784\u5efadistributed system\u7684\u57fa\u7840 2\u3001formal description","title":"Introduction"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/Happened-before/#wikipedia#happened-before","text":"For example, in some programming languages such as Java, C++ or Rust, a happens-before edge exists if memory written to by statement A is visible to statement B, that is, if statement A completes its write before statement B starts its read. NOTE: 1\u3001\u4e0a\u8ff0\u4e09\u79cdprogramming language\u7684memory model\u90fd\u662f\u57fa\u4e8ehappens-before relation\u800c\u5efa\u7acb\u7684 Like all strict partial orders, the happened-before relation is transitive , irreflexive and antisymmetric , i.e. The processes that make up a distributed system have no knowledge of the happened-before relation unless they use a logical clock , like a Lamport clock or a vector clock . This allows one to design algorithms for mutual exclusion , and tasks like debugging or optimising distributed systems. NOTE: 1\u3001\u5b83\u662f\u6784\u5efadistributed system\u7684\u57fa\u7840 2\u3001formal description","title":"wikipedia Happened-before"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/Lamport-timestamp/","text":"Lamport timestamps \u4e00\u3001Lamport timestamps\u5176\u5b9e\u975e\u5e38\u7b80\u5355\uff0c\u6b63\u5982\u5728 wikipedia Logical clock \u4e2d\u6240\u8ff0: 1\u3001 Lamport timestamps , which are monotonically increasing software counters. \u5b83\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u5355\u8c03\u9012\u589e\u7684\"software counter\"\uff1b \u867d\u7136\u975e\u5e38\u7b80\u5355\uff0c\u4f46\u662f\u5177\u6709\u91cd\u8981\u7684\u610f\u4e49\uff1b \u4e8c\u3001Lamport timestamps\u975e\u5e38\u91cd\u8981\uff0c\u5b83\u5728\u5404\u79cddistributed protocol\u4e2d\u6709\u7740\u5e7f\u6cdb\u7684\u8fd0\u7528 \u4e09\u3001\u8fd9\u4e2a\u9886\u57df\u7684\u5f00\u5c71\u4e4b\u4f5c wikipedia Lamport timestamps The algorithm of Lamport timestamps is a simple algorithm used to determine the order of events in a distributed computer system . As different nodes or processes will typically not be perfectly synchronized, this algorithm is used to provide a partial ordering of events with minimal overhead, and conceptually provide a starting point for the more advanced vector clock method. They are named after their creator, Leslie Lamport . NOTE: \u8fd9\u4e2a\u9886\u57df\u7684\u5f00\u7aef Distributed algorithms such as resource synchronization often depend on some method of ordering events to function. For example, consider a system with two processes and a disk. The processes send messages to each other, and also send messages to the disk requesting access. The disk grants access in the order the messages were sent. For example process $ A $ sends a message to the disk requesting write access, and then sends a read instruction message to process $ B $. Process $ B $ receives the message, and as a result sends its own read request message to the disk. If there is a timing delay causing the disk to receive both messages at the same time, it can determine which message happened-before the other: $ A $ happens-before $ B $ if one can get from $ A $ to $ B $ by a sequence of moves of two types: moving forward while remaining in the same process, and following a message from its sending to its reception. A logical clock algorithm provides a mechanism to determine facts about the order of such events. Lamport invented a simple mechanism by which the happened-before ordering can be captured numerically. A Lamport logical clock is an incrementing software counter maintained in each process. NOTE: \u603b\u7ed3\u5730\u6bd4\u8f83\u597d Conceptually, this logical clock can be thought of as a clock that only has meaning in relation to messages moving between processes. When a process receives a message , it resynchronizes its logical clock with that sender . The above mentioned vector clock is a generalization of the idea into the context of an arbitrary number of parallel, independent processes. Algorithm The algorithm follows some simple rules: 1\u3001 A process increments its counter before each event in that process; 2\u3001 When a process sends a message, it includes its counter value with the message; 3\u3001 On receiving a message, the counter of the recipient is updated, if necessary, to the greater of its current counter and the timestamp in the received message. The counter is then incremented by 1 before the message is considered received.[ 1] In pseudocode , the algorithm for sending is: # event is known time = time + 1 ; # event happens send ( message , time ); The algorithm for receiving a message is: ( message , time_stamp ) = receive (); time = max ( time_stamp , time ) + 1 ; Considerations For every two different events $ a $ and $ b $ occurring in the same process, and $ C(x) $ being the timestamp for a certain event $ x $, it is necessary that $ C(a) $ never equals $ C(b) $. Therefore it is necessary that: 1\u3001The logical clock be set so that there is a minimum of one clock \"tick\" (increment of the counter) between events $ a $ and $ b $; 2\u3001In a multiprocess or multithreaded environment, it might be necessary to attach the process ID (PID) or any other unique ID to the timestamp so that it is possible to differentiate between events $ a $ and $ b $ which may occur simultaneously in different processes. Causal ordering For any two events, $ a $ and $ b $, if there\u2019s any way that $ a $ could have influenced $ b $, then the Lamport timestamp of $ a $ will be less than the Lamport timestamp of $ b $. It\u2019s also possible to have two events where we can\u2019t say which came first; when that happens, it means that they couldn\u2019t have affected each other. If $ a $ and $ b $ can\u2019t have any effect on each other, then it doesn\u2019t matter which one came first. Implications A Lamport clock may be used to create a partial causal ordering of events between processes. Given a logical clock following these rules, the following relation is true: if $ a\\rightarrow b $ then $ C(a)<C(b) $, where $ \\rightarrow \\, $ means happened-before . This relation only goes one way, and is called the clock consistency condition : if one event comes before another, then that event's logical clock comes before the other's. The strong clock consistency condition , which is two way (if $ C(a)<C(b) $ then $ a\\rightarrow b $), can be obtained by other techniques such as vector clocks. Using only a simple Lamport clock, only a partial causal ordering can be inferred from the clock. However, via the contrapositive, it's true that $ C(a)\\nless C(b) $ implies $ a\\nrightarrow b $. So, for example, if $ C(a)\\geq C(b) $ then $ a $ cannot have happened-before $ b $. Another way of putting this is that $ C(a)<C(b) $ means that $ a $ may have happened-before $ b $, or be incomparable with $ b $ in the happened-before ordering, but $ a $ did not happen after $ b $. Nevertheless, Lamport timestamps can be used to create a total ordering of events in a distributed system by using some arbitrary mechanism to break ties (e.g., the ID of the process). The caveat is that this ordering is artifactual and cannot be depended on to imply a causal relationship. Lamport's logical clock in distributed systems 1\u3001In a distributed system, it is not possible in practice to synchronize time across entities (typically thought of as processes) within the system; hence, the entities can use the concept of a logical clock based on the events through which they communicate. NOTE: \u4e00\u3001\"it is not possible in practice to synchronize time across entities (typically thought of as processes) within the system\" \u5176\u5b9e\u968f\u7740\u79d1\u6280\u7684\u53d1\u5c55\uff0c\u73b0\u5728\u5df2\u7ecf\u80fd\u591f\u8fdb\u884c\u5168\u7403\u7684\u65f6\u95f4\u540c\u6b65\u4e86\uff0cGoogle spanner\u4f7f\u7528atomic clock + GPS clock\u5c31\u5b9e\u73b0\u4e86 2\u3001If two entities do not exchange any messages, then they probably do not need to share a common clock; events occurring on those entities are termed as concurrent events. 3\u3001Among the processes on the same local machine we can order the events based on the local clock of the system. 4\u3001When two entities communicate by message passing, then the send event is said to happen-before the receive event, and the logical order can be established among the events. 5\u3001A distributed system is said to have partial order if we can have a partial order relationship among the events in the system. If 'totality', i.e., causal relationship among all events in the system, can be established, then the system is said to have total order. 6\u3001A single entity cannot have two events occur simultaneously. If the system has total order we can determine the order among all events in the system. If the system has partial order between processes, which is the type of order Lamport's logical clock provides, then we can only tell the ordering between entities that interact. Lamport addressed ordering two events with the same timestamp (or counter): \"To break ties, we use any arbitrary total ordering $ < $ of the processes.\"[ 1] Thus two timestamps or counters may be the same within a distributed system, but in applying the logical clocks algorithm events that occur will always maintain at least a strict partial ordering. 7\u3001Lamport clocks lead to a situation where all events in a distributed system are totally ordered. That is, if $ a\\rightarrow b $, then we can say $ a $actually happened before $ b $. 8\u3001Unfortunately, with Lamport\u2019s clocks, nothing can be said about the actual time of $ a $ and $ b $. If the logical clock says $ a\\rightarrow b $, that does not mean in reality that $ a $ actually happened before $ b $ in terms of real time. 9\u3001Lamport clocks show non-causality, but not capture all causality. Knowing $ a\\rightarrow c $ and $ b\\rightarrow c $ shows $ c $ did not cause $ a $ or $ b $ but we cannot say which initiated $ c $. 10\u3001This kind of information can be important when trying to replay events in a distributed system (such as when trying to recover after a crash). 11\u3001The theory goes that if one node goes down, if we know the causal relationships between messages, then we can replay those messages and respect the causal relationship to get that node back up to the state it needs to be in.[ 2] TODO swizec Week 7: Time, Clocks, and Ordering of Events in a Distributed System medium Ordering Distributed Events mwhittaker Lamport's Logical Clocks","title":"Introduction"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/Lamport-timestamp/#lamport#timestamps","text":"\u4e00\u3001Lamport timestamps\u5176\u5b9e\u975e\u5e38\u7b80\u5355\uff0c\u6b63\u5982\u5728 wikipedia Logical clock \u4e2d\u6240\u8ff0: 1\u3001 Lamport timestamps , which are monotonically increasing software counters. \u5b83\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u5355\u8c03\u9012\u589e\u7684\"software counter\"\uff1b \u867d\u7136\u975e\u5e38\u7b80\u5355\uff0c\u4f46\u662f\u5177\u6709\u91cd\u8981\u7684\u610f\u4e49\uff1b \u4e8c\u3001Lamport timestamps\u975e\u5e38\u91cd\u8981\uff0c\u5b83\u5728\u5404\u79cddistributed protocol\u4e2d\u6709\u7740\u5e7f\u6cdb\u7684\u8fd0\u7528 \u4e09\u3001\u8fd9\u4e2a\u9886\u57df\u7684\u5f00\u5c71\u4e4b\u4f5c","title":"Lamport timestamps"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/Lamport-timestamp/#wikipedia#lamport#timestamps","text":"The algorithm of Lamport timestamps is a simple algorithm used to determine the order of events in a distributed computer system . As different nodes or processes will typically not be perfectly synchronized, this algorithm is used to provide a partial ordering of events with minimal overhead, and conceptually provide a starting point for the more advanced vector clock method. They are named after their creator, Leslie Lamport . NOTE: \u8fd9\u4e2a\u9886\u57df\u7684\u5f00\u7aef Distributed algorithms such as resource synchronization often depend on some method of ordering events to function. For example, consider a system with two processes and a disk. The processes send messages to each other, and also send messages to the disk requesting access. The disk grants access in the order the messages were sent. For example process $ A $ sends a message to the disk requesting write access, and then sends a read instruction message to process $ B $. Process $ B $ receives the message, and as a result sends its own read request message to the disk. If there is a timing delay causing the disk to receive both messages at the same time, it can determine which message happened-before the other: $ A $ happens-before $ B $ if one can get from $ A $ to $ B $ by a sequence of moves of two types: moving forward while remaining in the same process, and following a message from its sending to its reception. A logical clock algorithm provides a mechanism to determine facts about the order of such events. Lamport invented a simple mechanism by which the happened-before ordering can be captured numerically. A Lamport logical clock is an incrementing software counter maintained in each process. NOTE: \u603b\u7ed3\u5730\u6bd4\u8f83\u597d Conceptually, this logical clock can be thought of as a clock that only has meaning in relation to messages moving between processes. When a process receives a message , it resynchronizes its logical clock with that sender . The above mentioned vector clock is a generalization of the idea into the context of an arbitrary number of parallel, independent processes.","title":"wikipedia Lamport timestamps"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/Lamport-timestamp/#algorithm","text":"The algorithm follows some simple rules: 1\u3001 A process increments its counter before each event in that process; 2\u3001 When a process sends a message, it includes its counter value with the message; 3\u3001 On receiving a message, the counter of the recipient is updated, if necessary, to the greater of its current counter and the timestamp in the received message. The counter is then incremented by 1 before the message is considered received.[ 1] In pseudocode , the algorithm for sending is: # event is known time = time + 1 ; # event happens send ( message , time ); The algorithm for receiving a message is: ( message , time_stamp ) = receive (); time = max ( time_stamp , time ) + 1 ;","title":"Algorithm"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/Lamport-timestamp/#considerations","text":"For every two different events $ a $ and $ b $ occurring in the same process, and $ C(x) $ being the timestamp for a certain event $ x $, it is necessary that $ C(a) $ never equals $ C(b) $. Therefore it is necessary that: 1\u3001The logical clock be set so that there is a minimum of one clock \"tick\" (increment of the counter) between events $ a $ and $ b $; 2\u3001In a multiprocess or multithreaded environment, it might be necessary to attach the process ID (PID) or any other unique ID to the timestamp so that it is possible to differentiate between events $ a $ and $ b $ which may occur simultaneously in different processes.","title":"Considerations"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/Lamport-timestamp/#causal#ordering","text":"For any two events, $ a $ and $ b $, if there\u2019s any way that $ a $ could have influenced $ b $, then the Lamport timestamp of $ a $ will be less than the Lamport timestamp of $ b $. It\u2019s also possible to have two events where we can\u2019t say which came first; when that happens, it means that they couldn\u2019t have affected each other. If $ a $ and $ b $ can\u2019t have any effect on each other, then it doesn\u2019t matter which one came first.","title":"Causal ordering"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/Lamport-timestamp/#implications","text":"A Lamport clock may be used to create a partial causal ordering of events between processes. Given a logical clock following these rules, the following relation is true: if $ a\\rightarrow b $ then $ C(a)<C(b) $, where $ \\rightarrow \\, $ means happened-before . This relation only goes one way, and is called the clock consistency condition : if one event comes before another, then that event's logical clock comes before the other's. The strong clock consistency condition , which is two way (if $ C(a)<C(b) $ then $ a\\rightarrow b $), can be obtained by other techniques such as vector clocks. Using only a simple Lamport clock, only a partial causal ordering can be inferred from the clock. However, via the contrapositive, it's true that $ C(a)\\nless C(b) $ implies $ a\\nrightarrow b $. So, for example, if $ C(a)\\geq C(b) $ then $ a $ cannot have happened-before $ b $. Another way of putting this is that $ C(a)<C(b) $ means that $ a $ may have happened-before $ b $, or be incomparable with $ b $ in the happened-before ordering, but $ a $ did not happen after $ b $. Nevertheless, Lamport timestamps can be used to create a total ordering of events in a distributed system by using some arbitrary mechanism to break ties (e.g., the ID of the process). The caveat is that this ordering is artifactual and cannot be depended on to imply a causal relationship.","title":"Implications"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/Lamport-timestamp/#lamports#logical#clock#in#distributed#systems","text":"1\u3001In a distributed system, it is not possible in practice to synchronize time across entities (typically thought of as processes) within the system; hence, the entities can use the concept of a logical clock based on the events through which they communicate. NOTE: \u4e00\u3001\"it is not possible in practice to synchronize time across entities (typically thought of as processes) within the system\" \u5176\u5b9e\u968f\u7740\u79d1\u6280\u7684\u53d1\u5c55\uff0c\u73b0\u5728\u5df2\u7ecf\u80fd\u591f\u8fdb\u884c\u5168\u7403\u7684\u65f6\u95f4\u540c\u6b65\u4e86\uff0cGoogle spanner\u4f7f\u7528atomic clock + GPS clock\u5c31\u5b9e\u73b0\u4e86 2\u3001If two entities do not exchange any messages, then they probably do not need to share a common clock; events occurring on those entities are termed as concurrent events. 3\u3001Among the processes on the same local machine we can order the events based on the local clock of the system. 4\u3001When two entities communicate by message passing, then the send event is said to happen-before the receive event, and the logical order can be established among the events. 5\u3001A distributed system is said to have partial order if we can have a partial order relationship among the events in the system. If 'totality', i.e., causal relationship among all events in the system, can be established, then the system is said to have total order. 6\u3001A single entity cannot have two events occur simultaneously. If the system has total order we can determine the order among all events in the system. If the system has partial order between processes, which is the type of order Lamport's logical clock provides, then we can only tell the ordering between entities that interact. Lamport addressed ordering two events with the same timestamp (or counter): \"To break ties, we use any arbitrary total ordering $ < $ of the processes.\"[ 1] Thus two timestamps or counters may be the same within a distributed system, but in applying the logical clocks algorithm events that occur will always maintain at least a strict partial ordering. 7\u3001Lamport clocks lead to a situation where all events in a distributed system are totally ordered. That is, if $ a\\rightarrow b $, then we can say $ a $actually happened before $ b $. 8\u3001Unfortunately, with Lamport\u2019s clocks, nothing can be said about the actual time of $ a $ and $ b $. If the logical clock says $ a\\rightarrow b $, that does not mean in reality that $ a $ actually happened before $ b $ in terms of real time. 9\u3001Lamport clocks show non-causality, but not capture all causality. Knowing $ a\\rightarrow c $ and $ b\\rightarrow c $ shows $ c $ did not cause $ a $ or $ b $ but we cannot say which initiated $ c $. 10\u3001This kind of information can be important when trying to replay events in a distributed system (such as when trying to recover after a crash). 11\u3001The theory goes that if one node goes down, if we know the causal relationships between messages, then we can replay those messages and respect the causal relationship to get that node back up to the state it needs to be in.[ 2]","title":"Lamport's logical clock in distributed systems"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/Lamport-timestamp/#todo","text":"swizec Week 7: Time, Clocks, and Ordering of Events in a Distributed System medium Ordering Distributed Events mwhittaker Lamport's Logical Clocks","title":"TODO"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/Lamport-timestamp/draft-towardsdatascience-Understanding-Lamport-Timestamps/","text":"towardsdatascience Understanding Lamport Timestamps with Python\u2019s multiprocessing library Everyone who has been working with distributed systems or logs from such a systems, has directly or indirectly encountered Lamport Timestamps . Lamport Timestamps are used to (partially) order events in a distributed system. The algorithm is based on causal ordering of events and is the foundation of more advanced clocks such as Vector Clocks and Interval Tree Clocks (ITC) . In this article we will first briefly introduce the concept of logical clocks , explain why ordering of events in distributed systems is needed and discuss some alternatives. Then we\u2019ll go over the algorithm of Lamport Timestamps and work an example with three processes. Next, we\u2019ll implement this example in easy-to-understand code using Python\u2019s multiprocessing library. To top it all off, we\u2019ll transform our code into an implementation with Vector Clocks . Logical clocks To understand why logical clocks are needed, it is important to understand what a distributed system is. A distributed system is a system whose components (here called processes ) are located on different networked computers, which then coordinate their actions by passing messages to one other. One of the main properties of a distributed system is that it lacks a global clock . All the processes have their own local clock, but due to clock skew and clock drift they have no direct way to know if their clock is in check with the local clocks of the other processes in the system, this problem is sometimes referred to as the problem of clock synchronization . Solutions to this problem consist of using a central time server ( Cristian\u2019s Algorithm ) or a mechanism called a logical clock . The problem with a central time server is that its error depends on the round-trip time of the message from process to time server and back. Logical clocks are based on capturing chronological\uff08\u6309\u7167\u65f6\u95f4\u53d1\u751f\u7684\uff09 and causal relationships of processes and ordering events based on these relationships . The first implementation, the Lamport timestamps , was proposed by Leslie Lamport in 1978 and still forms the foundation of almost all logical clocks . Lamport Timestamps algorithm A Lamport logical clock is an incrementing counter maintained in each process. Conceptually, this logical clock can be thought of as a clock that only has meaning in relation to messages moving between processes. When a process receives a message, it resynchronizes its logical clock with that sender (causality\uff08\u56e0\u679c\u5173\u7cfb\uff09). NOTE : \u6bcf\u7c7bmessage\u6709\u4e00\u4e2a\u81ea\u5df1\u7684\u4e00\u4e2a\u72ec\u7acb\u7684clock\uff0c\u4e0d\u540c\u7c7b\u7684message\u6709\u5404\u81ea\u72ec\u7acb\u7684clock\uff0c\u663e\u7136\u4e0d\u540c\u7c7b\u7684message\u4e4b\u95f4\u662f\u65e0\u9700\u8fdb\u884ccompare\u7684\uff0c\u6240\u4ee5lamport timestamp\u662fpartial ordering\uff1b\u8fd9\u5728\u8fd9\u4e00\u8282\u7684\u6700\u540e\u4e00\u6bb5\u8fdb\u884c\u4e86\u4ecb\u7ecd\uff1b NOTE : distributed system\u4e2d\u7684\u5404\u4e2a\u8282\u70b9\u4e4b\u95f4\u662f\u4f7f\u7528message\u8fdb\u884c\u901a\u4fe1\u7684\uff08\u5f53\u8282\u70b9\u89c2\u5bdf\u5230\u67d0\u4e2aevent\u53d1\u751f\u7684\u65f6\u5019\uff0c\u5b83\u901a\u8fc7\u53d1\u9001message\u6765\u5c06\u8fd9\u4e2aevent\u544a\u77e5\u7ed9distributed system\u4e2d\u7684\u5176\u4ed6\u7684\u8282\u70b9\uff1b\u5f53distributed system\u4e2d\u6709\u591a\u4e2a\u8282\u70b9\u90fd\u89c2\u5bdf\u5230\u8be5event\u7684\u65b9\u5f0f\uff0c\u5219\u5b83\u4eec\u90fd\u4f1a\u901a\u8fc7message\u6765\u5c06\u8fd9\u4e2aevent\u544a\u77e5\u7ed9distributed system\u4e2d\u7684\u5176\u4ed6\u7684node\uff0c\u663e\u7136\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u591a\u4e2a\u53d1\u9001message\u7684\u8282\u70b9\u4e4b\u95f4\u662f\u5b58\u5728\u7740\u67d0\u79cd\u7ade\u4e89\u7684\uff1b\u5728\u5f88\u591a\u573a\u666f\u4e2d\uff0c\u5c31\u6709\u8fd9\u6837\u7684\u4e00\u4e2a\u9700\u6c42\uff1a\u9700\u8981\u786e\u5b9a\u5230\u5e95\u662f\u54ea\u4e2a\u8282\u70b9\u5148\u53d1\u9001\u7684message\uff0c\u54ea\u4e2a\u8282\u70b9\u540e\u53d1\u9001\u7684message\uff0c\u4ee5\u4fbf\u4fee\u6539\u89e3\u51b3\u8fd9\u79cd\u7ade\u4e89\uff09\uff0c\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u9700\u6c42\u662f\uff1a\u9700\u8981\u786e\u5b9a\u4e24\u4e2a\u540c\u7c7b\u7684message\u53d1\u751f\u7684\u5148\u540e\u6b21\u5e8f\uff0c\u5373\u5230\u5e95\u662f\u54ea\u4e2amessage\u5148\u53d1\u751f\u7684\uff1b The algorithm of Lamport Timestamps can be captured in a few rules: All the process counters start with value 0. A process increments its counter for each event (internal event, message sending, message receiving) in that process. When a process sends a message, it includes its (incremented) counter value with the message. On receiving a message, the counter of the recipient is updated to the greater of its current counter and the timestamp in the received message, and then incremented by one. Looking at these rules, we can see the algorithm will create a minimum overhead, since the counter consists of just one integer value and the messaging piggybacks on inter-process messages. One of the shortcomings of Lamport Timestamps is rooted in the fact that they only partially order events (as opposed to total order). Partial order indicates that not every pair of events need be comparable. If two events can\u2019t be compared, we call these events concurrent . The problem with Lamport Timestamps is that they can\u2019t tell if events are concurrent or not. This problem is solved by Vector Clocks . Vector Clocks As mentioned before, Lamport timestamp have one big shortcoming: they can\u2019t tell you when two events are concurrent . Going back to our example, by just checking the timestamps, we could conclude that event 3 in process 1 has happend before event 8 in process 3, but this isn\u2019t necessarily true.","title":"towardsdatascience [Understanding Lamport Timestamps with Python\u2019s multiprocessing library](https://towardsdatascience.com/understanding-lamport-timestamps-with-pythons-multiprocessing-library-12a6427881c6)"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/Lamport-timestamp/draft-towardsdatascience-Understanding-Lamport-Timestamps/#towardsdatascience#understanding#lamport#timestamps#with#pythons#multiprocessing#library","text":"Everyone who has been working with distributed systems or logs from such a systems, has directly or indirectly encountered Lamport Timestamps . Lamport Timestamps are used to (partially) order events in a distributed system. The algorithm is based on causal ordering of events and is the foundation of more advanced clocks such as Vector Clocks and Interval Tree Clocks (ITC) . In this article we will first briefly introduce the concept of logical clocks , explain why ordering of events in distributed systems is needed and discuss some alternatives. Then we\u2019ll go over the algorithm of Lamport Timestamps and work an example with three processes. Next, we\u2019ll implement this example in easy-to-understand code using Python\u2019s multiprocessing library. To top it all off, we\u2019ll transform our code into an implementation with Vector Clocks .","title":"towardsdatascience Understanding Lamport Timestamps with Python\u2019s multiprocessing library"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/Lamport-timestamp/draft-towardsdatascience-Understanding-Lamport-Timestamps/#logical#clocks","text":"To understand why logical clocks are needed, it is important to understand what a distributed system is. A distributed system is a system whose components (here called processes ) are located on different networked computers, which then coordinate their actions by passing messages to one other. One of the main properties of a distributed system is that it lacks a global clock . All the processes have their own local clock, but due to clock skew and clock drift they have no direct way to know if their clock is in check with the local clocks of the other processes in the system, this problem is sometimes referred to as the problem of clock synchronization . Solutions to this problem consist of using a central time server ( Cristian\u2019s Algorithm ) or a mechanism called a logical clock . The problem with a central time server is that its error depends on the round-trip time of the message from process to time server and back. Logical clocks are based on capturing chronological\uff08\u6309\u7167\u65f6\u95f4\u53d1\u751f\u7684\uff09 and causal relationships of processes and ordering events based on these relationships . The first implementation, the Lamport timestamps , was proposed by Leslie Lamport in 1978 and still forms the foundation of almost all logical clocks .","title":"Logical clocks"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/Lamport-timestamp/draft-towardsdatascience-Understanding-Lamport-Timestamps/#lamport#timestamps#algorithm","text":"A Lamport logical clock is an incrementing counter maintained in each process. Conceptually, this logical clock can be thought of as a clock that only has meaning in relation to messages moving between processes. When a process receives a message, it resynchronizes its logical clock with that sender (causality\uff08\u56e0\u679c\u5173\u7cfb\uff09). NOTE : \u6bcf\u7c7bmessage\u6709\u4e00\u4e2a\u81ea\u5df1\u7684\u4e00\u4e2a\u72ec\u7acb\u7684clock\uff0c\u4e0d\u540c\u7c7b\u7684message\u6709\u5404\u81ea\u72ec\u7acb\u7684clock\uff0c\u663e\u7136\u4e0d\u540c\u7c7b\u7684message\u4e4b\u95f4\u662f\u65e0\u9700\u8fdb\u884ccompare\u7684\uff0c\u6240\u4ee5lamport timestamp\u662fpartial ordering\uff1b\u8fd9\u5728\u8fd9\u4e00\u8282\u7684\u6700\u540e\u4e00\u6bb5\u8fdb\u884c\u4e86\u4ecb\u7ecd\uff1b NOTE : distributed system\u4e2d\u7684\u5404\u4e2a\u8282\u70b9\u4e4b\u95f4\u662f\u4f7f\u7528message\u8fdb\u884c\u901a\u4fe1\u7684\uff08\u5f53\u8282\u70b9\u89c2\u5bdf\u5230\u67d0\u4e2aevent\u53d1\u751f\u7684\u65f6\u5019\uff0c\u5b83\u901a\u8fc7\u53d1\u9001message\u6765\u5c06\u8fd9\u4e2aevent\u544a\u77e5\u7ed9distributed system\u4e2d\u7684\u5176\u4ed6\u7684\u8282\u70b9\uff1b\u5f53distributed system\u4e2d\u6709\u591a\u4e2a\u8282\u70b9\u90fd\u89c2\u5bdf\u5230\u8be5event\u7684\u65b9\u5f0f\uff0c\u5219\u5b83\u4eec\u90fd\u4f1a\u901a\u8fc7message\u6765\u5c06\u8fd9\u4e2aevent\u544a\u77e5\u7ed9distributed system\u4e2d\u7684\u5176\u4ed6\u7684node\uff0c\u663e\u7136\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u591a\u4e2a\u53d1\u9001message\u7684\u8282\u70b9\u4e4b\u95f4\u662f\u5b58\u5728\u7740\u67d0\u79cd\u7ade\u4e89\u7684\uff1b\u5728\u5f88\u591a\u573a\u666f\u4e2d\uff0c\u5c31\u6709\u8fd9\u6837\u7684\u4e00\u4e2a\u9700\u6c42\uff1a\u9700\u8981\u786e\u5b9a\u5230\u5e95\u662f\u54ea\u4e2a\u8282\u70b9\u5148\u53d1\u9001\u7684message\uff0c\u54ea\u4e2a\u8282\u70b9\u540e\u53d1\u9001\u7684message\uff0c\u4ee5\u4fbf\u4fee\u6539\u89e3\u51b3\u8fd9\u79cd\u7ade\u4e89\uff09\uff0c\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u9700\u6c42\u662f\uff1a\u9700\u8981\u786e\u5b9a\u4e24\u4e2a\u540c\u7c7b\u7684message\u53d1\u751f\u7684\u5148\u540e\u6b21\u5e8f\uff0c\u5373\u5230\u5e95\u662f\u54ea\u4e2amessage\u5148\u53d1\u751f\u7684\uff1b The algorithm of Lamport Timestamps can be captured in a few rules: All the process counters start with value 0. A process increments its counter for each event (internal event, message sending, message receiving) in that process. When a process sends a message, it includes its (incremented) counter value with the message. On receiving a message, the counter of the recipient is updated to the greater of its current counter and the timestamp in the received message, and then incremented by one. Looking at these rules, we can see the algorithm will create a minimum overhead, since the counter consists of just one integer value and the messaging piggybacks on inter-process messages. One of the shortcomings of Lamport Timestamps is rooted in the fact that they only partially order events (as opposed to total order). Partial order indicates that not every pair of events need be comparable. If two events can\u2019t be compared, we call these events concurrent . The problem with Lamport Timestamps is that they can\u2019t tell if events are concurrent or not. This problem is solved by Vector Clocks .","title":"Lamport Timestamps algorithm"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/Lamport-timestamp/draft-towardsdatascience-Understanding-Lamport-Timestamps/#vector#clocks","text":"As mentioned before, Lamport timestamp have one big shortcoming: they can\u2019t tell you when two events are concurrent . Going back to our example, by just checking the timestamps, we could conclude that event 3 in process 1 has happend before event 8 in process 3, but this isn\u2019t necessarily true.","title":"Vector Clocks"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/Lamport-timestamp/paper-Time-Clocks-and-the-Ordering-of-Events-in-a-Distributed-System/","text":"paper Time, Clocks, and the Ordering of Events in a Distributed System Leslie Lamport Massachusetts Computer Associates, Inc. The concept of one event happening before another in a distributed system is examined, and is shown to define a partial ordering of the events . A distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events. The use of the total ordering is illustrated with a method for solving synchronization problems. The algorithm is then specialized for synchronizing physical clocks, and a bound is derived on how far out of synchrony the clocks can become. NOTE: \u4e00\u3001\u7ffb\u8bd1: \"\u7814\u7a76\u4e86\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u4e00\u4e2a\u4e8b\u4ef6\u5148\u4e8e\u53e6\u4e00\u4e2a\u4e8b\u4ef6\u53d1\u751f\u7684\u6982\u5ff5\uff0c\u5e76\u663e\u793a\u4e86\u8be5\u4e8b\u4ef6\u4ee5\u5b9a\u4e49\u4e8b\u4ef6\u7684\u90e8\u5206\u6392\u5e8f\u3002 \u7ed9\u51fa\u4e86\u7528\u4e8e\u540c\u6b65\u903b\u8f91\u65f6\u949f\u7cfb\u7edf\u7684\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u8be5\u903b\u8f91\u65f6\u949f\u53ef\u7528\u4e8e\u5bf9\u4e8b\u4ef6\u8fdb\u884c\u6574\u4f53\u6392\u5e8f\u3002 \u901a\u8fc7\u89e3\u51b3\u540c\u6b65\u95ee\u9898\u7684\u65b9\u6cd5\u8bf4\u660e\u4e86\u603b\u6392\u5e8f\u7684\u4f7f\u7528\u3002 \u7136\u540e\uff0c\u8be5\u7b97\u6cd5\u4e13\u95e8\u7528\u4e8e\u540c\u6b65\u7269\u7406\u65f6\u949f\uff0c\u5e76\u5f97\u51fa\u65f6\u949f\u53ef\u4ee5\u53d8\u5f97\u591a\u4e0d\u540c\u6b65\u7684\u754c\u9650\u3002\" NOTE : \u4e0a\u8ff0**logical clocks**\uff0c\u5728 raft \u7b97\u6cd5\u4e2d\u662f\u4f7f\u7528term\u6765\u5b9e\u73b0\u7684\uff1b Key Words and Phrases : distributed systems, computer networks, clock synchronization, multiprocess systems CR Categories: 4.32, 5.29 Introduction The concept of time is fundamental to our way of thinking. It is derived from the more basic concept of the order in which events occur. We say that something happened at 3:15 if it occurred after our clock read 3:15 and before it read 3:16. The concept of the temporal ordering of events pervades\uff08\u904d\u5e03\uff09 our thinking about systems. For example, in an airline reservation system we specify that a request for a reservation should be granted if it is made before the flight is filled. However, we will see that this concept must be carefully reexamined when considering events in a distributed system. A distributed system consists of a collection of distinct processes which are spatially separated, and which communicate with one another by exchanging messages. A network of interconnected computers, such as the ARPA net, is a distributed system. A single computer can also be viewed as a distributed system in which the central control unit, the memory units, and the input-output channels are separate processes. A system is distributed if the message transmission delay is not negligible compared to the time between events in a single process. We will concern ourselves primarily with systems of spatially separated computers. However, many of our remarks will apply more generally. In particular, a multiprocessing system on a single computer involves problems similar to those of a distributed system because of the unpredictable order in which certain events can occur. In a distributed system, it is sometimes impossible to say that one of two events occurred first. The relation \"happened before\" is therefore only a partial ordering of the events in the system. We have found that problems often arise because people are not fully aware of this fact and its implications. In this paper, we discuss the partial ordering defined by the \"happened before\" relation, and give a distributed algorithm for extending it to a consistent total ordering of all the events. This algorithm can provide a useful mechanism for implementing a distributed system. We illustrate its use with a simple method for solving synchronization problems. Unexpected, anomalous behavior can occur if the ordering obtained by this algorithm differs from that perceived by the user. This can be avoided by introducing real, physical clocks. We describe a simple method for synchronizing these clocks, and derive an upper bound on how far out of synchrony they can drift.","title":"Introduction"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/Lamport-timestamp/paper-Time-Clocks-and-the-Ordering-of-Events-in-a-Distributed-System/#paper#time#clocks#and#the#ordering#of#events#in#a#distributed#system","text":"Leslie Lamport Massachusetts Computer Associates, Inc. The concept of one event happening before another in a distributed system is examined, and is shown to define a partial ordering of the events . A distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events. The use of the total ordering is illustrated with a method for solving synchronization problems. The algorithm is then specialized for synchronizing physical clocks, and a bound is derived on how far out of synchrony the clocks can become. NOTE: \u4e00\u3001\u7ffb\u8bd1: \"\u7814\u7a76\u4e86\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u4e00\u4e2a\u4e8b\u4ef6\u5148\u4e8e\u53e6\u4e00\u4e2a\u4e8b\u4ef6\u53d1\u751f\u7684\u6982\u5ff5\uff0c\u5e76\u663e\u793a\u4e86\u8be5\u4e8b\u4ef6\u4ee5\u5b9a\u4e49\u4e8b\u4ef6\u7684\u90e8\u5206\u6392\u5e8f\u3002 \u7ed9\u51fa\u4e86\u7528\u4e8e\u540c\u6b65\u903b\u8f91\u65f6\u949f\u7cfb\u7edf\u7684\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u8be5\u903b\u8f91\u65f6\u949f\u53ef\u7528\u4e8e\u5bf9\u4e8b\u4ef6\u8fdb\u884c\u6574\u4f53\u6392\u5e8f\u3002 \u901a\u8fc7\u89e3\u51b3\u540c\u6b65\u95ee\u9898\u7684\u65b9\u6cd5\u8bf4\u660e\u4e86\u603b\u6392\u5e8f\u7684\u4f7f\u7528\u3002 \u7136\u540e\uff0c\u8be5\u7b97\u6cd5\u4e13\u95e8\u7528\u4e8e\u540c\u6b65\u7269\u7406\u65f6\u949f\uff0c\u5e76\u5f97\u51fa\u65f6\u949f\u53ef\u4ee5\u53d8\u5f97\u591a\u4e0d\u540c\u6b65\u7684\u754c\u9650\u3002\" NOTE : \u4e0a\u8ff0**logical clocks**\uff0c\u5728 raft \u7b97\u6cd5\u4e2d\u662f\u4f7f\u7528term\u6765\u5b9e\u73b0\u7684\uff1b Key Words and Phrases : distributed systems, computer networks, clock synchronization, multiprocess systems CR Categories: 4.32, 5.29","title":"paper Time, Clocks, and the Ordering of Events in a Distributed System"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/Lamport-timestamp/paper-Time-Clocks-and-the-Ordering-of-Events-in-a-Distributed-System/#introduction","text":"The concept of time is fundamental to our way of thinking. It is derived from the more basic concept of the order in which events occur. We say that something happened at 3:15 if it occurred after our clock read 3:15 and before it read 3:16. The concept of the temporal ordering of events pervades\uff08\u904d\u5e03\uff09 our thinking about systems. For example, in an airline reservation system we specify that a request for a reservation should be granted if it is made before the flight is filled. However, we will see that this concept must be carefully reexamined when considering events in a distributed system. A distributed system consists of a collection of distinct processes which are spatially separated, and which communicate with one another by exchanging messages. A network of interconnected computers, such as the ARPA net, is a distributed system. A single computer can also be viewed as a distributed system in which the central control unit, the memory units, and the input-output channels are separate processes. A system is distributed if the message transmission delay is not negligible compared to the time between events in a single process. We will concern ourselves primarily with systems of spatially separated computers. However, many of our remarks will apply more generally. In particular, a multiprocessing system on a single computer involves problems similar to those of a distributed system because of the unpredictable order in which certain events can occur. In a distributed system, it is sometimes impossible to say that one of two events occurred first. The relation \"happened before\" is therefore only a partial ordering of the events in the system. We have found that problems often arise because people are not fully aware of this fact and its implications. In this paper, we discuss the partial ordering defined by the \"happened before\" relation, and give a distributed algorithm for extending it to a consistent total ordering of all the events. This algorithm can provide a useful mechanism for implementing a distributed system. We illustrate its use with a simple method for solving synchronization problems. Unexpected, anomalous behavior can occur if the ordering obtained by this algorithm differs from that perceived by the user. This can be avoided by introducing real, physical clocks. We describe a simple method for synchronizing these clocks, and derive an upper bound on how far out of synchrony they can drift.","title":"Introduction"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/Ordering-draft/scattered-thoughts-Causal-ordering/","text":"scattered-thoughts Causal ordering NOTE: \"causal\"\u7684\u610f\u601d\u662f\"\u56e0\u679c\" Causal ordering is a vital tool for thinking about distributed systems . Once you understand it, many other concepts become much simpler. We\u2019ll start with the fundamental property of distributed systems: Messages sent between machines may arrive zero or more times at any point after they are sent NOTE: \u5728\u673a\u5668\u4e4b\u95f4\u53d1\u9001\u7684\u6d88\u606f\u53ef\u80fd\u5728\u53d1\u9001\u540e\u7684\u4efb\u4f55\u65f6\u95f4\u5230\u8fbe\u96f6\u6b21\u6216\u591a\u6b21 This is the sole reason that building distributed systems is hard. NOTE: \u8fd9\u662f\u6784\u5efa\u5206\u5e03\u5f0f\u7cfb\u7edf\u56f0\u96be\u7684\u552f\u4e00\u539f\u56e0\u3002 For example, because of this property it is impossible for two computers communicating over a network to agree on the exact time. You can send me a message saying \u201cit is now 10:00:00\u201d but I don\u2019t know how long it took for that message to arrive. We can send messages back and forth all day but we will never know for sure that we are synchronised. NOTE: \u4f8b\u5982\uff0c\u7531\u4e8e\u8fd9\u4e2a\u7279\u6027\uff0c\u5728\u7f51\u7edc\u4e0a\u901a\u4fe1\u7684\u4e24\u53f0\u8ba1\u7b97\u673a\u4e0d\u53ef\u80fd\u5c31\u786e\u5207\u7684\u65f6\u95f4\u8fbe\u6210\u4e00\u81f4\u3002\u4f60\u53ef\u4ee5\u7ed9\u6211\u53d1\u4fe1\u606f\u8bf4\u201c\u73b0\u5728\u662f10\u70b9\u4e86\u201d\uff0c\u4f46\u6211\u4e0d\u77e5\u9053\u8fd9\u6761\u4fe1\u606f\u82b1\u4e86\u591a\u957f\u65f6\u95f4\u5230\u8fbe\u3002\u6211\u4eec\u53ef\u4ee5\u6574\u5929\u6765\u56de\u53d1\u9001\u4fe1\u606f\uff0c\u4f46\u6211\u4eec\u6c38\u8fdc\u65e0\u6cd5\u786e\u5b9a\u6211\u4eec\u662f\u5426\u540c\u6b65\u3002 If we can\u2019t agree on the time then we can\u2019t always agree on what order things happen in. Suppose I say \u201cmy user logged on at 10:00:00\u201d and you say \u201cmy user logged on at 10:00:01\u201d. Maybe mine was first or maybe my clock is just fast relative to yours. The only way to know for sure is if something connects those two events. For example, if my user logged on and then sent your user an email and if you received that email before your user logged on then we know for sure that mine was first. NOTE: \u4f8b\u5982\uff0c\u5982\u679c\u6211\u7684\u7528\u6237\u767b\u5f55\u540e\u5411\u60a8\u7684\u7528\u6237\u53d1\u9001\u4e86\u4e00\u5c01\u7535\u5b50\u90ae\u4ef6\uff0c\u5982\u679c\u60a8\u5728\u7528\u6237\u767b\u5f55\u4e4b\u524d\u6536\u5230\u4e86\u8fd9\u5c01\u7535\u5b50\u90ae\u4ef6\uff0c\u90a3\u4e48\u6211\u4eec\u53ef\u4ee5\u786e\u5b9a\u6211\u7684\u7528\u6237\u662f\u7b2c\u4e00\u4e2a\u767b\u5f55\u7684\u3002 This concept is called causal ordering and is written like this: A -> B (event A is causally ordered before event B) Let\u2019s define it a little more formally. We model the world as follows: We have a number of machines on which we observe a series of events. These events are either specific to one machine (eg user input) or are communications between machines. We define the causal ordering of these events by three rules: If A and B happen on the same machine and A happens before B then A -> B If I send you some message M and you receive it then (send M) -> (recv M) If A -> B and B -> C then A -> C We are used to thinking of ordering by time which is a total order - every pair of events can be placed in some order. In contrast, causal ordering is only a partial order - sometimes events happen with no possible causal relationship i.e. not (A -> B or B -> A). This image shows a nice way to picture these relationships. On a single machine causal ordering is exactly the same as time ordering (actually, on a multi-core machine the situation is more complicated , but let\u2019s forget about that for now). Between machines causal ordering is conveyed by messages . Since sending messages is the only way for machines to affect each other this gives rise to a nice property: If not(A -> B) then A cannot possibly have caused B Since we don\u2019t have a single global time this is the only thing that allows us to reason about causality in a distributed system. This is really important so let\u2019s say it again: Communication bounds causality The lack of a total global order is not just an accidental property of computer systems, it is a fundamental property of the laws of physics. I claimed that understanding causal order makes many other concepts much simpler. Let\u2019s skim over some examples. Vector Clocks Lamport clocks and Vector clocks are data-structures which efficiently approximate\uff08\u8fd1\u4f3c\uff09 the causal ordering and so can be used by programs to reason about causality . If A -> B then LC_A < LC_B If VC_A < VC_B then A -> B Different types of vector clock trade-off compression vs accuracy by storing smaller or larger portions of the causal history of an event. Consistency When mutable state is distributed over multiple machines each machine can receive update events at different times and in different orders. If the final state is dependent on the order of updates then the system must choose a single serialisation of the events, imposing a global total order. A distributed system is consistent exactly when the outside world can never observe two different serialisations. CAP Theorem The CAP (Consistency-Availability-Partition) theorem also boils down to causality. When a machine in a distributed system is asked to perform an action that depends on its current state it must decide that state by choosing a serialisation of the events it has seen. NOTE: CAP(\u4e00\u81f4\u6027-\u53ef\u7528\u6027-\u5212\u5206)\u5b9a\u7406\u4e5f\u5f52\u7ed3\u4e3a\u56e0\u679c\u5173\u7cfb\u3002\u5f53\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u673a\u5668\u88ab\u8981\u6c42\u6267\u884c\u4e00\u4e2a\u4f9d\u8d56\u4e8e\u5176\u5f53\u524d\u72b6\u6001\u7684\u64cd\u4f5c\u65f6\uff0c\u5b83\u5fc5\u987b\u901a\u8fc7\u9009\u62e9\u5b83\u6240\u770b\u5230\u7684\u4e8b\u4ef6\u7684\u5e8f\u5217\u5316\u6765\u51b3\u5b9a\u8be5\u72b6\u6001\u3002 \u5b83\u6709\u4e24\u4e2a\u9009\u62e9: It has two options: 1\u3001Choose a serialisation of its current events immediately 2\u3001Wait until it is sure it has seen all concurrent events before choosing a serialisation The first choice risks violating consistency if some other machine makes the same choice with a different set of events. The second violates availability by waiting for every other machine that could possibly have received a conflicting event before performing the requested action. There is no need for an actual network partition to happen - the trade-off between availability and consistency exists whenever communication between components is not instant. We can state this even more simply: Ordering requires waiting Even your hardware cannot escape this law. It provides the illusion of synchronous access to memory at the cost of availabilty. If you want to write fast parallel programs then you need to understand the messaging model used by the underlying hardware. Eventual Consistency A system is eventually consistent if the final state of each machine is the same regardless of how we choose to serialise update events. An eventually consistent system allows us to sacrifice consistency for availability without having the state of different machines diverge irreparably. It doesn\u2019t save us from having the outside world see different serialisations of update events. It is also difficult to construct eventually consistent data structures and to reason about their composition. Further reading CRDTs provide guidance on constructing eventually consistent data-structures. Bloom is a logic-based DSL for writing distributed systems. The core observation is that there is a natural connection between monotonic logic programs (logic programs which do not have to retract output when given additional inputs) and available distributed systems (where individual machines do not have to wait until all possible inputs have been received before producing output). Recent work from the Bloom group shows how to merge their approach with the CRDT approach to get the best of both worlds. Nathan Marz suggests an architecture for data processing systems which avoids much of the pain caused by the CAP theorem. In short, combine a consistent batch-processing layer with an available, eventually consistent real-time layer so that the system as a whole is available but any errors in the (complicated, difficult to program) eventually consistent layer are transient and cannot corrupt the consistent data store.","title":"scattered-thoughts [Causal ordering](https://scattered-thoughts.net/blog/2012/08/16/causal-ordering/)"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/Ordering-draft/scattered-thoughts-Causal-ordering/#scattered-thoughts#causal#ordering","text":"NOTE: \"causal\"\u7684\u610f\u601d\u662f\"\u56e0\u679c\" Causal ordering is a vital tool for thinking about distributed systems . Once you understand it, many other concepts become much simpler. We\u2019ll start with the fundamental property of distributed systems: Messages sent between machines may arrive zero or more times at any point after they are sent NOTE: \u5728\u673a\u5668\u4e4b\u95f4\u53d1\u9001\u7684\u6d88\u606f\u53ef\u80fd\u5728\u53d1\u9001\u540e\u7684\u4efb\u4f55\u65f6\u95f4\u5230\u8fbe\u96f6\u6b21\u6216\u591a\u6b21 This is the sole reason that building distributed systems is hard. NOTE: \u8fd9\u662f\u6784\u5efa\u5206\u5e03\u5f0f\u7cfb\u7edf\u56f0\u96be\u7684\u552f\u4e00\u539f\u56e0\u3002 For example, because of this property it is impossible for two computers communicating over a network to agree on the exact time. You can send me a message saying \u201cit is now 10:00:00\u201d but I don\u2019t know how long it took for that message to arrive. We can send messages back and forth all day but we will never know for sure that we are synchronised. NOTE: \u4f8b\u5982\uff0c\u7531\u4e8e\u8fd9\u4e2a\u7279\u6027\uff0c\u5728\u7f51\u7edc\u4e0a\u901a\u4fe1\u7684\u4e24\u53f0\u8ba1\u7b97\u673a\u4e0d\u53ef\u80fd\u5c31\u786e\u5207\u7684\u65f6\u95f4\u8fbe\u6210\u4e00\u81f4\u3002\u4f60\u53ef\u4ee5\u7ed9\u6211\u53d1\u4fe1\u606f\u8bf4\u201c\u73b0\u5728\u662f10\u70b9\u4e86\u201d\uff0c\u4f46\u6211\u4e0d\u77e5\u9053\u8fd9\u6761\u4fe1\u606f\u82b1\u4e86\u591a\u957f\u65f6\u95f4\u5230\u8fbe\u3002\u6211\u4eec\u53ef\u4ee5\u6574\u5929\u6765\u56de\u53d1\u9001\u4fe1\u606f\uff0c\u4f46\u6211\u4eec\u6c38\u8fdc\u65e0\u6cd5\u786e\u5b9a\u6211\u4eec\u662f\u5426\u540c\u6b65\u3002 If we can\u2019t agree on the time then we can\u2019t always agree on what order things happen in. Suppose I say \u201cmy user logged on at 10:00:00\u201d and you say \u201cmy user logged on at 10:00:01\u201d. Maybe mine was first or maybe my clock is just fast relative to yours. The only way to know for sure is if something connects those two events. For example, if my user logged on and then sent your user an email and if you received that email before your user logged on then we know for sure that mine was first. NOTE: \u4f8b\u5982\uff0c\u5982\u679c\u6211\u7684\u7528\u6237\u767b\u5f55\u540e\u5411\u60a8\u7684\u7528\u6237\u53d1\u9001\u4e86\u4e00\u5c01\u7535\u5b50\u90ae\u4ef6\uff0c\u5982\u679c\u60a8\u5728\u7528\u6237\u767b\u5f55\u4e4b\u524d\u6536\u5230\u4e86\u8fd9\u5c01\u7535\u5b50\u90ae\u4ef6\uff0c\u90a3\u4e48\u6211\u4eec\u53ef\u4ee5\u786e\u5b9a\u6211\u7684\u7528\u6237\u662f\u7b2c\u4e00\u4e2a\u767b\u5f55\u7684\u3002 This concept is called causal ordering and is written like this: A -> B (event A is causally ordered before event B) Let\u2019s define it a little more formally. We model the world as follows: We have a number of machines on which we observe a series of events. These events are either specific to one machine (eg user input) or are communications between machines. We define the causal ordering of these events by three rules: If A and B happen on the same machine and A happens before B then A -> B If I send you some message M and you receive it then (send M) -> (recv M) If A -> B and B -> C then A -> C We are used to thinking of ordering by time which is a total order - every pair of events can be placed in some order. In contrast, causal ordering is only a partial order - sometimes events happen with no possible causal relationship i.e. not (A -> B or B -> A). This image shows a nice way to picture these relationships. On a single machine causal ordering is exactly the same as time ordering (actually, on a multi-core machine the situation is more complicated , but let\u2019s forget about that for now). Between machines causal ordering is conveyed by messages . Since sending messages is the only way for machines to affect each other this gives rise to a nice property: If not(A -> B) then A cannot possibly have caused B Since we don\u2019t have a single global time this is the only thing that allows us to reason about causality in a distributed system. This is really important so let\u2019s say it again: Communication bounds causality The lack of a total global order is not just an accidental property of computer systems, it is a fundamental property of the laws of physics. I claimed that understanding causal order makes many other concepts much simpler. Let\u2019s skim over some examples.","title":"scattered-thoughts Causal ordering"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/Ordering-draft/scattered-thoughts-Causal-ordering/#vector#clocks","text":"Lamport clocks and Vector clocks are data-structures which efficiently approximate\uff08\u8fd1\u4f3c\uff09 the causal ordering and so can be used by programs to reason about causality . If A -> B then LC_A < LC_B If VC_A < VC_B then A -> B Different types of vector clock trade-off compression vs accuracy by storing smaller or larger portions of the causal history of an event.","title":"Vector Clocks"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/Ordering-draft/scattered-thoughts-Causal-ordering/#consistency","text":"When mutable state is distributed over multiple machines each machine can receive update events at different times and in different orders. If the final state is dependent on the order of updates then the system must choose a single serialisation of the events, imposing a global total order. A distributed system is consistent exactly when the outside world can never observe two different serialisations.","title":"Consistency"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/Ordering-draft/scattered-thoughts-Causal-ordering/#cap#theorem","text":"The CAP (Consistency-Availability-Partition) theorem also boils down to causality. When a machine in a distributed system is asked to perform an action that depends on its current state it must decide that state by choosing a serialisation of the events it has seen. NOTE: CAP(\u4e00\u81f4\u6027-\u53ef\u7528\u6027-\u5212\u5206)\u5b9a\u7406\u4e5f\u5f52\u7ed3\u4e3a\u56e0\u679c\u5173\u7cfb\u3002\u5f53\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u673a\u5668\u88ab\u8981\u6c42\u6267\u884c\u4e00\u4e2a\u4f9d\u8d56\u4e8e\u5176\u5f53\u524d\u72b6\u6001\u7684\u64cd\u4f5c\u65f6\uff0c\u5b83\u5fc5\u987b\u901a\u8fc7\u9009\u62e9\u5b83\u6240\u770b\u5230\u7684\u4e8b\u4ef6\u7684\u5e8f\u5217\u5316\u6765\u51b3\u5b9a\u8be5\u72b6\u6001\u3002 \u5b83\u6709\u4e24\u4e2a\u9009\u62e9: It has two options: 1\u3001Choose a serialisation of its current events immediately 2\u3001Wait until it is sure it has seen all concurrent events before choosing a serialisation The first choice risks violating consistency if some other machine makes the same choice with a different set of events. The second violates availability by waiting for every other machine that could possibly have received a conflicting event before performing the requested action. There is no need for an actual network partition to happen - the trade-off between availability and consistency exists whenever communication between components is not instant. We can state this even more simply: Ordering requires waiting Even your hardware cannot escape this law. It provides the illusion of synchronous access to memory at the cost of availabilty. If you want to write fast parallel programs then you need to understand the messaging model used by the underlying hardware.","title":"CAP Theorem"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/Ordering-draft/scattered-thoughts-Causal-ordering/#eventual#consistency","text":"A system is eventually consistent if the final state of each machine is the same regardless of how we choose to serialise update events. An eventually consistent system allows us to sacrifice consistency for availability without having the state of different machines diverge irreparably. It doesn\u2019t save us from having the outside world see different serialisations of update events. It is also difficult to construct eventually consistent data structures and to reason about their composition.","title":"Eventual Consistency"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/Ordering-draft/scattered-thoughts-Causal-ordering/#further#reading","text":"CRDTs provide guidance on constructing eventually consistent data-structures. Bloom is a logic-based DSL for writing distributed systems. The core observation is that there is a natural connection between monotonic logic programs (logic programs which do not have to retract output when given additional inputs) and available distributed systems (where individual machines do not have to wait until all possible inputs have been received before producing output). Recent work from the Bloom group shows how to merge their approach with the CRDT approach to get the best of both worlds. Nathan Marz suggests an architecture for data processing systems which avoids much of the pain caused by the CAP theorem. In short, combine a consistent batch-processing layer with an available, eventually consistent real-time layer so that the system as a whole is available but any errors in the (complicated, difficult to program) eventually consistent layer are transient and cannot corrupt the consistent data store.","title":"Further reading"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/Vector-clock/","text":"wanweibaike Vector clock NOTE: \u8fd8\u672a\u5b66\u4e60","title":"Introduction"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/Vector-clock/#wanweibaike#vector#clock","text":"NOTE: \u8fd8\u672a\u5b66\u4e60","title":"wanweibaike Vector clock"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/Version-vector/","text":"Version vector \u6682\u672a\u5b66\u4e60\u3002 wanweibaike Version vector Example zookeeper","title":"Introduction"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/Version-vector/#version#vector","text":"\u6682\u672a\u5b66\u4e60\u3002","title":"Version vector"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/Version-vector/#wanweibaike#version#vector","text":"","title":"wanweibaike Version vector"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/Version-vector/#example","text":"zookeeper","title":"Example"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/wikipedia-Logical-clock/","text":"wikipedia Logical clock A logical clock is a mechanism for capturing chronological and causal relationships in a distributed system . Distributed systems may have no physically synchronous global clock, so a logical clock allows global ordering on events from different processes in such systems. The first implementation, the Lamport timestamps , was proposed by Leslie Lamport in 1978 ( Turing Award in 2013). NOTE: \u4e00\u3001 chronological relationship \u65f6\u95f4\u5173\u7cfb causal relationship \u56e0\u679c\u5173\u7cfb \u4e8c\u3001\"Distributed systems may have no physically synchronous global clock\" \u8fd9\u662f\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u6311\u6218 Algorithms Some noteworthy logical clock algorithms are: 1\u3001 Lamport timestamps , which are monotonically increasing software counters. NOTE: \u975e\u5e38\u7b80\u5355\uff0c\u4f46\u662f\u5177\u6709\u91cd\u8981\u7684\u610f\u4e49 2\u3001 Vector clocks , that allow for partial ordering of events in a distributed system. 3\u3001 Version vectors , order replicas, according to updates, in an optimistic replicated system . 4\u3001 Matrix clocks , an extension of vector clocks that also contains information about other processes' views of the system. NOTE: \u4ece\u540d\u79f0\u5c31\u53ef\u4ee5\u770b\u51fa\u662fvector clock\u7684\u5347\u7ea7\u7248","title":"Introduction"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/wikipedia-Logical-clock/#wikipedia#logical#clock","text":"A logical clock is a mechanism for capturing chronological and causal relationships in a distributed system . Distributed systems may have no physically synchronous global clock, so a logical clock allows global ordering on events from different processes in such systems. The first implementation, the Lamport timestamps , was proposed by Leslie Lamport in 1978 ( Turing Award in 2013). NOTE: \u4e00\u3001 chronological relationship \u65f6\u95f4\u5173\u7cfb causal relationship \u56e0\u679c\u5173\u7cfb \u4e8c\u3001\"Distributed systems may have no physically synchronous global clock\" \u8fd9\u662f\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u6311\u6218","title":"wikipedia Logical clock"},{"location":"Distributed-computing/Theory/Time%26Ordering/Logical-clock/wikipedia-Logical-clock/#algorithms","text":"Some noteworthy logical clock algorithms are: 1\u3001 Lamport timestamps , which are monotonically increasing software counters. NOTE: \u975e\u5e38\u7b80\u5355\uff0c\u4f46\u662f\u5177\u6709\u91cd\u8981\u7684\u610f\u4e49 2\u3001 Vector clocks , that allow for partial ordering of events in a distributed system. 3\u3001 Version vectors , order replicas, according to updates, in an optimistic replicated system . 4\u3001 Matrix clocks , an extension of vector clocks that also contains information about other processes' views of the system. NOTE: \u4ece\u540d\u79f0\u5c31\u53ef\u4ee5\u770b\u51fa\u662fvector clock\u7684\u5347\u7ea7\u7248","title":"Algorithms"},{"location":"Distributed-computing/dist-prog-book/","text":"Programming Models for DISTRIBUTED COMPUTING","title":"Introduction"},{"location":"Distributed-computing/dist-prog-book/#programming#models#for#distributed#computing","text":"","title":"Programming Models for DISTRIBUTED COMPUTING"},{"location":"Distributed-computing/dist-prog-book/chapter-2/Futures-and-Promises/","text":"Futures and Promises BY KISALAYA PRASAD, AVANTI PATIL, AND HEATHER MILLER Futures and promises are a popular abstraction for asynchronous programming , especially in the context of distributed systems . We'll cover the motivation for and history of these abstractions, and see how they have evolved over time. We\u2019ll do a deep dive into their differing semantics, and various models of execution . This isn't only history and evolution; we\u2019ll also dive into futures and promises most widely utilized today in languages like JavaScript, Scala, and C++. (55 min read) NOTE: in distributed system\uff0casynchronous programming \u6709\u7740\u5e7f\u6cdb\u7684\u5e94\u7528\u3002 NOTE: \"model of execution\"\uff0c\u5728\u540e\u6587\u4e2d\u4f1a\u53cd\u590d\u51fa\u73b0\u3002 Introduction As human beings we have the ability to multitask i.e. we can walk, talk, and eat at the same time except when sneezing(\u6253\u55b7\u568f). Sneezing is a blocking activity because it forces you to stop what you\u2019re doing for a brief moment, and then you resume where you left off. One can think of the human sense of multitasking as multithreading in the context of computers. Consider for a moment a simple computer processor; no parallelism, just the ability to complete one task or process at a time. In this scenario, sometimes the processor is blocked when some blocking operation is called. Such blocking calls can include I/O operations like reading/writing to disk, or sending or receiving packets over the network. And as programmers, we know that blocking calls like I/O can take disproportionately more time than a typical CPU-bound task, like iterating over a list. The processor can handle blocking calls in two ways: 1) Synchronously : the processor waits until the blocking call completes its task and returns the result. Afterwards, the processor will move on to processing the next task. This can oftentimes be problematic because the CPU may not be utilized in an efficient manner; it may wait for long periods of time. 2) Asynchronously : When tasks are processed asynchronously, CPU time spent waiting in the synchronous case is instead spent processing some other task using a preemptive(\u62a2\u5360\u5f0f) time sharing algorithm. That is, rather than wait, process some other task instead. Thus, the processor is never left waiting as long as there is more work that can be done. NOTE:\u73b0\u4ee3CPU\u90fd\u662f\u4f7f\u7528\u7684\u8fd9\u79cd\u8c03\u5ea6\u7b56\u7565\u3002\u8054\u60f3\u5230concurrency programming\u4e2d\u7684\u9519\u53d1\uff0c\u5176\u5b9e\u548cCPU\u505a\u6cd5\u80cc\u540e\u7684\u601d\u60f3\u662f\u4e00\u6837\u7684\u3002\u8fd9\u63d0\u793a\u4e86\u6211\u4eec\uff0c\u76f8\u4f3c\u7684\u95ee\u9898\uff0c\u5728computer science\u7684\u5404\u4e2a\u5c42\u6b21\u90fd\u4f1a\u51fa\u73b0\uff0c\u53ef\u4ee5\u91c7\u7528\u76f8\u540c\u7684\u601d\u60f3\u3001\u76f8\u4f3c\u7684\u89e3\u51b3\u65b9\u6cd5\u3002 \u5f53\u4f7f\u7528asynchronous\u65b9\u5f0f\u7684\u65f6\u5019\uff0c\u5fc5\u7136\u4f1a\u6d89\u53ca\u5230\u7684\u4e00\u4e2a\u95ee\u9898\u662f: \u5f53asynchronous operation\u5b8c\u6210\u7684\u65f6\u5019\uff0c\u5982\u4f55\u901a\u77e5\uff1f\u8fd9\u5728hardware\u5c42\u662f\u901a\u8fc7interrupt\u7684\u65b9\u5f0f\u6765\u5b9e\u73b0\u7684\u3002 In the world of programming, many constructs have been introduced in order to help programmers reach ideal levels of resource utilization. Arguably one of the most widely-used of which are futures and/or promises . In this chapter, we\u2019ll do a deep dive into futures and promises, a popular abstraction for doing both synchronous and asynchronous programming . We\u2019ll go through the motivation for and history of these abstractions, understand what sort of scenarios they\u2019re useful for, and cover how they have evolved over time. We\u2019ll cover the various models of execution associated with these abstractions, and finally we\u2019ll touch on the futures and promises most widely utilized today in different general purpose programming languages such as JavaScript, Scala, and C++. NOTE: futures and promises is a popular abstraction for doing both synchronous and asynchronous programming . The Basic Idea As we\u2019ll see in subsequent sections, what we choose to call this concept, and its precise definition tends to vary. We will start with the widest possible definition of the concept of a future/promise, and later zoom in and cover the many semantic differences between different languages\u2019 interpretations of these constructs. NOTE: \u4e0d\u540c\u7684programminglanguage\uff0c\u5bf9future \u548c promise \u7684\u89e3\u91ca\u53ef\u80fd\u662f\u4e0d\u540c\u7684 In the broadest sense, A future or promise can be thought of as a value that will eventually become available. NOTE: future\u5728\u91d1\u878d\u9886\u57df\u7684\u542b\u4e49\u662f\u201c\u671f\u8d27\u201d\uff0c\u5728computer science\u4e2d\uff0c\u5b83\u7684\u542b\u4e49\u5219\u662f\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u6240\u63cf\u8ff0\u7684\uff0c\u5b83\u8868\u793a\u7684\u662f\u5728\u4e3a\u4e86\u67d0\u4e2a\u65f6\u523b\uff0cvalue\u5c06available\u3002\u5bf9\u6bd4future\u5728\u8fd9\u4e24\u4e2a\u9886\u57df\u4e2d\u7684\u542b\u4e49\uff0c\u53ef\u4ee5\u770b\u51fa\u8fd9\u4e2a\u8bcd\u8868\u793a\u7684\u662f\u67d0\u4e2a\u548c**\u672a\u6765**\u6709\u5173\u7684\u4e8b\u7269\uff0c\u5b83\u662f\u4e00\u79cd\u62bd\u8c61\uff0c\u4e5f\u5c31\u662f\u4e0b\u9762\u7684\u8fd9\u6bb5\u8bdd\u4e2d\u7684\"it is an abstraction which encodes a notion of time\"\uff1b\u663e\u7136\u8fd9\u662f\u548c**\u65f6\u95f4**\u76f8\u5173\u7684\u3002 \u5176\u5b9e\u5e76\u4e0d\u9700\u8981\u628afuture\u548cpromise\u60f3\u5f97\u7384\u4e4e\uff0c\u5b8c\u5168\u53ef\u4ee5\u4f7f\u7528function\u8c03\u7528\u5e76\u83b7\u5f97\u51fd\u6570\u7684\u8fd4\u56de\u503c\u7684\u8fc7\u7a0b\u5bf9\u5176\u8fdb\u884c\u7c7b\u6bd4\uff0c\u51fd\u6570\u7684\u6267\u884c\u8fc7\u7a0b\u53ef\u80fd\u9700\u8981\u8017\u8d39\u4e00\u5b9a\u7684\u65f6\u95f4\uff0c\u6240\u4ee5\u9700\u8981\u7b49\u5f85\u4e00\u5b9a\u7684\u65f6\u95f4\u624d\u80fd\u591f\u83b7\u5f97\u5176\u8fd4\u56de\u503c\uff0c\u663e\u7136\u7b49\u5f85\u4e00\u5b9a\u7684\u65f6\u95f4\u5c31\u53ef\u4ee5\u5c31\u53ef\u4ee5\u83b7\u5f97\u4e00\u4e2avalue\u7684\u8fc7\u7a0b\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528future\u5bf9\u5176\u8fdb\u884c\u63cf\u8ff0\uff1b\u73b0\u5728\u60f3\u6765\u8fd9\u79cd\u62bd\u8c61\u662f\u975e\u5e38\u9002\u5408\u4e8e**concurrency**\uff0c synchronous and asynchronous programming \u3002 \u5bf9future/promise\u7684\u7c7b\u6bd4: 1) \u6211\u89c9\u5f97promise/future\uff0c\u5176\u5b9e\u548cC++ vocabulary type\u4e2d\u7684 std::optional \u3001 std::any \u3001 std::variant \u6709\u4e9b\u7c7b\u4f3c; 2) \u5728wikipedia Futures and promises \u4e2d\uff0c\u5c06promise/future\u6bd4\u4f5cplaceholder 3) future/promise\u662fsymbol\uff0c\u53c2\u89c1symbolic programming\uff0c\u5b83\u4ee3\u8868\u4e86\u67d0\u4e2a\u672a\u6765\u7684\u503c Or said another way, it is an abstraction which encodes a notion of time . State By choosing to use this construct, it is assumed that your value can now have many possible states , depending on the point in time which we request it. The simplest variation includes two time-dependent states; a future/promise is either: 1) completed/determined : the computation is complete and the future/promise\u2019s value is available. 2) incomplete/undetermined : the computation is not yet complete. As we\u2019ll later see, other states have been introduced in some variations of futures/promises to better support needs like error-handling and cancellation . Importantly, futures/promises typically enable some degree of concurrency . That is, in one of first defitions of futures: The construct ( future X ) immediately returns a future for the value of the expression X and concurrently begins evaluating X . When the evaluation of X yields a value, that value replaces the future. (Halstead, 1985) NOTE: In wikipedia Futures and promises it calls this property a placeholder . Some interpretations(\u89e3\u91ca) of futures/promises have a type associated with them, others not. Typically a future/promise is single-assignment ; that is, it can only be written to once. Some interpretations are blocking ( synchronous ), others are completely non-blocking ( asynchronous ). Some interpretations must be explicitly kicked off (i.e. manually started), while in other interpretations, computation is started implicitly. Functional programming: pipeline/composition NOTE: future/promise\u662f\u8d77\u6e90\u81eafunctional programming\u7684 Inspired by functional programming , one of the major distinctions between different interpretations of this construct have to do with pipelineing or composition . Some of the more popular interpretations of futures/promises make it possible to chain operations, or define a pipeline of operations to be invoked upon completion of the computation represented by the future/promise . This is in contrast to callback-heavy or more imperative direct blocking approaches. Motivation and Uses NOTE: \u8fd9\u6bb5\u603b\u7ed3\u4e86future/promise\u7684use case\u3002 The rise of promises and futures as a topic of relevance has for the most part occurred alongside of the rise of parallel and concurrent programming and distributed systems . This follows somewhat naturally, since, as an abstraction which encodes time, futures/promises introduce a nice way to reason about state changes when latency becomes an issue; a common concern faced by programmers when a node must communicate with another node in a distributed system. NOTE: \u73b0\u5728\u770b\u6765\u5728\u79d1\u5b66\u7684\u6bcf\u4e2a\u9886\u57df\uff0c\u521b\u9020\u51fa\u5bf9\u95ee\u9898\u7684\u63cf\u8ff0\uff0c\u523b\u753b\u65b9\u5f0f\u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u5c31\u597d\u6bd4\u6b64\u5904\u7684future/promise\u4e4b\u4e8e parallel and concurrent programming and distributed systems \u3002 However promises and futures are considered useful in a number of other contexts as well, both distributed and not. Some such contexts include: 1) Request-Response Patterns , such as web service calls over HTTP. A future may be used to represent the value of the response of the HTTP request. 2) Input/Output , such as UI dialogs requiring user input, or operations such as reading large files from disk. A future may be used to represent the IO call and the resulting value of the IO (e.g., terminal input, array of bytes of a file that was read). 3) Long-Running Computations . Imagine you would like the process which initiated a long-running computation, such as a complex numerical algorithm, not to wait on the completion of that long-running computation and to instead move on to process some other task. A future may be used to represent this long-running computation and the value of its result. 4) Database Queries . Like long-running computations, database queries can be time-consuming. Thus, like above, it may be desirable to offload the work of doing the query to another process and move on to processing the next task. A future may be used to represent the query and resulting value of the query. 5) RPC (Remote Procedure Call) . Network latency is typically an issue when making an RPC call to a server. Like above, it may be desirable to not have to wait on the result of the RPC invocation by instead offloading it to another process. A future may be used to represent the RPC call and its result; when the server responds with a result, the future is completed and its value is the server\u2019s response. 6) Reading Data from a Socket can be time-consuming particularly due to network latency. It may thus be desirable to not to have to wait on incoming data, and instead to offload it to another process. A future may be used to represent the reading operation and the resulting value of what it reads when the future is completed. 7) Timeouts , such as managing timeouts in a web service. A future representing a timeout could simply return no result or some kind of empty result like the Unit type in typed programming languages. Many real world services and systems today make heavy use of futures/promises in popular contexts such as these, thanks to the notion of a future or a promise having been introduced in popular languages and frameworks such as JavaScript, Node.js , Scala, Java, C++, amongst many others. As we will see in further sections, this proliferation\uff08\u6269\u6563\uff09 of futures/promises has resulted in futures/promises changing meanings and names over time and across languages. NOTE: \u5728**futures/promises**\u7684\u53d1\u5c55\u8fc7\u7a0b\u4e2d\uff0c\u5176\u542b\u4e49\u4ee5\u53ca\u53d1\u751f\u4e86\u53d8\u5316\u3002 Diverging Terminology Future , Promise , Delay or Deferred generally refer to roughly the same synchronization mechanism where an object acts as a proxy for as-of-yet unknown result. When the result is available, some other code then gets executed. Over the years, these terms have come to refer to slightly different semantic meanings between languages and ecosystems. NOTE: \u663e\u7136\uff0c\u4e0a\u9762\u6240\u8bf4\u7684proxy\uff0c\u5e94\u8be5\u662f virtual proxy \u3002 Sometimes, a language may have one construct named future , promise , delay , deferred , etc. However, in other cases, a language may have two constructs, typically referred to as futures and promises. Languages like Scala, Java, and Dart fall into this category. In this case, A Future is a read-only reference to a yet-to-be-computed(\u5c06\u8981\u88ab\u8ba1\u7b97\u7684) value. A Promise (or a CompletableFuture / Completer /etc.) is a single-assignment variable which the Future refers to. In other words, a future is a read-only window to a value written into a promise . You can get the Future associated with a Promise by calling the future method on it, but conversion in the other direction is not possible. Another way to look at it would be, if you promise something to someone, you are responsible for keeping it, but if someone else makes a promise to you, you expect them to honor it in the future \uff08\u53e6\u4e00\u79cd\u770b\u5f85\u5b83\u7684\u65b9\u6cd5\u662f\uff0c\u5982\u679c\u4f60\u5411\u67d0\u4eba\u627f\u8bfa\uff0c\u4f60\u6709\u8d23\u4efb\u4fdd\u7559\u5b83\uff0c\u4f46\u5982\u679c\u5176\u4ed6\u4eba\u5411\u4f60\u505a\u51fa\u627f\u8bfa\uff0c\u4f60\u5e0c\u671b\u4ed6\u4eec\u5728\u5c06\u6765\u5c0a\u91cd\u5b83\uff09. NOTE:\u53ef\u4ee5\u901a\u8fc7\u5728 Future \u4e0a\u8c03\u7528 future \u53d1\u653e\u6765\u83b7\u5f97\u8be5 Future \u76f8\u5173\u8054\u7684 Promise \uff0c\u4f46\u662f\u53cd\u65b9\u5411\u7684\u8f6c\u6362\u662f\u4e0d\u53ef\u80fd\u7684\uff0c\u4e5f\u5c31\u662f\u7ed9\u5b9a**promise**\u65e0\u6cd5\u5f97\u5230\u5b83\u7684**future**\u3002\u4e4b\u4e8e\u4e3a\u4ec0\u4e48\u8fd9\u6837\uff0c\u6211\u89c9\u5f97\u53ef\u80fd\u662f\u76ee\u524d**promise**\u8fd8\u6ca1\u6709\u6267\u884c\uff0c\u6240\u4ee5\u5b83\u7684future\u6ca1\u6709\u503c\u3002\u6700\u540e\u4e00\u6bb5\u8bdd\u662f\u4f5c\u8005\u7684\u4e00\u4e2a\u6bd4\u55bb\u3002 Scala In Scala, they are defined as follows: A future is a placeholder object for a result that does not yet exist. A promise is a writable, single-assignment container, which completes a future. Promises can complete the future with a result to indicate success, or with an exception to indicate failure. (Haller et al., 2013) An important difference between Scala and Java (6) futures is that Scala futures are asynchronous in nature. Java\u2019s future, at least till Java 6, were blocking. Java 7 introduced asynchronous futures to great fanfare. Java In Java 8, the Future<T> interface has methods to check if the computation is complete, to wait for its completion, and to retrieve the result of the computation when it is complete. CompletableFutures can be thought of as a promise , since their value can be explicitly set. However, CompletableFuture also implements the Future interface allowing it to be used as a Future as well. Promises can be thought of as a future with a public set method which the caller (or anybody else) can use to set the value of the future. NOTE:The last paragraph is a metaphor\u3002 JQuery In the JavaScript world, JQuery introduces a notion of Deferred objects which are used to represent a unit of work which is not yet finished. The Deferred object contains a promise object which represents the result of that unit of work. Promises are values returned by a function. The deferred object can also be canceled by its caller. C# Like Scala and Java, C# also makes the distinction between the future and promise constructs described above. In C#, futures are called Task<T> s, and promises are called TaskCompletionSource<T> . The result of the future is available in the read-only property Task<T>.Result which returns T , and TaskCompletionSource<T>.Task<TResult> has methods to complete the Task object with a result of type T or with an exception or cancellation. Important to note; Task s are asynchronous in C#. JavaScript And confusingly, the JavaScript community has standardized on a single construct known as a Promise which can be used like other languages\u2019 notions of futures. The Promises specification (Promises/A+, 2013) defines only a single interface and leaves the details of completing (or fulfilling ) the promise to the implementer of the spec. Promises in JavaScript are also asynchronous and able to be pipelined . JavaScript promises are enabled by default in browsers that support ECMAScript 6 (EC6), or are available in a number of libraries such as Bluebird and Q . Summary As we can see, concepts, semantics, and terminology seem to differ between languages and library implementations of futures/promises. These differences in terminology and semantics arise from the long history and independent language communities that have proliferated\uff08\u6269\u6563\uff09 the use of futures/promises. Brief History Here\u2019s a brief glimpse at a timeline spanning the history of futures and promises as we know them today: The first concept which eventually led to futures/promises appeared in 1961, with so-called thunks . Thunks can be thought of as a primitive , sequential notion of a future or promise. According to its inventor P. Z. Ingerman, thunks are: A piece of coding which provides an address (Ingerman, 1961) Thunks were designed as a way of binding actual parameters to their formal definitions in Algol-60 procedure calls. If a procedure is called with an expression in the place of a formal parameter, the compiler generates a thunk which computes the expression and leaves the address of the result in some standard location. Think of a thunk as a continuation or a function that was intended to be evaluated in a single-threaded environment. The first mention of Futures was by Baker and Hewitt in a paper on Incremental Garbage Collection of Processes (Baker & Hewitt, 1977) . They coined the term, call-by-futures , to describe a calling convention in which each formal parameter to a method is bound to a process which evaluates the expression in the parameter in parallel with other parameters. Before this paper, Algol 68 (missing reference) also presented a way to make this kind of concurrent parameter evaluation possible, using the collateral clauses and parallel clauses for parameter binding. In their paper, Baker and Hewitt introduced a notion of Futures as a 3-tuple representing an expression E consisting of: A process which evaluates E , A memory location where the result of E needs to be stored, A list of processes which are waiting on E . Though importantly, the focus of their work was not on role of futures and the role they play in asynchronous distributed computing. Instead, it was focused on garbage collecting the processes which evaluate expressions that were not needed by the function. The Multilisp language (Halstead, 1985) , presented by Halestead in 1985 built upon this call-by-future with a future annotation. In Multilisp, binding a variable to a future expression would in turn create a new process which evaluates that expression and binds it to the variable reference which represents its (eventual) result. That is, Multilisp introduced a way to compute arbitrary expressions concurrently in a new process. This made it possible to move past the actual computation and continue processing without waiting for the future to complete. If the result value of the future is never used, the initiating process will not block, thus eliminating a potential source of deadlock. MultiLisp also included a lazy future variant, called delay , which would only be evaluated the first time the value is required elsewhere in the program. This design of futures in Multilisp in turn influenced a construct dubbed promises in the Argus programming language (Liskov & Shrira, 1988; Liskov, 1988) by Liskov and Shrira in 1988. Like futures in MultiLisp, promises in Argus intended to be a placeholder for the result of a value that will be available in the future. Unlike Multilisp which was focused on single-machine concurrency, however, Argus was designed to facilitate distributed programming, and in particular focused on promises as a way to integrate asynchronous RPC into the Argus programming language. Importantly, promises were extended beyond futures in Multilisp through the introduction of types to promises. Therefore, in Argus, when a call to promise is made, a promise is created which immediately returns, and a type-safe asynchronous RPC call is made in a new process. When the RPC completes, the returned value can be claimed by the caller. Argus also introduced call streams, which can be thought of as a way to enforce an ordering of concurrently executing calls. A sender and a receiver are connected by a stream, over which it is possible make normal (synchronous) RPC calls, or stream calls , in which the sender may make more calls before receiving the reply. The underlying runtime however ensures that, despite the non-blocking stream calls fired off, that all calls and subsequent replies happen in call order. That is, call-streams ensure exactly-once, ordered delivery. Argus also introduces constructs to compose call-streams, in order to build up pipelines of computation, or directed acyclic graphs (DAGs) of computation. These semantics are an early precursor to what we refer to today as promise pipelining . E is an object-oriented programming language for secure distributed computing (Miller, Tribble, & Shapiro, 2005) , created by Mark S. Miller, Dan Bornstein, and others at Electric Communities in 1997. A major contribution of E is its interpretation and implementation of promises. It traces its routes to Joule (Tribble, Miller, Hardy, & Krieger, 1995) , a dataflow programming language predecessor to E. Importantly, E introduced an eventually operator, * <- * which enabled what is called an eventual send in E; that is, the program doesn\u2019t wait for the operation to complete and moves to next sequential statement. This is in contrast to the expected semantics of an immediate call which looks like a normal method invocation in E. Eventual sends queue a pending delivery and complete immediately, returning a promise. A pending delivery includes a resolver for the promise. Subsequent messages can also be eventually sent to a promise before it is resolved. In this case, these messages are queued up and forwarded once the promise is resolved. That is, once we have a promise, we are able to chain make several pipelined eventual sends as if the initial promise was already resolved. This notion of promise pipelining (Miller, Tribble, & Jellinghaus, 2007) has been adopted by a majority of contemporary futures/promises interpretations. Futures and promises remained primarily an academic fascination until the early 2000s, upon the rise of web application development, networked system development, and an increased need for responsive user interfaces. Among the mainstream programming languages, Python was perhaps the first, in 2002, to get a library which introduced a construct along the same lines as E\u2019s promises in the Twisted library (Lefkowitz, 2002) . Twisted introduced the notion of Deferred objects, are used to receive the result of an operation not yet completed. In Twisted, deferred objects are just like normal first-class objects; they can be passed along anywhere a normal object can, the only difference is that deferred objects don\u2019t have values. Deferred objects support callbacks, which are called once the result of the operation is complete. Perhaps most famous in recent memory is that of promises in JavaScript. In 2007, inspired by Python\u2019s Twisted library, the authors of the Dojo Toolkit came up a JavaScript implementation of Twisted\u2019s deferred objects, known as dojo.Deferred . This in turn inspired Kris Zyp to propose the CommonJS Promises/A spec in 2009 (Zyp, 2009) . The same year, Ryan Dahl introduced Node.js. In it\u2019s early versions, Node.js used promises in its non-blocking API. However, when Node.js moved away from promises to its now familiar error-first callback API (the first argument for the callback should be an error object), it left a void to fill for a promises API. Q.js is an implementation of Promises/A spec by Kris Kowal around this time (Kowal, 2009) . The FuturesJS library by AJ O\u2019Neal was another library which aimed to solve flow-control problems without using Promises in the strictest of senses. In 2011, JQuery v1.5 introduced Promises to its wider and ever-growing audience. However, JQuery\u2019s promises API was subtly different than the Promises/A spec (Gheri, 2013) . With the rise of HTML5 and different APIs, there came a problem of different and messy interfaces which added to the already infamous callback hell. The Promises/A+ spec (Promises/A+, 2013) aimed to solve this problem. Following the broad community acceptance of the Promises/A+ spec, promises were finally made a part of the ECMAScript\u00ae 2015 Language Specification (ECMAScript, Association, & others, 2015) . However, a lack of backward compatibility and additional features missing in the Promises/A+ spec means that libraries like BlueBird and Q.js still have a place in the JavaScript ecosystem. SUMMARY :\u4e0a\u9762\u7684\u5185\u5bb9\u6ca1\u6709\u9605\u8bfb Semantics of Execution \u6267\u884c\u7684\u8bed\u4e49 As architectures and runtimes have developed and changed over the years, so too have the techniques for implementing futures/promises such that the abstraction translates into efficient utilization of system resources(\u968f\u7740architectures \u548cruntimes \u591a\u5e74\u6765\u7684\u53d1\u5c55\u548c\u53d8\u5316\uff0c\u5b9e\u73b0 futures/promises\u7684\u6280\u672f\u4e5f\u8d8a\u6765\u8d8a\u591a\uff0c\u62bd\u8c61\u8f6c\u5316\u4e3a\u7cfb\u7edf\u8d44\u6e90\u7684\u6709\u6548\u5229\u7528). In this section, we\u2019ll cover the three primary executions models upon which futures/promises are built on top of in popular languages and libraries. That is, we\u2019ll see the different ways in which futures and promises actually get executed and resolved underneath their APIs. Thread Pools A thread pool is an abstraction that gives users access to a group of ready, idle threads which can be given work. Thread pool implementations take care of worker creation, management, and scheduling, which can easily become tricky and costly if not handled carefully. Thread pools come in many different flavors, with many different techniques for scheduling and executing tasks, and with fixed numbers of threads or the ability of the pool to dynamically resize itself depending on load. A classic thread pool implementation is Java\u2019s Executor , which is an object which executes the Runnable tasks. Executor s provide a way of abstracting out how the details of how a task will actually run. These details, like selecting a thread to run the task, how the task is scheduled are managed by the underlying implementation of the Executor interface. SUMMARY : Executror is a implementation of thread pool,it is elegant and fit to Java's thinking. Similar to Executor , Scala includes is a ExecutionContext s as part of the scala.concurrent package. The basic intent behind Scala\u2019s ExecutionContext is the same as Java\u2019s Executor ; it is responsible for efficiently executing computations concurrently without requiring the user of the pool to have to worry about things like scheduling. Importantly, ExecutionContext can be thought of as an interface ; that is, it is possible to swap in different underlying thread pool implementations and keep the same thread pool interface. While it\u2019s possible to use different thread pool implementations, Scala\u2019s default ExecutionContext implementation is backed by Java\u2019s ForkJoinPool ; a thread pool implementation that features a work-stealing algorithm in which idle threads pick up tasks previously scheduled to other busy threads. The ForkJoinPool is a popular thread pool implementation due to its improved performance over Executor s, its ability to better avoid pool-induced deadlock , and for minimizing the amount of time spent switching between threads. Scala\u2019s futures (and promises ) are based on this ExecutionContext interface to an underlying thread pool. While typically users use the underlying default ExecutionContext which is backed by a ForkJoinPool , users may also elect to provide (or implement) their own ExecutionContext if they need a specific behavior, like blocking futures. THINKING :\u4e3a\u4ec0\u4e48Scala\u7684future\u548cpromise\u9700\u8981\u57fa\u4e8e ExecutionContext interface?future\u548cpromise\u662f\u4e00\u79cd\u62bd\u8c61\uff0c\u6211\u4eec\u5b83\u8981\u548c ExecutionContext interface\u6346\u7ed1\u5728\u4e00\u8d77\uff1f In Scala, every usage of a future or promise requires some kind of ExecutionContext to be passed along. This parameter is implicit, and is usually ExecutionContext.global (the default underlying ForkJoinPool ExecutionContext ). For example, a creating and running a basic future: implicit val ec = ExecutionContext . global val f : Future [ String ] = Future { \u201c hello world \u201d } In this example, the global execution context is used to asynchronously run the created future. As mentioned earlier, the ExecutionContext parameter to the Future is implicit . That means that if the compiler finds an instance of an ExecutionContext in so-called implicit scope , it is automatically passed to the call to Future without the user having to explicitly pass it. In the example above, ec is put into implicit scope through the use of the implicit keyword when declaring ec . As mentioned earlier, futures and promises in Scala are asynchronous , which is achieved through the use of callbacks . For example: implicit val ec = ExecutionContext . global val f = Future { Http ( \"http://api.fixed.io/latest?base=USD\" ). asString } f . onComplete { case Success ( response ) => println ( response ) case Failure ( t ) => println ( t . getMessage ()) } In this example, we first create a future f , and when it completes, we provide two possible expressions that can be invoked depending on whether the future was executed successfully or if there was an error. In this case, if successful, we get the result of the computation an HTTP string, and we print it. If an exception was thrown, we get the message string contained within the exception and we print that. So, how does it all work together? As we mentioned, Futures require an ExecutionContext , which is an implicit parameter to virtually all of the futures API. This ExecutionContext is used to execute the future. Scala is flexible enough to let users implement their own Execution Contexts, but let\u2019s talk about the default ExecutionContext , which is a ForkJoinPool . ForkJoinPool is ideal for many small computations that spawn off and then come back together( ForkJoinPool \u662f\u8bb8\u591a\u5c0f\u578b\u8ba1\u7b97\u7684\u7406\u60f3\u9009\u62e9\uff0c\u5b83\u4eec\u4f1a\u4ea7\u751f\u5e76\u4e00\u8d77\u56de\u5f52). Scala\u2019s ForkJoinPool requires the tasks submitted to it to be a ForkJoinTask . The tasks submitted to the global ExecutionContext is quietly wrapped inside a ForkJoinTask and then executed. ForkJoinPool also supports a possibly blocking task, using the ManagedBlock method which creates a spare thread if required to ensure that there is sufficient parallelism if the current thread is blocked. To summarize, ForkJoinPool is an really good general purpose ExecutionContext , which works really well in most of the scenarios. Event Loops Modern platforms and runtimes typically rely on many underlying system layers to operate. For example, there\u2019s an underlying file system, a database system, and other web services that may be relied on by a given language implementation, library, or framework. Interaction with these components typically involves a period where we\u2019re doing nothing but waiting for the response. This can be a very large waste of computing resources. JavaScript is a single threaded asynchronous runtime . Now, conventionally(\u60ef\u5e38\u5730) async programming is generally associated with multi-threading, but we\u2019re not allowed to create new threads in JavaScript. Instead, asynchronicity in JavaScript is achieved using an event-loop mechanism. JavaScript has historically been used to interact with the DOM and user interactions in the browser, and thus an event-driven programming model was a natural fit for the language. This has scaled up surprisingly well in high throughput scenarios in Node.js. The general idea behind event-driven programming model is that the logic flow control is determined by the order in which events are processed. This is underpinned\uff08\u5de9\u56fa\uff09 by a mechanism which is constantly listening for events and fires a callback when it is detected. This is the JavaScript\u2019s event loop in a nutshell. SUMMARY :event loop\u662f\u4e00\u79cd\u4e0d\u540c\u4e8ethread pool\u7684\u6267\u884c\u6a21\u578b\uff1b A typical JavaScript engine has a few basic components. They are : Heap Used to allocate memory for objects Stack Function call frames go into a stack from where they\u2019re picked up from top to be executed\uff08\u4ece\u6808\u9876\u5f00\u59cb\u6267\u884c\uff09. Queue A message queue holds the messages to be processed. Each message has a callback function which is fired when the message is processed. These messages can be generated by user actions like button clicks or scrolling, or by actions like HTTP requests, request to a database to fetch records or reading/writing to a file. Separating when a message is queued from when it is executed means the single thread doesn\u2019t have to wait for an action to complete before moving on to another\uff08\u4f7f\u7528\u961f\u5217\u663e\u7136\u662f\u5f02\u6b65\u7684\uff0c\u65e0\u9700\u7b49\u5f85action\u5b8c\u6210\u5373\u53ef\u5904\u7406\u4e0b\u4e00\u4e2a\uff09. We attach a callback to the action we want to do, and when the time comes, the callback is run with the result of our action\uff08\u5373\u5c06\u4e0a\u4e00\u4e2a\u7684\u8f93\u51fa\u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u7684\u8f93\u5165\uff0c\u7c7b\u4f3cpipeline\uff09. Callbacks work good in isolation, but they force us into a continuation passing style of execution, what is otherwise known as Callback hell . SUMMARY :\u4e0a\u9762\u8fd9\u6bb5\u63cf\u8ff0\u4e86\u5d4c\u5957\u4f7f\u7528callback\u7684\u4f18\u52bf\uff1aisolation\uff0c\u52a3\u52bf\uff1acallback hell\u3002\u4e0b\u9762\u7684\u8fd9\u4e2a\u5c0f\u4f8b\u5b50\u5c31\u5c55\u793a\u4e86callback hell\u3002 getData = function ( param , callback ){ $ . get ( 'http://example.com/get/' + param , function ( responseText ){ callback ( responseText ); }); } getData ( 0 , function ( a ){ getData ( a , function ( b ){ getData ( b , function ( c ){ getData ( c , function ( d ){ getData ( d , function ( e ){ // ... }); }); }); }); }); VS getData = function ( param , callback ){ return new Promise ( function ( resolve , reject ) { $ . get ( 'http://example.com/get/' + param , function ( responseText ){ resolve ( responseText ); }); }); } getData ( 0 ). then ( getData ) . then ( getData ) . then ( getData ) . then ( getData ); Programs must be written for people to read, and only incidentally for machines to execute. - Harold Abelson and Gerald Jay Sussman Promises are an abstraction which make working with async operations in JavaScript much more fun. Callbacks lead to inversion of control , which is difficult to reason about at scale(\u8fdb\u4e00\u6b65\u89e3\u91cacallback hell:\u4ee3\u7801\u53ef\u8bfb\u6027\u964d\u4f4e\u4e86). Moving on from a continuation passing style , where you specify what needs to be done once the action is done, the callee\uff08\u88ab\u8c03\u51fd\u6570\uff09 simply returns a Promise object . This inverts\uff08\u98a0\u5012\uff09 the chain of responsibility, as now the caller is responsible for handling the result of the promise when it is settled. SUMMARY : The ES2015 spec specifies that \u201c promises must not fire their resolution/rejection function on the same turn of the event loop that they are created on.\u201d This is an important property because it ensures deterministic\uff08\u786e\u5b9a\u7684\uff09 order of execution. Also, once a promise is fulfilled or failed, the promise\u2019s value MUST not be changed. This ensures that a promise cannot be resolved more than once. Let\u2019s take an example to understand the promise resolution workflow as it happens inside the JavaScript Engine. Suppose we execute a function, here g() which in turn, calls another function f() . Function f returns a promise, which, after counting down for 1000 ms, resolves the promise with a single value, true. Once f gets resolved, a value true or false is alerted based on the value of the promise. Now, JavaScript\u2019s runtime is single threaded. This statement is true, and not true. The thread which executes the user code is single threaded. It executes what is on top of the stack, runs it to completion, and then moves onto what is next on the stack. But, there are also a number of helper threads which handle things like network or timer/settimeout type events. This timing thread handles the counter for setTimeout. Once the timer expires, the timer thread puts a message on the message queue. The queued up messages are then handled by the event loop. The event loop as described above, is simply an infinite loop which checks if a message is ready to be processed, picks it up and puts it on the stack for it\u2019s callback to be executed. Here, since the future is resolved with a value of true , we are alerted with a value true when the callback is picked up for execution. We\u2019ve ignored the heap here, but all the functions, variables and callbacks are stored on heap. As we\u2019ve seen here, even though JavaScript is said to be single threaded, there are number of helper threads to help main thread do things like timeout, UI, network operations, file operations etc. Run-to-completion helps us reason about the code in a nice way. Whenever a function starts, it needs to finish before yielding the main thread. The data it accesses cannot be modified by someone else. This also means every function needs to finish in a reasonable amount of time, otherwise the program seems hung. This makes JavaScript well suited for I/O tasks which are queued up and then picked up when finished, but not for data processing intensive tasks which generally take long time to finish. We haven\u2019t talked about error handling, but it gets handled the same exact way, with the error callback being called with the error object the promise is rejected with. Event loops have proven to be surprisingly performant. When network servers are designed around multithreading, as soon as you end up with a few hundred concurrent connections, the CPU spends so much of its time task switching that you start to lose overall performance. Switching from one thread to another has overhead which can add up significantly at scale. Apache used to choke even as low as a few hundred concurrent users when using a thread per connection while Node.js can scale up to a 100,000 concurrent connections based on event loops and asynchronous IO. SUMMARY \uff1a\u4e0a\u9762\u7684\u4f8b\u5b50\u6ca1\u6709\u9605\u8bfb Thread Model The Oz programming language introduced an idea of dataflow concurrency model . In Oz, whenever the program comes across an unbound variable, it waits for it to be resolved. This dataflow property of variables helps us write threads in Oz that communicate through streams in a producer-consumer pattern. The major benefit of dataflow based concurrency model is that it\u2019s deterministic - same operation called with same parameters always produces the same result. It makes it a lot easier to reason about concurrent programs, if the code is side-effect free. SUMMARY :\u4e0a\u6587\u6240\u63d0\u5230\u7684**dataflow concurrency model**\u6bd4\u8f83\u6709\u4ef7\u503c\u3002 Alice ML is a dialect of Standard ML with support for lazy evaluation , concurrent, distributed, and constraint programming. The early aim of Alice project was to reconstruct the functionalities of Oz programming language on top of a typed programming language. Building on the Standard ML dialect, Alice also provides concurrency features as part of the language through the use of a future type. Futures in Alice represent an undetermined result of a concurrent operation. Promises in Alice ML are explicit handles for futures. Any expression in Alice can be evaluated in it\u2019s own thread using spawn keyword. Spawn always returns a future which acts as a placeholder for the result of the operation. Futures in Alice ML can be thought of as functional threads, in a sense that threads in Alice always have a result. A thread is said to be touching a future if it performs an operation that requires the value future is a placeholder for. All threads touching a future are blocked until the future is resolved. If a thread raises an exception, the future is failed and this exception is re-raised in the threads touching it. Futures can also be passed along as values. This helps us achieve the dataflow model of concurrency in Alice. Alice also allows for lazy evaluation of expressions. Expressions preceded with the lazy keyword are evaluated to a lazy future. The lazy future is evaluated when it is needed. If the computation associated with a concurrent or lazy future ends with an exception, it results in a failed future. Requesting a failed future does not block, it simply raises the exception that was the cause of the failure. Implicit vs. Explicit Promises We define implicit promises as ones where we don\u2019t have to manually trigger the computation vs Explicit promises where we have to trigger the resolution of future manually, either by calling a start function or by requiring the value. This distinction can be understood in terms of what triggers the calculation: With implicit promises , the creation of a promise also triggers the computation, while with explicit futures , one needs to triggers the resolution of a promise. This trigger can in turn be explicit, like calling a start method, or implicit, like lazy evaluation where the first use of a promise\u2019s value triggers its evaluation. The idea for explicit futures were introduced in the Baker and Hewitt paper. They\u2019re a little trickier to implement, and require some support from the underlying language, and as such they aren\u2019t that common. The Baker and Hewitt paper talked about using futures as placeholders for arguments to a function, which get evaluated in parallel, but when they\u2019re needed. MultiLisp also had a mechanism to delay the evaluation of the future to the time when it\u2019s value is first used, using the defer construct. Lazy futures in Alice ML have a similar explicit invocation mechanism, the first thread touching a future triggers its evaluation. An example of explicit futures would be (from AliceML): fun enum n = lazy n :: enum (n+1) This example generates an infinite stream of integers and if stated when it is created, will compete for the system resources. Implicit futures were introduced originally by Friedman and Wise in a paper in 1978. The ideas presented in that paper inspired the design of promises in MultiLisp. Futures are also implicit in Scala and JavaScript, where they\u2019re supported as libraries on top of the core languages. Implicit futures can be implemented this way as they don\u2019t require support from language itself. Alice ML\u2019s concurrent futures are also an example of implicit invocation. In Scala, we can see an example of an implicit future when making an HTTP request. val f = Future { Http ( \"http://api.fixer.io/latest?base=USD\" ). asString } f onComplete { case Success ( response ) => println ( response . body ) case Failure ( t ) => println ( t ) } This sends the HTTP call as soon as it the Future is created. In Scala, although the futures are implicit, Promises can be used to have an explicit-like behavior. This is useful in a scenario where we need to stack up some computations and then resolve the Promise. val p = Promise [ Foo ]() p . future . map ( ... ). filter ( ... ) foreach println p . complete ( new Foo ) Here, we create a Promise, and complete it later. Between creation and completion we stack up a set of computations which then get executed once the promise is completed. Promise Pipelining One of the criticism of traditional RPC systems would be that they\u2019re blocking. Imagine a scenario where you need to call an API \u2018A\u2019 and another API \u2018B\u2019, then aggregate the results of both the calls and use that result as a parameter to another API \u2018C\u2019. Now, the logical way to go about doing this would be to call A and B in parallel, then once both finish, aggregate the result and call C. Unfortunately, in a blocking system, the way to go about is call A, wait for it to finish, call B, wait, then aggregate and call C. This seems like a waste of time, but in absence of asynchronicity, it is impossible. Even with asynchronicity, it gets a little difficult to manage or scale up the system linearly. Fortunately, we have promises. Futures/Promises can be passed along, waited upon, or chained and joined together. These properties helps make life easier for the programmers working with them. This also reduces the latency associated with distributed computing. Promises enable dataflow concurrency , which is also deterministic, and easier to reason about. The history of promise pipelining can be traced back to the call-streams in Argus. In Argus, call-streams are a mechanism for communication between distributed components. The communicating entities, a sender and a receiver are connected by a stream, and sender can make calls to receiver over it. Streams can be thought of as RPC, except that these allow callers to run in parallel with the receiver while processing the call. When making a call in Argus, the caller receives a promise for the result. In the paper on Promises by Liskov and Shrira, they mention that having integrated Promises into call streams, next logical step would be to talk about stream composition. This means arranging streams into pipelines where output of one stream can be used as input of the next stream. They talk about composing streams using fork and coenter. Channels in Joule were a similar idea, providing a channel which connects an acceptor and a distributor. Joule was a direct ancestor to E language, and talked about it in more detail. t3 := (x <- a()) <- c(y <- b()) t1 := x <- a() t2 := y <- b() t3 := t1 <- c(t2) Without pipelining in E, this call will require three round trips. First to send a() to x , then b() to y then finally c to the result t1 with t2 as an argument. But with pipelining, the later messages can be sent with promises as result of earlier messages as argument. This allowed sending all the messages together, thereby saving the costly round trips. This is assuming x and y are on the same remote machine, otherwise we can still evaluate t1 and t2 parallely. Notice that this pipelining mechanism is different from asynchronous message passing, as in asynchronous message passing, even if t1 and t2 get evaluated in parallel, to resolve t3 we still wait for t1 and t2 to be resolved, and send it again in another call to the remote machine. Modern promise specifications, like one in JavaScript comes with methods which help working with promise pipelining easier. In JavaScript, a Promise.all method is provided, which takes in an iterable and returns a new Promise which gets resolved when all the promises in the iterable get resolved. There\u2019s also a Promise.race method, which returns a promise which is resolved when the first promise in the iterable gets resolved. Examples using these methods are shown below. var a = Promise . resolve ( 1 ); var b = new Promise ( function ( resolve , reject ) { setTimeout ( resolve , 100 , 2 ); }); Promise . all ([ p1 , p2 ]). then ( values => { console . log ( values ); // [1,2] }); Promise . race ([ p1 , p2 ]). then ( function ( value ) { console . log ( value ); // 1 }); In Scala, futures have an onSuccess method which acts as a callback to when the future is complete. This callback itself can be used to sequentially chain futures together. But this results in bulkier code. Fortunately, the Scala API has combinators which allow for easier combination of results from futures. Examples of combinators are map , flatMap , filter , withFilter . Handling Errors If the world ran without errors we would rejoice in unison, but this is not the case in the programming world. When you run a program you either receive an expected output or an error. Error can be defined as wrong output or an exception. In a synchronous programming model, the most logical way of handling errors is a try...catch block. try { do something1; do something2; do something3; // ... } catch (exception) { HandleException; } Unfortunately, the same thing doesn\u2019t directly translate to asynchronous code. foo = doSomethingAsync(); try { foo(); // This doesn\u2019t work as the error might not have been thrown yet } catch (exception) { handleException; } Although most of the earlier papers did not talk about error handling, the Promises paper by Liskov and Shrira did acknowledge the possibility of failure in a distributed environment. To put this in Argus\u2019s perspective, the \u2018claim\u2019 operation waits until the promise is ready. Then it returns normally if the call terminated normally, and otherwise it signals the appropriate \u2018exception\u2019, e.g. y: real := pt$claim(x) except when foo: ... when unavailable(s: string): . when failure(s: string): . . end Here x is a promise object of type pt; the form pi$claim illustrates the way Argus identifies an operation of a type by concatenating the type name with the operation name. When there are communication problems, RPCs in Argus terminate either with the \u2018unavailable\u2019 exception or the \u2018failure\u2019 exception. Unavailable means that the problem is temporary e.g. communication is impossible right now. Failure means that the problem is permanent e.g. the handler\u2019s guardian does not exist. Thus stream calls (and sends) whose replies are lost because of broken streams will terminate with one of these exceptions. Both exceptions have a string argument that explains the reason for the failure, e.g., future(\"handler does not exist\") or unavailable(\"cannot communicate\") . Since any call can fail, every handler can raise the exceptions failure and unavailable. In this paper they also talked about propagation of exceptions from the called procedure to the caller. In the paper about the E language they talk about broken promises and setting a promise to the exception of broken references. Modern Languages Scala In modern languages like Scala, Promises generally come with two callbacks. One to handle the success case and other to handle the failure. e.g. f onComplete { case Success ( data ) => handleSuccess ( data ) case Failure ( e ) => handleFailure ( e ) } In Scala, the Try type represents a computation that may either result in an exception, or return a successfully computed value. For example, Try[Int] represents a computation which can either result in Int if it\u2019s successful, or return a Throwable if something is wrong. val a : Int = 100 val b : Int = 0 def divide : Try [ Int ] = Try ( a / b ) divide match { case Success ( v ) => println ( v ) case Failure ( e ) => println ( e ) // java.lang.ArithmeticException: / by zero } The Try type can be pipelined, allowing for catching exceptions and recovering from them along the way. A similar pattern for handling exceptions can be seen in JavaScript. promise . then ( function ( data ) { // success callback console . log ( data ); }, function ( error ) { // failure callback console . error ( error ); }); Scala futures exception handling: When asynchronous computations throw unhandled exceptions, futures associated with those computations fail. Failed futures store an instance of Throwable instead of the result value. Futures provide the onFailure callback method, which accepts a PartialFunction to be applied to a Throwable . TimeoutException , scala.runtime.NonLocalReturnControl[] and ExecutionException exceptions are treated differently Scala promises exception handling: When failing a promise with an exception, three subtypes of Throwable are handled specially. If the Throwable used to break the promise is a scala.runtime.NonLocalReturnControl , then the promise is completed with the corresponding value. If the Throwable used to break the promise is an instance of Error , InterruptedException , or scala.util.control.ControlThrowable , the Throwable is wrapped as the cause of a new ExecutionException which, in turn, is failing the promise. To handle errors with asynchronous methods and callbacks, the error-first callback style (which we\u2019ve seen before, and has also been adopted by Node.js) is the most common convention. Although this works, but it is not very composable, and eventually takes us back to what is called callback hell. Fortunately, Promises allow asynchronous code to apply structured error handling. The Promises.then method takes in two callbacks, a onFulfilled to handle when a promise is resolved successfully and a onRejected to handle if the promise is rejected. var p = new Promise ( function ( resolve , reject ) { resolve ( 100 ); }); p . then ( function ( data ) { console . log ( data ); // 100 }, function ( error ) { console . err ( error ); }); var q = new Promise ( function ( resolve , reject ) { reject ( new Error ( { 'message' : 'Divide by zero' } )); }); q . then ( function ( data ) { console . log ( data ); }, function ( error ) { console . err ( error ); // {'message':'Divide by zero'} }); Promises also have a catch method, which work the same way as onFailure callback, but also help deal with errors in a composition. Exceptions in promises behave the same way as they do in a synchronous block of code : they jump to the nearest exception handler. function work(data) { return Promise.resolve(data + \"1\"); } function error(data) { return Promise.reject(data + \"2\"); } function handleError(error) { return error + \"3\"; } work(\"\") .then(work) .then(error) .then(work) // this will be skipped .then(work, handleError) .then(check); function check(data) { console.log(data == \"1123\"); return Promise.resolve(); } The same behavior can be written using catch block. work(\"\") .then(work) .then(error) .then(work) .catch(handleError) .then(check); function check(data) { console.log(data == \"1123\"); return Promise.resolve(); } Futures and Promises in Action Twitter Finagle Finagle is a protocol-agnostic, asynchronous RPC system for the JVM that makes it easy to build robust clients and servers in Java, Scala, or any other JVM language. It uses Futures to encapsulate concurrent tasks. Finagle introduces two other abstractions built on top of Futures to reason about distributed software: Services are asynchronous functions which represent system boundaries. Filters are application-independent blocks of logic like handling timeouts and authentication. In Finagle, operations describe what needs to be done, while the actual execution is left to be handled by the runtime. The runtime comes with a robust implementation of connection pooling, failure detection and recovery and load balancers. An example of a Service : val service = new Service[HttpRequest, HttpResponse] { def apply(request: HttpRequest) = Future(new DefaultHttpResponse(HTTP_1_1, OK)) } A timeout filter can be implemented as: def timeoutFilter(d: Duration) = { (req, service) => service(req).within(d) } Correctables Correctables were introduced by Rachid Guerraoui, Matej Pavlovic, and Dragos-Adrian Seredinschi at OSDI \u201816, in a paper titled Incremental Consistency Guarantees for Replicated Objects. As the title suggests, Correctables aim to solve the problems with consistency in replicated objects. They provide incremental consistency guarantees by capturing successive changes to the value of a replicated object. Applications can opt to receive a fast but possibly inconsistent result if eventual consistency is acceptable, or to wait for a strongly consistent result. Correctables API draws inspiration from, and builds on the API of Promises. Promises have a two state model to represent an asynchronous task, it starts in blocked state and proceeds to a ready state when the value is available. This cannot represent the incremental nature of correctables. Instead, Correctables have a updating state when they start. From there on, they remain in an updating state during intermediate updates, and when the final result is available, they transition to final state. If an error occurs in between, they move into an error state. Each state change triggers a callback. Folly Futures Folly is a library by Facebook for asynchronous C++ inspired by the implementation of Futures by Twitter for Scala. It builds upon the Futures in the C++11 Standard. Like Scala\u2019s futures, they also allow for implementing a custom executor which provides different ways of running a Future (thread pool, event loop etc). Node.js Fibers Fibers provide coroutine support for V8 and Node.js. Applications can use Fibers to allow users to write code without using a ton of callbacks, without sacrificing the performance benefits of asynchronous IO. Think of fibers as light-weight threads for Node.js where the scheduling is in the hands of the programmer. The node-fibers library doesn\u2019t recommend using raw API and code together without any abstractions, and provides a Futures implementation which is \u2018fiber-aware\u2019.","title":"[Futures and Promises](http://dist-prog-book.com/chapter/2/futures.html)"},{"location":"Distributed-computing/dist-prog-book/chapter-2/Futures-and-Promises/#futures#and#promises","text":"BY KISALAYA PRASAD, AVANTI PATIL, AND HEATHER MILLER Futures and promises are a popular abstraction for asynchronous programming , especially in the context of distributed systems . We'll cover the motivation for and history of these abstractions, and see how they have evolved over time. We\u2019ll do a deep dive into their differing semantics, and various models of execution . This isn't only history and evolution; we\u2019ll also dive into futures and promises most widely utilized today in languages like JavaScript, Scala, and C++. (55 min read) NOTE: in distributed system\uff0casynchronous programming \u6709\u7740\u5e7f\u6cdb\u7684\u5e94\u7528\u3002 NOTE: \"model of execution\"\uff0c\u5728\u540e\u6587\u4e2d\u4f1a\u53cd\u590d\u51fa\u73b0\u3002","title":"Futures and Promises"},{"location":"Distributed-computing/dist-prog-book/chapter-2/Futures-and-Promises/#introduction","text":"As human beings we have the ability to multitask i.e. we can walk, talk, and eat at the same time except when sneezing(\u6253\u55b7\u568f). Sneezing is a blocking activity because it forces you to stop what you\u2019re doing for a brief moment, and then you resume where you left off. One can think of the human sense of multitasking as multithreading in the context of computers. Consider for a moment a simple computer processor; no parallelism, just the ability to complete one task or process at a time. In this scenario, sometimes the processor is blocked when some blocking operation is called. Such blocking calls can include I/O operations like reading/writing to disk, or sending or receiving packets over the network. And as programmers, we know that blocking calls like I/O can take disproportionately more time than a typical CPU-bound task, like iterating over a list. The processor can handle blocking calls in two ways: 1) Synchronously : the processor waits until the blocking call completes its task and returns the result. Afterwards, the processor will move on to processing the next task. This can oftentimes be problematic because the CPU may not be utilized in an efficient manner; it may wait for long periods of time. 2) Asynchronously : When tasks are processed asynchronously, CPU time spent waiting in the synchronous case is instead spent processing some other task using a preemptive(\u62a2\u5360\u5f0f) time sharing algorithm. That is, rather than wait, process some other task instead. Thus, the processor is never left waiting as long as there is more work that can be done. NOTE:\u73b0\u4ee3CPU\u90fd\u662f\u4f7f\u7528\u7684\u8fd9\u79cd\u8c03\u5ea6\u7b56\u7565\u3002\u8054\u60f3\u5230concurrency programming\u4e2d\u7684\u9519\u53d1\uff0c\u5176\u5b9e\u548cCPU\u505a\u6cd5\u80cc\u540e\u7684\u601d\u60f3\u662f\u4e00\u6837\u7684\u3002\u8fd9\u63d0\u793a\u4e86\u6211\u4eec\uff0c\u76f8\u4f3c\u7684\u95ee\u9898\uff0c\u5728computer science\u7684\u5404\u4e2a\u5c42\u6b21\u90fd\u4f1a\u51fa\u73b0\uff0c\u53ef\u4ee5\u91c7\u7528\u76f8\u540c\u7684\u601d\u60f3\u3001\u76f8\u4f3c\u7684\u89e3\u51b3\u65b9\u6cd5\u3002 \u5f53\u4f7f\u7528asynchronous\u65b9\u5f0f\u7684\u65f6\u5019\uff0c\u5fc5\u7136\u4f1a\u6d89\u53ca\u5230\u7684\u4e00\u4e2a\u95ee\u9898\u662f: \u5f53asynchronous operation\u5b8c\u6210\u7684\u65f6\u5019\uff0c\u5982\u4f55\u901a\u77e5\uff1f\u8fd9\u5728hardware\u5c42\u662f\u901a\u8fc7interrupt\u7684\u65b9\u5f0f\u6765\u5b9e\u73b0\u7684\u3002 In the world of programming, many constructs have been introduced in order to help programmers reach ideal levels of resource utilization. Arguably one of the most widely-used of which are futures and/or promises . In this chapter, we\u2019ll do a deep dive into futures and promises, a popular abstraction for doing both synchronous and asynchronous programming . We\u2019ll go through the motivation for and history of these abstractions, understand what sort of scenarios they\u2019re useful for, and cover how they have evolved over time. We\u2019ll cover the various models of execution associated with these abstractions, and finally we\u2019ll touch on the futures and promises most widely utilized today in different general purpose programming languages such as JavaScript, Scala, and C++. NOTE: futures and promises is a popular abstraction for doing both synchronous and asynchronous programming .","title":"Introduction"},{"location":"Distributed-computing/dist-prog-book/chapter-2/Futures-and-Promises/#the#basic#idea","text":"As we\u2019ll see in subsequent sections, what we choose to call this concept, and its precise definition tends to vary. We will start with the widest possible definition of the concept of a future/promise, and later zoom in and cover the many semantic differences between different languages\u2019 interpretations of these constructs. NOTE: \u4e0d\u540c\u7684programminglanguage\uff0c\u5bf9future \u548c promise \u7684\u89e3\u91ca\u53ef\u80fd\u662f\u4e0d\u540c\u7684 In the broadest sense, A future or promise can be thought of as a value that will eventually become available. NOTE: future\u5728\u91d1\u878d\u9886\u57df\u7684\u542b\u4e49\u662f\u201c\u671f\u8d27\u201d\uff0c\u5728computer science\u4e2d\uff0c\u5b83\u7684\u542b\u4e49\u5219\u662f\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u6240\u63cf\u8ff0\u7684\uff0c\u5b83\u8868\u793a\u7684\u662f\u5728\u4e3a\u4e86\u67d0\u4e2a\u65f6\u523b\uff0cvalue\u5c06available\u3002\u5bf9\u6bd4future\u5728\u8fd9\u4e24\u4e2a\u9886\u57df\u4e2d\u7684\u542b\u4e49\uff0c\u53ef\u4ee5\u770b\u51fa\u8fd9\u4e2a\u8bcd\u8868\u793a\u7684\u662f\u67d0\u4e2a\u548c**\u672a\u6765**\u6709\u5173\u7684\u4e8b\u7269\uff0c\u5b83\u662f\u4e00\u79cd\u62bd\u8c61\uff0c\u4e5f\u5c31\u662f\u4e0b\u9762\u7684\u8fd9\u6bb5\u8bdd\u4e2d\u7684\"it is an abstraction which encodes a notion of time\"\uff1b\u663e\u7136\u8fd9\u662f\u548c**\u65f6\u95f4**\u76f8\u5173\u7684\u3002 \u5176\u5b9e\u5e76\u4e0d\u9700\u8981\u628afuture\u548cpromise\u60f3\u5f97\u7384\u4e4e\uff0c\u5b8c\u5168\u53ef\u4ee5\u4f7f\u7528function\u8c03\u7528\u5e76\u83b7\u5f97\u51fd\u6570\u7684\u8fd4\u56de\u503c\u7684\u8fc7\u7a0b\u5bf9\u5176\u8fdb\u884c\u7c7b\u6bd4\uff0c\u51fd\u6570\u7684\u6267\u884c\u8fc7\u7a0b\u53ef\u80fd\u9700\u8981\u8017\u8d39\u4e00\u5b9a\u7684\u65f6\u95f4\uff0c\u6240\u4ee5\u9700\u8981\u7b49\u5f85\u4e00\u5b9a\u7684\u65f6\u95f4\u624d\u80fd\u591f\u83b7\u5f97\u5176\u8fd4\u56de\u503c\uff0c\u663e\u7136\u7b49\u5f85\u4e00\u5b9a\u7684\u65f6\u95f4\u5c31\u53ef\u4ee5\u5c31\u53ef\u4ee5\u83b7\u5f97\u4e00\u4e2avalue\u7684\u8fc7\u7a0b\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528future\u5bf9\u5176\u8fdb\u884c\u63cf\u8ff0\uff1b\u73b0\u5728\u60f3\u6765\u8fd9\u79cd\u62bd\u8c61\u662f\u975e\u5e38\u9002\u5408\u4e8e**concurrency**\uff0c synchronous and asynchronous programming \u3002 \u5bf9future/promise\u7684\u7c7b\u6bd4: 1) \u6211\u89c9\u5f97promise/future\uff0c\u5176\u5b9e\u548cC++ vocabulary type\u4e2d\u7684 std::optional \u3001 std::any \u3001 std::variant \u6709\u4e9b\u7c7b\u4f3c; 2) \u5728wikipedia Futures and promises \u4e2d\uff0c\u5c06promise/future\u6bd4\u4f5cplaceholder 3) future/promise\u662fsymbol\uff0c\u53c2\u89c1symbolic programming\uff0c\u5b83\u4ee3\u8868\u4e86\u67d0\u4e2a\u672a\u6765\u7684\u503c Or said another way, it is an abstraction which encodes a notion of time .","title":"The Basic Idea"},{"location":"Distributed-computing/dist-prog-book/chapter-2/Futures-and-Promises/#state","text":"By choosing to use this construct, it is assumed that your value can now have many possible states , depending on the point in time which we request it. The simplest variation includes two time-dependent states; a future/promise is either: 1) completed/determined : the computation is complete and the future/promise\u2019s value is available. 2) incomplete/undetermined : the computation is not yet complete. As we\u2019ll later see, other states have been introduced in some variations of futures/promises to better support needs like error-handling and cancellation . Importantly, futures/promises typically enable some degree of concurrency . That is, in one of first defitions of futures: The construct ( future X ) immediately returns a future for the value of the expression X and concurrently begins evaluating X . When the evaluation of X yields a value, that value replaces the future. (Halstead, 1985) NOTE: In wikipedia Futures and promises it calls this property a placeholder . Some interpretations(\u89e3\u91ca) of futures/promises have a type associated with them, others not. Typically a future/promise is single-assignment ; that is, it can only be written to once. Some interpretations are blocking ( synchronous ), others are completely non-blocking ( asynchronous ). Some interpretations must be explicitly kicked off (i.e. manually started), while in other interpretations, computation is started implicitly.","title":"State"},{"location":"Distributed-computing/dist-prog-book/chapter-2/Futures-and-Promises/#functional#programming#pipelinecomposition","text":"NOTE: future/promise\u662f\u8d77\u6e90\u81eafunctional programming\u7684 Inspired by functional programming , one of the major distinctions between different interpretations of this construct have to do with pipelineing or composition . Some of the more popular interpretations of futures/promises make it possible to chain operations, or define a pipeline of operations to be invoked upon completion of the computation represented by the future/promise . This is in contrast to callback-heavy or more imperative direct blocking approaches.","title":"Functional programming: pipeline/composition"},{"location":"Distributed-computing/dist-prog-book/chapter-2/Futures-and-Promises/#motivation#and#uses","text":"NOTE: \u8fd9\u6bb5\u603b\u7ed3\u4e86future/promise\u7684use case\u3002 The rise of promises and futures as a topic of relevance has for the most part occurred alongside of the rise of parallel and concurrent programming and distributed systems . This follows somewhat naturally, since, as an abstraction which encodes time, futures/promises introduce a nice way to reason about state changes when latency becomes an issue; a common concern faced by programmers when a node must communicate with another node in a distributed system. NOTE: \u73b0\u5728\u770b\u6765\u5728\u79d1\u5b66\u7684\u6bcf\u4e2a\u9886\u57df\uff0c\u521b\u9020\u51fa\u5bf9\u95ee\u9898\u7684\u63cf\u8ff0\uff0c\u523b\u753b\u65b9\u5f0f\u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u5c31\u597d\u6bd4\u6b64\u5904\u7684future/promise\u4e4b\u4e8e parallel and concurrent programming and distributed systems \u3002 However promises and futures are considered useful in a number of other contexts as well, both distributed and not. Some such contexts include: 1) Request-Response Patterns , such as web service calls over HTTP. A future may be used to represent the value of the response of the HTTP request. 2) Input/Output , such as UI dialogs requiring user input, or operations such as reading large files from disk. A future may be used to represent the IO call and the resulting value of the IO (e.g., terminal input, array of bytes of a file that was read). 3) Long-Running Computations . Imagine you would like the process which initiated a long-running computation, such as a complex numerical algorithm, not to wait on the completion of that long-running computation and to instead move on to process some other task. A future may be used to represent this long-running computation and the value of its result. 4) Database Queries . Like long-running computations, database queries can be time-consuming. Thus, like above, it may be desirable to offload the work of doing the query to another process and move on to processing the next task. A future may be used to represent the query and resulting value of the query. 5) RPC (Remote Procedure Call) . Network latency is typically an issue when making an RPC call to a server. Like above, it may be desirable to not have to wait on the result of the RPC invocation by instead offloading it to another process. A future may be used to represent the RPC call and its result; when the server responds with a result, the future is completed and its value is the server\u2019s response. 6) Reading Data from a Socket can be time-consuming particularly due to network latency. It may thus be desirable to not to have to wait on incoming data, and instead to offload it to another process. A future may be used to represent the reading operation and the resulting value of what it reads when the future is completed. 7) Timeouts , such as managing timeouts in a web service. A future representing a timeout could simply return no result or some kind of empty result like the Unit type in typed programming languages. Many real world services and systems today make heavy use of futures/promises in popular contexts such as these, thanks to the notion of a future or a promise having been introduced in popular languages and frameworks such as JavaScript, Node.js , Scala, Java, C++, amongst many others. As we will see in further sections, this proliferation\uff08\u6269\u6563\uff09 of futures/promises has resulted in futures/promises changing meanings and names over time and across languages. NOTE: \u5728**futures/promises**\u7684\u53d1\u5c55\u8fc7\u7a0b\u4e2d\uff0c\u5176\u542b\u4e49\u4ee5\u53ca\u53d1\u751f\u4e86\u53d8\u5316\u3002","title":"Motivation and Uses"},{"location":"Distributed-computing/dist-prog-book/chapter-2/Futures-and-Promises/#diverging#terminology","text":"Future , Promise , Delay or Deferred generally refer to roughly the same synchronization mechanism where an object acts as a proxy for as-of-yet unknown result. When the result is available, some other code then gets executed. Over the years, these terms have come to refer to slightly different semantic meanings between languages and ecosystems. NOTE: \u663e\u7136\uff0c\u4e0a\u9762\u6240\u8bf4\u7684proxy\uff0c\u5e94\u8be5\u662f virtual proxy \u3002 Sometimes, a language may have one construct named future , promise , delay , deferred , etc. However, in other cases, a language may have two constructs, typically referred to as futures and promises. Languages like Scala, Java, and Dart fall into this category. In this case, A Future is a read-only reference to a yet-to-be-computed(\u5c06\u8981\u88ab\u8ba1\u7b97\u7684) value. A Promise (or a CompletableFuture / Completer /etc.) is a single-assignment variable which the Future refers to. In other words, a future is a read-only window to a value written into a promise . You can get the Future associated with a Promise by calling the future method on it, but conversion in the other direction is not possible. Another way to look at it would be, if you promise something to someone, you are responsible for keeping it, but if someone else makes a promise to you, you expect them to honor it in the future \uff08\u53e6\u4e00\u79cd\u770b\u5f85\u5b83\u7684\u65b9\u6cd5\u662f\uff0c\u5982\u679c\u4f60\u5411\u67d0\u4eba\u627f\u8bfa\uff0c\u4f60\u6709\u8d23\u4efb\u4fdd\u7559\u5b83\uff0c\u4f46\u5982\u679c\u5176\u4ed6\u4eba\u5411\u4f60\u505a\u51fa\u627f\u8bfa\uff0c\u4f60\u5e0c\u671b\u4ed6\u4eec\u5728\u5c06\u6765\u5c0a\u91cd\u5b83\uff09. NOTE:\u53ef\u4ee5\u901a\u8fc7\u5728 Future \u4e0a\u8c03\u7528 future \u53d1\u653e\u6765\u83b7\u5f97\u8be5 Future \u76f8\u5173\u8054\u7684 Promise \uff0c\u4f46\u662f\u53cd\u65b9\u5411\u7684\u8f6c\u6362\u662f\u4e0d\u53ef\u80fd\u7684\uff0c\u4e5f\u5c31\u662f\u7ed9\u5b9a**promise**\u65e0\u6cd5\u5f97\u5230\u5b83\u7684**future**\u3002\u4e4b\u4e8e\u4e3a\u4ec0\u4e48\u8fd9\u6837\uff0c\u6211\u89c9\u5f97\u53ef\u80fd\u662f\u76ee\u524d**promise**\u8fd8\u6ca1\u6709\u6267\u884c\uff0c\u6240\u4ee5\u5b83\u7684future\u6ca1\u6709\u503c\u3002\u6700\u540e\u4e00\u6bb5\u8bdd\u662f\u4f5c\u8005\u7684\u4e00\u4e2a\u6bd4\u55bb\u3002","title":"Diverging Terminology"},{"location":"Distributed-computing/dist-prog-book/chapter-2/Futures-and-Promises/#scala","text":"In Scala, they are defined as follows: A future is a placeholder object for a result that does not yet exist. A promise is a writable, single-assignment container, which completes a future. Promises can complete the future with a result to indicate success, or with an exception to indicate failure. (Haller et al., 2013) An important difference between Scala and Java (6) futures is that Scala futures are asynchronous in nature. Java\u2019s future, at least till Java 6, were blocking. Java 7 introduced asynchronous futures to great fanfare.","title":"Scala"},{"location":"Distributed-computing/dist-prog-book/chapter-2/Futures-and-Promises/#java","text":"In Java 8, the Future<T> interface has methods to check if the computation is complete, to wait for its completion, and to retrieve the result of the computation when it is complete. CompletableFutures can be thought of as a promise , since their value can be explicitly set. However, CompletableFuture also implements the Future interface allowing it to be used as a Future as well. Promises can be thought of as a future with a public set method which the caller (or anybody else) can use to set the value of the future. NOTE:The last paragraph is a metaphor\u3002","title":"Java"},{"location":"Distributed-computing/dist-prog-book/chapter-2/Futures-and-Promises/#jquery","text":"In the JavaScript world, JQuery introduces a notion of Deferred objects which are used to represent a unit of work which is not yet finished. The Deferred object contains a promise object which represents the result of that unit of work. Promises are values returned by a function. The deferred object can also be canceled by its caller.","title":"JQuery"},{"location":"Distributed-computing/dist-prog-book/chapter-2/Futures-and-Promises/#c","text":"Like Scala and Java, C# also makes the distinction between the future and promise constructs described above. In C#, futures are called Task<T> s, and promises are called TaskCompletionSource<T> . The result of the future is available in the read-only property Task<T>.Result which returns T , and TaskCompletionSource<T>.Task<TResult> has methods to complete the Task object with a result of type T or with an exception or cancellation. Important to note; Task s are asynchronous in C#.","title":"C#"},{"location":"Distributed-computing/dist-prog-book/chapter-2/Futures-and-Promises/#javascript","text":"And confusingly, the JavaScript community has standardized on a single construct known as a Promise which can be used like other languages\u2019 notions of futures. The Promises specification (Promises/A+, 2013) defines only a single interface and leaves the details of completing (or fulfilling ) the promise to the implementer of the spec. Promises in JavaScript are also asynchronous and able to be pipelined . JavaScript promises are enabled by default in browsers that support ECMAScript 6 (EC6), or are available in a number of libraries such as Bluebird and Q .","title":"JavaScript"},{"location":"Distributed-computing/dist-prog-book/chapter-2/Futures-and-Promises/#summary","text":"As we can see, concepts, semantics, and terminology seem to differ between languages and library implementations of futures/promises. These differences in terminology and semantics arise from the long history and independent language communities that have proliferated\uff08\u6269\u6563\uff09 the use of futures/promises.","title":"Summary"},{"location":"Distributed-computing/dist-prog-book/chapter-2/Futures-and-Promises/#brief#history","text":"Here\u2019s a brief glimpse at a timeline spanning the history of futures and promises as we know them today: The first concept which eventually led to futures/promises appeared in 1961, with so-called thunks . Thunks can be thought of as a primitive , sequential notion of a future or promise. According to its inventor P. Z. Ingerman, thunks are: A piece of coding which provides an address (Ingerman, 1961) Thunks were designed as a way of binding actual parameters to their formal definitions in Algol-60 procedure calls. If a procedure is called with an expression in the place of a formal parameter, the compiler generates a thunk which computes the expression and leaves the address of the result in some standard location. Think of a thunk as a continuation or a function that was intended to be evaluated in a single-threaded environment. The first mention of Futures was by Baker and Hewitt in a paper on Incremental Garbage Collection of Processes (Baker & Hewitt, 1977) . They coined the term, call-by-futures , to describe a calling convention in which each formal parameter to a method is bound to a process which evaluates the expression in the parameter in parallel with other parameters. Before this paper, Algol 68 (missing reference) also presented a way to make this kind of concurrent parameter evaluation possible, using the collateral clauses and parallel clauses for parameter binding. In their paper, Baker and Hewitt introduced a notion of Futures as a 3-tuple representing an expression E consisting of: A process which evaluates E , A memory location where the result of E needs to be stored, A list of processes which are waiting on E . Though importantly, the focus of their work was not on role of futures and the role they play in asynchronous distributed computing. Instead, it was focused on garbage collecting the processes which evaluate expressions that were not needed by the function. The Multilisp language (Halstead, 1985) , presented by Halestead in 1985 built upon this call-by-future with a future annotation. In Multilisp, binding a variable to a future expression would in turn create a new process which evaluates that expression and binds it to the variable reference which represents its (eventual) result. That is, Multilisp introduced a way to compute arbitrary expressions concurrently in a new process. This made it possible to move past the actual computation and continue processing without waiting for the future to complete. If the result value of the future is never used, the initiating process will not block, thus eliminating a potential source of deadlock. MultiLisp also included a lazy future variant, called delay , which would only be evaluated the first time the value is required elsewhere in the program. This design of futures in Multilisp in turn influenced a construct dubbed promises in the Argus programming language (Liskov & Shrira, 1988; Liskov, 1988) by Liskov and Shrira in 1988. Like futures in MultiLisp, promises in Argus intended to be a placeholder for the result of a value that will be available in the future. Unlike Multilisp which was focused on single-machine concurrency, however, Argus was designed to facilitate distributed programming, and in particular focused on promises as a way to integrate asynchronous RPC into the Argus programming language. Importantly, promises were extended beyond futures in Multilisp through the introduction of types to promises. Therefore, in Argus, when a call to promise is made, a promise is created which immediately returns, and a type-safe asynchronous RPC call is made in a new process. When the RPC completes, the returned value can be claimed by the caller. Argus also introduced call streams, which can be thought of as a way to enforce an ordering of concurrently executing calls. A sender and a receiver are connected by a stream, over which it is possible make normal (synchronous) RPC calls, or stream calls , in which the sender may make more calls before receiving the reply. The underlying runtime however ensures that, despite the non-blocking stream calls fired off, that all calls and subsequent replies happen in call order. That is, call-streams ensure exactly-once, ordered delivery. Argus also introduces constructs to compose call-streams, in order to build up pipelines of computation, or directed acyclic graphs (DAGs) of computation. These semantics are an early precursor to what we refer to today as promise pipelining . E is an object-oriented programming language for secure distributed computing (Miller, Tribble, & Shapiro, 2005) , created by Mark S. Miller, Dan Bornstein, and others at Electric Communities in 1997. A major contribution of E is its interpretation and implementation of promises. It traces its routes to Joule (Tribble, Miller, Hardy, & Krieger, 1995) , a dataflow programming language predecessor to E. Importantly, E introduced an eventually operator, * <- * which enabled what is called an eventual send in E; that is, the program doesn\u2019t wait for the operation to complete and moves to next sequential statement. This is in contrast to the expected semantics of an immediate call which looks like a normal method invocation in E. Eventual sends queue a pending delivery and complete immediately, returning a promise. A pending delivery includes a resolver for the promise. Subsequent messages can also be eventually sent to a promise before it is resolved. In this case, these messages are queued up and forwarded once the promise is resolved. That is, once we have a promise, we are able to chain make several pipelined eventual sends as if the initial promise was already resolved. This notion of promise pipelining (Miller, Tribble, & Jellinghaus, 2007) has been adopted by a majority of contemporary futures/promises interpretations. Futures and promises remained primarily an academic fascination until the early 2000s, upon the rise of web application development, networked system development, and an increased need for responsive user interfaces. Among the mainstream programming languages, Python was perhaps the first, in 2002, to get a library which introduced a construct along the same lines as E\u2019s promises in the Twisted library (Lefkowitz, 2002) . Twisted introduced the notion of Deferred objects, are used to receive the result of an operation not yet completed. In Twisted, deferred objects are just like normal first-class objects; they can be passed along anywhere a normal object can, the only difference is that deferred objects don\u2019t have values. Deferred objects support callbacks, which are called once the result of the operation is complete. Perhaps most famous in recent memory is that of promises in JavaScript. In 2007, inspired by Python\u2019s Twisted library, the authors of the Dojo Toolkit came up a JavaScript implementation of Twisted\u2019s deferred objects, known as dojo.Deferred . This in turn inspired Kris Zyp to propose the CommonJS Promises/A spec in 2009 (Zyp, 2009) . The same year, Ryan Dahl introduced Node.js. In it\u2019s early versions, Node.js used promises in its non-blocking API. However, when Node.js moved away from promises to its now familiar error-first callback API (the first argument for the callback should be an error object), it left a void to fill for a promises API. Q.js is an implementation of Promises/A spec by Kris Kowal around this time (Kowal, 2009) . The FuturesJS library by AJ O\u2019Neal was another library which aimed to solve flow-control problems without using Promises in the strictest of senses. In 2011, JQuery v1.5 introduced Promises to its wider and ever-growing audience. However, JQuery\u2019s promises API was subtly different than the Promises/A spec (Gheri, 2013) . With the rise of HTML5 and different APIs, there came a problem of different and messy interfaces which added to the already infamous callback hell. The Promises/A+ spec (Promises/A+, 2013) aimed to solve this problem. Following the broad community acceptance of the Promises/A+ spec, promises were finally made a part of the ECMAScript\u00ae 2015 Language Specification (ECMAScript, Association, & others, 2015) . However, a lack of backward compatibility and additional features missing in the Promises/A+ spec means that libraries like BlueBird and Q.js still have a place in the JavaScript ecosystem. SUMMARY :\u4e0a\u9762\u7684\u5185\u5bb9\u6ca1\u6709\u9605\u8bfb","title":"Brief History"},{"location":"Distributed-computing/dist-prog-book/chapter-2/Futures-and-Promises/#semantics#of#execution","text":"As architectures and runtimes have developed and changed over the years, so too have the techniques for implementing futures/promises such that the abstraction translates into efficient utilization of system resources(\u968f\u7740architectures \u548cruntimes \u591a\u5e74\u6765\u7684\u53d1\u5c55\u548c\u53d8\u5316\uff0c\u5b9e\u73b0 futures/promises\u7684\u6280\u672f\u4e5f\u8d8a\u6765\u8d8a\u591a\uff0c\u62bd\u8c61\u8f6c\u5316\u4e3a\u7cfb\u7edf\u8d44\u6e90\u7684\u6709\u6548\u5229\u7528). In this section, we\u2019ll cover the three primary executions models upon which futures/promises are built on top of in popular languages and libraries. That is, we\u2019ll see the different ways in which futures and promises actually get executed and resolved underneath their APIs.","title":"Semantics of Execution \u6267\u884c\u7684\u8bed\u4e49"},{"location":"Distributed-computing/dist-prog-book/chapter-2/Futures-and-Promises/#thread#pools","text":"A thread pool is an abstraction that gives users access to a group of ready, idle threads which can be given work. Thread pool implementations take care of worker creation, management, and scheduling, which can easily become tricky and costly if not handled carefully. Thread pools come in many different flavors, with many different techniques for scheduling and executing tasks, and with fixed numbers of threads or the ability of the pool to dynamically resize itself depending on load. A classic thread pool implementation is Java\u2019s Executor , which is an object which executes the Runnable tasks. Executor s provide a way of abstracting out how the details of how a task will actually run. These details, like selecting a thread to run the task, how the task is scheduled are managed by the underlying implementation of the Executor interface. SUMMARY : Executror is a implementation of thread pool,it is elegant and fit to Java's thinking. Similar to Executor , Scala includes is a ExecutionContext s as part of the scala.concurrent package. The basic intent behind Scala\u2019s ExecutionContext is the same as Java\u2019s Executor ; it is responsible for efficiently executing computations concurrently without requiring the user of the pool to have to worry about things like scheduling. Importantly, ExecutionContext can be thought of as an interface ; that is, it is possible to swap in different underlying thread pool implementations and keep the same thread pool interface. While it\u2019s possible to use different thread pool implementations, Scala\u2019s default ExecutionContext implementation is backed by Java\u2019s ForkJoinPool ; a thread pool implementation that features a work-stealing algorithm in which idle threads pick up tasks previously scheduled to other busy threads. The ForkJoinPool is a popular thread pool implementation due to its improved performance over Executor s, its ability to better avoid pool-induced deadlock , and for minimizing the amount of time spent switching between threads. Scala\u2019s futures (and promises ) are based on this ExecutionContext interface to an underlying thread pool. While typically users use the underlying default ExecutionContext which is backed by a ForkJoinPool , users may also elect to provide (or implement) their own ExecutionContext if they need a specific behavior, like blocking futures. THINKING :\u4e3a\u4ec0\u4e48Scala\u7684future\u548cpromise\u9700\u8981\u57fa\u4e8e ExecutionContext interface?future\u548cpromise\u662f\u4e00\u79cd\u62bd\u8c61\uff0c\u6211\u4eec\u5b83\u8981\u548c ExecutionContext interface\u6346\u7ed1\u5728\u4e00\u8d77\uff1f In Scala, every usage of a future or promise requires some kind of ExecutionContext to be passed along. This parameter is implicit, and is usually ExecutionContext.global (the default underlying ForkJoinPool ExecutionContext ). For example, a creating and running a basic future: implicit val ec = ExecutionContext . global val f : Future [ String ] = Future { \u201c hello world \u201d } In this example, the global execution context is used to asynchronously run the created future. As mentioned earlier, the ExecutionContext parameter to the Future is implicit . That means that if the compiler finds an instance of an ExecutionContext in so-called implicit scope , it is automatically passed to the call to Future without the user having to explicitly pass it. In the example above, ec is put into implicit scope through the use of the implicit keyword when declaring ec . As mentioned earlier, futures and promises in Scala are asynchronous , which is achieved through the use of callbacks . For example: implicit val ec = ExecutionContext . global val f = Future { Http ( \"http://api.fixed.io/latest?base=USD\" ). asString } f . onComplete { case Success ( response ) => println ( response ) case Failure ( t ) => println ( t . getMessage ()) } In this example, we first create a future f , and when it completes, we provide two possible expressions that can be invoked depending on whether the future was executed successfully or if there was an error. In this case, if successful, we get the result of the computation an HTTP string, and we print it. If an exception was thrown, we get the message string contained within the exception and we print that. So, how does it all work together? As we mentioned, Futures require an ExecutionContext , which is an implicit parameter to virtually all of the futures API. This ExecutionContext is used to execute the future. Scala is flexible enough to let users implement their own Execution Contexts, but let\u2019s talk about the default ExecutionContext , which is a ForkJoinPool . ForkJoinPool is ideal for many small computations that spawn off and then come back together( ForkJoinPool \u662f\u8bb8\u591a\u5c0f\u578b\u8ba1\u7b97\u7684\u7406\u60f3\u9009\u62e9\uff0c\u5b83\u4eec\u4f1a\u4ea7\u751f\u5e76\u4e00\u8d77\u56de\u5f52). Scala\u2019s ForkJoinPool requires the tasks submitted to it to be a ForkJoinTask . The tasks submitted to the global ExecutionContext is quietly wrapped inside a ForkJoinTask and then executed. ForkJoinPool also supports a possibly blocking task, using the ManagedBlock method which creates a spare thread if required to ensure that there is sufficient parallelism if the current thread is blocked. To summarize, ForkJoinPool is an really good general purpose ExecutionContext , which works really well in most of the scenarios.","title":"Thread Pools"},{"location":"Distributed-computing/dist-prog-book/chapter-2/Futures-and-Promises/#event#loops","text":"Modern platforms and runtimes typically rely on many underlying system layers to operate. For example, there\u2019s an underlying file system, a database system, and other web services that may be relied on by a given language implementation, library, or framework. Interaction with these components typically involves a period where we\u2019re doing nothing but waiting for the response. This can be a very large waste of computing resources. JavaScript is a single threaded asynchronous runtime . Now, conventionally(\u60ef\u5e38\u5730) async programming is generally associated with multi-threading, but we\u2019re not allowed to create new threads in JavaScript. Instead, asynchronicity in JavaScript is achieved using an event-loop mechanism. JavaScript has historically been used to interact with the DOM and user interactions in the browser, and thus an event-driven programming model was a natural fit for the language. This has scaled up surprisingly well in high throughput scenarios in Node.js. The general idea behind event-driven programming model is that the logic flow control is determined by the order in which events are processed. This is underpinned\uff08\u5de9\u56fa\uff09 by a mechanism which is constantly listening for events and fires a callback when it is detected. This is the JavaScript\u2019s event loop in a nutshell. SUMMARY :event loop\u662f\u4e00\u79cd\u4e0d\u540c\u4e8ethread pool\u7684\u6267\u884c\u6a21\u578b\uff1b A typical JavaScript engine has a few basic components. They are : Heap Used to allocate memory for objects Stack Function call frames go into a stack from where they\u2019re picked up from top to be executed\uff08\u4ece\u6808\u9876\u5f00\u59cb\u6267\u884c\uff09. Queue A message queue holds the messages to be processed. Each message has a callback function which is fired when the message is processed. These messages can be generated by user actions like button clicks or scrolling, or by actions like HTTP requests, request to a database to fetch records or reading/writing to a file. Separating when a message is queued from when it is executed means the single thread doesn\u2019t have to wait for an action to complete before moving on to another\uff08\u4f7f\u7528\u961f\u5217\u663e\u7136\u662f\u5f02\u6b65\u7684\uff0c\u65e0\u9700\u7b49\u5f85action\u5b8c\u6210\u5373\u53ef\u5904\u7406\u4e0b\u4e00\u4e2a\uff09. We attach a callback to the action we want to do, and when the time comes, the callback is run with the result of our action\uff08\u5373\u5c06\u4e0a\u4e00\u4e2a\u7684\u8f93\u51fa\u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u7684\u8f93\u5165\uff0c\u7c7b\u4f3cpipeline\uff09. Callbacks work good in isolation, but they force us into a continuation passing style of execution, what is otherwise known as Callback hell . SUMMARY :\u4e0a\u9762\u8fd9\u6bb5\u63cf\u8ff0\u4e86\u5d4c\u5957\u4f7f\u7528callback\u7684\u4f18\u52bf\uff1aisolation\uff0c\u52a3\u52bf\uff1acallback hell\u3002\u4e0b\u9762\u7684\u8fd9\u4e2a\u5c0f\u4f8b\u5b50\u5c31\u5c55\u793a\u4e86callback hell\u3002 getData = function ( param , callback ){ $ . get ( 'http://example.com/get/' + param , function ( responseText ){ callback ( responseText ); }); } getData ( 0 , function ( a ){ getData ( a , function ( b ){ getData ( b , function ( c ){ getData ( c , function ( d ){ getData ( d , function ( e ){ // ... }); }); }); }); }); VS getData = function ( param , callback ){ return new Promise ( function ( resolve , reject ) { $ . get ( 'http://example.com/get/' + param , function ( responseText ){ resolve ( responseText ); }); }); } getData ( 0 ). then ( getData ) . then ( getData ) . then ( getData ) . then ( getData ); Programs must be written for people to read, and only incidentally for machines to execute. - Harold Abelson and Gerald Jay Sussman Promises are an abstraction which make working with async operations in JavaScript much more fun. Callbacks lead to inversion of control , which is difficult to reason about at scale(\u8fdb\u4e00\u6b65\u89e3\u91cacallback hell:\u4ee3\u7801\u53ef\u8bfb\u6027\u964d\u4f4e\u4e86). Moving on from a continuation passing style , where you specify what needs to be done once the action is done, the callee\uff08\u88ab\u8c03\u51fd\u6570\uff09 simply returns a Promise object . This inverts\uff08\u98a0\u5012\uff09 the chain of responsibility, as now the caller is responsible for handling the result of the promise when it is settled. SUMMARY : The ES2015 spec specifies that \u201c promises must not fire their resolution/rejection function on the same turn of the event loop that they are created on.\u201d This is an important property because it ensures deterministic\uff08\u786e\u5b9a\u7684\uff09 order of execution. Also, once a promise is fulfilled or failed, the promise\u2019s value MUST not be changed. This ensures that a promise cannot be resolved more than once. Let\u2019s take an example to understand the promise resolution workflow as it happens inside the JavaScript Engine. Suppose we execute a function, here g() which in turn, calls another function f() . Function f returns a promise, which, after counting down for 1000 ms, resolves the promise with a single value, true. Once f gets resolved, a value true or false is alerted based on the value of the promise. Now, JavaScript\u2019s runtime is single threaded. This statement is true, and not true. The thread which executes the user code is single threaded. It executes what is on top of the stack, runs it to completion, and then moves onto what is next on the stack. But, there are also a number of helper threads which handle things like network or timer/settimeout type events. This timing thread handles the counter for setTimeout. Once the timer expires, the timer thread puts a message on the message queue. The queued up messages are then handled by the event loop. The event loop as described above, is simply an infinite loop which checks if a message is ready to be processed, picks it up and puts it on the stack for it\u2019s callback to be executed. Here, since the future is resolved with a value of true , we are alerted with a value true when the callback is picked up for execution. We\u2019ve ignored the heap here, but all the functions, variables and callbacks are stored on heap. As we\u2019ve seen here, even though JavaScript is said to be single threaded, there are number of helper threads to help main thread do things like timeout, UI, network operations, file operations etc. Run-to-completion helps us reason about the code in a nice way. Whenever a function starts, it needs to finish before yielding the main thread. The data it accesses cannot be modified by someone else. This also means every function needs to finish in a reasonable amount of time, otherwise the program seems hung. This makes JavaScript well suited for I/O tasks which are queued up and then picked up when finished, but not for data processing intensive tasks which generally take long time to finish. We haven\u2019t talked about error handling, but it gets handled the same exact way, with the error callback being called with the error object the promise is rejected with. Event loops have proven to be surprisingly performant. When network servers are designed around multithreading, as soon as you end up with a few hundred concurrent connections, the CPU spends so much of its time task switching that you start to lose overall performance. Switching from one thread to another has overhead which can add up significantly at scale. Apache used to choke even as low as a few hundred concurrent users when using a thread per connection while Node.js can scale up to a 100,000 concurrent connections based on event loops and asynchronous IO. SUMMARY \uff1a\u4e0a\u9762\u7684\u4f8b\u5b50\u6ca1\u6709\u9605\u8bfb","title":"Event Loops"},{"location":"Distributed-computing/dist-prog-book/chapter-2/Futures-and-Promises/#thread#model","text":"The Oz programming language introduced an idea of dataflow concurrency model . In Oz, whenever the program comes across an unbound variable, it waits for it to be resolved. This dataflow property of variables helps us write threads in Oz that communicate through streams in a producer-consumer pattern. The major benefit of dataflow based concurrency model is that it\u2019s deterministic - same operation called with same parameters always produces the same result. It makes it a lot easier to reason about concurrent programs, if the code is side-effect free. SUMMARY :\u4e0a\u6587\u6240\u63d0\u5230\u7684**dataflow concurrency model**\u6bd4\u8f83\u6709\u4ef7\u503c\u3002 Alice ML is a dialect of Standard ML with support for lazy evaluation , concurrent, distributed, and constraint programming. The early aim of Alice project was to reconstruct the functionalities of Oz programming language on top of a typed programming language. Building on the Standard ML dialect, Alice also provides concurrency features as part of the language through the use of a future type. Futures in Alice represent an undetermined result of a concurrent operation. Promises in Alice ML are explicit handles for futures. Any expression in Alice can be evaluated in it\u2019s own thread using spawn keyword. Spawn always returns a future which acts as a placeholder for the result of the operation. Futures in Alice ML can be thought of as functional threads, in a sense that threads in Alice always have a result. A thread is said to be touching a future if it performs an operation that requires the value future is a placeholder for. All threads touching a future are blocked until the future is resolved. If a thread raises an exception, the future is failed and this exception is re-raised in the threads touching it. Futures can also be passed along as values. This helps us achieve the dataflow model of concurrency in Alice. Alice also allows for lazy evaluation of expressions. Expressions preceded with the lazy keyword are evaluated to a lazy future. The lazy future is evaluated when it is needed. If the computation associated with a concurrent or lazy future ends with an exception, it results in a failed future. Requesting a failed future does not block, it simply raises the exception that was the cause of the failure.","title":"Thread Model"},{"location":"Distributed-computing/dist-prog-book/chapter-2/Futures-and-Promises/#implicit#vs#explicit#promises","text":"We define implicit promises as ones where we don\u2019t have to manually trigger the computation vs Explicit promises where we have to trigger the resolution of future manually, either by calling a start function or by requiring the value. This distinction can be understood in terms of what triggers the calculation: With implicit promises , the creation of a promise also triggers the computation, while with explicit futures , one needs to triggers the resolution of a promise. This trigger can in turn be explicit, like calling a start method, or implicit, like lazy evaluation where the first use of a promise\u2019s value triggers its evaluation. The idea for explicit futures were introduced in the Baker and Hewitt paper. They\u2019re a little trickier to implement, and require some support from the underlying language, and as such they aren\u2019t that common. The Baker and Hewitt paper talked about using futures as placeholders for arguments to a function, which get evaluated in parallel, but when they\u2019re needed. MultiLisp also had a mechanism to delay the evaluation of the future to the time when it\u2019s value is first used, using the defer construct. Lazy futures in Alice ML have a similar explicit invocation mechanism, the first thread touching a future triggers its evaluation. An example of explicit futures would be (from AliceML): fun enum n = lazy n :: enum (n+1) This example generates an infinite stream of integers and if stated when it is created, will compete for the system resources. Implicit futures were introduced originally by Friedman and Wise in a paper in 1978. The ideas presented in that paper inspired the design of promises in MultiLisp. Futures are also implicit in Scala and JavaScript, where they\u2019re supported as libraries on top of the core languages. Implicit futures can be implemented this way as they don\u2019t require support from language itself. Alice ML\u2019s concurrent futures are also an example of implicit invocation. In Scala, we can see an example of an implicit future when making an HTTP request. val f = Future { Http ( \"http://api.fixer.io/latest?base=USD\" ). asString } f onComplete { case Success ( response ) => println ( response . body ) case Failure ( t ) => println ( t ) } This sends the HTTP call as soon as it the Future is created. In Scala, although the futures are implicit, Promises can be used to have an explicit-like behavior. This is useful in a scenario where we need to stack up some computations and then resolve the Promise. val p = Promise [ Foo ]() p . future . map ( ... ). filter ( ... ) foreach println p . complete ( new Foo ) Here, we create a Promise, and complete it later. Between creation and completion we stack up a set of computations which then get executed once the promise is completed.","title":"Implicit vs. Explicit Promises"},{"location":"Distributed-computing/dist-prog-book/chapter-2/Futures-and-Promises/#promise#pipelining","text":"One of the criticism of traditional RPC systems would be that they\u2019re blocking. Imagine a scenario where you need to call an API \u2018A\u2019 and another API \u2018B\u2019, then aggregate the results of both the calls and use that result as a parameter to another API \u2018C\u2019. Now, the logical way to go about doing this would be to call A and B in parallel, then once both finish, aggregate the result and call C. Unfortunately, in a blocking system, the way to go about is call A, wait for it to finish, call B, wait, then aggregate and call C. This seems like a waste of time, but in absence of asynchronicity, it is impossible. Even with asynchronicity, it gets a little difficult to manage or scale up the system linearly. Fortunately, we have promises. Futures/Promises can be passed along, waited upon, or chained and joined together. These properties helps make life easier for the programmers working with them. This also reduces the latency associated with distributed computing. Promises enable dataflow concurrency , which is also deterministic, and easier to reason about. The history of promise pipelining can be traced back to the call-streams in Argus. In Argus, call-streams are a mechanism for communication between distributed components. The communicating entities, a sender and a receiver are connected by a stream, and sender can make calls to receiver over it. Streams can be thought of as RPC, except that these allow callers to run in parallel with the receiver while processing the call. When making a call in Argus, the caller receives a promise for the result. In the paper on Promises by Liskov and Shrira, they mention that having integrated Promises into call streams, next logical step would be to talk about stream composition. This means arranging streams into pipelines where output of one stream can be used as input of the next stream. They talk about composing streams using fork and coenter. Channels in Joule were a similar idea, providing a channel which connects an acceptor and a distributor. Joule was a direct ancestor to E language, and talked about it in more detail. t3 := (x <- a()) <- c(y <- b()) t1 := x <- a() t2 := y <- b() t3 := t1 <- c(t2) Without pipelining in E, this call will require three round trips. First to send a() to x , then b() to y then finally c to the result t1 with t2 as an argument. But with pipelining, the later messages can be sent with promises as result of earlier messages as argument. This allowed sending all the messages together, thereby saving the costly round trips. This is assuming x and y are on the same remote machine, otherwise we can still evaluate t1 and t2 parallely. Notice that this pipelining mechanism is different from asynchronous message passing, as in asynchronous message passing, even if t1 and t2 get evaluated in parallel, to resolve t3 we still wait for t1 and t2 to be resolved, and send it again in another call to the remote machine. Modern promise specifications, like one in JavaScript comes with methods which help working with promise pipelining easier. In JavaScript, a Promise.all method is provided, which takes in an iterable and returns a new Promise which gets resolved when all the promises in the iterable get resolved. There\u2019s also a Promise.race method, which returns a promise which is resolved when the first promise in the iterable gets resolved. Examples using these methods are shown below. var a = Promise . resolve ( 1 ); var b = new Promise ( function ( resolve , reject ) { setTimeout ( resolve , 100 , 2 ); }); Promise . all ([ p1 , p2 ]). then ( values => { console . log ( values ); // [1,2] }); Promise . race ([ p1 , p2 ]). then ( function ( value ) { console . log ( value ); // 1 }); In Scala, futures have an onSuccess method which acts as a callback to when the future is complete. This callback itself can be used to sequentially chain futures together. But this results in bulkier code. Fortunately, the Scala API has combinators which allow for easier combination of results from futures. Examples of combinators are map , flatMap , filter , withFilter .","title":"Promise Pipelining"},{"location":"Distributed-computing/dist-prog-book/chapter-2/Futures-and-Promises/#handling#errors","text":"If the world ran without errors we would rejoice in unison, but this is not the case in the programming world. When you run a program you either receive an expected output or an error. Error can be defined as wrong output or an exception. In a synchronous programming model, the most logical way of handling errors is a try...catch block. try { do something1; do something2; do something3; // ... } catch (exception) { HandleException; } Unfortunately, the same thing doesn\u2019t directly translate to asynchronous code. foo = doSomethingAsync(); try { foo(); // This doesn\u2019t work as the error might not have been thrown yet } catch (exception) { handleException; } Although most of the earlier papers did not talk about error handling, the Promises paper by Liskov and Shrira did acknowledge the possibility of failure in a distributed environment. To put this in Argus\u2019s perspective, the \u2018claim\u2019 operation waits until the promise is ready. Then it returns normally if the call terminated normally, and otherwise it signals the appropriate \u2018exception\u2019, e.g. y: real := pt$claim(x) except when foo: ... when unavailable(s: string): . when failure(s: string): . . end Here x is a promise object of type pt; the form pi$claim illustrates the way Argus identifies an operation of a type by concatenating the type name with the operation name. When there are communication problems, RPCs in Argus terminate either with the \u2018unavailable\u2019 exception or the \u2018failure\u2019 exception. Unavailable means that the problem is temporary e.g. communication is impossible right now. Failure means that the problem is permanent e.g. the handler\u2019s guardian does not exist. Thus stream calls (and sends) whose replies are lost because of broken streams will terminate with one of these exceptions. Both exceptions have a string argument that explains the reason for the failure, e.g., future(\"handler does not exist\") or unavailable(\"cannot communicate\") . Since any call can fail, every handler can raise the exceptions failure and unavailable. In this paper they also talked about propagation of exceptions from the called procedure to the caller. In the paper about the E language they talk about broken promises and setting a promise to the exception of broken references.","title":"Handling Errors"},{"location":"Distributed-computing/dist-prog-book/chapter-2/Futures-and-Promises/#modern#languages","text":"","title":"Modern Languages"},{"location":"Distributed-computing/dist-prog-book/chapter-2/Futures-and-Promises/#scala_1","text":"In modern languages like Scala, Promises generally come with two callbacks. One to handle the success case and other to handle the failure. e.g. f onComplete { case Success ( data ) => handleSuccess ( data ) case Failure ( e ) => handleFailure ( e ) } In Scala, the Try type represents a computation that may either result in an exception, or return a successfully computed value. For example, Try[Int] represents a computation which can either result in Int if it\u2019s successful, or return a Throwable if something is wrong. val a : Int = 100 val b : Int = 0 def divide : Try [ Int ] = Try ( a / b ) divide match { case Success ( v ) => println ( v ) case Failure ( e ) => println ( e ) // java.lang.ArithmeticException: / by zero } The Try type can be pipelined, allowing for catching exceptions and recovering from them along the way. A similar pattern for handling exceptions can be seen in JavaScript. promise . then ( function ( data ) { // success callback console . log ( data ); }, function ( error ) { // failure callback console . error ( error ); }); Scala futures exception handling: When asynchronous computations throw unhandled exceptions, futures associated with those computations fail. Failed futures store an instance of Throwable instead of the result value. Futures provide the onFailure callback method, which accepts a PartialFunction to be applied to a Throwable . TimeoutException , scala.runtime.NonLocalReturnControl[] and ExecutionException exceptions are treated differently Scala promises exception handling: When failing a promise with an exception, three subtypes of Throwable are handled specially. If the Throwable used to break the promise is a scala.runtime.NonLocalReturnControl , then the promise is completed with the corresponding value. If the Throwable used to break the promise is an instance of Error , InterruptedException , or scala.util.control.ControlThrowable , the Throwable is wrapped as the cause of a new ExecutionException which, in turn, is failing the promise. To handle errors with asynchronous methods and callbacks, the error-first callback style (which we\u2019ve seen before, and has also been adopted by Node.js) is the most common convention. Although this works, but it is not very composable, and eventually takes us back to what is called callback hell. Fortunately, Promises allow asynchronous code to apply structured error handling. The Promises.then method takes in two callbacks, a onFulfilled to handle when a promise is resolved successfully and a onRejected to handle if the promise is rejected. var p = new Promise ( function ( resolve , reject ) { resolve ( 100 ); }); p . then ( function ( data ) { console . log ( data ); // 100 }, function ( error ) { console . err ( error ); }); var q = new Promise ( function ( resolve , reject ) { reject ( new Error ( { 'message' : 'Divide by zero' } )); }); q . then ( function ( data ) { console . log ( data ); }, function ( error ) { console . err ( error ); // {'message':'Divide by zero'} }); Promises also have a catch method, which work the same way as onFailure callback, but also help deal with errors in a composition. Exceptions in promises behave the same way as they do in a synchronous block of code : they jump to the nearest exception handler. function work(data) { return Promise.resolve(data + \"1\"); } function error(data) { return Promise.reject(data + \"2\"); } function handleError(error) { return error + \"3\"; } work(\"\") .then(work) .then(error) .then(work) // this will be skipped .then(work, handleError) .then(check); function check(data) { console.log(data == \"1123\"); return Promise.resolve(); } The same behavior can be written using catch block. work(\"\") .then(work) .then(error) .then(work) .catch(handleError) .then(check); function check(data) { console.log(data == \"1123\"); return Promise.resolve(); }","title":"Scala"},{"location":"Distributed-computing/dist-prog-book/chapter-2/Futures-and-Promises/#futures#and#promises#in#action","text":"","title":"Futures and Promises in Action"},{"location":"Distributed-computing/dist-prog-book/chapter-2/Futures-and-Promises/#twitter#finagle","text":"Finagle is a protocol-agnostic, asynchronous RPC system for the JVM that makes it easy to build robust clients and servers in Java, Scala, or any other JVM language. It uses Futures to encapsulate concurrent tasks. Finagle introduces two other abstractions built on top of Futures to reason about distributed software: Services are asynchronous functions which represent system boundaries. Filters are application-independent blocks of logic like handling timeouts and authentication. In Finagle, operations describe what needs to be done, while the actual execution is left to be handled by the runtime. The runtime comes with a robust implementation of connection pooling, failure detection and recovery and load balancers. An example of a Service : val service = new Service[HttpRequest, HttpResponse] { def apply(request: HttpRequest) = Future(new DefaultHttpResponse(HTTP_1_1, OK)) } A timeout filter can be implemented as: def timeoutFilter(d: Duration) = { (req, service) => service(req).within(d) }","title":"Twitter Finagle"},{"location":"Distributed-computing/dist-prog-book/chapter-2/Futures-and-Promises/#correctables","text":"Correctables were introduced by Rachid Guerraoui, Matej Pavlovic, and Dragos-Adrian Seredinschi at OSDI \u201816, in a paper titled Incremental Consistency Guarantees for Replicated Objects. As the title suggests, Correctables aim to solve the problems with consistency in replicated objects. They provide incremental consistency guarantees by capturing successive changes to the value of a replicated object. Applications can opt to receive a fast but possibly inconsistent result if eventual consistency is acceptable, or to wait for a strongly consistent result. Correctables API draws inspiration from, and builds on the API of Promises. Promises have a two state model to represent an asynchronous task, it starts in blocked state and proceeds to a ready state when the value is available. This cannot represent the incremental nature of correctables. Instead, Correctables have a updating state when they start. From there on, they remain in an updating state during intermediate updates, and when the final result is available, they transition to final state. If an error occurs in between, they move into an error state. Each state change triggers a callback.","title":"Correctables"},{"location":"Distributed-computing/dist-prog-book/chapter-2/Futures-and-Promises/#folly#futures","text":"Folly is a library by Facebook for asynchronous C++ inspired by the implementation of Futures by Twitter for Scala. It builds upon the Futures in the C++11 Standard. Like Scala\u2019s futures, they also allow for implementing a custom executor which provides different ways of running a Future (thread pool, event loop etc).","title":"Folly Futures"},{"location":"Distributed-computing/dist-prog-book/chapter-2/Futures-and-Promises/#nodejs#fibers","text":"Fibers provide coroutine support for V8 and Node.js. Applications can use Fibers to allow users to write code without using a ton of callbacks, without sacrificing the performance benefits of asynchronous IO. Think of fibers as light-weight threads for Node.js where the scheduling is in the hands of the programmer. The node-fibers library doesn\u2019t recommend using raw API and code together without any abstractions, and provides a Futures implementation which is \u2018fiber-aware\u2019.","title":"Node.js Fibers"},{"location":"Distributed-computing/draft-Book-Distributed-operating-systems/","text":"Distributed operating systems","title":"[Distributed operating systems](https://www.e-reading.club/book.php?book=143358)"},{"location":"Distributed-computing/draft-Book-Distributed-operating-systems/#distributed#operating#systems","text":"","title":"Distributed operating systems"},{"location":"Distributed-computing/draft-Book-Distributed-operating-systems/6-Distributed-Shared-Memory/6.3.5-Weak-Consistency/","text":"","title":"6.3.5 Weak Consistency"},{"location":"Distributed-computing/draft-Book-Distributed-operating-systems/6-Distributed-Shared-Memory/6.3.8-Summary-of-Consistency-Models/","text":"6.3.8. Summary of Consistency Models Although other consistency models have been proposed, the main ones are discussed above. They differ in how restrictive they are, how complex their implementations are, their ease of programming, and their performance. Strict consistency is the most restrictive , but because its implementation in a DSM system is essentially impossible, it is never used. Sequential consistency is feasible, popular with programmers, and widely used. It has the problem of poor performance , however. The way to get around this result is to relax the consistency model . Some of the possibilities are shown in Fig. 6-24(a), roughly in order of decreasing restrictiveness. Consistency Description Strict Absolute time ordering of all shared accesses matters Sequential All processes see all shared accesses in the same order Causal All processes see all casually-related shared accesses in the same order Processor PRAM consistency + memory coherence PRAM All processes see writes from each processor in the order they were issued. Writes from different processors may not always be seen in the same order (a) Weak Shared data can only be counted on to be consistent after a synchronization is done Release Shared data are made consistent when a critical region is exited Entry Shared data pertaining to a critical region are made consistent when a critical region is entered (b) Fig. 6-24. (a) Consistency models not using synchronization operations . (b) Models with synchronization operations .","title":"[6.3.8. Summary of Consistency Models](https://www.e-reading.club/chapter.php/143358/224/Tanenbaum_-_Distributed_operating_systems.html)"},{"location":"Distributed-computing/draft-Book-Distributed-operating-systems/6-Distributed-Shared-Memory/6.3.8-Summary-of-Consistency-Models/#638#summary#of#consistency#models","text":"Although other consistency models have been proposed, the main ones are discussed above. They differ in how restrictive they are, how complex their implementations are, their ease of programming, and their performance. Strict consistency is the most restrictive , but because its implementation in a DSM system is essentially impossible, it is never used. Sequential consistency is feasible, popular with programmers, and widely used. It has the problem of poor performance , however. The way to get around this result is to relax the consistency model . Some of the possibilities are shown in Fig. 6-24(a), roughly in order of decreasing restrictiveness. Consistency Description Strict Absolute time ordering of all shared accesses matters Sequential All processes see all shared accesses in the same order Causal All processes see all casually-related shared accesses in the same order Processor PRAM consistency + memory coherence PRAM All processes see writes from each processor in the order they were issued. Writes from different processors may not always be seen in the same order (a) Weak Shared data can only be counted on to be consistent after a synchronization is done Release Shared data are made consistent when a critical region is exited Entry Shared data pertaining to a critical region are made consistent when a critical region is entered (b) Fig. 6-24. (a) Consistency models not using synchronization operations . (b) Models with synchronization operations .","title":"6.3.8. Summary of Consistency Models"},{"location":"Distributed-computing/draft-Book-Distributed-operating-systems/6-Distributed-Shared-Memory/6.3-Consistency-model/","text":"6.3. CONSISTENCY MODELS Although modern multiprocessors have a great deal in common with distributed shared memory systems , it is time to leave the subject of multiprocessors and move on. In our brief introduction to DSM(distributed shared memory) systems at the start of this chapter, we said that they have one or more copies of each read-only page and one copy of each writable page. In the simplest implementation, when a writable page is referenced by a remote machine, a trap occurs and the page is fetched. However, if some writable pages are heavily shared, having only a single copy of each one can be a serious performance bottleneck. \u7b2c\u4e00\u53e5\u8bdd\u7684\u610f\u601d\u662f: \u5c3d\u7ba1\u73b0\u4ee3\u591a\u5904\u7406\u5668\u4e0e\u5206\u5e03\u5f0f\u5171\u4eab\u5185\u5b58\u7cfb\u7edf\u6709\u8bb8\u591a\u5171\u540c\u4e4b\u5904 Allowing multiple copies eases the performance problem, since it is then sufficient to update any copy, but doing so introduces a new problem: how to keep all the copies consistent . Maintaining perfect consistency is especially painful when the various copies are on different machines that can only communicate by sending messages over a slow (compared to memory speeds) network. In some DSM (and multiprocessor) systems, the solution is to accept less than perfect consistency as the price for better performance . Precisely what consistency means and how it can be relaxed without making programming unbearable is a major issue among DSM researchers. NOTE: tradeoff: \u964d\u4f4econsistency\u6765\u83b7\u53d6performance\u3002 A consistency model is essentially a contract (\u7ea6\u5b9a) between the software and the memory (Adve and Hill, 1990). It says that if the software agrees to obey certain rules , the memory promises to work correctly. If the software violates these rules , all bets are off and correctness of memory operation is no longer guaranteed. A wide spectrum of contracts have been devised, ranging from contracts that place only minor restrictions on the software to those that make normal programming nearly impossible. As you probably already guessed, the ones with minor restrictions do not perform nearly as well as the ones with major restrictions. Such is life. In this section we will study a variety of consistency models used in DSM systems. For additional information, see the paper by Mosberger (1993). NOTE: consistency model\u662f\u6db5\u76d6\u975e\u5e38\u5e7f\u6cdb\u7684\u6846\u67b6\uff0c\u5728uniprocessor\uff0cmultiple threading\uff0cdistributed system\u4e2d\u90fd\u53ef\u4ee5\u627e\u5230\u5b83\u7684\u5f71\u5b50\uff1b","title":"[6.3. CONSISTENCY MODELS](https://www.e-reading.club/chapter.php/143358/216/Tanenbaum_-_Distributed_operating_systems.html)"},{"location":"Distributed-computing/draft-Book-Distributed-operating-systems/6-Distributed-Shared-Memory/6.3-Consistency-model/#63#consistency#models","text":"Although modern multiprocessors have a great deal in common with distributed shared memory systems , it is time to leave the subject of multiprocessors and move on. In our brief introduction to DSM(distributed shared memory) systems at the start of this chapter, we said that they have one or more copies of each read-only page and one copy of each writable page. In the simplest implementation, when a writable page is referenced by a remote machine, a trap occurs and the page is fetched. However, if some writable pages are heavily shared, having only a single copy of each one can be a serious performance bottleneck. \u7b2c\u4e00\u53e5\u8bdd\u7684\u610f\u601d\u662f: \u5c3d\u7ba1\u73b0\u4ee3\u591a\u5904\u7406\u5668\u4e0e\u5206\u5e03\u5f0f\u5171\u4eab\u5185\u5b58\u7cfb\u7edf\u6709\u8bb8\u591a\u5171\u540c\u4e4b\u5904 Allowing multiple copies eases the performance problem, since it is then sufficient to update any copy, but doing so introduces a new problem: how to keep all the copies consistent . Maintaining perfect consistency is especially painful when the various copies are on different machines that can only communicate by sending messages over a slow (compared to memory speeds) network. In some DSM (and multiprocessor) systems, the solution is to accept less than perfect consistency as the price for better performance . Precisely what consistency means and how it can be relaxed without making programming unbearable is a major issue among DSM researchers. NOTE: tradeoff: \u964d\u4f4econsistency\u6765\u83b7\u53d6performance\u3002 A consistency model is essentially a contract (\u7ea6\u5b9a) between the software and the memory (Adve and Hill, 1990). It says that if the software agrees to obey certain rules , the memory promises to work correctly. If the software violates these rules , all bets are off and correctness of memory operation is no longer guaranteed. A wide spectrum of contracts have been devised, ranging from contracts that place only minor restrictions on the software to those that make normal programming nearly impossible. As you probably already guessed, the ones with minor restrictions do not perform nearly as well as the ones with major restrictions. Such is life. In this section we will study a variety of consistency models used in DSM systems. For additional information, see the paper by Mosberger (1993). NOTE: consistency model\u662f\u6db5\u76d6\u975e\u5e38\u5e7f\u6cdb\u7684\u6846\u67b6\uff0c\u5728uniprocessor\uff0cmultiple threading\uff0cdistributed system\u4e2d\u90fd\u53ef\u4ee5\u627e\u5230\u5b83\u7684\u5f71\u5b50\uff1b","title":"6.3. CONSISTENCY MODELS"},{"location":"Distributed-computing/draft-Book-Distributed-operating-systems/6-Distributed-Shared-Memory/6.3.1-Strict-Consistency/","text":"6.3.1. Strict Consistency What is strict consistency The most stringent\uff08\u4e25\u5389\u7684\uff09 consistency model is called strict consistency . It is defined by the following condition: Any read to a memory location x returns the value stored by the most recent write operation to x . This definition is natural and obvious, although it implicitly assumes the existence of absolute global time (as in Newtonian physics(\u725b\u987f\u7269\u7406\u5b66)) so that the determination of \"most recent\" is unambiguous. NOTE: stricken consistency\u7684\u8981\u6c42\u662f\u975e\u5e38\u82db\u523b\u7684: \u5b83\u7684\u5b9e\u73b0\u8981\u6c42**\u7edd\u5bf9\u7684\u5b9e\u65f6**\uff0c\u4f46\u662f\u4ee5\u76ee\u524d\u7684\u6280\u672f\u6c34\u5e73\uff0c\u8fd9\u662f\u65e0\u6cd5\u5b9e\u73b0\u7684\u3002\u4ece\u4e0a\u9762\u7684\u5b9a\u4e49\u53ef\u4ee5\u770b\u51fastrict consistency\u7684\u5b9a\u4e49\u662f\u57fa\u4e8e\u65f6\u95f4\u7684\u3002 \u5728**\u5206\u5e03\u5f0f**\u6216\u8005**\u591a\u7ebf\u7a0b**\u7684\u60c5\u51b5\u4e0b\u5176\u5b9e\u5b9e\u73b0\u8d77\u6765\u662f\u9700\u8981\u8003\u8651\u5f88\u591a\u95ee\u9898\u7684: \u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u6311\u6218: lack of global clock\uff0c\u90a3\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\uff0c\u5b9e\u73b0strict consistency\u53ef\u80fd\u975e\u5e38\u56f0\u96be\u7684\u3002 \u5206\u60c5\u51b5\u8ba8\u8bba NOTE: \u539f\u6587\u7684\u540e\u9762\u90e8\u5206\uff0c\u4f5c\u8005\u5bf9\u5404\u79cd\u53ef\u80fd\u7684\u60c5\u51b5\u4e0bstrict consistency\u7684\u60c5\u51b5\u8fdb\u884c\u4e86\u5206\u7c7b\u8ba8\u8bba Uniprocessors Uniprocessors have traditionally observed strict consistency and uniprocessor programmers have come to expect such behavior as a matter of course. A system on which the program a = 1 ; a = 2 ; print ( a ); printed 1 or any value other than 2 would quickly lead to a lot of very agitated\uff08\u6fc0\u52a8\u7684\uff09 programmers (in this chapter, print is a procedure that prints its parameter or parameters). DSM In a DSM (distributed shared memory) system, the matter is more complicated. Suppose x is a variable stored only on machine B. Imagine that a process on machine A reads x at time T1 , which means that a message is then sent to B to get x . Slightly later\uff08\u8bf4\u660e T2 \u662f\u665a\u4e8e T1 \u7684\uff09, at T2 , a process on B does a write to x . If strict consistency holds, the read should always return the old value regardless of where the machines are and how close T2 is to T1 . However, if T2\u2013T1 is, say, 1 nanosecond(\u4e24\u8005\u4e4b\u95f4\u7684\u65f6\u95f4\u95f4\u9694\u53ea\u67091NS), and the machines are 3 meters apart, in order to propagate the read request from A to B to get there before the write, the signal would have to travel at 10 times the speed of light, something forbidden by Einstein's special theory of relativity(\u72ed\u4e49\u76f8\u5bf9\u8bba). Is it reasonable for programmers to demand that the system be strictly consistent , even if this requires violating the laws of physics? NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u542b\u4e49\u662f: \u5728\u76ee\u524d\u7684\u6280\u672f\u6c34\u5e73\uff0c\u5b9e\u73b0strict ordering\u662f\u4e0d\u53ef\u80fd\u7684 Multiprocessor This brings us to the matter of the contract between the software and the memory . If the contract implicitly or explicitly promises strict consistency , then the memory had better deliver it. On the other hand, a programmer who really expects strict consistency , and whose program fails if it is not present\uff08\u5982\u679c\u5b83\u4e0d\u51fa\u73b0\uff0c\u5219\u7a0b\u5e8f\u5c31\u4f1a\u51fa\u73b0\u9519\u8bef\uff09, is living dangerously. Even on a small multiprocessor, if one processor starts to write memory location a , and a nanosecond later another processor starts to read a the reader will probably get the old value from its local cache. Anyone who writes programs that fail under these circumstances should be made to stay after school and write a program to print 100 times: \"I will avoid race conditions.\" NOTE: \u5373\u4f7f\u5728multiprocessor computer\u4e2d\uff0c\u5b9e\u73b0strict consistency\u4e5f\u662f\u975e\u5e38\u56f0\u96be\u7684 Do we really need strict consistency? NOTE: \u539f\u6587\u5e76\u6ca1\u6709\u8fd9\u6837\u7684\u6807\u9898\uff0c\u5b83\u662f\u6211\u4eec\u6dfb\u52a0\u7684\u3002\u6211\u4eec\u9700\u8981\u601d\u8003\uff0c\u6211\u4eec\u662f\u5426\u9700\u8981strict consistency\u7684\uff0c\u539f\u6587\u4e2d\uff0c\u4f5c\u8005\u4ee5\u8db3\u7403\u76f4\u64ad\u4e3a\u4f8b\u6765\u8fdb\u884c\u8bf4\u660e\uff0c\u4ece\u8fd9\u4e2a\u4e3e\u4f8b\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u77e5\u9053\uff0c\u73b0\u5b9e\u4e2d\uff0c \u6211\u4eec\u5f80\u5f80\u662f\u4e0d\u9700\u8981strict consistency\u4e5f\u80fd\u591f\u6ee1\u8db3\u6211\u4eec\u7684\u9700\u6c42\u7684\u3002 As a more realistic example, one could imagine a system to provide sports fans with up-to-the-minute (but perhaps not up-to-the-nanosecond) scores for sporting events worldwide(\u5176\u5b9e\u6240\u8c13\u7684\u5b9e\u65f6\uff0c\u90fd\u5b58\u5728\u4e00\u5b9a\u7684\u5ef6\u65f6). Answering a query as if it had been made 2 nanoseconds earlier or later might well be acceptable in this case, especially if it gave much better performance by allowing multiple copies of the data to be stored. In this case strict consistency is not promised, delivered, or needed. Notation to illustrate consistency NOTE: \u672c\u8282\u6807\u9898\u7684\u542b\u4e49\u662f: illustrate consistency\u7684\u7b26\u53f7\uff0c\u8fd9\u662f\u4f5c\u8005\u521b\u9020\u7684\u5c55\u793aconsistency\u7684\u4e00\u79cd\u975e\u5e38\u7b80\u4fbf\u7684\u65b9\u6cd5\u3002 To study consistency in detail, we will give numerous examples. To make these examples precise, we need a special notation. In this notation, several processes, P1 , P2 , and so on can be shown at different heights in the figure. The operations done by each process are shown horizontally, with time increasing to the right. Straight lines separate the processes. The symbols W(x)a and R(y)b mean that a write to x with the value a and a read from y returning b have been done, respectively. The initial value of all variables in these diagrams throughout this chapter is assumed to be 0. As an example, in Fig. 6-12(a) P1 does a write to location x , storing the value 1. Later, P2 reads x and sees the 1. This behavior is correct for a strictly consistent memory . Fig. 6-12. Behavior of two processes. The horizontal axis is time. (a) Strictly consistent memory . (b) Memory that is not strictly consistent . In contrast, in Fig. 6-12(b), P2 does a read after the write (possibly only a nanosecond after it, but still after it), and gets 0. A subsequent read gives 1. Such behavior is incorrect for a strictly consistent memory . Summary In summary, when memory is strictly consistent , all writes are instantaneously\uff08\u5373\u53ef\uff09 visible to all processes and an absolute global time order is maintained. If a memory location is changed, all subsequent reads from that location see the new value, no matter how soon after the change the reads are done and no matter which processes are doing the reading and where they are located. Similarly, if a read is done, it gets the then-current value \uff08\u5f53\u65f6\u7684\u5f53\u524d\u503c\uff09, no matter how quickly the next write is done. NOTE: \u8fd9\u662f\u5bf9strict consistency\u518d\u4e00\u6b21\u8ba8\u8bba\uff0c\u663e\u7136\u5728\u5f53\u524d\u6280\u672f\u6c34\u5e73\u662f\u65e0\u6cd5\u5b9e\u73b0\u7684\u3002","title":"[6.3.1. Strict Consistency](https://www.e-reading.club/chapter.php/143358/217/Tanenbaum_-_Distributed_operating_systems.html)"},{"location":"Distributed-computing/draft-Book-Distributed-operating-systems/6-Distributed-Shared-Memory/6.3.1-Strict-Consistency/#631#strict#consistency","text":"","title":"6.3.1. Strict Consistency"},{"location":"Distributed-computing/draft-Book-Distributed-operating-systems/6-Distributed-Shared-Memory/6.3.1-Strict-Consistency/#what#is#strict#consistency","text":"The most stringent\uff08\u4e25\u5389\u7684\uff09 consistency model is called strict consistency . It is defined by the following condition: Any read to a memory location x returns the value stored by the most recent write operation to x . This definition is natural and obvious, although it implicitly assumes the existence of absolute global time (as in Newtonian physics(\u725b\u987f\u7269\u7406\u5b66)) so that the determination of \"most recent\" is unambiguous. NOTE: stricken consistency\u7684\u8981\u6c42\u662f\u975e\u5e38\u82db\u523b\u7684: \u5b83\u7684\u5b9e\u73b0\u8981\u6c42**\u7edd\u5bf9\u7684\u5b9e\u65f6**\uff0c\u4f46\u662f\u4ee5\u76ee\u524d\u7684\u6280\u672f\u6c34\u5e73\uff0c\u8fd9\u662f\u65e0\u6cd5\u5b9e\u73b0\u7684\u3002\u4ece\u4e0a\u9762\u7684\u5b9a\u4e49\u53ef\u4ee5\u770b\u51fastrict consistency\u7684\u5b9a\u4e49\u662f\u57fa\u4e8e\u65f6\u95f4\u7684\u3002 \u5728**\u5206\u5e03\u5f0f**\u6216\u8005**\u591a\u7ebf\u7a0b**\u7684\u60c5\u51b5\u4e0b\u5176\u5b9e\u5b9e\u73b0\u8d77\u6765\u662f\u9700\u8981\u8003\u8651\u5f88\u591a\u95ee\u9898\u7684: \u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u6311\u6218: lack of global clock\uff0c\u90a3\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\uff0c\u5b9e\u73b0strict consistency\u53ef\u80fd\u975e\u5e38\u56f0\u96be\u7684\u3002","title":"What is strict consistency"},{"location":"Distributed-computing/draft-Book-Distributed-operating-systems/6-Distributed-Shared-Memory/6.3.1-Strict-Consistency/#_1","text":"NOTE: \u539f\u6587\u7684\u540e\u9762\u90e8\u5206\uff0c\u4f5c\u8005\u5bf9\u5404\u79cd\u53ef\u80fd\u7684\u60c5\u51b5\u4e0bstrict consistency\u7684\u60c5\u51b5\u8fdb\u884c\u4e86\u5206\u7c7b\u8ba8\u8bba","title":"\u5206\u60c5\u51b5\u8ba8\u8bba"},{"location":"Distributed-computing/draft-Book-Distributed-operating-systems/6-Distributed-Shared-Memory/6.3.1-Strict-Consistency/#uniprocessors","text":"Uniprocessors have traditionally observed strict consistency and uniprocessor programmers have come to expect such behavior as a matter of course. A system on which the program a = 1 ; a = 2 ; print ( a ); printed 1 or any value other than 2 would quickly lead to a lot of very agitated\uff08\u6fc0\u52a8\u7684\uff09 programmers (in this chapter, print is a procedure that prints its parameter or parameters).","title":"Uniprocessors"},{"location":"Distributed-computing/draft-Book-Distributed-operating-systems/6-Distributed-Shared-Memory/6.3.1-Strict-Consistency/#dsm","text":"In a DSM (distributed shared memory) system, the matter is more complicated. Suppose x is a variable stored only on machine B. Imagine that a process on machine A reads x at time T1 , which means that a message is then sent to B to get x . Slightly later\uff08\u8bf4\u660e T2 \u662f\u665a\u4e8e T1 \u7684\uff09, at T2 , a process on B does a write to x . If strict consistency holds, the read should always return the old value regardless of where the machines are and how close T2 is to T1 . However, if T2\u2013T1 is, say, 1 nanosecond(\u4e24\u8005\u4e4b\u95f4\u7684\u65f6\u95f4\u95f4\u9694\u53ea\u67091NS), and the machines are 3 meters apart, in order to propagate the read request from A to B to get there before the write, the signal would have to travel at 10 times the speed of light, something forbidden by Einstein's special theory of relativity(\u72ed\u4e49\u76f8\u5bf9\u8bba). Is it reasonable for programmers to demand that the system be strictly consistent , even if this requires violating the laws of physics? NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u542b\u4e49\u662f: \u5728\u76ee\u524d\u7684\u6280\u672f\u6c34\u5e73\uff0c\u5b9e\u73b0strict ordering\u662f\u4e0d\u53ef\u80fd\u7684","title":"DSM"},{"location":"Distributed-computing/draft-Book-Distributed-operating-systems/6-Distributed-Shared-Memory/6.3.1-Strict-Consistency/#multiprocessor","text":"This brings us to the matter of the contract between the software and the memory . If the contract implicitly or explicitly promises strict consistency , then the memory had better deliver it. On the other hand, a programmer who really expects strict consistency , and whose program fails if it is not present\uff08\u5982\u679c\u5b83\u4e0d\u51fa\u73b0\uff0c\u5219\u7a0b\u5e8f\u5c31\u4f1a\u51fa\u73b0\u9519\u8bef\uff09, is living dangerously. Even on a small multiprocessor, if one processor starts to write memory location a , and a nanosecond later another processor starts to read a the reader will probably get the old value from its local cache. Anyone who writes programs that fail under these circumstances should be made to stay after school and write a program to print 100 times: \"I will avoid race conditions.\" NOTE: \u5373\u4f7f\u5728multiprocessor computer\u4e2d\uff0c\u5b9e\u73b0strict consistency\u4e5f\u662f\u975e\u5e38\u56f0\u96be\u7684","title":"Multiprocessor"},{"location":"Distributed-computing/draft-Book-Distributed-operating-systems/6-Distributed-Shared-Memory/6.3.1-Strict-Consistency/#do#we#really#need#strict#consistency","text":"NOTE: \u539f\u6587\u5e76\u6ca1\u6709\u8fd9\u6837\u7684\u6807\u9898\uff0c\u5b83\u662f\u6211\u4eec\u6dfb\u52a0\u7684\u3002\u6211\u4eec\u9700\u8981\u601d\u8003\uff0c\u6211\u4eec\u662f\u5426\u9700\u8981strict consistency\u7684\uff0c\u539f\u6587\u4e2d\uff0c\u4f5c\u8005\u4ee5\u8db3\u7403\u76f4\u64ad\u4e3a\u4f8b\u6765\u8fdb\u884c\u8bf4\u660e\uff0c\u4ece\u8fd9\u4e2a\u4e3e\u4f8b\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u77e5\u9053\uff0c\u73b0\u5b9e\u4e2d\uff0c \u6211\u4eec\u5f80\u5f80\u662f\u4e0d\u9700\u8981strict consistency\u4e5f\u80fd\u591f\u6ee1\u8db3\u6211\u4eec\u7684\u9700\u6c42\u7684\u3002 As a more realistic example, one could imagine a system to provide sports fans with up-to-the-minute (but perhaps not up-to-the-nanosecond) scores for sporting events worldwide(\u5176\u5b9e\u6240\u8c13\u7684\u5b9e\u65f6\uff0c\u90fd\u5b58\u5728\u4e00\u5b9a\u7684\u5ef6\u65f6). Answering a query as if it had been made 2 nanoseconds earlier or later might well be acceptable in this case, especially if it gave much better performance by allowing multiple copies of the data to be stored. In this case strict consistency is not promised, delivered, or needed.","title":"Do we really need strict consistency?"},{"location":"Distributed-computing/draft-Book-Distributed-operating-systems/6-Distributed-Shared-Memory/6.3.1-Strict-Consistency/#notation#to#illustrate#consistency","text":"NOTE: \u672c\u8282\u6807\u9898\u7684\u542b\u4e49\u662f: illustrate consistency\u7684\u7b26\u53f7\uff0c\u8fd9\u662f\u4f5c\u8005\u521b\u9020\u7684\u5c55\u793aconsistency\u7684\u4e00\u79cd\u975e\u5e38\u7b80\u4fbf\u7684\u65b9\u6cd5\u3002 To study consistency in detail, we will give numerous examples. To make these examples precise, we need a special notation. In this notation, several processes, P1 , P2 , and so on can be shown at different heights in the figure. The operations done by each process are shown horizontally, with time increasing to the right. Straight lines separate the processes. The symbols W(x)a and R(y)b mean that a write to x with the value a and a read from y returning b have been done, respectively. The initial value of all variables in these diagrams throughout this chapter is assumed to be 0. As an example, in Fig. 6-12(a) P1 does a write to location x , storing the value 1. Later, P2 reads x and sees the 1. This behavior is correct for a strictly consistent memory . Fig. 6-12. Behavior of two processes. The horizontal axis is time. (a) Strictly consistent memory . (b) Memory that is not strictly consistent . In contrast, in Fig. 6-12(b), P2 does a read after the write (possibly only a nanosecond after it, but still after it), and gets 0. A subsequent read gives 1. Such behavior is incorrect for a strictly consistent memory .","title":"Notation to illustrate consistency"},{"location":"Distributed-computing/draft-Book-Distributed-operating-systems/6-Distributed-Shared-Memory/6.3.1-Strict-Consistency/#summary","text":"In summary, when memory is strictly consistent , all writes are instantaneously\uff08\u5373\u53ef\uff09 visible to all processes and an absolute global time order is maintained. If a memory location is changed, all subsequent reads from that location see the new value, no matter how soon after the change the reads are done and no matter which processes are doing the reading and where they are located. Similarly, if a read is done, it gets the then-current value \uff08\u5f53\u65f6\u7684\u5f53\u524d\u503c\uff09, no matter how quickly the next write is done. NOTE: \u8fd9\u662f\u5bf9strict consistency\u518d\u4e00\u6b21\u8ba8\u8bba\uff0c\u663e\u7136\u5728\u5f53\u524d\u6280\u672f\u6c34\u5e73\u662f\u65e0\u6cd5\u5b9e\u73b0\u7684\u3002","title":"Summary"},{"location":"Distributed-computing/draft-Book-Distributed-operating-systems/6-Distributed-Shared-Memory/6.3.2-Sequential-Consistency/","text":"6.3.2. Sequential Consistency","title":"[6.3.2. Sequential Consistency](https://www.e-reading.club/chapter.php/143358/218/Tanenbaum_-_Distributed_operating_systems.html)"},{"location":"Distributed-computing/draft-Book-Distributed-operating-systems/6-Distributed-Shared-Memory/6.3.2-Sequential-Consistency/#632#sequential#consistency","text":"","title":"6.3.2. Sequential Consistency"},{"location":"Distributed-data-store/","text":"Distributed data store \u611f\u89c9distributed data store/partition\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u662f\u6e90\u81ea\u4e8e**\u5206\u800c\u6cbb\u4e4b\u601d\u60f3**\u3002 wikipedia Distributed data store A distributed data store is a computer network where information is stored on more than one node , often in a replicated fashion.[ 1] It is usually specifically used to refer to either a distributed database where users store information on a number of nodes , or a computer network in which users store information on a number of peer network nodes . Distributed databases Distributed databases are usually non-relational databases that enable a quick access to data over a large number of nodes. Some distributed databases expose rich query abilities while others are limited to a key-value store semantics. Examples of limited distributed databases are Google 's Bigtable , which is much more than a distributed file system or a peer-to-peer network ,[ 2] Amazon 's Dynamo [ 3] and Windows Azure Storage .[ 4] As the ability of arbitrary querying is not as important as the availability , designers of distributed data stores have increased the latter at an expense of consistency . But the high-speed read/write access results in reduced consistency, as it is not possible to have both consistency , availability, and partition tolerance of the network , as it has been proven by the CAP theorem . Application \u5206\u5e93\u5206\u8868 zhuanlan MySQL\uff1a\u4e92\u8054\u7f51\u516c\u53f8\u5e38\u7528\u5206\u5e93\u5206\u8868\u65b9\u6848\u6c47\u603b\uff01 \u5185\u5bb9\u8f83\u597d csdn \u5f7b\u5e95\u641e\u6e05\u5206\u5e93\u5206\u8868\uff08\u5782\u76f4\u5206\u5e93\uff0c\u5782\u76f4\u5206\u8868\uff0c\u6c34\u5e73\u5206\u5e93\uff0c\u6c34\u5e73\u5206\u8868\uff09 meituan \u5927\u4f17\u70b9\u8bc4\u8ba2\u5355\u7cfb\u7edf\u5206\u5e93\u5206\u8868\u5b9e\u8df5 Redis redis Partitioning: how to split data among multiple Redis instances. redis Redis cluster tutorial Distributed hash table \u53c2\u89c1 Distributed-hash-table \u7ae0\u8282\u3002","title":"Introduction"},{"location":"Distributed-data-store/#distributed#data#store","text":"\u611f\u89c9distributed data store/partition\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u662f\u6e90\u81ea\u4e8e**\u5206\u800c\u6cbb\u4e4b\u601d\u60f3**\u3002","title":"Distributed data store"},{"location":"Distributed-data-store/#wikipedia#distributed#data#store","text":"A distributed data store is a computer network where information is stored on more than one node , often in a replicated fashion.[ 1] It is usually specifically used to refer to either a distributed database where users store information on a number of nodes , or a computer network in which users store information on a number of peer network nodes .","title":"wikipedia Distributed data store"},{"location":"Distributed-data-store/#distributed#databases","text":"Distributed databases are usually non-relational databases that enable a quick access to data over a large number of nodes. Some distributed databases expose rich query abilities while others are limited to a key-value store semantics. Examples of limited distributed databases are Google 's Bigtable , which is much more than a distributed file system or a peer-to-peer network ,[ 2] Amazon 's Dynamo [ 3] and Windows Azure Storage .[ 4] As the ability of arbitrary querying is not as important as the availability , designers of distributed data stores have increased the latter at an expense of consistency . But the high-speed read/write access results in reduced consistency, as it is not possible to have both consistency , availability, and partition tolerance of the network , as it has been proven by the CAP theorem .","title":"Distributed databases"},{"location":"Distributed-data-store/#application","text":"","title":"Application"},{"location":"Distributed-data-store/#_1","text":"zhuanlan MySQL\uff1a\u4e92\u8054\u7f51\u516c\u53f8\u5e38\u7528\u5206\u5e93\u5206\u8868\u65b9\u6848\u6c47\u603b\uff01 \u5185\u5bb9\u8f83\u597d csdn \u5f7b\u5e95\u641e\u6e05\u5206\u5e93\u5206\u8868\uff08\u5782\u76f4\u5206\u5e93\uff0c\u5782\u76f4\u5206\u8868\uff0c\u6c34\u5e73\u5206\u5e93\uff0c\u6c34\u5e73\u5206\u8868\uff09 meituan \u5927\u4f17\u70b9\u8bc4\u8ba2\u5355\u7cfb\u7edf\u5206\u5e93\u5206\u8868\u5b9e\u8df5","title":"\u5206\u5e93\u5206\u8868"},{"location":"Distributed-data-store/#redis","text":"redis Partitioning: how to split data among multiple Redis instances. redis Redis cluster tutorial","title":"Redis"},{"location":"Distributed-data-store/#distributed#hash#table","text":"\u53c2\u89c1 Distributed-hash-table \u7ae0\u8282\u3002","title":"Distributed hash table"},{"location":"Distributed-data-store/Amazon-Dynamo/","text":"Amazon Dynamo CSDN Amazon Dynamo\u7cfb\u7edf\u67b6\u6784","title":"Introduction"},{"location":"Distributed-data-store/Amazon-Dynamo/#amazon#dynamo","text":"","title":"Amazon Dynamo"},{"location":"Distributed-data-store/Amazon-Dynamo/#csdn#amazon#dynamo","text":"","title":"CSDN Amazon Dynamo\u7cfb\u7edf\u67b6\u6784"},{"location":"Distributed-data-store/Data-Sharding-Strategy/","text":"Data sharding strategy \u672c\u7ae0\u8ba8\u8bba\u5728distributed data store\u4e2d\u6d89\u53ca\u7684data sharding strategy\uff0c\u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\uff0c\u5bf9\u5404\u79cddata sharding strategy\u3001\u4e3b\u6d41\u7684\u5206\u5e03\u5f0fdata store\u7684\u5b9e\u73b0\u8fdb\u884c\u4e86\u603b\u7ed3: 1\u3001yugabyte Four Data Sharding Strategies We Analyzed in Building a Distributed SQL Database \u8fd9\u7bc7\u6587\u7ae0\u975e\u5e38\u597d\uff0c\u9996\u5148\u63a8\u8350\u9605\u8bfb wanweibaike Partition (database) A partition is a division of a logical database or its constituent elements into distinct independent parts. Database partitioning is normally done for manageability, performance or availability [ 1] reasons, or for load balancing . Partitioning criteria They take a partitioning key and assign a partition based on certain criteria. Some common criteria include: Range partitioning Round-robin partitioning Partitioning methods Horizontal partitioning NOTE: \u6c34\u5e73 Vertical partitioning NOTE: \u5782\u76f4 wanweibaike Shard (database architecture) A database shard , or simply a shard , is a horizontal partition of data in a database or search engine . Each shard is held on a separate database server instance, to spread load. Notable implementations NOTE: \u53ef\u4ee5\u770b\u5230\uff0c\u4e3b\u6d41\u7684\u90fd\u662f\u91c7\u7528\u7684shared\uff0c\u5373horizontal partition Google Spanner Spanner , Google's global-scale distributed database, shards across multiple Paxos state machines to scale to \"millions of machines across hundreds of data centers and trillions of database rows\".[ 19] \u8bc4\u4ef7\u6307\u6807 1\u3001\u5747\u5300\u3001\u8d1f\u8f7d\u5747\u8861 TODO gitbook Systems Design Glossary","title":"Introduction"},{"location":"Distributed-data-store/Data-Sharding-Strategy/#data#sharding#strategy","text":"\u672c\u7ae0\u8ba8\u8bba\u5728distributed data store\u4e2d\u6d89\u53ca\u7684data sharding strategy\uff0c\u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\uff0c\u5bf9\u5404\u79cddata sharding strategy\u3001\u4e3b\u6d41\u7684\u5206\u5e03\u5f0fdata store\u7684\u5b9e\u73b0\u8fdb\u884c\u4e86\u603b\u7ed3: 1\u3001yugabyte Four Data Sharding Strategies We Analyzed in Building a Distributed SQL Database \u8fd9\u7bc7\u6587\u7ae0\u975e\u5e38\u597d\uff0c\u9996\u5148\u63a8\u8350\u9605\u8bfb","title":"Data sharding strategy"},{"location":"Distributed-data-store/Data-Sharding-Strategy/#wanweibaike#partition#database","text":"A partition is a division of a logical database or its constituent elements into distinct independent parts. Database partitioning is normally done for manageability, performance or availability [ 1] reasons, or for load balancing .","title":"wanweibaike Partition (database)"},{"location":"Distributed-data-store/Data-Sharding-Strategy/#partitioning#criteria","text":"They take a partitioning key and assign a partition based on certain criteria. Some common criteria include: Range partitioning Round-robin partitioning","title":"Partitioning criteria"},{"location":"Distributed-data-store/Data-Sharding-Strategy/#partitioning#methods","text":"Horizontal partitioning NOTE: \u6c34\u5e73 Vertical partitioning NOTE: \u5782\u76f4","title":"Partitioning methods"},{"location":"Distributed-data-store/Data-Sharding-Strategy/#wanweibaike#shard#database#architecture","text":"A database shard , or simply a shard , is a horizontal partition of data in a database or search engine . Each shard is held on a separate database server instance, to spread load.","title":"wanweibaike Shard (database architecture)"},{"location":"Distributed-data-store/Data-Sharding-Strategy/#notable#implementations","text":"NOTE: \u53ef\u4ee5\u770b\u5230\uff0c\u4e3b\u6d41\u7684\u90fd\u662f\u91c7\u7528\u7684shared\uff0c\u5373horizontal partition","title":"Notable implementations"},{"location":"Distributed-data-store/Data-Sharding-Strategy/#google#spanner","text":"Spanner , Google's global-scale distributed database, shards across multiple Paxos state machines to scale to \"millions of machines across hundreds of data centers and trillions of database rows\".[ 19]","title":"Google Spanner"},{"location":"Distributed-data-store/Data-Sharding-Strategy/#_1","text":"1\u3001\u5747\u5300\u3001\u8d1f\u8f7d\u5747\u8861","title":"\u8bc4\u4ef7\u6307\u6807"},{"location":"Distributed-data-store/Data-Sharding-Strategy/#todo","text":"gitbook Systems Design Glossary","title":"TODO"},{"location":"Distributed-data-store/Data-Sharding-Strategy/TODO-Consistent-hash-VS-rendezvous-hash/","text":"Rendezvous hashing vs consistent hashing https://github.com/clohfink/RendezvousHash https://stackoverflow.com/questions/20790898/consistent-hashing-vs-rendezvous-hrw-hashing-what-are-the-tradeoffs https://blog.kevingomez.fr/2019/04/11/clusters-and-data-sharding-introducing-rendezvous-hashing/ https://www.pvk.ca/Blog/2017/09/24/rendezvous-hashing-my-baseline-consistent-distribution-method/","title":"TODO-Consistent-hash-VS-rendezvous-hash"},{"location":"Distributed-data-store/Data-Sharding-Strategy/TODO-Consistent-hash-VS-rendezvous-hash/#rendezvous#hashing#vs#consistent#hashing","text":"https://github.com/clohfink/RendezvousHash https://stackoverflow.com/questions/20790898/consistent-hashing-vs-rendezvous-hrw-hashing-what-are-the-tradeoffs https://blog.kevingomez.fr/2019/04/11/clusters-and-data-sharding-introducing-rendezvous-hashing/ https://www.pvk.ca/Blog/2017/09/24/rendezvous-hashing-my-baseline-consistent-distribution-method/","title":"Rendezvous hashing vs consistent hashing"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Consistent-hashing/","text":"Consistent hashing \u4e00\u81f4\u6027\u54c8\u5e0c\u3002 wikipedia Consistent hashing In computer science , consistent hashing is a special kind of hashing such that when a hash table is resized, only $ K/n $ keys need to be remapped(\u91cd\u65b0\u7b56\u5212) on average, where $ K $ is the number of keys, and $ n $ is the number of slots. In contrast, in most traditional hash tables , a change in the number of array slots causes nearly all keys to be remapped because the mapping between the keys and the slots is defined by a modular operation . Consistent hashing achieves some of the goals of rendezvous hashing (also called HRW Hashing), which is more general, since consistent hashing has been shown to be a special case of rendezvous hashing . Rendezvous hashing was first described in 1996, while consistent hashing appeared in 1997. The two techniques use different algorithms. History The term \" consistent hashing \" was introduced by Karger et al. at MIT for use in distributed caching . This academic paper from 1997 introduced the term \"consistent hashing\" as a way of distributing\uff08\u5206\u5e03\uff09 requests among a changing population of Web servers. Each slot is then represented by a node in a distributed system. The addition (joins) and removal (leaves/failures) of nodes only requires $ K/n $ items to be re-shuffled when the number of slots/nodes change. The authors mention Linear hashing and its ability to handle sequential addition and removal of nodes, while consistent hashing allows buckets to be added and removed in arbitrary order. [ 1] Teradata used this technique in their distributed database, released in 1986, although they did not use this term. Teradata still use the concept of a Hash table to fulfill exactly this purpose. Akamai Technologies was founded in 1998 by the scientists Daniel Lewin and F. Thomson Leighton (co-authors of the article coining \"consistent hashing\") to apply this algorithm, which gave birth to the content delivery network industry. Consistent hashing has also been used to reduce the impact of partial system failures in large Web applications as to allow for robust caches without incurring the system wide fallout of a failure.[ 2] The consistent hashing concept also applies to the design of distributed hash tables (DHTs). DHTs use consistent hashing to partition a keyspace among a distributed set of nodes, and additionally provide an overlay network that connects nodes such that the node responsible for any key can be efficiently located. Rendezvous hashing , designed at the same time as consistent hashing, achieves the same goals using the very different Highest Random Weight (HRW) algorithm. Need for consistent hashing While running collections of caching machines some limitations are experienced. A common way of load balancing $ n $ cache machines is to put object $ o $ in cache machine number $ {\\text{hash}}(o)\\;\\left({\\text{mod }}n\\right) $. But this will not work if a cache machine is added or removed because $ n $ changes and every object is hashed to a new location. This can be disastrous(\u707e\u96be\u6027\u7684) since the originating content servers are flooded with requests from the cache machines. Hence consistent hashing is needed to avoid swamping(\u8986\u6ca1) of servers. Consistent hashing maps objects to the same cache machine, as far as possible. It means when a cache machine is added, it takes its share of objects from all the other cache machines and when it is removed, its objects are shared among the remaining machines. The main idea behind the consistent hashing algorithm is to associate each cache with one or more hash value intervals(\u95f4\u9694) where the interval boundaries are determined by calculating the hash of each cache identifier. (The hash function used to define the intervals does not have to be the same function used to hash the cached values. Only the range of the two functions need match.) If the cache is removed its interval is taken over by a cache with an adjacent interval. All the remaining caches are unchanged. Technique Consistent hashing is based on mapping each object to a point on a circle (or equivalently, mapping each object to a real angle). The system maps each available machine (or other storage bucket) to many pseudo-randomly distributed points on the same circle. To find where an object should be placed, the system finds the location of that object's key on the circle; then walks around the circle until falling into the first bucket it encounters (or equivalently, the first available bucket with a higher angle). The result is that each bucket contains all the resources located between each one of its points and the previous points that belong to other buckets. If a bucket becomes unavailable (for example because the computer it resides on is not reachable), then the points it maps to will be removed. Requests for resources that would have mapped to each of those points now map to the next highest points. Since each bucket is associated with many pseudo-randomly distributed points, the resources that were held by that bucket will now map to many different buckets. The items that mapped to the lost bucket must be redistributed among the remaining ones, but values mapping to other buckets will still do so and do not need to be moved. A similar process occurs when a bucket is added. By adding new bucket points, we make any resources between those and the points corresponding to the next smaller angles map to the new bucket. These resources will no longer be associated with the previous buckets, and any value previously stored there will not be found by the selection method described above. The portion of the keys associated with each bucket can be altered by altering the number of angles that bucket maps to. Complexity Asymptotic time complexities for N nodes (or slots) and K keys Classic hash table Consistent hashing add a node O(K) O(K/N + log(N)) remove a node O(K) O(K/N + log(N)) add a key O(1) O(log(N)) remove a key O(1) O(log(N)) The O(log(N)) complexity for consistent hashing comes from the fact that a binary search among nodes angles is required to find the next node on the ring. Examples of use Some known instances where consistent hashing is used are: 1\u3001 Couchbase automated data partitioning 2\u3001Partitioning component of Amazon's storage system Dynamo [ 4] 3\u3001Data partitioning in Apache Cassandra [ 5] NOTE: \u53c2\u89c1 datastax Consistent hashing 4\u3001Data Partitioning in Voldemort [ 6] 5\u3001 Akka 's consistent hashing router[ 7] 6\u3001 Riak , a distributed key-value database[ 8] \u8865\u5145\u5185\u5bb9 1\u3001cnblogs \u7ed9\u9762\u8bd5\u5b98\u8bb2\u660e\u767d\uff1a\u4e00\u81f4\u6027Hash\u7684\u539f\u7406\u548c\u5b9e\u8df5 \u5176\u4e2d\u7ed3\u5408\u4e86\u5177\u4f53\u7684\u6848\u4f8b\u6765\u8bf4\u660e\uff0c\u6bd4\u8f83\u5bb9\u6613\u7406\u89e3\uff0c\u5e76\u4e14\u8fd8\u8bf4\u660e\u4e86virtual node trick 2\u3001wikipedia Hash table # Monotonic keys \u5176\u4e2d\u4e5f\u8ba8\u8bba\u4e86consistent hashing\u3002 3\u3001toptal A Guide to Consistent Hashing Virtual node trick \u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\uff0c\u5bf9virtual node trick\u8fdb\u884c\u4e86\u8bf4\u660e: 1\u3001cnblogs \u7ed9\u9762\u8bd5\u5b98\u8bb2\u660e\u767d\uff1a\u4e00\u81f4\u6027Hash\u7684\u539f\u7406\u548c\u5b9e\u8df5 stackoverflow does redis cluster use consistent hashing A You are right, virtual nodes is quite simalar with hash slot. But virtual nodes is not an original concept of consistent hashing, but more like a trick used by Cassandra based on consistent hashing. So it's also ok for redis to say not using consistent hashing. So, don't bother with phraseology. datastax Virtual nodes # Distributing data using vnodes Virtual vs single-token architecture \u4f18\u52bf \u4ece\u5176\u4e2d\u7684\u5185\u5bb9\u53ef\u77e5\uff0cvirtual node\u6709\u5982\u4e0b\u4f18\u52bf: 1\u3001load balance 2\u3001\u964d\u4f4e\u8fc1\u79fb\u91cf Implementation github mixu / vnodehash TODO aaronice.gitbook Consistent Hashing \u5176\u4e2d\u7ed9\u51fa\u4e86\u4e24\u79cdconsistent hash\u7684\u5b9e\u73b0","title":"Introduction"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Consistent-hashing/#consistent#hashing","text":"\u4e00\u81f4\u6027\u54c8\u5e0c\u3002","title":"Consistent hashing"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Consistent-hashing/#wikipedia#consistent#hashing","text":"In computer science , consistent hashing is a special kind of hashing such that when a hash table is resized, only $ K/n $ keys need to be remapped(\u91cd\u65b0\u7b56\u5212) on average, where $ K $ is the number of keys, and $ n $ is the number of slots. In contrast, in most traditional hash tables , a change in the number of array slots causes nearly all keys to be remapped because the mapping between the keys and the slots is defined by a modular operation . Consistent hashing achieves some of the goals of rendezvous hashing (also called HRW Hashing), which is more general, since consistent hashing has been shown to be a special case of rendezvous hashing . Rendezvous hashing was first described in 1996, while consistent hashing appeared in 1997. The two techniques use different algorithms.","title":"wikipedia Consistent hashing"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Consistent-hashing/#history","text":"The term \" consistent hashing \" was introduced by Karger et al. at MIT for use in distributed caching . This academic paper from 1997 introduced the term \"consistent hashing\" as a way of distributing\uff08\u5206\u5e03\uff09 requests among a changing population of Web servers. Each slot is then represented by a node in a distributed system. The addition (joins) and removal (leaves/failures) of nodes only requires $ K/n $ items to be re-shuffled when the number of slots/nodes change. The authors mention Linear hashing and its ability to handle sequential addition and removal of nodes, while consistent hashing allows buckets to be added and removed in arbitrary order. [ 1] Teradata used this technique in their distributed database, released in 1986, although they did not use this term. Teradata still use the concept of a Hash table to fulfill exactly this purpose. Akamai Technologies was founded in 1998 by the scientists Daniel Lewin and F. Thomson Leighton (co-authors of the article coining \"consistent hashing\") to apply this algorithm, which gave birth to the content delivery network industry. Consistent hashing has also been used to reduce the impact of partial system failures in large Web applications as to allow for robust caches without incurring the system wide fallout of a failure.[ 2] The consistent hashing concept also applies to the design of distributed hash tables (DHTs). DHTs use consistent hashing to partition a keyspace among a distributed set of nodes, and additionally provide an overlay network that connects nodes such that the node responsible for any key can be efficiently located. Rendezvous hashing , designed at the same time as consistent hashing, achieves the same goals using the very different Highest Random Weight (HRW) algorithm.","title":"History"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Consistent-hashing/#need#for#consistent#hashing","text":"While running collections of caching machines some limitations are experienced. A common way of load balancing $ n $ cache machines is to put object $ o $ in cache machine number $ {\\text{hash}}(o)\\;\\left({\\text{mod }}n\\right) $. But this will not work if a cache machine is added or removed because $ n $ changes and every object is hashed to a new location. This can be disastrous(\u707e\u96be\u6027\u7684) since the originating content servers are flooded with requests from the cache machines. Hence consistent hashing is needed to avoid swamping(\u8986\u6ca1) of servers. Consistent hashing maps objects to the same cache machine, as far as possible. It means when a cache machine is added, it takes its share of objects from all the other cache machines and when it is removed, its objects are shared among the remaining machines. The main idea behind the consistent hashing algorithm is to associate each cache with one or more hash value intervals(\u95f4\u9694) where the interval boundaries are determined by calculating the hash of each cache identifier. (The hash function used to define the intervals does not have to be the same function used to hash the cached values. Only the range of the two functions need match.) If the cache is removed its interval is taken over by a cache with an adjacent interval. All the remaining caches are unchanged.","title":"Need for consistent hashing"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Consistent-hashing/#technique","text":"Consistent hashing is based on mapping each object to a point on a circle (or equivalently, mapping each object to a real angle). The system maps each available machine (or other storage bucket) to many pseudo-randomly distributed points on the same circle. To find where an object should be placed, the system finds the location of that object's key on the circle; then walks around the circle until falling into the first bucket it encounters (or equivalently, the first available bucket with a higher angle). The result is that each bucket contains all the resources located between each one of its points and the previous points that belong to other buckets. If a bucket becomes unavailable (for example because the computer it resides on is not reachable), then the points it maps to will be removed. Requests for resources that would have mapped to each of those points now map to the next highest points. Since each bucket is associated with many pseudo-randomly distributed points, the resources that were held by that bucket will now map to many different buckets. The items that mapped to the lost bucket must be redistributed among the remaining ones, but values mapping to other buckets will still do so and do not need to be moved. A similar process occurs when a bucket is added. By adding new bucket points, we make any resources between those and the points corresponding to the next smaller angles map to the new bucket. These resources will no longer be associated with the previous buckets, and any value previously stored there will not be found by the selection method described above. The portion of the keys associated with each bucket can be altered by altering the number of angles that bucket maps to.","title":"Technique"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Consistent-hashing/#complexity","text":"Asymptotic time complexities for N nodes (or slots) and K keys Classic hash table Consistent hashing add a node O(K) O(K/N + log(N)) remove a node O(K) O(K/N + log(N)) add a key O(1) O(log(N)) remove a key O(1) O(log(N)) The O(log(N)) complexity for consistent hashing comes from the fact that a binary search among nodes angles is required to find the next node on the ring.","title":"Complexity"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Consistent-hashing/#examples#of#use","text":"Some known instances where consistent hashing is used are: 1\u3001 Couchbase automated data partitioning 2\u3001Partitioning component of Amazon's storage system Dynamo [ 4] 3\u3001Data partitioning in Apache Cassandra [ 5] NOTE: \u53c2\u89c1 datastax Consistent hashing 4\u3001Data Partitioning in Voldemort [ 6] 5\u3001 Akka 's consistent hashing router[ 7] 6\u3001 Riak , a distributed key-value database[ 8]","title":"Examples of use"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Consistent-hashing/#_1","text":"1\u3001cnblogs \u7ed9\u9762\u8bd5\u5b98\u8bb2\u660e\u767d\uff1a\u4e00\u81f4\u6027Hash\u7684\u539f\u7406\u548c\u5b9e\u8df5 \u5176\u4e2d\u7ed3\u5408\u4e86\u5177\u4f53\u7684\u6848\u4f8b\u6765\u8bf4\u660e\uff0c\u6bd4\u8f83\u5bb9\u6613\u7406\u89e3\uff0c\u5e76\u4e14\u8fd8\u8bf4\u660e\u4e86virtual node trick 2\u3001wikipedia Hash table # Monotonic keys \u5176\u4e2d\u4e5f\u8ba8\u8bba\u4e86consistent hashing\u3002 3\u3001toptal A Guide to Consistent Hashing","title":"\u8865\u5145\u5185\u5bb9"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Consistent-hashing/#virtual#node#trick","text":"\u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\uff0c\u5bf9virtual node trick\u8fdb\u884c\u4e86\u8bf4\u660e: 1\u3001cnblogs \u7ed9\u9762\u8bd5\u5b98\u8bb2\u660e\u767d\uff1a\u4e00\u81f4\u6027Hash\u7684\u539f\u7406\u548c\u5b9e\u8df5","title":"Virtual node trick"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Consistent-hashing/#stackoverflow#does#redis#cluster#use#consistent#hashing","text":"A You are right, virtual nodes is quite simalar with hash slot. But virtual nodes is not an original concept of consistent hashing, but more like a trick used by Cassandra based on consistent hashing. So it's also ok for redis to say not using consistent hashing. So, don't bother with phraseology.","title":"stackoverflow does redis cluster use consistent hashing"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Consistent-hashing/#datastax#virtual#nodes#distributing#data#using#vnodes","text":"Virtual vs single-token architecture","title":"datastax Virtual nodes # Distributing data using vnodes"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Consistent-hashing/#_2","text":"\u4ece\u5176\u4e2d\u7684\u5185\u5bb9\u53ef\u77e5\uff0cvirtual node\u6709\u5982\u4e0b\u4f18\u52bf: 1\u3001load balance 2\u3001\u964d\u4f4e\u8fc1\u79fb\u91cf","title":"\u4f18\u52bf"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Consistent-hashing/#implementation","text":"github mixu / vnodehash","title":"Implementation"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Consistent-hashing/#todo","text":"aaronice.gitbook Consistent Hashing \u5176\u4e2d\u7ed9\u51fa\u4e86\u4e24\u79cdconsistent hash\u7684\u5b9e\u73b0","title":"TODO"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Consistent-hashing/cnblogs-%E4%B8%80%E8%87%B4%E6%80%A7Hash%E7%9A%84%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E8%B7%B5/","text":"cnblogs \u7ed9\u9762\u8bd5\u5b98\u8bb2\u660e\u767d\uff1a\u4e00\u81f4\u6027Hash\u7684\u539f\u7406\u548c\u5b9e\u8df5 \u201c\u4e00\u81f4\u6027hash\u7684\u8bbe\u8ba1\u521d\u8877\u662f\u89e3\u51b3\u5206\u5e03\u5f0f\u7f13\u5b58\u95ee\u9898\uff0c\u5b83\u4e0d\u4ec5\u80fd\u8d77\u5230hash\u4f5c\u7528\uff0c\u8fd8\u53ef\u4ee5\u5728\u670d\u52a1\u5668\u5b95\u673a\u65f6\uff0c\u5c3d\u91cf\u5c11\u5730\u8fc1\u79fb\u6570\u636e\u3002\u56e0\u6b64\u88ab\u5e7f\u6cdb\u7528\u4e8e**\u72b6\u6001\u670d\u52a1**\u7684**\u8def\u7531\u529f\u80fd**\u201d 01\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u8def\u7531\u7b97\u6cd5 \u5047\u8bbe\u6709\u4e00\u4e2a\u6d88\u606f\u63a8\u9001\u7cfb\u7edf\uff0c\u5176\u7b80\u6613\u67b6\u6784\u5982\u4e0b ) \u8bbe\u5907\u63a5\u5165\u5c42**\u4e0d\u4ec5\u8981\u63a5\u6536\u8bbe\u5907\u7684\u767b\u5f55\u3001\u4e0b\u7ebf\u7b49\u72b6\u6001\u547d\u4ee4\uff0c\u8fd8\u8981\u628a\u5f00\u53d1\u8005\u7684\u6d88\u606f\u63a8\u9001\u7ed9\u8bbe\u5907\u3002\u8fd9\u4e2a\u65f6\u5019\u8bbe\u5907\u63a5\u5165\u5c42\u5c31\u9700\u8981\u7ef4\u62a4\u8bbe\u5907\u7684**\u72b6\u6001\u4fe1\u606f \uff08\u5f53\u7136\u53ef\u4ee5\u4e13\u95e8\u62c6\u4e00\u4e2a**\u72b6\u6001\u670d\u52a1**\u53bb\u7ef4\u62a4\u8fd9\u4e9b\u4fe1\u606f\uff0c\u8981\u6c42\u8fd9\u90e8\u5206\u5fc5\u987b\u5c11\u6709\u4ee3\u7801\u66f4\u65b0\uff0c\u5177\u4f53\u539f\u56e0\u81ea\u5df1\u53bb\u60f3\u54e6=_=\uff09\u3002\u8fd9\u4e2a\u65f6\u5019\u8bbe\u5907\u63a5\u5165\u5c42\u7684\u6bcf\u53f0server\u90fd\u4fdd\u7559\u4e00\u6279\u8bbe\u5907\u7684\u72b6\u6001\u4fe1\u606fcache\uff0c\u8bbe\u5907\u5e94\u8be5\u8fde\u63a5\u54ea\u53f0server\u53bb\u83b7\u53d6\u6570\u636e\uff0c\u540c\u65f6**\u4e2d\u95f4\u5c42**\u7684\u6d88\u606f\u53c8\u8be5\u53d1\u5f80\u54ea\u4e2aserver\u53bb\u63a8\u9001\u5462\uff1f\u8fd9\u5c31\u7528\u5230\u4e86\u4e00\u81f4\u6027hash\u7b97\u6cd5\u3002 02\u4ec0\u4e48\u662f\u4e00\u81f4\u6027hash\u7b97\u6cd5 \u4e00\u81f4\u6027hash\u7531\u5bf9\u8c61\u3001\u8d44\u6e90\u3001\u7b97\u6cd5\u548c\u673a\u5668\u7ec4\u6210\u3002\u5b83\u8981\u505a\u7684\u662f\uff1a\u5bf9\u8c61\u901a\u8fc7\u7b97\u6cd5\u5224\u65ad\u8fde\u54ea\u53f0\u673a\u5668\u3002\u5728\u5982\u4e0a\u7cfb\u7edf\u4e2d\uff1a\u8bbe\u5907id\uff08 userID \uff09\u4e3a\u5bf9\u8c61\uff1b\u5176\u5bf9\u5e94\u7684\u72b6\u6001\u6570\u636e(cache)\u4e3a\u8d44\u6e90\uff1b\u670d\u52a1\u5668\u4e3a\u673a\u5668\u3002 NOTE: 1\u3001\u76ee\u7684\u662f\u5c06\u8bbe\u5907\u7684\u8bf7\u6c42\u5206\u6563\u5230\u96c6\u7fa4\u4e2d\uff0c\u56e0\u6b64\u4f7f\u7528\u8bbe\u5907id\uff08 userID \uff09\u4f5c\u4e3akey\uff0c\u5c06\u5b83\u8fdb\u884c\u5212\u5206 2\u3001\u4e00\u822c\u8981\u4fdd\u6301 userID \u4e3a**monotonic**\uff0c\u8fd9\u6837\u624d\u80fd\u591f\u5c06\u5b83\u5212\u5206\u4e3a\u591a\u4e2a\u533a\u95f4 \u5728\u4e00\u81f4\u6027hash\u7b97\u6cd5\u4e2d\uff0c\u8fd9\u4e9b\u8d44\u6e90\u56f4\u6210\u4e86\u4e00\u4e2a\u95ed\u73af\uff0c\u6bcf\u53f0\u673a\u5668\u53c8\u4fdd\u5b58\u7740\u4e00\u4e2a\u8d44\u6e90\u6bb5\uff0c\u6bcf\u4e2a\u8d44\u6e90\u6bb5\u5bf9\u5e94\u4e00\u6279\u5bf9\u8c61/\u8bbe\u5907\uff1b\u8fd9\u6837\u5982\u679c\u67d0\u53f0\u673a\u5668\u6302\u4e86\uff0c\u90a3\u5b83\u5bf9\u5e94\u7684\u8d44\u6e90\u8f6c\u79fb\u5230\u79bb\u5b83\u8f83\u8fd1\u7684**\u673a\u5668x**\uff0c\u8fd9\u53f0dead server\u5bf9\u5e94\u7684\u8bbe\u5907\u8fde\u63a5\u5230\u673a\u5668x\u5c31\u884c\u3002 \u73b0\u5728\u5047\u8bbe\u8fd9\u56db\u4e2a\u8d44\u6e90\u6bb5\u5bf9\u5e94\u7684\u8bbe\u5907\uff0c\u6d3b\u8dc3\u60c5\u51b5\u76f8\u5dee\u8f83\u5927\u3002\u6bd4\u5982\u8bf4\u8d44\u6e90\u6bb51\u30012\u5bf9\u5e94\u7684\u8bbe\u5907\u7279\u522b\u6d3b\u8dc3\uff0c\u800c\u8d44\u6e90\u6bb53\u548c4\u51e0\u4e4e\u6ca1\u6d3b\u52a8\u3002\u8fd9\u6837\u673a\u56681-2\u9700\u8981\u4fdd\u5b58\u5927\u91cf\u7684\u72b6\u6001\u6570\u636e\uff0c\u800c3-4\u5219\u6709\u5927\u91cf\u7684\u7a7a\u7f6e\uff0c\u663e\u7136\u662f\u4e0d\u5408\u7406\u7684\u3002\u6539\u8fdb\u7248\u7684\u4e00\u81f4\u6027hash\u7b97\u6cd5\u662f\u8fd9\u6837\u64cd\u4f5c\u7684\uff1a\u5b83\u4e0d\u518d\u662f\u6bcf\u53f0\u673a\u5668\u53bb\u4fdd\u5b58\u4e00\u4e2a\u8fde\u7eed\u7684\u8d44\u6e90\u6bb5\uff0c\u800c\u662f\u8ba9\u6bcf\u53f0\u673a\u5668\u90fd\u4fdd\u5b58\u591a\u4e2a\u533a\u57df\u7684\u90e8\u5206\u8d44\u6e90\u6bb5\u3002\u5982\u673a\u56681\u4fdd\u5b58\u6bcf\u4e2a\u8d44\u6e90\u6bb5\u7684\u00bc\uff0c\u673a\u56682\u4fdd\u5b58\u6bcf\u4e2a\u8d44\u6e90\u6bb5\u7684\u00bc\uff0c\u673a\u56683\u30014\u540c\u6837\u5982\u6b64\u3002\u8fd9\u6837\u5373\u4f7f\u4e2a\u522b\u53f7\u6bb5\u6709\u70ed\u70b9\uff0c\u4e5f\u4f1a**\u5747\u644a**\u5230\u4e0d\u540c\u7684\u673a\u5668\u3002 NOTE: \u4e00\u3001\u4e0a\u9762\u63d0\u5230\u7684\u8fd9\u79cd\"\u5747\u644a\u6765\u907f\u514d\u70ed\u70b9\"\u7684\u505a\u6cd5\u662f\u503c\u5f97\u501f\u9274\u7684\uff0c\"amortize-to-avoid-hot-point-\u5747\u644a\u907f\u514d\u96c6\u4e2d\u70ed\u70b9\" \u4e8c\u3001\u8fd9\u5176\u5b9e\u662fCassandra\u4e2d\u7684virtual node trick\uff0cvirtual node\u662f\u975e\u5e38\u7c7b\u4f3c\u4e8eRedis\u7684hash slot\u7684 03\u4e00\u81f4\u6027hash\u5728\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528 \u5982\u4e0a\u4ecb\u7ecd\u4e86\u4e00\u81f4\u6027hash\u7684\u6982\u5ff5\u548c\u6539\u8fdb\uff0c\u5728\u7cfb\u7edf\u5b9e\u8df5\u4e2d\uff0c\u6211\u4eec\u7528\u6237\u91cf\u975e\u5e38\u5927\uff0c\u5f80\u5f80\u4e0d\u53ea\u4e00\u4e2a\u96c6\u7fa4\u3002\u6211\u4eec\u662f\u5982\u6b64\u4f7f\u7528\u4e00\u81f4\u6027hash\uff1a 1\u3001\u9996\u5148\u6839\u636e\u4e0d\u540c\u53f7\u6bb5\u9009\u62e9\u5bf9\u5e94\u7684\u96c6\u7fa4\uff0c\u8fd9\u90e8\u5206\u662f\u53ef\u914d\u7f6e\u7684 2\u3001\u786e\u5b9a\u96c6\u7fa4\u540e\uff0c\u6839\u636e\u4e00\u81f4\u6027hash\u628a\u8bbe\u5907\u5339\u914d\u5230server\u7684\u67d0\u4e2ainstance\u4e0a(\u6bcf\u53f0server\u90e8\u7f72\u591a\u4e2a\u8bbe\u5907\u63a5\u5165\u5c42\u5b9e\u4f8b\uff081.\u6bcf\u4e2ainstance\u4fdd\u5b58\u7684\u72b6\u6001\u4fe1\u606f\u66f4\u5206\u6563;2.\u670d\u52a1\u7684gc\u95ee\u9898\u4f1a\u6709\u7f13\u89e3\uff09 3\u3001\u5efa\u7acb\u673a\u5668\u865a\u62df\u8282\u70b9\uff1a\u628auser\u9006\u5e8f(\u6253\u4e71\u4e4b\u524d\u8fde\u7eed userId )\uff0c\u7ec4\u6210\u65b0\u7684\u8d44\u6e90\u6bb5;\u76f8\u5f53\u4e8e\u5efa\u7acb\u4e86server\u865a\u62df\u8282\u70b9 NOTE: virtual node 4\u3001\u8bb0\u5f55\u6bcf\u53f0server\u9501\u670d\u52a1\u7684\u8bbe\u5907\u6570\uff0c\u5982\u679c\u673a\u5668A\u6302\u4e86\uff0c\u6311\u9009\u670d\u52a1\u8bbe\u5907\u6570\u6700\u5c11\u7684\u673a\u5668\u53bb\u627f\u63a5kicked-device 04\u4e0d\u662f\u6240\u6709\u60c5\u51b5\u90fd\u9002\u5408\u4e00\u81f4\u6027hash \u4ee5\u4e0a\u4ecb\u7ecd\u4e86\u4e00\u81f4\u6027Hash\u7684\u539f\u7406\u548c\u5b9e\u8df5\uff0c\u4f46\u4e0d\u662f\u6240\u6709\u7684\u670d\u52a1\u90fd\u9002\u5408\u7528\u4e00\u81f4\u6027hash\u6765\u8def\u7531\u3002\u6bd4\u598201\u8282\u4e2d\u7684\u6d88\u606f\u63a8\u9001\u7cfb\u7edf\uff0c\u4e2d\u95f4\u5c42\u662f\u65e0\u72b6\u6001\u7684\uff0c\u5f00\u53d1\u8005\u63a5\u5165\u5c42\u8bf7\u6c42cluter-A\u7684\u54ea\u53f0\u673a\u5668\u90fd\u884c\uff0c\u5b83\u53ea\u8981\u505a\u5b8c\u57fa\u672c\u6821\u9a8c\u540e\uff0c\u628a\u6d88\u606f\u5f02\u6b65\u53d1\u7ed9MQ\u5373\u53ef\uff0c\u65e0\u9700\u7b49\u5f85\u7ed3\u679c\u76f4\u63a5\u8fd4\u56de; \u800c\u8bbe\u5907\u63a5\u5165\u5c42\u662f\u6709\u72b6\u6001\u7684\uff0c\u4e14\u5bf9\u8f83\u9ad8\u65f6\u5ef6\u65e0\u6cd5\u5fcd\u53d7\uff0c\u66f4\u9002\u5408\u4e00\u81f4\u6027Hash\u9009\u62e9\u597dserver-instance\uff0c\u7136\u540e\u901a\u8fc7TCP/UDP\u6765\u901a\u4fe1\u3002","title":"Introduction"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Consistent-hashing/cnblogs-%E4%B8%80%E8%87%B4%E6%80%A7Hash%E7%9A%84%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E8%B7%B5/#cnblogs#hash","text":"\u201c\u4e00\u81f4\u6027hash\u7684\u8bbe\u8ba1\u521d\u8877\u662f\u89e3\u51b3\u5206\u5e03\u5f0f\u7f13\u5b58\u95ee\u9898\uff0c\u5b83\u4e0d\u4ec5\u80fd\u8d77\u5230hash\u4f5c\u7528\uff0c\u8fd8\u53ef\u4ee5\u5728\u670d\u52a1\u5668\u5b95\u673a\u65f6\uff0c\u5c3d\u91cf\u5c11\u5730\u8fc1\u79fb\u6570\u636e\u3002\u56e0\u6b64\u88ab\u5e7f\u6cdb\u7528\u4e8e**\u72b6\u6001\u670d\u52a1**\u7684**\u8def\u7531\u529f\u80fd**\u201d","title":"cnblogs \u7ed9\u9762\u8bd5\u5b98\u8bb2\u660e\u767d\uff1a\u4e00\u81f4\u6027Hash\u7684\u539f\u7406\u548c\u5b9e\u8df5"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Consistent-hashing/cnblogs-%E4%B8%80%E8%87%B4%E6%80%A7Hash%E7%9A%84%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E8%B7%B5/#01","text":"\u5047\u8bbe\u6709\u4e00\u4e2a\u6d88\u606f\u63a8\u9001\u7cfb\u7edf\uff0c\u5176\u7b80\u6613\u67b6\u6784\u5982\u4e0b ) \u8bbe\u5907\u63a5\u5165\u5c42**\u4e0d\u4ec5\u8981\u63a5\u6536\u8bbe\u5907\u7684\u767b\u5f55\u3001\u4e0b\u7ebf\u7b49\u72b6\u6001\u547d\u4ee4\uff0c\u8fd8\u8981\u628a\u5f00\u53d1\u8005\u7684\u6d88\u606f\u63a8\u9001\u7ed9\u8bbe\u5907\u3002\u8fd9\u4e2a\u65f6\u5019\u8bbe\u5907\u63a5\u5165\u5c42\u5c31\u9700\u8981\u7ef4\u62a4\u8bbe\u5907\u7684**\u72b6\u6001\u4fe1\u606f \uff08\u5f53\u7136\u53ef\u4ee5\u4e13\u95e8\u62c6\u4e00\u4e2a**\u72b6\u6001\u670d\u52a1**\u53bb\u7ef4\u62a4\u8fd9\u4e9b\u4fe1\u606f\uff0c\u8981\u6c42\u8fd9\u90e8\u5206\u5fc5\u987b\u5c11\u6709\u4ee3\u7801\u66f4\u65b0\uff0c\u5177\u4f53\u539f\u56e0\u81ea\u5df1\u53bb\u60f3\u54e6=_=\uff09\u3002\u8fd9\u4e2a\u65f6\u5019\u8bbe\u5907\u63a5\u5165\u5c42\u7684\u6bcf\u53f0server\u90fd\u4fdd\u7559\u4e00\u6279\u8bbe\u5907\u7684\u72b6\u6001\u4fe1\u606fcache\uff0c\u8bbe\u5907\u5e94\u8be5\u8fde\u63a5\u54ea\u53f0server\u53bb\u83b7\u53d6\u6570\u636e\uff0c\u540c\u65f6**\u4e2d\u95f4\u5c42**\u7684\u6d88\u606f\u53c8\u8be5\u53d1\u5f80\u54ea\u4e2aserver\u53bb\u63a8\u9001\u5462\uff1f\u8fd9\u5c31\u7528\u5230\u4e86\u4e00\u81f4\u6027hash\u7b97\u6cd5\u3002","title":"01\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u8def\u7531\u7b97\u6cd5"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Consistent-hashing/cnblogs-%E4%B8%80%E8%87%B4%E6%80%A7Hash%E7%9A%84%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E8%B7%B5/#02hash","text":"\u4e00\u81f4\u6027hash\u7531\u5bf9\u8c61\u3001\u8d44\u6e90\u3001\u7b97\u6cd5\u548c\u673a\u5668\u7ec4\u6210\u3002\u5b83\u8981\u505a\u7684\u662f\uff1a\u5bf9\u8c61\u901a\u8fc7\u7b97\u6cd5\u5224\u65ad\u8fde\u54ea\u53f0\u673a\u5668\u3002\u5728\u5982\u4e0a\u7cfb\u7edf\u4e2d\uff1a\u8bbe\u5907id\uff08 userID \uff09\u4e3a\u5bf9\u8c61\uff1b\u5176\u5bf9\u5e94\u7684\u72b6\u6001\u6570\u636e(cache)\u4e3a\u8d44\u6e90\uff1b\u670d\u52a1\u5668\u4e3a\u673a\u5668\u3002 NOTE: 1\u3001\u76ee\u7684\u662f\u5c06\u8bbe\u5907\u7684\u8bf7\u6c42\u5206\u6563\u5230\u96c6\u7fa4\u4e2d\uff0c\u56e0\u6b64\u4f7f\u7528\u8bbe\u5907id\uff08 userID \uff09\u4f5c\u4e3akey\uff0c\u5c06\u5b83\u8fdb\u884c\u5212\u5206 2\u3001\u4e00\u822c\u8981\u4fdd\u6301 userID \u4e3a**monotonic**\uff0c\u8fd9\u6837\u624d\u80fd\u591f\u5c06\u5b83\u5212\u5206\u4e3a\u591a\u4e2a\u533a\u95f4 \u5728\u4e00\u81f4\u6027hash\u7b97\u6cd5\u4e2d\uff0c\u8fd9\u4e9b\u8d44\u6e90\u56f4\u6210\u4e86\u4e00\u4e2a\u95ed\u73af\uff0c\u6bcf\u53f0\u673a\u5668\u53c8\u4fdd\u5b58\u7740\u4e00\u4e2a\u8d44\u6e90\u6bb5\uff0c\u6bcf\u4e2a\u8d44\u6e90\u6bb5\u5bf9\u5e94\u4e00\u6279\u5bf9\u8c61/\u8bbe\u5907\uff1b\u8fd9\u6837\u5982\u679c\u67d0\u53f0\u673a\u5668\u6302\u4e86\uff0c\u90a3\u5b83\u5bf9\u5e94\u7684\u8d44\u6e90\u8f6c\u79fb\u5230\u79bb\u5b83\u8f83\u8fd1\u7684**\u673a\u5668x**\uff0c\u8fd9\u53f0dead server\u5bf9\u5e94\u7684\u8bbe\u5907\u8fde\u63a5\u5230\u673a\u5668x\u5c31\u884c\u3002 \u73b0\u5728\u5047\u8bbe\u8fd9\u56db\u4e2a\u8d44\u6e90\u6bb5\u5bf9\u5e94\u7684\u8bbe\u5907\uff0c\u6d3b\u8dc3\u60c5\u51b5\u76f8\u5dee\u8f83\u5927\u3002\u6bd4\u5982\u8bf4\u8d44\u6e90\u6bb51\u30012\u5bf9\u5e94\u7684\u8bbe\u5907\u7279\u522b\u6d3b\u8dc3\uff0c\u800c\u8d44\u6e90\u6bb53\u548c4\u51e0\u4e4e\u6ca1\u6d3b\u52a8\u3002\u8fd9\u6837\u673a\u56681-2\u9700\u8981\u4fdd\u5b58\u5927\u91cf\u7684\u72b6\u6001\u6570\u636e\uff0c\u800c3-4\u5219\u6709\u5927\u91cf\u7684\u7a7a\u7f6e\uff0c\u663e\u7136\u662f\u4e0d\u5408\u7406\u7684\u3002\u6539\u8fdb\u7248\u7684\u4e00\u81f4\u6027hash\u7b97\u6cd5\u662f\u8fd9\u6837\u64cd\u4f5c\u7684\uff1a\u5b83\u4e0d\u518d\u662f\u6bcf\u53f0\u673a\u5668\u53bb\u4fdd\u5b58\u4e00\u4e2a\u8fde\u7eed\u7684\u8d44\u6e90\u6bb5\uff0c\u800c\u662f\u8ba9\u6bcf\u53f0\u673a\u5668\u90fd\u4fdd\u5b58\u591a\u4e2a\u533a\u57df\u7684\u90e8\u5206\u8d44\u6e90\u6bb5\u3002\u5982\u673a\u56681\u4fdd\u5b58\u6bcf\u4e2a\u8d44\u6e90\u6bb5\u7684\u00bc\uff0c\u673a\u56682\u4fdd\u5b58\u6bcf\u4e2a\u8d44\u6e90\u6bb5\u7684\u00bc\uff0c\u673a\u56683\u30014\u540c\u6837\u5982\u6b64\u3002\u8fd9\u6837\u5373\u4f7f\u4e2a\u522b\u53f7\u6bb5\u6709\u70ed\u70b9\uff0c\u4e5f\u4f1a**\u5747\u644a**\u5230\u4e0d\u540c\u7684\u673a\u5668\u3002 NOTE: \u4e00\u3001\u4e0a\u9762\u63d0\u5230\u7684\u8fd9\u79cd\"\u5747\u644a\u6765\u907f\u514d\u70ed\u70b9\"\u7684\u505a\u6cd5\u662f\u503c\u5f97\u501f\u9274\u7684\uff0c\"amortize-to-avoid-hot-point-\u5747\u644a\u907f\u514d\u96c6\u4e2d\u70ed\u70b9\" \u4e8c\u3001\u8fd9\u5176\u5b9e\u662fCassandra\u4e2d\u7684virtual node trick\uff0cvirtual node\u662f\u975e\u5e38\u7c7b\u4f3c\u4e8eRedis\u7684hash slot\u7684","title":"02\u4ec0\u4e48\u662f\u4e00\u81f4\u6027hash\u7b97\u6cd5"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Consistent-hashing/cnblogs-%E4%B8%80%E8%87%B4%E6%80%A7Hash%E7%9A%84%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E8%B7%B5/#03hash","text":"\u5982\u4e0a\u4ecb\u7ecd\u4e86\u4e00\u81f4\u6027hash\u7684\u6982\u5ff5\u548c\u6539\u8fdb\uff0c\u5728\u7cfb\u7edf\u5b9e\u8df5\u4e2d\uff0c\u6211\u4eec\u7528\u6237\u91cf\u975e\u5e38\u5927\uff0c\u5f80\u5f80\u4e0d\u53ea\u4e00\u4e2a\u96c6\u7fa4\u3002\u6211\u4eec\u662f\u5982\u6b64\u4f7f\u7528\u4e00\u81f4\u6027hash\uff1a 1\u3001\u9996\u5148\u6839\u636e\u4e0d\u540c\u53f7\u6bb5\u9009\u62e9\u5bf9\u5e94\u7684\u96c6\u7fa4\uff0c\u8fd9\u90e8\u5206\u662f\u53ef\u914d\u7f6e\u7684 2\u3001\u786e\u5b9a\u96c6\u7fa4\u540e\uff0c\u6839\u636e\u4e00\u81f4\u6027hash\u628a\u8bbe\u5907\u5339\u914d\u5230server\u7684\u67d0\u4e2ainstance\u4e0a(\u6bcf\u53f0server\u90e8\u7f72\u591a\u4e2a\u8bbe\u5907\u63a5\u5165\u5c42\u5b9e\u4f8b\uff081.\u6bcf\u4e2ainstance\u4fdd\u5b58\u7684\u72b6\u6001\u4fe1\u606f\u66f4\u5206\u6563;2.\u670d\u52a1\u7684gc\u95ee\u9898\u4f1a\u6709\u7f13\u89e3\uff09 3\u3001\u5efa\u7acb\u673a\u5668\u865a\u62df\u8282\u70b9\uff1a\u628auser\u9006\u5e8f(\u6253\u4e71\u4e4b\u524d\u8fde\u7eed userId )\uff0c\u7ec4\u6210\u65b0\u7684\u8d44\u6e90\u6bb5;\u76f8\u5f53\u4e8e\u5efa\u7acb\u4e86server\u865a\u62df\u8282\u70b9 NOTE: virtual node 4\u3001\u8bb0\u5f55\u6bcf\u53f0server\u9501\u670d\u52a1\u7684\u8bbe\u5907\u6570\uff0c\u5982\u679c\u673a\u5668A\u6302\u4e86\uff0c\u6311\u9009\u670d\u52a1\u8bbe\u5907\u6570\u6700\u5c11\u7684\u673a\u5668\u53bb\u627f\u63a5kicked-device","title":"03\u4e00\u81f4\u6027hash\u5728\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Consistent-hashing/cnblogs-%E4%B8%80%E8%87%B4%E6%80%A7Hash%E7%9A%84%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E8%B7%B5/#04hash","text":"\u4ee5\u4e0a\u4ecb\u7ecd\u4e86\u4e00\u81f4\u6027Hash\u7684\u539f\u7406\u548c\u5b9e\u8df5\uff0c\u4f46\u4e0d\u662f\u6240\u6709\u7684\u670d\u52a1\u90fd\u9002\u5408\u7528\u4e00\u81f4\u6027hash\u6765\u8def\u7531\u3002\u6bd4\u598201\u8282\u4e2d\u7684\u6d88\u606f\u63a8\u9001\u7cfb\u7edf\uff0c\u4e2d\u95f4\u5c42\u662f\u65e0\u72b6\u6001\u7684\uff0c\u5f00\u53d1\u8005\u63a5\u5165\u5c42\u8bf7\u6c42cluter-A\u7684\u54ea\u53f0\u673a\u5668\u90fd\u884c\uff0c\u5b83\u53ea\u8981\u505a\u5b8c\u57fa\u672c\u6821\u9a8c\u540e\uff0c\u628a\u6d88\u606f\u5f02\u6b65\u53d1\u7ed9MQ\u5373\u53ef\uff0c\u65e0\u9700\u7b49\u5f85\u7ed3\u679c\u76f4\u63a5\u8fd4\u56de; \u800c\u8bbe\u5907\u63a5\u5165\u5c42\u662f\u6709\u72b6\u6001\u7684\uff0c\u4e14\u5bf9\u8f83\u9ad8\u65f6\u5ef6\u65e0\u6cd5\u5fcd\u53d7\uff0c\u66f4\u9002\u5408\u4e00\u81f4\u6027Hash\u9009\u62e9\u597dserver-instance\uff0c\u7136\u540e\u901a\u8fc7TCP/UDP\u6765\u901a\u4fe1\u3002","title":"04\u4e0d\u662f\u6240\u6709\u60c5\u51b5\u90fd\u9002\u5408\u4e00\u81f4\u6027hash"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Consistent-hashing/toptal-A-Guide-to-Consistent-Hashing/","text":"toptal A Guide to Consistent Hashing In recent years, with the advent of concepts such as cloud computing and big data, distributed systems have gained popularity and relevance. One such type of system, distributed caches that power many high-traffic dynamic websites and web applications, typically consist of a particular case of distributed hashing . These take advantage of an algorithm known as consistent hashing . What is consistent hashing? What\u2019s the motivation behind it, and why should you care? In this article, I\u2019ll first review the general concept of hashing and its purpose, followed by a description of distributed hashing and the problems it entails. In turn, that will lead us to our title subject. What Is Hashing? What is \u201chashing\u201d all about? Merriam-Webster defines the noun hash as \u201cchopped meat mixed with potatoes and browned,\u201d and the verb as \u201cto chop (as meat and potatoes) into small pieces.\u201d So, culinary details aside, hash roughly means \u201cchop and mix\u201d\u2014and that\u2019s precisely where the technical term comes from. NOTE: \u54c8\u5e0c\u662f\u4ec0\u4e48\u610f\u601d?\u300a\u97e6\u6c0f\u8bcd\u5178\u300b\u5c06\u540d\u8bcd\u201chash\u201d\u5b9a\u4e49\u4e3a\u201c\u5c06\u788e\u8089\u4e0e\u571f\u8c46\u6df7\u5408\u5e76\u714e\u6210\u8910\u8272\u201d\uff0c\u52a8\u8bcd\u201c\u5c06(\u8089\u548c\u571f\u8c46)\u5207\u6210\u5c0f\u5757\u201d\u3002\u6240\u4ee5\uff0c\u6487\u5f00\u70f9\u996a\u7ec6\u8282\u4e0d\u8c08\uff0chash\u5927\u81f4\u7684\u610f\u601d\u662f\u201c\u5207\u788e\u548c\u6df7\u5408\u201d\u2014\u2014\u8fd9\u6b63\u662f\u8fd9\u4e2a\u6280\u672f\u672f\u8bed\u7684\u6765\u6e90\u3002 A hash function is a function that maps one piece of data\u2014typically describing some kind of object, often of arbitrary size\u2014to another piece of data, typically an integer, known as hash code , or simply hash . For instance, some hash function designed to hash strings, with an output range of 0 .. 100 , may map the string Hello to, say, the number 57 , Hasta la vista, baby to the number 33 , and any other possible string to some number within that range. Since there are way more possible inputs than outputs, any given number will have many different strings mapped to it, a phenomenon known as collision . Good hash functions should somehow \u201cchop and mix\u201d (hence the term) the input data in such a way that the outputs for different input values are spread as evenly as possible over the output range. Hash functions have many uses and for each one, different properties may be desired. There is a type of hash function known as cryptographic hash functions , which must meet a restrictive set of properties and are used for security purposes\u2014including applications such as password protection, integrity(\u5b8c\u6574\u6027) checking and fingerprinting of messages, and data corruption detection, among others, but those are outside the scope of this article. Non-cryptographic hash functions have several uses as well, the most common being their use in hash tables , which is the one that concerns us and which we\u2019ll explore in more detail. Introducing Hash Tables (Hash Maps) Imagine we needed to keep a list of all the members of some club while being able to search for any specific member. We could handle it by keeping the list in an array (or linked list) and, to perform a search, iterate the elements until we find the desired one (we might be searching based on their name, for instance). In the worst case, that would mean checking all members (if the one we\u2019re searching for is last, or not present at all), or half of them on average. In complexity theory terms, the search would then have complexity O(n) , and it would be reasonably fast for a small list, but it would get slower and slower in direct proportion to the number of members. How could that be improved? Let\u2019s suppose all these club members had a member ID , which happened to be a sequential number reflecting the order in which they joined the club. Assuming that searching by ID were acceptable, we could place all members in an array, with their indexes matching their ID s (for example, a member with ID=10 would be at the index 10 in the array). This would allow us to access each member directly, with no search at all. That would be very efficient, in fact, as efficient as it can possibly be, corresponding to the lowest complexity possible, O(1) , also known as constant time . But, admittedly, our club member ID scenario is somewhat contrived. What if ID s were big, non-sequential or random numbers? Or, if searching by ID were not acceptable, and we needed to search by name (or some other field) instead? It would certainly be useful to keep our fast direct access (or something close) while at the same time being able to handle arbitrary datasets and less restrictive search criteria. Here\u2019s where hash functions come to the rescue. A suitable hash function can be used to map an arbitrary piece of data to an integer, which will play a similar role to that of our club member ID , albeit with a few important differences. First, a good hash function generally has a wide output range (typically, the whole range of a 32 or 64-bit integer), so building an array for all possible indices would be either impractical or plain impossible, and a colossal waste of memory. To overcome that, we can have a reasonably sized array (say, just twice the number of elements we expect to store) and perform a modulo operation on the hash to get the array index. So, the index would be index = hash(object) mod N , where N is the size of the array. Second, object hashes will not be unique (unless we\u2019re working with a fixed dataset and a custom-built perfect hash function , but we won\u2019t discuss that here). There will be collisions (further increased by the modulo operation), and therefore a simple direct index access won\u2019t work. There are several ways to handle this, but a typical one is to attach a list, commonly known as a bucket , to each array index to hold all the objects sharing a given index. So, we have an array of size N , with each entry pointing to an object bucket. To add a new object, we need to calculate its hash modulo N , and check the bucket at the resulting index, adding the object if it\u2019s not already there. To search for an object, we do the same, just looking into the bucket to check if the object is there. Such a structure is called a hash table , and although the searches within buckets are linear, a properly sized hash table should have a reasonably small number of objects per bucket, resulting in almost constant time access (an average complexity of O(N/k) , where k is the number of buckets). With complex objects, the hash function is typically not applied to the whole object, but to a key*instead. In our club member example, each object might contain several fields (like name, age, address, email, phone), but we could pick, say, the email to act as the key so that the hash function would be applied to the email only. In fact, the key need not be part of the object; it is common to store key/value pairs, where the key is usually a relatively short string, and the value can be an arbitrary piece of data. In such cases, the hash table or hash map is used as a *dictionary , and that\u2019s the way some high-level languages implement objects or associative arrays. Scaling Out: Distributed Hashing Now that we have discussed hashing, we\u2019re ready to look into distributed hashing . In some situations, it may be necessary or desirable to split a hash table into several parts, hosted by different servers. One of the main motivations for this is to bypass the memory limitations of using a single computer, allowing for the construction of arbitrarily large hash tables (given enough servers). NOTE: 1\u3001Redis cluster\u5c31\u662f\u8fd9\u6837\u505a\u7684 In such a scenario, the objects (and their keys) are distributed among several servers, hence the name. A typical use case for this is the implementation of in-memory caches, such as Memcached . Such setups consist of a pool of caching servers that host many key/value pairs and are used to provide fast access to data originally stored (or computed) elsewhere. For example, to reduce the load on a database server and at the same time improve performance, an application can be designed to first fetch data from the cache servers, and only if it\u2019s not present there\u2014a situation known as cache miss \u2014resort to the database, running the relevant query and caching the results with an appropriate key, so that it can be found next time it\u2019s needed. Now, how does distribution take place? What criteria are used to determine which keys to host in which servers? The simplest way is to take the hash modulo of the number of servers. That is, server = hash(key) mod N , where N is the size of the pool. To store or retrieve a key, the client first computes the hash, applies a modulo N operation, and uses the resulting index to contact the appropriate server (probably by using a lookup table of IP addresses). Note that the hash function used for key distribution must be the same one across all clients, but it need not be the same one used internally by the caching servers. NOTE: 1\u3001\u5176\u5b9e\u5206\u5e03\u5f0fhash table\u4e2d\uff0c\u6bcf\u4e2anode\u5c31\u662f\u4e00\u4e2aslot\uff0c\u8fd9\u5c31\u662f\"key distribution\" Let\u2019s see an example. Say we have three servers, A , B and C , and we have some string keys with their hashes: KEY HASH HASH mod 3 \"john\" 1633428562 2 \"bill\" 7594634739 0 \"jane\" 5000799124 1 \"steve\" 9787173343 0 \"kate\" 3421657995 2 A client wants to retrieve the value for key john . Its hash modulo 3 is 2 , so it must contact server C . The key is not found there, so the client fetches the data from the source and adds it. The pool looks like this: A B C \"john\" Next another client (or the same one) wants to retrieve the value for key bill . Its hash modulo 3 is 0 , so it must contact server A . The key is not found there, so the client fetches the data from the source and adds it. The pool looks like this now: A B C \"bill\" \"john\" After the remaining keys are added, the pool looks like this: A B C \"bill\" \"jane\" \"john\" \"steve\" \"kate\" The Rehashing Problem This distribution scheme is simple, intuitive, and works fine. That is, until the number of servers changes. What happens if one of the servers crashes or becomes unavailable? Keys need to be redistributed to account for the missing server\uff08key\u9700\u8981\u91cd\u65b0\u5206\u53d1\uff0c\u4ee5\u5f25\u8865\u4e22\u5931\u7684\u670d\u52a1\u5668\uff09, of course. The same applies if one or more new servers are added to the pool;keys need to be redistributed to include the new servers. This is true for any distribution scheme, but the problem with our simple modulo distribution is that when the number of servers changes, most hashes modulo N will change, so most keys will need to be moved to a different server. So, even if a single server is removed or added, all keys will likely need to be rehashed into a different server. NOTE: \u4e0a\u8ff0\u5bf9\u95ee\u9898\u7684\u603b\u7ed3\u975e\u5e38\u7684\u597d\uff1b From our previous example, if we removed server C , we\u2019d have to rehash all the keys using hash modulo 2 instead of hash modulo 3 , and the new locations for the keys would become: KEY HASH HASH mod 2 \"john\" 1633428562 0 \"bill\" 7594634739 1 \"jane\" 5000799124 0 \"steve\" 9787173343 1 \"kate\" 3421657995 1 A B \"john\" \"bill\" \"jane\" \"steve\" \"kate\" Note that all key locations changed, not only the ones from server C . In the typical use case we mentioned before (caching), this would mean that, all of a sudden, the keys won\u2019t be found because they won\u2019t yet be present at their new location. So, most queries will result in misses, and the original data will likely need retrieving again from the source to be rehashed, thus placing a heavy load on the origin server(s) (typically a database). This may very well degrade performance severely and possibly crash the origin servers. The Solution: Consistent Hashing So, how can this problem be solved? We need a distribution scheme that does not depend directly on the number of servers , so that, when adding or removing servers, the number of keys that need to be relocated is minimized. One such scheme\u2014a clever, yet surprisingly simple one\u2014is called consistent hashing , and was first described by Karger et al. at MIT in an academic paper from 1997 (according to Wikipedia). Consistent Hashing is a distributed hashing scheme that operates independently of the number of servers or objects in a distributed hash table by assigning them a position on an abstract circle , or hash ring . This allows servers and objects to scale without affecting the overall system. Imagine we mapped the hash output range onto the edge of a circle\uff08\u5c06hash\u8f93\u51fa\u503c\u7684\u503c\u57df\u6620\u5c04\u5230circle\u4e0a\uff09. That means that the minimum possible hash value , zero, would correspond to an angle of zero , the maximum possible value (some big integer we\u2019ll call INT_MAX ) would correspond to an angle of 2\ud835\udf45 radians (or 360 degrees), and all other hash values would linearly fit somewhere in between. So, we could take a key, compute its hash, and find out where it lies on the circle\u2019s edge. Assuming an INT_MAX of 1010 (for example\u2019s sake), the keys from our previous example would look like this: KEY HASH ANGLE (DEG) \"john\" 1633428562 58.8 \"bill\" 7594634739 273.4 \"jane\" 5000799124 180 \"steve\" 9787173343 352.3 \"kate\" 3421657995 123.2 Now imagine we also placed the servers on the edge of the circle , by pseudo-randomly assigning them angles too. This should be done in a repeatable\uff08\u53ef\u91cd\u590d\u7684\uff09 way (or at least in such a way that all clients agree on the servers\u2019 angles). A convenient way of doing this is by hashing the server name (or IP address, or some ID)\u2014as we\u2019d do with any other key\u2014to come up with its angle. In our example, things might look like this: KEY HASH ANGLE (DEG) \"john\" 1633428562 58.8 \"bill\" 7594634739 273.4 \"jane\" 5000799124 180 \"steve\" 9787173343 352.3 \"kate\" 3421657995 123.2 \"A\" 5572014558 200.6 \"B\" 8077113362 290.8 \"C\" 2269549488 81.7 Since we have the keys for both the objects and the servers on the same circle , we may define a simple rule to associate the former with the latter: Each object key will belong in the server whose key is closest , in a counterclockwise direction (or clockwise, depending on the conventions used). In other words, to find out which server to ask for a given key, we need to locate the key on the circle and move in the ascending angle direction until we find a server. In our example: KEY HASH ANGLE (DEG) \"john\" 1633428562 58.7 \"C\" 2269549488 81.7 \"kate\" 3421657995 123.1 \"jane\" 5000799124 180 \"A\" 5572014557 200.5 \"bill\" 7594634739 273.4 \"B\" 8077113361 290.7 \"steve\" 787173343 352.3 KEY HASH ANGLE (DEG) LABEL SERVER \"john\" 1632929716 58.7 \"C\" C \"kate\" 3421831276 123.1 \"A\" A \"jane\" 5000648311 180 \"A\" A \"bill\" 7594873884 273.4 \"B\" B \"steve\" 9786437450 352.3 \"C\" C From a programming perspective, what we would do is keep a sorted list of server values (which could be angles or numbers in any real interval), and walk this list (or use a binary search) to find the first server with a value greater than, or equal to, that of the desired key. If no such value is found, we need to wrap around, taking the first one from the list. To ensure object keys are evenly distributed among servers, we need to apply a simple trick: To assign not one, but many labels (angles) to each server . So instead of having labels A , B and C , we could have, say, A0 .. A9 , B0 .. B9 and C0 .. C9 , all interspersed along the circle. The factor by which to increase the number of labels (server keys), known as weight , depends on the situation (and may even be different for each server) to adjust the probability of keys ending up on each. For example, if server B were twice as powerful as the rest, it could be assigned twice as many labels, and as a result, it would end up holding twice as many objects (on average). For our example we\u2019ll assume all three servers have an equal weight of 10 (this works well for three servers, for 10 to 50 servers, a weight in the range 100 to 500 would work better, and bigger pools may need even higher weights): KEY HASH ANGLE (DEG) \"C6\" 408965526 14.7 \"A1\" 473914830 17 \"A2\" 548798874 19.7 \"A3\" 1466730567 52.8 \"C4\" 1493080938 53.7 \"john\" 1633428562 58.7 \"B2\" 1808009038 65 \"C0\" 1982701318 71.3 \"B3\" 2058758486 74.1 \"A7\" 2162578920 77.8 \"B4\" 2660265921 95.7 \"C9\" 3359725419 120.9 \"kate\" 3421657995 123.1 \"A5\" 3434972143 123.6 \"C1\" 3672205973 132.1 \"C8\" 3750588567 135 \"B0\" 4049028775 145.7 \"B8\" 4755525684 171.1 \"A9\" 4769549830 171.7 \"jane\" 5000799124 180 \"C7\" 5014097839 180.5 \"B1\" 5444659173 196 \"A6\" 6210502707 223.5 \"A0\" 6511384141 234.4 \"B9\" 7292819872 262.5 \"C3\" 7330467663 263.8 \"C5\" 7502566333 270 \"bill\" 7594634739 273.4 \"A4\" 8047401090 289.7 \"C2\" 8605012288 309.7 \"A8\" 8997397092 323.9 \"B7\" 9038880553 325.3 \"B5\" 9368225254 337.2 \"B6\" 9379713761 337.6 \"steve\" 9787173343 352.3 KEY HASH ANGLE (DEG) LABEL SERVER \"john\" 1632929716 58.7 \"B2\" B \"kate\" 3421831276 123.1 \"A5\" A \"jane\" 5000648311 180 \"C7\" C \"bill\" 7594873884 273.4 \"A4\" A \"steve\" 9786437450 352.3 \"C6\" C So, what\u2019s the benefit of all this circle approach? Imagine server C is removed. To account for\uff08\u5f25\u8865\uff09 this, we must remove labels C0 .. C9 from the circle. This results in the object keys formerly adjacent to the deleted labels now being randomly labeled Ax and Bx , reassigning them to servers A and B . But what happens with the other object keys, the ones that originally belonged in A and B ? Nothing! That\u2019s the beauty of it: The absence of Cx labels does not affect those keys in any way. So, removing a server results in its object keys being randomly reassigned to the rest of the servers, leaving all other keys untouched : KEY HASH ANGLE (DEG) \"A1\" 473914830 17 \"A2\" 548798874 19.7 \"A3\" 1466730567 52.8 \"john\" 1633428562 58.7 \"B2\" 1808009038 65 \"B3\" 2058758486 74.1 \"A7\" 2162578920 77.8 \"B4\" 2660265921 95.7 \"kate\" 3421657995 123.1 \"A5\" 3434972143 123.6 \"B0\" 4049028775 145.7 \"B8\" 4755525684 171.1 \"A9\" 4769549830 171.7 \"jane\" 5000799124 180 \"B1\" 5444659173 196 \"A6\" 6210502707 223.5 \"A0\" 6511384141 234.4 \"B9\" 7292819872 262.5 \"bill\" 7594634739 273.4 \"A4\" 8047401090 289.7 \"A8\" 8997397092 323.9 \"B7\" 9038880553 325.3 \"B5\" 9368225254 337.2 \"B6\" 9379713761 337.6 \"steve\" 9787173343 352.3 KEY HASH ANGLE (DEG) LABEL SERVER \"john\" 1632929716 58.7 \"B2\" B \"kate\" 3421831276 123.1 \"A5\" A \"jane\" 5000648311 180 \"B1\" B \"bill\" 7594873884 273.4 \"A4\" A \"steve\" 9786437450 352.3 \"A1\" A Something similar happens if, instead of removing a server, we add one. If we wanted to add server D to our example (say, as a replacement for C ), we would need to add labels D0 .. D9 . The result would be that roughly one-third of the existing keys (all belonging to A or B ) would be reassigned to D , and, again, the rest would stay the same: KEY HASH ANGLE (DEG) \"D2\" 439890723 15.8 \"A1\" 473914830 17 \"A2\" 548798874 19.7 \"D8\" 796709216 28.6 \"D1\" 1008580939 36.3 \"A3\" 1466730567 52.8 \"D5\" 1587548309 57.1 \"john\" 1633428562 58.7 \"B2\" 1808009038 65 \"B3\" 2058758486 74.1 \"A7\" 2162578920 77.8 \"B4\" 2660265921 95.7 \"D4\" 2909395217 104.7 \"kate\" 3421657995 123.1 \"A5\" 3434972143 123.6 \"D7\" 3567129743 128.4 \"B0\" 4049028775 145.7 \"B8\" 4755525684 171.1 \"A9\" 4769549830 171.7 \"jane\" 5000799124 180 \"B1\" 5444659173 196 \"D6\" 5703092354 205.3 \"A6\" 6210502707 223.5 \"A0\" 6511384141 234.4 \"B9\" 7292819872 262.5 \"bill\" 7594634739 273.4 \"A4\" 8047401090 289.7 \"D0\" 8272587142 297.8 \"A8\" 8997397092 323.9 \"B7\" 9038880553 325.3 \"D3\" 9048608874 325.7 \"D9\" 9314459653 335.3 \"B5\" 9368225254 337.2 \"B6\" 9379713761 337.6 \"steve\" 9787173343 352.3 KEY HASH ANGLE (DEG) LABEL SERVER \"john\" 1632929716 58.7 \"B2\" B \"kate\" 3421831276 123.1 \"A5\" A \"jane\" 5000648311 180 \"B1\" B \"bill\" 7594873884 273.4 \"A4\" A \"steve\" 9786437450 352.3 \"D2\" D This is how consistent hashing solves the rehashing problem. In general, only k/N keys need to be remapped when k is the number of keys and N is the number of servers (more specifically, the maximum of the initial and final number of servers). What Next? We observed that when using distributed caching to optimize performance, it may happen that the number of caching servers changes (reasons for this may be a server crashing, or the need to add or remove a server to increase or decrease overall capacity). By using consistent hashing to distribute keys between the servers, we can rest assured that should that happen, the number of keys being rehashed\u2014and therefore, the impact on origin servers\u2014will be minimized, preventing potential downtime or performance issues. There are clients for several systems, such as Memcached and Redis, that include support for consistent hashing out of the box. Alternatively, you can implement the algorithm yourself, in your language of choice, and that should be relatively easy once the concept is understood. If data science interests you, Toptal has some of the best articles on the subject at the blog","title":"Introduction"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Consistent-hashing/toptal-A-Guide-to-Consistent-Hashing/#toptal#a#guide#to#consistent#hashing","text":"In recent years, with the advent of concepts such as cloud computing and big data, distributed systems have gained popularity and relevance. One such type of system, distributed caches that power many high-traffic dynamic websites and web applications, typically consist of a particular case of distributed hashing . These take advantage of an algorithm known as consistent hashing . What is consistent hashing? What\u2019s the motivation behind it, and why should you care? In this article, I\u2019ll first review the general concept of hashing and its purpose, followed by a description of distributed hashing and the problems it entails. In turn, that will lead us to our title subject.","title":"toptal A Guide to Consistent Hashing"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Consistent-hashing/toptal-A-Guide-to-Consistent-Hashing/#what#is#hashing","text":"What is \u201chashing\u201d all about? Merriam-Webster defines the noun hash as \u201cchopped meat mixed with potatoes and browned,\u201d and the verb as \u201cto chop (as meat and potatoes) into small pieces.\u201d So, culinary details aside, hash roughly means \u201cchop and mix\u201d\u2014and that\u2019s precisely where the technical term comes from. NOTE: \u54c8\u5e0c\u662f\u4ec0\u4e48\u610f\u601d?\u300a\u97e6\u6c0f\u8bcd\u5178\u300b\u5c06\u540d\u8bcd\u201chash\u201d\u5b9a\u4e49\u4e3a\u201c\u5c06\u788e\u8089\u4e0e\u571f\u8c46\u6df7\u5408\u5e76\u714e\u6210\u8910\u8272\u201d\uff0c\u52a8\u8bcd\u201c\u5c06(\u8089\u548c\u571f\u8c46)\u5207\u6210\u5c0f\u5757\u201d\u3002\u6240\u4ee5\uff0c\u6487\u5f00\u70f9\u996a\u7ec6\u8282\u4e0d\u8c08\uff0chash\u5927\u81f4\u7684\u610f\u601d\u662f\u201c\u5207\u788e\u548c\u6df7\u5408\u201d\u2014\u2014\u8fd9\u6b63\u662f\u8fd9\u4e2a\u6280\u672f\u672f\u8bed\u7684\u6765\u6e90\u3002 A hash function is a function that maps one piece of data\u2014typically describing some kind of object, often of arbitrary size\u2014to another piece of data, typically an integer, known as hash code , or simply hash . For instance, some hash function designed to hash strings, with an output range of 0 .. 100 , may map the string Hello to, say, the number 57 , Hasta la vista, baby to the number 33 , and any other possible string to some number within that range. Since there are way more possible inputs than outputs, any given number will have many different strings mapped to it, a phenomenon known as collision . Good hash functions should somehow \u201cchop and mix\u201d (hence the term) the input data in such a way that the outputs for different input values are spread as evenly as possible over the output range. Hash functions have many uses and for each one, different properties may be desired. There is a type of hash function known as cryptographic hash functions , which must meet a restrictive set of properties and are used for security purposes\u2014including applications such as password protection, integrity(\u5b8c\u6574\u6027) checking and fingerprinting of messages, and data corruption detection, among others, but those are outside the scope of this article. Non-cryptographic hash functions have several uses as well, the most common being their use in hash tables , which is the one that concerns us and which we\u2019ll explore in more detail.","title":"What Is Hashing?"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Consistent-hashing/toptal-A-Guide-to-Consistent-Hashing/#introducing#hash#tables#hash#maps","text":"Imagine we needed to keep a list of all the members of some club while being able to search for any specific member. We could handle it by keeping the list in an array (or linked list) and, to perform a search, iterate the elements until we find the desired one (we might be searching based on their name, for instance). In the worst case, that would mean checking all members (if the one we\u2019re searching for is last, or not present at all), or half of them on average. In complexity theory terms, the search would then have complexity O(n) , and it would be reasonably fast for a small list, but it would get slower and slower in direct proportion to the number of members. How could that be improved? Let\u2019s suppose all these club members had a member ID , which happened to be a sequential number reflecting the order in which they joined the club. Assuming that searching by ID were acceptable, we could place all members in an array, with their indexes matching their ID s (for example, a member with ID=10 would be at the index 10 in the array). This would allow us to access each member directly, with no search at all. That would be very efficient, in fact, as efficient as it can possibly be, corresponding to the lowest complexity possible, O(1) , also known as constant time . But, admittedly, our club member ID scenario is somewhat contrived. What if ID s were big, non-sequential or random numbers? Or, if searching by ID were not acceptable, and we needed to search by name (or some other field) instead? It would certainly be useful to keep our fast direct access (or something close) while at the same time being able to handle arbitrary datasets and less restrictive search criteria. Here\u2019s where hash functions come to the rescue. A suitable hash function can be used to map an arbitrary piece of data to an integer, which will play a similar role to that of our club member ID , albeit with a few important differences. First, a good hash function generally has a wide output range (typically, the whole range of a 32 or 64-bit integer), so building an array for all possible indices would be either impractical or plain impossible, and a colossal waste of memory. To overcome that, we can have a reasonably sized array (say, just twice the number of elements we expect to store) and perform a modulo operation on the hash to get the array index. So, the index would be index = hash(object) mod N , where N is the size of the array. Second, object hashes will not be unique (unless we\u2019re working with a fixed dataset and a custom-built perfect hash function , but we won\u2019t discuss that here). There will be collisions (further increased by the modulo operation), and therefore a simple direct index access won\u2019t work. There are several ways to handle this, but a typical one is to attach a list, commonly known as a bucket , to each array index to hold all the objects sharing a given index. So, we have an array of size N , with each entry pointing to an object bucket. To add a new object, we need to calculate its hash modulo N , and check the bucket at the resulting index, adding the object if it\u2019s not already there. To search for an object, we do the same, just looking into the bucket to check if the object is there. Such a structure is called a hash table , and although the searches within buckets are linear, a properly sized hash table should have a reasonably small number of objects per bucket, resulting in almost constant time access (an average complexity of O(N/k) , where k is the number of buckets). With complex objects, the hash function is typically not applied to the whole object, but to a key*instead. In our club member example, each object might contain several fields (like name, age, address, email, phone), but we could pick, say, the email to act as the key so that the hash function would be applied to the email only. In fact, the key need not be part of the object; it is common to store key/value pairs, where the key is usually a relatively short string, and the value can be an arbitrary piece of data. In such cases, the hash table or hash map is used as a *dictionary , and that\u2019s the way some high-level languages implement objects or associative arrays.","title":"Introducing Hash Tables (Hash Maps)"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Consistent-hashing/toptal-A-Guide-to-Consistent-Hashing/#scaling#out#distributed#hashing","text":"Now that we have discussed hashing, we\u2019re ready to look into distributed hashing . In some situations, it may be necessary or desirable to split a hash table into several parts, hosted by different servers. One of the main motivations for this is to bypass the memory limitations of using a single computer, allowing for the construction of arbitrarily large hash tables (given enough servers). NOTE: 1\u3001Redis cluster\u5c31\u662f\u8fd9\u6837\u505a\u7684 In such a scenario, the objects (and their keys) are distributed among several servers, hence the name. A typical use case for this is the implementation of in-memory caches, such as Memcached . Such setups consist of a pool of caching servers that host many key/value pairs and are used to provide fast access to data originally stored (or computed) elsewhere. For example, to reduce the load on a database server and at the same time improve performance, an application can be designed to first fetch data from the cache servers, and only if it\u2019s not present there\u2014a situation known as cache miss \u2014resort to the database, running the relevant query and caching the results with an appropriate key, so that it can be found next time it\u2019s needed. Now, how does distribution take place? What criteria are used to determine which keys to host in which servers? The simplest way is to take the hash modulo of the number of servers. That is, server = hash(key) mod N , where N is the size of the pool. To store or retrieve a key, the client first computes the hash, applies a modulo N operation, and uses the resulting index to contact the appropriate server (probably by using a lookup table of IP addresses). Note that the hash function used for key distribution must be the same one across all clients, but it need not be the same one used internally by the caching servers. NOTE: 1\u3001\u5176\u5b9e\u5206\u5e03\u5f0fhash table\u4e2d\uff0c\u6bcf\u4e2anode\u5c31\u662f\u4e00\u4e2aslot\uff0c\u8fd9\u5c31\u662f\"key distribution\" Let\u2019s see an example. Say we have three servers, A , B and C , and we have some string keys with their hashes: KEY HASH HASH mod 3 \"john\" 1633428562 2 \"bill\" 7594634739 0 \"jane\" 5000799124 1 \"steve\" 9787173343 0 \"kate\" 3421657995 2 A client wants to retrieve the value for key john . Its hash modulo 3 is 2 , so it must contact server C . The key is not found there, so the client fetches the data from the source and adds it. The pool looks like this: A B C \"john\" Next another client (or the same one) wants to retrieve the value for key bill . Its hash modulo 3 is 0 , so it must contact server A . The key is not found there, so the client fetches the data from the source and adds it. The pool looks like this now: A B C \"bill\" \"john\" After the remaining keys are added, the pool looks like this: A B C \"bill\" \"jane\" \"john\" \"steve\" \"kate\"","title":"Scaling Out: Distributed Hashing"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Consistent-hashing/toptal-A-Guide-to-Consistent-Hashing/#the#rehashing#problem","text":"This distribution scheme is simple, intuitive, and works fine. That is, until the number of servers changes. What happens if one of the servers crashes or becomes unavailable? Keys need to be redistributed to account for the missing server\uff08key\u9700\u8981\u91cd\u65b0\u5206\u53d1\uff0c\u4ee5\u5f25\u8865\u4e22\u5931\u7684\u670d\u52a1\u5668\uff09, of course. The same applies if one or more new servers are added to the pool;keys need to be redistributed to include the new servers. This is true for any distribution scheme, but the problem with our simple modulo distribution is that when the number of servers changes, most hashes modulo N will change, so most keys will need to be moved to a different server. So, even if a single server is removed or added, all keys will likely need to be rehashed into a different server. NOTE: \u4e0a\u8ff0\u5bf9\u95ee\u9898\u7684\u603b\u7ed3\u975e\u5e38\u7684\u597d\uff1b From our previous example, if we removed server C , we\u2019d have to rehash all the keys using hash modulo 2 instead of hash modulo 3 , and the new locations for the keys would become: KEY HASH HASH mod 2 \"john\" 1633428562 0 \"bill\" 7594634739 1 \"jane\" 5000799124 0 \"steve\" 9787173343 1 \"kate\" 3421657995 1 A B \"john\" \"bill\" \"jane\" \"steve\" \"kate\" Note that all key locations changed, not only the ones from server C . In the typical use case we mentioned before (caching), this would mean that, all of a sudden, the keys won\u2019t be found because they won\u2019t yet be present at their new location. So, most queries will result in misses, and the original data will likely need retrieving again from the source to be rehashed, thus placing a heavy load on the origin server(s) (typically a database). This may very well degrade performance severely and possibly crash the origin servers.","title":"The Rehashing Problem"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Consistent-hashing/toptal-A-Guide-to-Consistent-Hashing/#the#solution#consistent#hashing","text":"So, how can this problem be solved? We need a distribution scheme that does not depend directly on the number of servers , so that, when adding or removing servers, the number of keys that need to be relocated is minimized. One such scheme\u2014a clever, yet surprisingly simple one\u2014is called consistent hashing , and was first described by Karger et al. at MIT in an academic paper from 1997 (according to Wikipedia). Consistent Hashing is a distributed hashing scheme that operates independently of the number of servers or objects in a distributed hash table by assigning them a position on an abstract circle , or hash ring . This allows servers and objects to scale without affecting the overall system. Imagine we mapped the hash output range onto the edge of a circle\uff08\u5c06hash\u8f93\u51fa\u503c\u7684\u503c\u57df\u6620\u5c04\u5230circle\u4e0a\uff09. That means that the minimum possible hash value , zero, would correspond to an angle of zero , the maximum possible value (some big integer we\u2019ll call INT_MAX ) would correspond to an angle of 2\ud835\udf45 radians (or 360 degrees), and all other hash values would linearly fit somewhere in between. So, we could take a key, compute its hash, and find out where it lies on the circle\u2019s edge. Assuming an INT_MAX of 1010 (for example\u2019s sake), the keys from our previous example would look like this: KEY HASH ANGLE (DEG) \"john\" 1633428562 58.8 \"bill\" 7594634739 273.4 \"jane\" 5000799124 180 \"steve\" 9787173343 352.3 \"kate\" 3421657995 123.2 Now imagine we also placed the servers on the edge of the circle , by pseudo-randomly assigning them angles too. This should be done in a repeatable\uff08\u53ef\u91cd\u590d\u7684\uff09 way (or at least in such a way that all clients agree on the servers\u2019 angles). A convenient way of doing this is by hashing the server name (or IP address, or some ID)\u2014as we\u2019d do with any other key\u2014to come up with its angle. In our example, things might look like this: KEY HASH ANGLE (DEG) \"john\" 1633428562 58.8 \"bill\" 7594634739 273.4 \"jane\" 5000799124 180 \"steve\" 9787173343 352.3 \"kate\" 3421657995 123.2 \"A\" 5572014558 200.6 \"B\" 8077113362 290.8 \"C\" 2269549488 81.7 Since we have the keys for both the objects and the servers on the same circle , we may define a simple rule to associate the former with the latter: Each object key will belong in the server whose key is closest , in a counterclockwise direction (or clockwise, depending on the conventions used). In other words, to find out which server to ask for a given key, we need to locate the key on the circle and move in the ascending angle direction until we find a server. In our example: KEY HASH ANGLE (DEG) \"john\" 1633428562 58.7 \"C\" 2269549488 81.7 \"kate\" 3421657995 123.1 \"jane\" 5000799124 180 \"A\" 5572014557 200.5 \"bill\" 7594634739 273.4 \"B\" 8077113361 290.7 \"steve\" 787173343 352.3 KEY HASH ANGLE (DEG) LABEL SERVER \"john\" 1632929716 58.7 \"C\" C \"kate\" 3421831276 123.1 \"A\" A \"jane\" 5000648311 180 \"A\" A \"bill\" 7594873884 273.4 \"B\" B \"steve\" 9786437450 352.3 \"C\" C From a programming perspective, what we would do is keep a sorted list of server values (which could be angles or numbers in any real interval), and walk this list (or use a binary search) to find the first server with a value greater than, or equal to, that of the desired key. If no such value is found, we need to wrap around, taking the first one from the list. To ensure object keys are evenly distributed among servers, we need to apply a simple trick: To assign not one, but many labels (angles) to each server . So instead of having labels A , B and C , we could have, say, A0 .. A9 , B0 .. B9 and C0 .. C9 , all interspersed along the circle. The factor by which to increase the number of labels (server keys), known as weight , depends on the situation (and may even be different for each server) to adjust the probability of keys ending up on each. For example, if server B were twice as powerful as the rest, it could be assigned twice as many labels, and as a result, it would end up holding twice as many objects (on average). For our example we\u2019ll assume all three servers have an equal weight of 10 (this works well for three servers, for 10 to 50 servers, a weight in the range 100 to 500 would work better, and bigger pools may need even higher weights): KEY HASH ANGLE (DEG) \"C6\" 408965526 14.7 \"A1\" 473914830 17 \"A2\" 548798874 19.7 \"A3\" 1466730567 52.8 \"C4\" 1493080938 53.7 \"john\" 1633428562 58.7 \"B2\" 1808009038 65 \"C0\" 1982701318 71.3 \"B3\" 2058758486 74.1 \"A7\" 2162578920 77.8 \"B4\" 2660265921 95.7 \"C9\" 3359725419 120.9 \"kate\" 3421657995 123.1 \"A5\" 3434972143 123.6 \"C1\" 3672205973 132.1 \"C8\" 3750588567 135 \"B0\" 4049028775 145.7 \"B8\" 4755525684 171.1 \"A9\" 4769549830 171.7 \"jane\" 5000799124 180 \"C7\" 5014097839 180.5 \"B1\" 5444659173 196 \"A6\" 6210502707 223.5 \"A0\" 6511384141 234.4 \"B9\" 7292819872 262.5 \"C3\" 7330467663 263.8 \"C5\" 7502566333 270 \"bill\" 7594634739 273.4 \"A4\" 8047401090 289.7 \"C2\" 8605012288 309.7 \"A8\" 8997397092 323.9 \"B7\" 9038880553 325.3 \"B5\" 9368225254 337.2 \"B6\" 9379713761 337.6 \"steve\" 9787173343 352.3 KEY HASH ANGLE (DEG) LABEL SERVER \"john\" 1632929716 58.7 \"B2\" B \"kate\" 3421831276 123.1 \"A5\" A \"jane\" 5000648311 180 \"C7\" C \"bill\" 7594873884 273.4 \"A4\" A \"steve\" 9786437450 352.3 \"C6\" C So, what\u2019s the benefit of all this circle approach? Imagine server C is removed. To account for\uff08\u5f25\u8865\uff09 this, we must remove labels C0 .. C9 from the circle. This results in the object keys formerly adjacent to the deleted labels now being randomly labeled Ax and Bx , reassigning them to servers A and B . But what happens with the other object keys, the ones that originally belonged in A and B ? Nothing! That\u2019s the beauty of it: The absence of Cx labels does not affect those keys in any way. So, removing a server results in its object keys being randomly reassigned to the rest of the servers, leaving all other keys untouched : KEY HASH ANGLE (DEG) \"A1\" 473914830 17 \"A2\" 548798874 19.7 \"A3\" 1466730567 52.8 \"john\" 1633428562 58.7 \"B2\" 1808009038 65 \"B3\" 2058758486 74.1 \"A7\" 2162578920 77.8 \"B4\" 2660265921 95.7 \"kate\" 3421657995 123.1 \"A5\" 3434972143 123.6 \"B0\" 4049028775 145.7 \"B8\" 4755525684 171.1 \"A9\" 4769549830 171.7 \"jane\" 5000799124 180 \"B1\" 5444659173 196 \"A6\" 6210502707 223.5 \"A0\" 6511384141 234.4 \"B9\" 7292819872 262.5 \"bill\" 7594634739 273.4 \"A4\" 8047401090 289.7 \"A8\" 8997397092 323.9 \"B7\" 9038880553 325.3 \"B5\" 9368225254 337.2 \"B6\" 9379713761 337.6 \"steve\" 9787173343 352.3 KEY HASH ANGLE (DEG) LABEL SERVER \"john\" 1632929716 58.7 \"B2\" B \"kate\" 3421831276 123.1 \"A5\" A \"jane\" 5000648311 180 \"B1\" B \"bill\" 7594873884 273.4 \"A4\" A \"steve\" 9786437450 352.3 \"A1\" A Something similar happens if, instead of removing a server, we add one. If we wanted to add server D to our example (say, as a replacement for C ), we would need to add labels D0 .. D9 . The result would be that roughly one-third of the existing keys (all belonging to A or B ) would be reassigned to D , and, again, the rest would stay the same: KEY HASH ANGLE (DEG) \"D2\" 439890723 15.8 \"A1\" 473914830 17 \"A2\" 548798874 19.7 \"D8\" 796709216 28.6 \"D1\" 1008580939 36.3 \"A3\" 1466730567 52.8 \"D5\" 1587548309 57.1 \"john\" 1633428562 58.7 \"B2\" 1808009038 65 \"B3\" 2058758486 74.1 \"A7\" 2162578920 77.8 \"B4\" 2660265921 95.7 \"D4\" 2909395217 104.7 \"kate\" 3421657995 123.1 \"A5\" 3434972143 123.6 \"D7\" 3567129743 128.4 \"B0\" 4049028775 145.7 \"B8\" 4755525684 171.1 \"A9\" 4769549830 171.7 \"jane\" 5000799124 180 \"B1\" 5444659173 196 \"D6\" 5703092354 205.3 \"A6\" 6210502707 223.5 \"A0\" 6511384141 234.4 \"B9\" 7292819872 262.5 \"bill\" 7594634739 273.4 \"A4\" 8047401090 289.7 \"D0\" 8272587142 297.8 \"A8\" 8997397092 323.9 \"B7\" 9038880553 325.3 \"D3\" 9048608874 325.7 \"D9\" 9314459653 335.3 \"B5\" 9368225254 337.2 \"B6\" 9379713761 337.6 \"steve\" 9787173343 352.3 KEY HASH ANGLE (DEG) LABEL SERVER \"john\" 1632929716 58.7 \"B2\" B \"kate\" 3421831276 123.1 \"A5\" A \"jane\" 5000648311 180 \"B1\" B \"bill\" 7594873884 273.4 \"A4\" A \"steve\" 9786437450 352.3 \"D2\" D This is how consistent hashing solves the rehashing problem. In general, only k/N keys need to be remapped when k is the number of keys and N is the number of servers (more specifically, the maximum of the initial and final number of servers).","title":"The Solution: Consistent Hashing"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Consistent-hashing/toptal-A-Guide-to-Consistent-Hashing/#what#next","text":"We observed that when using distributed caching to optimize performance, it may happen that the number of caching servers changes (reasons for this may be a server crashing, or the need to add or remove a server to increase or decrease overall capacity). By using consistent hashing to distribute keys between the servers, we can rest assured that should that happen, the number of keys being rehashed\u2014and therefore, the impact on origin servers\u2014will be minimized, preventing potential downtime or performance issues. There are clients for several systems, such as Memcached and Redis, that include support for consistent hashing out of the box. Alternatively, you can implement the algorithm yourself, in your language of choice, and that should be relatively easy once the concept is understood. If data science interests you, Toptal has some of the best articles on the subject at the blog","title":"What Next?"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Rendezvous-hashing/","text":"Rendezvous hashing \"rendezvous\"\u7684\u610f\u601d\u662f\u7ea6\u4f1a wikipedia Rendezvous hashing Rendezvous or highest random weight (HRW) hashing [ 1] [ 2] is an algorithm that allows clients to achieve distributed agreement on a set of k options out of a possible set of n options. A typical application is when clients need to agree on which sites (or proxies) objects are assigned to. As shown below, Rendezvous hashing is more general than Consistent hashing , which becomes a special case (for k = 1) of Rendezvous hashing. NOTE: \u96c6\u5408\u6216\u6700\u9ad8\u968f\u673a\u6743\u91cd\uff08HRW\uff09\u6563\u5217[1] [2]\u662f\u4e00\u79cd\u7b97\u6cd5\uff0c\u5141\u8bb8\u5ba2\u6237\u7aef\u5728\u4e00\u7ec4\u53ef\u80fd\u7684n\u4e2a\u9009\u9879\u4e2d\u5b9e\u73b0\u5bf9\u4e00\u7ec4k\u4e2a\u9009\u9879\u7684\u5206\u5e03\u5f0f\u534f\u8bae\u3002 \u5178\u578b\u7684\u5e94\u7528\u662f\u5f53\u5ba2\u6237\u9700\u8981\u5c31\u5c06objects\u5206\u914d\u4e86\u54ea\u4e9b\u7ad9\u70b9\uff08\u6216\u4ee3\u7406\uff09\u8fbe\u6210\u4e00\u81f4\u3002 \u5982\u4e0b\u6240\u793a\uff0cRendezvous\u54c8\u5e0c\u6bd4\u4e00\u81f4\u54c8\u5e0c\u66f4\u901a\u7528\uff0c\u5b83Rendezvous\u54c8\u5e0c\u7684\u4e00\u79cd\u7279\u6b8a\u60c5\u51b5\uff08\u5bf9\u4e8ek = 1\uff09\u3002 History Rendezvous hashing was invented by David Thaler and Chinya Ravishankar at the University of Michigan in 1996[ 1] . Consistent hashing appeared a year later in the literature. One of the first applications of rendezvous hashing was to enable multicast clients on the Internet (in contexts such as the MBONE ) to identify multicast rendezvous points in a distributed fashion.[ 3] [ 4] It was used in 1998 by Microsoft's Cache Array Routing Protocol (CARP) for distributed cache coordination and routing.[ 5] [ 6] Some Protocol Independent Multicast routing protocols use rendezvous hashing to pick a rendezvous point.[ 1] NOTE: Rendezvous\u54c8\u5e0c\u662f1996\u5e74\u7531\u5bc6\u6b47\u6839\u5927\u5b66\u7684David Thaler\u548cChinya Ravishankar\u53d1\u660e\u7684\u3002 \u4e00\u5e74\u540e\uff0c\u6587\u732e\u4e2d\u51fa\u73b0\u4e86\u4e00\u81f4\u7684\u6563\u5217\u3002 \u96c6\u5408\u6563\u5217\u7684\u7b2c\u4e00\u4e2a\u5e94\u7528\u4e4b\u4e00\u662f\u4f7fInternet\u4e0a\u7684\u591a\u64ad\u5ba2\u6237\u7aef\u80fd\u591f\u4ee5\u5206\u5e03\u5f0f\u65b9\u5f0f\u8bc6\u522b\u591a\u64ad\u96c6\u5408\u70b9\u3002 \u5b83\u4e8e1998\u5e74\u88ab\u5fae\u8f6f\u7684\u7f13\u5b58\u9635\u5217\u8def\u7531\u534f\u8bae\u7528\u4e8e\u5206\u5e03\u5f0f\u7f13\u5b58\u534f\u8c03\u548c\u8def\u7531\u3002 \u67d0\u4e9b\u534f\u8bae\u72ec\u7acb\u591a\u64ad\u8def\u7531\u534f\u8bae\u4f7f\u7528\u96c6\u5408\u6563\u5217\u6765\u9009\u62e9\u96c6\u5408\u70b9\u3002 Given its simplicity and generality, rendezvous hashing has been applied in a wide variety of applications, including mobile caching,[ 7] router design,[ 8] secure key establishment ,[ 9] and sharding and distributed databases.[ 10] NOTE: \u4e0a\u8ff0\u8fd9\u4e9b\u9886\u57df\u662f\u975e\u5e38\u91cd\u8981\u7684 The HRW algorithm for rendezvous hashing NOTE: \u672a\u8bfb\u61c2 TODO https://github.com/drainingsun/ared https://en.wikipedia.org/wiki/Distributed_hash_table https://medium.com/@dgryski/consistent-hashing-algorithmic-tradeoffs-ef6b8e2fcae8 https://link.springer.com/chapter/10.1007/978-3-030-29859-3_3 https://blog.kevingomez.fr/2019/04/11/clusters-and-data-sharding-introducing-rendezvous-hashing/","title":"Introduction"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Rendezvous-hashing/#rendezvous#hashing","text":"\"rendezvous\"\u7684\u610f\u601d\u662f\u7ea6\u4f1a","title":"Rendezvous hashing"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Rendezvous-hashing/#wikipedia#rendezvous#hashing","text":"Rendezvous or highest random weight (HRW) hashing [ 1] [ 2] is an algorithm that allows clients to achieve distributed agreement on a set of k options out of a possible set of n options. A typical application is when clients need to agree on which sites (or proxies) objects are assigned to. As shown below, Rendezvous hashing is more general than Consistent hashing , which becomes a special case (for k = 1) of Rendezvous hashing. NOTE: \u96c6\u5408\u6216\u6700\u9ad8\u968f\u673a\u6743\u91cd\uff08HRW\uff09\u6563\u5217[1] [2]\u662f\u4e00\u79cd\u7b97\u6cd5\uff0c\u5141\u8bb8\u5ba2\u6237\u7aef\u5728\u4e00\u7ec4\u53ef\u80fd\u7684n\u4e2a\u9009\u9879\u4e2d\u5b9e\u73b0\u5bf9\u4e00\u7ec4k\u4e2a\u9009\u9879\u7684\u5206\u5e03\u5f0f\u534f\u8bae\u3002 \u5178\u578b\u7684\u5e94\u7528\u662f\u5f53\u5ba2\u6237\u9700\u8981\u5c31\u5c06objects\u5206\u914d\u4e86\u54ea\u4e9b\u7ad9\u70b9\uff08\u6216\u4ee3\u7406\uff09\u8fbe\u6210\u4e00\u81f4\u3002 \u5982\u4e0b\u6240\u793a\uff0cRendezvous\u54c8\u5e0c\u6bd4\u4e00\u81f4\u54c8\u5e0c\u66f4\u901a\u7528\uff0c\u5b83Rendezvous\u54c8\u5e0c\u7684\u4e00\u79cd\u7279\u6b8a\u60c5\u51b5\uff08\u5bf9\u4e8ek = 1\uff09\u3002","title":"wikipedia Rendezvous hashing"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Rendezvous-hashing/#history","text":"Rendezvous hashing was invented by David Thaler and Chinya Ravishankar at the University of Michigan in 1996[ 1] . Consistent hashing appeared a year later in the literature. One of the first applications of rendezvous hashing was to enable multicast clients on the Internet (in contexts such as the MBONE ) to identify multicast rendezvous points in a distributed fashion.[ 3] [ 4] It was used in 1998 by Microsoft's Cache Array Routing Protocol (CARP) for distributed cache coordination and routing.[ 5] [ 6] Some Protocol Independent Multicast routing protocols use rendezvous hashing to pick a rendezvous point.[ 1] NOTE: Rendezvous\u54c8\u5e0c\u662f1996\u5e74\u7531\u5bc6\u6b47\u6839\u5927\u5b66\u7684David Thaler\u548cChinya Ravishankar\u53d1\u660e\u7684\u3002 \u4e00\u5e74\u540e\uff0c\u6587\u732e\u4e2d\u51fa\u73b0\u4e86\u4e00\u81f4\u7684\u6563\u5217\u3002 \u96c6\u5408\u6563\u5217\u7684\u7b2c\u4e00\u4e2a\u5e94\u7528\u4e4b\u4e00\u662f\u4f7fInternet\u4e0a\u7684\u591a\u64ad\u5ba2\u6237\u7aef\u80fd\u591f\u4ee5\u5206\u5e03\u5f0f\u65b9\u5f0f\u8bc6\u522b\u591a\u64ad\u96c6\u5408\u70b9\u3002 \u5b83\u4e8e1998\u5e74\u88ab\u5fae\u8f6f\u7684\u7f13\u5b58\u9635\u5217\u8def\u7531\u534f\u8bae\u7528\u4e8e\u5206\u5e03\u5f0f\u7f13\u5b58\u534f\u8c03\u548c\u8def\u7531\u3002 \u67d0\u4e9b\u534f\u8bae\u72ec\u7acb\u591a\u64ad\u8def\u7531\u534f\u8bae\u4f7f\u7528\u96c6\u5408\u6563\u5217\u6765\u9009\u62e9\u96c6\u5408\u70b9\u3002 Given its simplicity and generality, rendezvous hashing has been applied in a wide variety of applications, including mobile caching,[ 7] router design,[ 8] secure key establishment ,[ 9] and sharding and distributed databases.[ 10] NOTE: \u4e0a\u8ff0\u8fd9\u4e9b\u9886\u57df\u662f\u975e\u5e38\u91cd\u8981\u7684","title":"History"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Rendezvous-hashing/#the#hrw#algorithm#for#rendezvous#hashing","text":"NOTE: \u672a\u8bfb\u61c2","title":"The HRW algorithm for rendezvous hashing"},{"location":"Distributed-data-store/Data-Sharding-Strategy/Rendezvous-hashing/#todo","text":"https://github.com/drainingsun/ared https://en.wikipedia.org/wiki/Distributed_hash_table https://medium.com/@dgryski/consistent-hashing-algorithmic-tradeoffs-ef6b8e2fcae8 https://link.springer.com/chapter/10.1007/978-3-030-29859-3_3 https://blog.kevingomez.fr/2019/04/11/clusters-and-data-sharding-introducing-rendezvous-hashing/","title":"TODO"},{"location":"Distributed-data-store/Data-system/Redo-log/","text":"Redo log infogalactic Redo log","title":"Introduction"},{"location":"Distributed-data-store/Data-system/Redo-log/#redo#log","text":"","title":"Redo log"},{"location":"Distributed-data-store/Data-system/Redo-log/#infogalactic#redo#log","text":"","title":"infogalactic Redo log"},{"location":"Distributed-data-store/Data-system/Schema/","text":"Schema \u5728\u9605\u8bfb What are the general differences between a format and a protocol \u65f6\uff0c\u5176\u4e2d\u6709\u4f7f\u7528schema\u8fd9\u4e2a\u8bcd\u8bed\uff0c\u8fd9\u8ba9\u6211\u60f3\u8d77\u4e86database scheme\uff0c\u6240\u4ee5\u51b3\u5b9a\u5bf9schema\u8fd9\u4e2a\u8bcd\u8bed\u8fdb\u884c\u6df1\u5165\u5206\u6790\u3001\u603b\u7ed3\u3002\u4e0b\u9762\u662f\u5bf9\u5b83\u7684\u76f4\u89c2\u611f\u53d7: \u5b83\u6240\u63cf\u8ff0\u7684\u662fabstract structure\uff0c\u4e00\u4e2a\u5178\u578b\u7684\u4f8b\u5b50\u662f: database table\u662f\u62bd\u8c61\u7684structure\uff0c\u5b83\u548centity\u90fd\u662f\u5bf9\u5177\u4f53\u7684\u62bd\u8c61\u3002 What is schema stanford Stanford Encyclopedia of Philosophy # Schema NOTE: \u8fd9\u7bc7\u6587\u7ae0\u5bf9schema\u7684\u89e3\u91ca\u662f\u975e\u5e38\u597d\u7684 wikipedia Schema In psychology and cognitive science, a scheme (plural(\u590d\u6570) schemata or schemes) describes a pattern of thought or behavior that organizes categories of information and the relationships among them. Schemes have frequently been mistranslated from the original French as 'Schema' It can also be described as a mental structure of preconceived(\u9884\u60f3) ideas, a framework representing some aspect of the world, or a system of organizing and perceiving new information. Schemata influence attention and the absorption(\u5438\u6536) of new knowledge: people are more likely to notice things that fit into their schema, while re-interpreting contradictions to the schema as exceptions or distorting them to fit. Schemata have a tendency to remain unchanged, even in the face of contradictory information. Schemata can help in understanding the world and the rapidly changing environment. People can organize new perceptions(\u89c2\u5ff5) into schemata quickly as most situations do not require complex thought when using schema, since automatic thought is all that is required. Schema and format \u5728 What are the general differences between a format and a protocol \u7684\u7b2c\u4e8c\u4e2a\u56de\u7b54\u4e2d\uff0c\u6709\u8fd9\u6837\u7684\u63cf\u8ff0:A format describes the structure of some data\u8fd9\u63d0\u793a\u4e86\u6211\uff0cformat\u4e5f\u662f\u4e00\u79cdschema\u3002\u4e0b\u9762\u5c31\u6709xml schema\u3001json schema\u7b49\u7b49\u3002\u5728\u6587\u7ae0medium Use Binary Encoding Instead of JSON \u4e2d\uff0c\u5c31\u53cd\u590d\u4f7f\u7528\u4e86schema\u8fd9\u4e2a\u672f\u8bed\u3002 XML schema http://en.wikipedia.org/wiki/XML_schemahttps://www.w3schools.com/xml/schema_intro.asp JSON Schema https://json-schema.org/ Database schema https://en.wikipedia.org/wiki/Database_schema Axiom schema \u53c2\u89c1Mathematical-logic\u4e2d\u7684 Axiom Schema \u7ae0\u8282\u3002","title":"Introduction"},{"location":"Distributed-data-store/Data-system/Schema/#schema","text":"\u5728\u9605\u8bfb What are the general differences between a format and a protocol \u65f6\uff0c\u5176\u4e2d\u6709\u4f7f\u7528schema\u8fd9\u4e2a\u8bcd\u8bed\uff0c\u8fd9\u8ba9\u6211\u60f3\u8d77\u4e86database scheme\uff0c\u6240\u4ee5\u51b3\u5b9a\u5bf9schema\u8fd9\u4e2a\u8bcd\u8bed\u8fdb\u884c\u6df1\u5165\u5206\u6790\u3001\u603b\u7ed3\u3002\u4e0b\u9762\u662f\u5bf9\u5b83\u7684\u76f4\u89c2\u611f\u53d7: \u5b83\u6240\u63cf\u8ff0\u7684\u662fabstract structure\uff0c\u4e00\u4e2a\u5178\u578b\u7684\u4f8b\u5b50\u662f: database table\u662f\u62bd\u8c61\u7684structure\uff0c\u5b83\u548centity\u90fd\u662f\u5bf9\u5177\u4f53\u7684\u62bd\u8c61\u3002","title":"Schema"},{"location":"Distributed-data-store/Data-system/Schema/#what#is#schema","text":"","title":"What is schema"},{"location":"Distributed-data-store/Data-system/Schema/#stanford#stanford#encyclopedia#of#philosophy#schema","text":"NOTE: \u8fd9\u7bc7\u6587\u7ae0\u5bf9schema\u7684\u89e3\u91ca\u662f\u975e\u5e38\u597d\u7684","title":"stanford Stanford Encyclopedia of Philosophy # Schema"},{"location":"Distributed-data-store/Data-system/Schema/#wikipedia#schema","text":"In psychology and cognitive science, a scheme (plural(\u590d\u6570) schemata or schemes) describes a pattern of thought or behavior that organizes categories of information and the relationships among them. Schemes have frequently been mistranslated from the original French as 'Schema' It can also be described as a mental structure of preconceived(\u9884\u60f3) ideas, a framework representing some aspect of the world, or a system of organizing and perceiving new information. Schemata influence attention and the absorption(\u5438\u6536) of new knowledge: people are more likely to notice things that fit into their schema, while re-interpreting contradictions to the schema as exceptions or distorting them to fit. Schemata have a tendency to remain unchanged, even in the face of contradictory information. Schemata can help in understanding the world and the rapidly changing environment. People can organize new perceptions(\u89c2\u5ff5) into schemata quickly as most situations do not require complex thought when using schema, since automatic thought is all that is required.","title":"wikipedia Schema"},{"location":"Distributed-data-store/Data-system/Schema/#schema#and#format","text":"\u5728 What are the general differences between a format and a protocol \u7684\u7b2c\u4e8c\u4e2a\u56de\u7b54\u4e2d\uff0c\u6709\u8fd9\u6837\u7684\u63cf\u8ff0:A format describes the structure of some data\u8fd9\u63d0\u793a\u4e86\u6211\uff0cformat\u4e5f\u662f\u4e00\u79cdschema\u3002\u4e0b\u9762\u5c31\u6709xml schema\u3001json schema\u7b49\u7b49\u3002\u5728\u6587\u7ae0medium Use Binary Encoding Instead of JSON \u4e2d\uff0c\u5c31\u53cd\u590d\u4f7f\u7528\u4e86schema\u8fd9\u4e2a\u672f\u8bed\u3002","title":"Schema and format"},{"location":"Distributed-data-store/Data-system/Schema/#xml#schema","text":"http://en.wikipedia.org/wiki/XML_schemahttps://www.w3schools.com/xml/schema_intro.asp","title":"XML schema"},{"location":"Distributed-data-store/Data-system/Schema/#json#schema","text":"https://json-schema.org/","title":"JSON Schema"},{"location":"Distributed-data-store/Data-system/Schema/#database#schema","text":"https://en.wikipedia.org/wiki/Database_schema","title":"Database schema"},{"location":"Distributed-data-store/Data-system/Schema/#axiom#schema","text":"\u53c2\u89c1Mathematical-logic\u4e2d\u7684 Axiom Schema \u7ae0\u8282\u3002","title":"Axiom schema"},{"location":"Distributed-data-store/Data-system/Transaction-log/","text":"Transaction Log \u662f\u5728\u9605\u8bfb ZooKeeper Getting Started Guide \u65f6\uff0c\u53d1\u73b0\u5176\u4e2d\u6709\u5173\u4e8etranslation log\u7684\u63cf\u8ff0\u3002 infogalactic Transaction log","title":"Introduction"},{"location":"Distributed-data-store/Data-system/Transaction-log/#transaction#log","text":"\u662f\u5728\u9605\u8bfb ZooKeeper Getting Started Guide \u65f6\uff0c\u53d1\u73b0\u5176\u4e2d\u6709\u5173\u4e8etranslation log\u7684\u63cf\u8ff0\u3002","title":"Transaction Log"},{"location":"Distributed-data-store/Data-system/Transaction-log/#infogalactic#transaction#log","text":"","title":"infogalactic Transaction log"},{"location":"Distributed-data-store/Distributed-database/","text":"Distributed database TODO zhihu \u5bf9\u6bd4\u4e00\u4e0bYugabyte DB\u548cTIDB?","title":"Introduction"},{"location":"Distributed-data-store/Distributed-database/#distributed#database","text":"","title":"Distributed database"},{"location":"Distributed-data-store/Distributed-database/#todo","text":"zhihu \u5bf9\u6bd4\u4e00\u4e0bYugabyte DB\u548cTIDB?","title":"TODO"},{"location":"Distributed-data-store/Distributed-database/CockroachDB/","text":"CockroachDB The world's most powerful global database is now also the easiest The Most Highly Evolved Database on the Planet. NOTE: \u4e00\u3001\u53f7\u79f0\u6700\u5148\u8fdb\u7684\u6570\u636e\u5e93 \u4e8c\u3001\u4f7f\u7528golang\u5f00\u53d1 zhihu \u5982\u4f55\u8bc4\u4ef7cockroachdb\uff1f triump\u7684\u56de\u7b54 cockroachDB,yugabyteDB,kudu, TIDB \u8fd9\u4e9b\u90fd\u662f \u53c2\u8003\u4e86google spanner \u8bba\u6587 \u7684\u5f00\u6e90\u5b9e\u73b0. github cockroachdb / cockroach","title":"Introduction"},{"location":"Distributed-data-store/Distributed-database/CockroachDB/#cockroachdb","text":"The world's most powerful global database is now also the easiest The Most Highly Evolved Database on the Planet. NOTE: \u4e00\u3001\u53f7\u79f0\u6700\u5148\u8fdb\u7684\u6570\u636e\u5e93 \u4e8c\u3001\u4f7f\u7528golang\u5f00\u53d1","title":"CockroachDB"},{"location":"Distributed-data-store/Distributed-database/CockroachDB/#zhihu#cockroachdb","text":"","title":"zhihu \u5982\u4f55\u8bc4\u4ef7cockroachdb\uff1f"},{"location":"Distributed-data-store/Distributed-database/CockroachDB/#triump","text":"cockroachDB,yugabyteDB,kudu, TIDB \u8fd9\u4e9b\u90fd\u662f \u53c2\u8003\u4e86google spanner \u8bba\u6587 \u7684\u5f00\u6e90\u5b9e\u73b0.","title":"triump\u7684\u56de\u7b54"},{"location":"Distributed-data-store/Distributed-database/CockroachDB/#github#cockroachdbcockroach","text":"","title":"github cockroachdb/cockroach"},{"location":"Distributed-data-store/Distributed-database/CockroachDB/Living-Without-Atomic-Clocks/","text":"cockroachlabs Living Without Atomic Clocks NOTE: \u8fd9\u7bc7\u6587\u7ae0\uff0c\u8bb2\u5f97\u4e0d\u9519","title":"Introduction"},{"location":"Distributed-data-store/Distributed-database/CockroachDB/Living-Without-Atomic-Clocks/#cockroachlabs#living#without#atomic#clocks","text":"NOTE: \u8fd9\u7bc7\u6587\u7ae0\uff0c\u8bb2\u5f97\u4e0d\u9519","title":"cockroachlabs Living Without Atomic Clocks"},{"location":"Distributed-data-store/Distributed-database/Google-Spanner/","text":"Google Spanner paper Spanner: Google\u2019s Globally Distributed Database oschina Google Spanner \u5168\u7403\u7ea7\u7684\u5206\u5e03\u5f0f\u6570\u636e\u5e93 Spanner \u662f Google \u7684\u5168\u7403\u7ea7\u7684\u5206\u5e03\u5f0f\u6570\u636e\u5e93 (Globally-Distributed Database) \u3002Spanner \u53ef\u4ee5\u6269\u5c55\u5230\u6570\u767e\u4e07\u7684\u673a\u5668\uff0c\u6570\u5df2\u767e\u8ba1\u7684\u6570\u636e\u4e2d\u5fc3\uff0c\u4e0a\u4e07\u4ebf\u7684\u884c\u3002\u66f4\u7ed9\u529b\u7684\u662f\uff0c\u9664\u4e86\u5938\u5f20\u7684\u6269\u5c55\u6027\u4e4b\u5916\uff0c\u5b83\u8fd8\u80fd\u540c\u65f6\u901a\u8fc7\u540c\u6b65\u590d\u5236\u548c\u591a\u7248\u672c\u6765\u6ee1\u8db3\u5916\u90e8\u4e00\u81f4\u6027\uff0c\u53ef\u7528\u6027\u4e5f\u662f\u5f88\u597d\u7684\u3002\u51b2\u7834CAP\u7684\u67b7\u9501\uff0c\u5728\u4e09\u8005\u4e4b\u95f4\u5b8c\u7f8e\u5e73\u8861\u3002 NOTE: \u4e00\u3001\"\u540c\u6b65\u590d\u5236\"\u5373 \"synchronous replication\" \u4e8c\u3001\"\u591a\u7248\u672c\"\u6307\u7684\u662f\u4ec0\u4e48\uff1f \u4e09\u3001\u5982\u4f55\u5728CAP\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff1f Spanner\u662f\u4e2a\u53ef\u6269\u5c55\uff0c\u591a\u7248\u672c\uff0c\u5168\u7403\u5206\u5e03\u5f0f\u8fd8\u652f\u6301\u540c\u6b65\u590d\u5236\u7684\u6570\u636e\u5e93\u3002\u4ed6\u662fGoogle\u7684\u7b2c\u4e00\u4e2a\u53ef\u4ee5\u5168\u7403\u6269\u5c55\u5e76\u4e14\u652f\u6301\u5916\u90e8\u4e00\u81f4 \u7684\u4e8b\u52a1\u3002Spanner\u80fd\u505a\u5230\u8fd9\u4e9b\uff0c\u79bb\u4e0d\u5f00\u4e00\u4e2a\u7528GPS\u548c\u539f\u5b50\u949f\u5b9e\u73b0\u7684\u65f6\u95f4API\u3002\u8fd9\u4e2aAPI\u80fd\u5c06\u6570\u636e\u4e2d\u5fc3\u4e4b\u95f4\u7684\u65f6\u95f4\u540c\u6b65\u7cbe\u786e\u523010ms\u4ee5\u5185\u3002\u56e0\u6b64\u6709\u51e0\u4e2a \u7ed9\u529b\u7684\u529f\u80fd\uff1a\u65e0\u9501\u8bfb\u4e8b\u52a1\uff0c\u539f\u5b50schema\u4fee\u6539\uff0c\u8bfb\u5386\u53f2\u6570\u636e\u65e0block\u3002 NOTE: \u4e00\u3001 distributed computing \u7684\u4e00\u4e2a\u6311\u6218\u662f lack of a global clock \uff0c\u4ece\u4e0a\u9762\u8fd9\u6bb5\u7684\u63cf\u8ff0\u53ef\u77e5\uff0cGoogle spanner\u5b9e\u73b0\u4e86clock synchronization\uff0c\u7cbe\u5ea6\u572810ms\u5185\uff0c\u663e\u7136\u8fd9\u662f\u80fd\u591f\u5e2e\u52a9\u89e3\u51b3distributed computing\u4e2d\u7684\u5f88\u591a\u95ee\u9898\u7684\uff0c\u6b63\u5982\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u6240\u603b\u7ed3\u7684: \"\u56e0\u6b64\u6709\u51e0\u4e2a \u7ed9\u529b\u7684\u529f\u80fd\uff1a\u65e0\u9501\u8bfb\u4e8b\u52a1\uff0c\u539f\u5b50schema\u4fee\u6539\uff0c\u8bfb\u5386\u53f2\u6570\u636e\u65e0block\u3002\" \u53e6\u5916\u53c2\u89c1: cockroachlabs Living Without Atomic Clocks TODO csdn Spanner\u7684TrueTime\u4e0e\u5206\u5e03\u5f0f\u4e8b\u52a1","title":"Introduction"},{"location":"Distributed-data-store/Distributed-database/Google-Spanner/#google#spanner","text":"","title":"Google Spanner"},{"location":"Distributed-data-store/Distributed-database/Google-Spanner/#paper#spanner#googles#globally#distributed#database","text":"","title":"paper Spanner: Google\u2019s Globally Distributed Database"},{"location":"Distributed-data-store/Distributed-database/Google-Spanner/#oschina#google#spanner","text":"Spanner \u662f Google \u7684\u5168\u7403\u7ea7\u7684\u5206\u5e03\u5f0f\u6570\u636e\u5e93 (Globally-Distributed Database) \u3002Spanner \u53ef\u4ee5\u6269\u5c55\u5230\u6570\u767e\u4e07\u7684\u673a\u5668\uff0c\u6570\u5df2\u767e\u8ba1\u7684\u6570\u636e\u4e2d\u5fc3\uff0c\u4e0a\u4e07\u4ebf\u7684\u884c\u3002\u66f4\u7ed9\u529b\u7684\u662f\uff0c\u9664\u4e86\u5938\u5f20\u7684\u6269\u5c55\u6027\u4e4b\u5916\uff0c\u5b83\u8fd8\u80fd\u540c\u65f6\u901a\u8fc7\u540c\u6b65\u590d\u5236\u548c\u591a\u7248\u672c\u6765\u6ee1\u8db3\u5916\u90e8\u4e00\u81f4\u6027\uff0c\u53ef\u7528\u6027\u4e5f\u662f\u5f88\u597d\u7684\u3002\u51b2\u7834CAP\u7684\u67b7\u9501\uff0c\u5728\u4e09\u8005\u4e4b\u95f4\u5b8c\u7f8e\u5e73\u8861\u3002 NOTE: \u4e00\u3001\"\u540c\u6b65\u590d\u5236\"\u5373 \"synchronous replication\" \u4e8c\u3001\"\u591a\u7248\u672c\"\u6307\u7684\u662f\u4ec0\u4e48\uff1f \u4e09\u3001\u5982\u4f55\u5728CAP\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff1f Spanner\u662f\u4e2a\u53ef\u6269\u5c55\uff0c\u591a\u7248\u672c\uff0c\u5168\u7403\u5206\u5e03\u5f0f\u8fd8\u652f\u6301\u540c\u6b65\u590d\u5236\u7684\u6570\u636e\u5e93\u3002\u4ed6\u662fGoogle\u7684\u7b2c\u4e00\u4e2a\u53ef\u4ee5\u5168\u7403\u6269\u5c55\u5e76\u4e14\u652f\u6301\u5916\u90e8\u4e00\u81f4 \u7684\u4e8b\u52a1\u3002Spanner\u80fd\u505a\u5230\u8fd9\u4e9b\uff0c\u79bb\u4e0d\u5f00\u4e00\u4e2a\u7528GPS\u548c\u539f\u5b50\u949f\u5b9e\u73b0\u7684\u65f6\u95f4API\u3002\u8fd9\u4e2aAPI\u80fd\u5c06\u6570\u636e\u4e2d\u5fc3\u4e4b\u95f4\u7684\u65f6\u95f4\u540c\u6b65\u7cbe\u786e\u523010ms\u4ee5\u5185\u3002\u56e0\u6b64\u6709\u51e0\u4e2a \u7ed9\u529b\u7684\u529f\u80fd\uff1a\u65e0\u9501\u8bfb\u4e8b\u52a1\uff0c\u539f\u5b50schema\u4fee\u6539\uff0c\u8bfb\u5386\u53f2\u6570\u636e\u65e0block\u3002 NOTE: \u4e00\u3001 distributed computing \u7684\u4e00\u4e2a\u6311\u6218\u662f lack of a global clock \uff0c\u4ece\u4e0a\u9762\u8fd9\u6bb5\u7684\u63cf\u8ff0\u53ef\u77e5\uff0cGoogle spanner\u5b9e\u73b0\u4e86clock synchronization\uff0c\u7cbe\u5ea6\u572810ms\u5185\uff0c\u663e\u7136\u8fd9\u662f\u80fd\u591f\u5e2e\u52a9\u89e3\u51b3distributed computing\u4e2d\u7684\u5f88\u591a\u95ee\u9898\u7684\uff0c\u6b63\u5982\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u6240\u603b\u7ed3\u7684: \"\u56e0\u6b64\u6709\u51e0\u4e2a \u7ed9\u529b\u7684\u529f\u80fd\uff1a\u65e0\u9501\u8bfb\u4e8b\u52a1\uff0c\u539f\u5b50schema\u4fee\u6539\uff0c\u8bfb\u5386\u53f2\u6570\u636e\u65e0block\u3002\" \u53e6\u5916\u53c2\u89c1: cockroachlabs Living Without Atomic Clocks","title":"oschina Google Spanner \u5168\u7403\u7ea7\u7684\u5206\u5e03\u5f0f\u6570\u636e\u5e93"},{"location":"Distributed-data-store/Distributed-database/Google-Spanner/#todo","text":"csdn Spanner\u7684TrueTime\u4e0e\u5206\u5e03\u5f0f\u4e8b\u52a1","title":"TODO"},{"location":"Distributed-data-store/Distributed-database/Google-Spanner/paper-Spanner-Google%E2%80%99s-Globally-Distributed-Database/","text":"acm Spanner: Google\u2019s Globally Distributed Database jianshu Google-Spanner\u8bba\u6587\u7684\u601d\u8003","title":"Introduction"},{"location":"Distributed-data-store/Distributed-database/Google-Spanner/paper-Spanner-Google%E2%80%99s-Globally-Distributed-Database/#acm#spanner#googles#globally#distributed#database","text":"","title":"acm Spanner: Google\u2019s Globally Distributed Database"},{"location":"Distributed-data-store/Distributed-database/Google-Spanner/paper-Spanner-Google%E2%80%99s-Globally-Distributed-Database/#jianshu#google-spanner","text":"","title":"jianshu Google-Spanner\u8bba\u6587\u7684\u601d\u8003"},{"location":"Distributed-data-store/Distributed-database/MySQL-Cluster/","text":"","title":"Introduction"},{"location":"Distributed-data-store/Distributed-database/NewSQL/","text":"NewSQL zhihu \u672a\u6765\u5df2\u6765\uff0cGoogle Cloud Spanner \u5c55\u5f00 NewSQL \u65f6\u4ee3","title":"Introduction"},{"location":"Distributed-data-store/Distributed-database/NewSQL/#newsql","text":"","title":"NewSQL"},{"location":"Distributed-data-store/Distributed-database/NewSQL/#zhihu#google#cloud#spanner#newsql","text":"","title":"zhihu \u672a\u6765\u5df2\u6765\uff0cGoogle Cloud Spanner \u5c55\u5f00 NewSQL \u65f6\u4ee3"},{"location":"Distributed-data-store/Distributed-database/YugabyteDB/","text":"Yugabyte DB github yugabyte github yugabyte / yugabyte-db NOTE\uff1a \u4f7f\u7528C\u5f00\u53d1\u7684 TODO ericfu YugabyteDB \u4ecb\u7ecd","title":"Introduction"},{"location":"Distributed-data-store/Distributed-database/YugabyteDB/#yugabyte#db","text":"","title":"Yugabyte DB"},{"location":"Distributed-data-store/Distributed-database/YugabyteDB/#github#yugabyte","text":"","title":"github yugabyte"},{"location":"Distributed-data-store/Distributed-database/YugabyteDB/#github#yugabyteyugabyte-db","text":"NOTE\uff1a \u4f7f\u7528C\u5f00\u53d1\u7684","title":"github yugabyte/yugabyte-db"},{"location":"Distributed-data-store/Distributed-database/YugabyteDB/#todo","text":"ericfu YugabyteDB \u4ecb\u7ecd","title":"TODO"},{"location":"Distributed-data-store/Distributed-database/YugabyteDB/blog-4-Data-Sharding-Strategies-in-Building-Distributed-SQL-Database/","text":"yugabyte Four Data Sharding Strategies We Analyzed in Building a Distributed SQL Database NOTE: \u5bf9\u5404\u79cd\u7b56\u7565\u603b\u7ed3\u5730\u6bd4\u8f83\u597d A distributed SQL database needs to automatically partition the data in a table and distribute it across nodes. This is known as data sharding and it can be achieved through different strategies, each with its own tradeoffs. In this post, we will examine various data sharding strategies for a distributed SQL database, analyze the tradeoffs, explain the rationale for which of these strategies YugabyteDB supports and what we picked as the default sharding strategy . Lessons from building sharded data systems Data sharding helps in scalability and geo-distribution by horizontally partitioning data. A SQL table is decomposed into multiple sets of rows according to a specific sharding strategy. Each of these sets of rows is called a shard. These shards are distributed across multiple server nodes (containers, VMs, bare-metal) in a shared-nothing architecture . This ensures that the shards do not get bottlenecked by the compute, storage and networking resources available at a single node. High availability is achieved by replicating each shard across multiple nodes. However, the application interacts with a SQL table as one logical unit and remains agnostic(\u4e0d\u77e5) to the physical placement of the shards. In this section, we will outline the pros, cons and our practical learnings from the sharding strategies adopted by these databases. Memcached and Redis \u2013 Algorithmic Sharding Distributed caches have had to distribute data across multiple nodes for a while. A commonly used technique is algorithmic sharding , where each key consistently maps to the same node. This is achieved by computing a numeric hash value out of the key and computing a modulo of that hash using the total number of nodes to compute which node owns the key. Part of the image from source: How Sharding Works Pros In algorithmic sharding, the client can determine a given partition\u2019s database without any help. Cons When a new node is added or removed, the ownership of almost all keys would be affected, resulting in a massive redistribution of all the data across nodes of the cluster. While this is not a correctness issue in a distributed cache (because cache misses will repopulate the data), it can have a huge performance impact since the entire cache will have to be warmed again. NOTE: \u4e00\u3001\u4e0a\u8ff0\u5206\u6790\u4ec5\u4ec5\u662f\u4ece\u7b97\u6cd5\u672c\u8eab\u6765\u8fdb\u884c\u7684\uff0c\u5e76\u6ca1\u6709\u7ed3\u5408 Redis \u548c memcache \u7684\u5177\u4f53\u60c5\u51b5\u6765\u8fdb\u884c\u5206\u6790\uff0c\u4e3a\u4ec0\u4e48\u5b83\u4eec\u8981\u8fd9\u6837\u505a\uff1b \u4e8c\u3001Redis\u4e2d\uff0c\u5c06\u5b83\u7684shard strategy\u79f0\u4e3ahash slot\uff1b Analysis Adding and removing nodes is fundamental to a distributed database, and these operations need to be efficient. This makes this type of sharding a poor option and is not implemented in YugabyteDB. References: 1\u3001Sharding data across a memcache tier from \u201c Distributed Caching with Memcached\u201d 2\u3001Algorithmic sharding from \u201c How Sharding Works\u201d Initial Implementation in Cassandra \u2013 Linear Hash Sharding NOTE: \u5e76\u6ca1\u6709\u83b7\u5f97\u5b9e\u9645\u7684\u8fd0\u7528 Linear hash sharding is a hybrid between hash and range sharding that preserves the sort order of the rows by utilizing a linear hash function instead of a regular random hash function to compute how to shard the rows. A linear hash function, sometimes referred to as an order preserving hash, is a hash function that maintains the relative ordering of input values while changing their distribution spacing. This type of sharding preserves the sort ordering of the rows, while redistributing these rows across a larger key space. The idea is that the larger key space over which the redistribution of rows is done can be pre-sharded, enabling the table to be spread across multiple nodes. Pros In theory, this type of sharding allows efficiently querying a range of rows by the primary key values while enabling pre-splitting of the table into multiple shards. Cons In practice, this sharding strategy was problematic because it was impossible to pick good shard split boundaries ahead of time. The primary disadvantage of linear hash sharding is that the data is never evenly distributed between partitions, and thus results in hotspots. NOTE: \u5b9e\u9645\u4f7f\u7528\u7684\u56f0\u96be: \u65e0\u6cd5\u6311\u9009\u4e00\u4e2a\u597d\u7684\u7684\"shard split boundaries ahead of time\" Analysis While useful in theory, these are only a narrow set of use-cases that can leverage this sharding strategy effectively. In the case of Cassandra, the default sharding strategy was changed from linear hash sharding to consistent sharding to improve scalability and performance. Hence, this is a poor sharding strategy and is not implemented in YugabyteDB. DynamoDB and Cassandra \u2013 Consistent Hash Sharding NOTE: \u8fd9\u79cd\u662f\u88ab\u5e7f\u6cdb\u91c7\u7528\u7684 With consistent hash sharding, data is evenly and randomly distributed across shards using a partitioning algorithm. Each row of the table is placed into a shard determined by computing a consistent hash on the partition column values of that row. This is shown in the figure below. Cons Performing range queries could be inefficient. Examples of range queries are finding rows greater than a lower bound or less than an upper bound (as opposed to point lookups). Google Spanner and HBase \u2013 Range Sharding Range sharding involves splitting the rows of a table into contiguous ranges that respect the sort order of the table based on the primary key column values. The tables that are range sharded usually start out with a single shard. As data is inserted into the table, it is dynamically split into multiple shards because it is not always possible to know the distribution of keys in the table ahead of time. The basic idea behind range sharding is shown in the figure below. Pros This type of sharding allows efficiently querying a range of rows by the primary key values. Examples of such a query is to look up all keys that lie between a lower bound and an upper bound. Cons Range sharding leads to a number of issues in practice at scale, some of which are similar to that of linear hash sharding. Putting it all together Support for consistent hash and range sharding Because all of the above properties are desirable, we decided to include both consistent hash and range sharding in YugabyteDB.","title":"Introduction"},{"location":"Distributed-data-store/Distributed-database/YugabyteDB/blog-4-Data-Sharding-Strategies-in-Building-Distributed-SQL-Database/#yugabyte#four#data#sharding#strategies#we#analyzed#in#building#a#distributed#sql#database","text":"NOTE: \u5bf9\u5404\u79cd\u7b56\u7565\u603b\u7ed3\u5730\u6bd4\u8f83\u597d A distributed SQL database needs to automatically partition the data in a table and distribute it across nodes. This is known as data sharding and it can be achieved through different strategies, each with its own tradeoffs. In this post, we will examine various data sharding strategies for a distributed SQL database, analyze the tradeoffs, explain the rationale for which of these strategies YugabyteDB supports and what we picked as the default sharding strategy .","title":"yugabyte Four Data Sharding Strategies We Analyzed in Building a Distributed SQL Database"},{"location":"Distributed-data-store/Distributed-database/YugabyteDB/blog-4-Data-Sharding-Strategies-in-Building-Distributed-SQL-Database/#lessons#from#building#sharded#data#systems","text":"Data sharding helps in scalability and geo-distribution by horizontally partitioning data. A SQL table is decomposed into multiple sets of rows according to a specific sharding strategy. Each of these sets of rows is called a shard. These shards are distributed across multiple server nodes (containers, VMs, bare-metal) in a shared-nothing architecture . This ensures that the shards do not get bottlenecked by the compute, storage and networking resources available at a single node. High availability is achieved by replicating each shard across multiple nodes. However, the application interacts with a SQL table as one logical unit and remains agnostic(\u4e0d\u77e5) to the physical placement of the shards. In this section, we will outline the pros, cons and our practical learnings from the sharding strategies adopted by these databases.","title":"Lessons from building sharded data systems"},{"location":"Distributed-data-store/Distributed-database/YugabyteDB/blog-4-Data-Sharding-Strategies-in-Building-Distributed-SQL-Database/#memcached#and#redis#algorithmic#sharding","text":"Distributed caches have had to distribute data across multiple nodes for a while. A commonly used technique is algorithmic sharding , where each key consistently maps to the same node. This is achieved by computing a numeric hash value out of the key and computing a modulo of that hash using the total number of nodes to compute which node owns the key. Part of the image from source: How Sharding Works Pros In algorithmic sharding, the client can determine a given partition\u2019s database without any help. Cons When a new node is added or removed, the ownership of almost all keys would be affected, resulting in a massive redistribution of all the data across nodes of the cluster. While this is not a correctness issue in a distributed cache (because cache misses will repopulate the data), it can have a huge performance impact since the entire cache will have to be warmed again. NOTE: \u4e00\u3001\u4e0a\u8ff0\u5206\u6790\u4ec5\u4ec5\u662f\u4ece\u7b97\u6cd5\u672c\u8eab\u6765\u8fdb\u884c\u7684\uff0c\u5e76\u6ca1\u6709\u7ed3\u5408 Redis \u548c memcache \u7684\u5177\u4f53\u60c5\u51b5\u6765\u8fdb\u884c\u5206\u6790\uff0c\u4e3a\u4ec0\u4e48\u5b83\u4eec\u8981\u8fd9\u6837\u505a\uff1b \u4e8c\u3001Redis\u4e2d\uff0c\u5c06\u5b83\u7684shard strategy\u79f0\u4e3ahash slot\uff1b Analysis Adding and removing nodes is fundamental to a distributed database, and these operations need to be efficient. This makes this type of sharding a poor option and is not implemented in YugabyteDB. References: 1\u3001Sharding data across a memcache tier from \u201c Distributed Caching with Memcached\u201d 2\u3001Algorithmic sharding from \u201c How Sharding Works\u201d","title":"Memcached and Redis \u2013 Algorithmic Sharding"},{"location":"Distributed-data-store/Distributed-database/YugabyteDB/blog-4-Data-Sharding-Strategies-in-Building-Distributed-SQL-Database/#initial#implementation#in#cassandra#linear#hash#sharding","text":"NOTE: \u5e76\u6ca1\u6709\u83b7\u5f97\u5b9e\u9645\u7684\u8fd0\u7528 Linear hash sharding is a hybrid between hash and range sharding that preserves the sort order of the rows by utilizing a linear hash function instead of a regular random hash function to compute how to shard the rows. A linear hash function, sometimes referred to as an order preserving hash, is a hash function that maintains the relative ordering of input values while changing their distribution spacing. This type of sharding preserves the sort ordering of the rows, while redistributing these rows across a larger key space. The idea is that the larger key space over which the redistribution of rows is done can be pre-sharded, enabling the table to be spread across multiple nodes. Pros In theory, this type of sharding allows efficiently querying a range of rows by the primary key values while enabling pre-splitting of the table into multiple shards. Cons In practice, this sharding strategy was problematic because it was impossible to pick good shard split boundaries ahead of time. The primary disadvantage of linear hash sharding is that the data is never evenly distributed between partitions, and thus results in hotspots. NOTE: \u5b9e\u9645\u4f7f\u7528\u7684\u56f0\u96be: \u65e0\u6cd5\u6311\u9009\u4e00\u4e2a\u597d\u7684\u7684\"shard split boundaries ahead of time\" Analysis While useful in theory, these are only a narrow set of use-cases that can leverage this sharding strategy effectively. In the case of Cassandra, the default sharding strategy was changed from linear hash sharding to consistent sharding to improve scalability and performance. Hence, this is a poor sharding strategy and is not implemented in YugabyteDB.","title":"Initial Implementation in Cassandra \u2013 Linear Hash Sharding"},{"location":"Distributed-data-store/Distributed-database/YugabyteDB/blog-4-Data-Sharding-Strategies-in-Building-Distributed-SQL-Database/#dynamodb#and#cassandra#consistent#hash#sharding","text":"NOTE: \u8fd9\u79cd\u662f\u88ab\u5e7f\u6cdb\u91c7\u7528\u7684 With consistent hash sharding, data is evenly and randomly distributed across shards using a partitioning algorithm. Each row of the table is placed into a shard determined by computing a consistent hash on the partition column values of that row. This is shown in the figure below. Cons Performing range queries could be inefficient. Examples of range queries are finding rows greater than a lower bound or less than an upper bound (as opposed to point lookups).","title":"DynamoDB and Cassandra \u2013 Consistent Hash Sharding"},{"location":"Distributed-data-store/Distributed-database/YugabyteDB/blog-4-Data-Sharding-Strategies-in-Building-Distributed-SQL-Database/#google#spanner#and#hbase#range#sharding","text":"Range sharding involves splitting the rows of a table into contiguous ranges that respect the sort order of the table based on the primary key column values. The tables that are range sharded usually start out with a single shard. As data is inserted into the table, it is dynamically split into multiple shards because it is not always possible to know the distribution of keys in the table ahead of time. The basic idea behind range sharding is shown in the figure below. Pros This type of sharding allows efficiently querying a range of rows by the primary key values. Examples of such a query is to look up all keys that lie between a lower bound and an upper bound. Cons Range sharding leads to a number of issues in practice at scale, some of which are similar to that of linear hash sharding.","title":"Google Spanner and HBase \u2013 Range Sharding"},{"location":"Distributed-data-store/Distributed-database/YugabyteDB/blog-4-Data-Sharding-Strategies-in-Building-Distributed-SQL-Database/#putting#it#all#together","text":"","title":"Putting it all together"},{"location":"Distributed-data-store/Distributed-database/YugabyteDB/blog-4-Data-Sharding-Strategies-in-Building-Distributed-SQL-Database/#support#for#consistent#hash#and#range#sharding","text":"Because all of the above properties are desirable, we decided to include both consistent hash and range sharding in YugabyteDB.","title":"Support for consistent hash and range sharding"},{"location":"Distributed-data-store/Distributed-hash-table/","text":"Distributed hash table \u5b58\u5728\u4e24\u6b21map/hash 1\u3001\u6839\u636ekey\u627e\u5230server\uff0c\u56e0\u6b64\u9700\u8981\u56e0\u6b64Keyspace partitioning\uff0c\u540e\u9762\u4f1a\u5bf9\u6b64\u8fdb\u884c\u8bf4\u660e 2\u3001\u5728\u5bf9\u5e94\u7684server\u4e2d\uff0c\u6839\u636ekey\u627e\u5230value Rehash of keyspace \u5728 distributed hash table \u4e2d\uff0c\u5bf9 keyspace \u7684 rehash \u4e0d\u662f\u548c\u666e\u901a\u7684hash table\u90a3\u6837\uff0c\u7531\u4e8eload factor\u800c\u5f15\u8d77\u7684\uff0c\u5728 distributed hash table \u4e2d\uff0c\u5bf9 keyspace \u7684 rehash \u662f\u7531\u4e8e cluster \u4e2d node \u7684 \u589e\u52a0\u3001\u9000\u51fa \u800c\u5f15\u8d77\u7684\u3002 wikipedia Distributed hash table NOTE : redis cluster\u53ef\u4ee5\u770b\u505a\u662f Distributed hash table \uff1b A distributed hash table ( DHT ) is a class of a decentralized distributed system that provides a lookup service similar to a hash table : ( key , value ) pairs are stored in a DHT, and any participating node can efficiently retrieve the value associated with a given key. Keys are unique identifiers which map to particular values , which in turn can be anything from addresses, to documents , to arbitrary data .[ 1] Responsibility for maintaining the mapping from keys to values is distributed among the nodes, in such a way that a change in the set of participants causes a minimal amount of disruption(\u4e2d\u65ad). This allows a DHT to scale to extremely large numbers of nodes and to handle continual node arrivals, departures, and failures. DHTs form an infrastructure that can be used to build more complex services, such as anycast , cooperative Web caching , distributed file systems , domain name services , instant messaging , multicast , and also peer-to-peer file sharing and content distribution systems. Notable distributed networks that use DHTs include BitTorrent 's distributed tracker, the Coral Content Distribution Network , the Kad network , the Storm botnet , the Tox instant messenger , Freenet , the YaCy search engine, and the InterPlanetary File System . Distributed hash tables History DHT research was originally motivated, in part, by peer-to-peer systems such as Freenet , gnutella , BitTorrent and Napster , which took advantage of resources distributed across the Internet to provide a single useful application. In particular, they took advantage of increased bandwidth and hard disk capacity to provide a file-sharing service.[ 2] NOTE: DHT\u7814\u7a76\u6700\u521d\u7684\u52a8\u673a\u90e8\u5206\u662f\u7531Freenet\uff0cgnutella\uff0cBitTorrent\u548cNapster\u7b49\u5bf9\u7b49\u7cfb\u7edf\u63a8\u52a8\u7684\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u5229\u7528\u5206\u5e03\u5728Internet\u4e0a\u7684\u8d44\u6e90\u6765\u63d0\u4f9b\u5355\u4e00\u6709\u7528\u7684\u5e94\u7528\u7a0b\u5e8f\u3002\u7279\u522b\u662f\uff0c\u4ed6\u4eec\u5229\u7528\u589e\u52a0\u7684\u5e26\u5bbd\u548c\u786c\u76d8\u5bb9\u91cf\u6765\u63d0\u4f9b\u6587\u4ef6\u5171\u4eab\u670d\u52a1\u3002 These systems differed in how they located the data offered by their peers. Napster, the first large-scale P2P content delivery system, required a central index server : each node, upon joining, would send a list of locally held files to the server, which would perform searches and refer the queries to the nodes that held the results. This central component left the system vulnerable to attacks and lawsuits. NOTE: \u8fd9\u4e9b\u7cfb\u7edf\u5728\u5b9a\u4f4d\u540c\u884c\u63d0\u4f9b\u7684\u6570\u636e\u65b9\u9762\u5b58\u5728\u5dee\u5f02\u3002 Napster\u662f\u7b2c\u4e00\u4e2a\u5927\u89c4\u6a21P2P\u5185\u5bb9\u4f20\u9001\u7cfb\u7edf\uff0c\u9700\u8981\u4e00\u4e2a\u4e2d\u592e\u7d22\u5f15\u670d\u52a1\u5668\uff1a\u6bcf\u4e2a\u8282\u70b9\u5728\u52a0\u5165\u65f6\uff0c\u4f1a\u5411\u670d\u52a1\u5668\u53d1\u9001\u4e00\u4e2a\u672c\u5730\u4fdd\u5b58\u6587\u4ef6\u5217\u8868\uff0c\u8fd9\u5c06\u6267\u884c\u641c\u7d22\u5e76\u5c06\u67e5\u8be2\u5f15\u7528\u5230\u6301\u6709\u7ed3\u679c\u3002\u8fd9\u4e00\u6838\u5fc3\u7ec4\u4ef6\u4f7f\u7cfb\u7edf\u5bb9\u6613\u53d7\u5230\u653b\u51fb\u548c\u8bc9\u8bbc Gnutella and similar networks moved to a flooding query model \u2013 in essence, each search would result in a message being broadcast to every other machine in the network. While avoiding a single point of failure , this method was significantly less efficient than Napster. Later versions of Gnutella clients moved to a dynamic querying model which vastly improved efficiency.[ 3] NOTE: Gnutella\u548c\u7c7b\u4f3c\u7684\u7f51\u7edc\u8f6c\u79fb\u5230\u6d2a\u6cdb\u67e5\u8be2\u6a21\u578b - \u5b9e\u8d28\u4e0a\uff0c\u6bcf\u6b21\u641c\u7d22\u90fd\u4f1a\u5bfc\u81f4\u6d88\u606f\u88ab\u5e7f\u64ad\u5230\u7f51\u7edc\u4e2d\u7684\u6bcf\u53f0\u5176\u4ed6\u673a\u5668\u3002\u867d\u7136\u907f\u514d\u4e86\u5355\u70b9\u6545\u969c\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u7684\u6548\u7387\u660e\u663e\u4f4e\u4e8eNapster\u3002\u66f4\u9ad8\u7248\u672c\u7684Gnutella\u5ba2\u6237\u8f6c\u5411\u52a8\u6001\u67e5\u8be2\u6a21\u578b\uff0c\u5927\u5927\u63d0\u9ad8\u4e86\u6548\u7387\u3002 Freenet is fully distributed, but employs a heuristic key-based routing in which each file is associated with a key, and files with similar keys tend to cluster on a similar set of nodes. Queries are likely to be routed through the network to such a cluster without needing to visit many peers.[ 4] However, Freenet does not guarantee that data will be found. NOTE: Freenet\u662f\u5b8c\u5168\u5206\u5e03\u5f0f\u7684\uff0c\u4f46\u91c7\u7528\u57fa\u4e8e\u5bc6\u94a5\u7684\u542f\u53d1\u5f0f\u8def\u7531\uff0c\u5176\u4e2d\u6bcf\u4e2a\u6587\u4ef6\u4e0e\u5bc6\u94a5\u76f8\u5173\u8054\uff0c\u5177\u6709\u76f8\u4f3c\u5bc6\u94a5\u7684\u6587\u4ef6\u503e\u5411\u4e8e\u5728\u7c7b\u4f3c\u7684\u8282\u70b9\u96c6\u4e0a\u96c6\u7fa4\u3002\u67e5\u8be2\u5f88\u53ef\u80fd\u901a\u8fc7\u7f51\u7edc\u8def\u7531\u5230\u8fd9\u6837\u7684\u96c6\u7fa4\uff0c\u800c\u65e0\u9700\u8bbf\u95ee\u8bb8\u591a\u5bf9\u7b49\u4f53\u3002[4]\u4f46\u662f\uff0cFreenet\u4e0d\u4fdd\u8bc1\u4f1a\u627e\u5230\u6570\u636e\u3002 Distributed hash table**s use a more structured key-based routing in order to attain both the **decentralization of Freenet and gnutella, and the efficiency and guaranteed results of Napster. One drawback is that, like Freenet, DHTs only directly support exact-match search , rather than keyword search , although Freenet's routing algorithm can be generalized to any key type where a closeness operation can be defined.[ 5] NOTE: \u5206\u5e03\u5f0f\u54c8\u5e0c\u8868\u4f7f\u7528\u66f4\u52a0\u7ed3\u6784\u5316\u7684\u57fa\u4e8e\u5bc6\u94a5\u7684\u8def\u7531\uff0c\u4ee5\u5b9e\u73b0Freenet\u548cgnutella\u7684\u5206\u6563\u5316\uff0c\u4ee5\u53caNapster\u7684\u6548\u7387\u548c\u4fdd\u8bc1\u7ed3\u679c\u3002 \u4e00\u4e2a\u7f3a\u70b9\u662f\uff0c\u50cfFreenet\u4e00\u6837\uff0cDHT\u53ea\u76f4\u63a5\u652f\u6301\u7cbe\u786e\u5339\u914d\u641c\u7d22\uff0c\u800c\u4e0d\u662f\u5173\u952e\u5b57\u641c\u7d22\uff0c\u5c3d\u7ba1Freenet\u7684\u8def\u7531\u7b97\u6cd5\u53ef\u4ee5\u63a8\u5e7f\u5230\u4efb\u4f55\u53ef\u4ee5\u5b9a\u4e49\u63a5\u8fd1\u5ea6\u64cd\u4f5c\u7684\u5bc6\u94a5\u7c7b\u578b\u3002 In 2001, four systems\u2014 CAN ,[ 6] Chord ,[ 7] Pastry , and Tapestry \u2014ignited\uff08\u6fc0\u8d77\uff09 DHTs as a popular research topic. A project called the Infrastructure for Resilient Internet Systems (Iris) was funded by a $12 million grant from the US National Science Foundation in 2002.[ 8] Researchers included Sylvia Ratnasamy , Ion Stoica , Hari Balakrishnan and Scott Shenker .[ 9] Outside academia, DHT technology has been adopted as a component of BitTorrent and in the Coral Content Distribution Network . NOTE: 2001\u5e74\uff0c\u56db\u4e2a\u7cfb\u7edf-CAN\uff0c[6] Chord\uff0c[7] Pastry\u548cTapestry-\u70b9\u71c3\u4e86DHT\u4f5c\u4e3a\u4e00\u4e2a\u70ed\u95e8\u7684\u7814\u7a76\u8bfe\u9898\u3002 \u4e00\u4e2a\u540d\u4e3a\u201c\u5f39\u6027\u4e92\u8054\u7f51\u7cfb\u7edf\u57fa\u7840\u8bbe\u65bd\u201d\uff08Iris\uff09\u7684\u9879\u76ee\u75312002\u5e74\u7f8e\u56fd\u56fd\u5bb6\u79d1\u5b66\u57fa\u91d1\u4f1a\u62e8\u6b3e1200\u4e07\u7f8e\u5143\u8d44\u52a9\u3002[8] \u7814\u7a76\u4eba\u5458\u5305\u62ecSylvia Ratnasamy\uff0cIon Stoica\uff0cHari Balakrishnan\u548cScott Shenker\u3002[9] \u5728\u5b66\u672f\u754c\u4e4b\u5916\uff0cDHT\u6280\u672f\u5df2\u88ab\u91c7\u7eb3\u4e3aBitTorrent\u548cCoral\u5185\u5bb9\u5206\u53d1\u7f51\u7edc\u7684\u4e00\u4e2a\u7ec4\u6210\u90e8\u5206\u3002 Properties DHTs characteristically emphasize the following properties: 1\u3001 Autonomy\uff08\u81ea\u6cbb\u7684\uff09 and decentralization : the nodes collectively form the system without any central coordination . 2\u3001 Fault tolerance : the system should be reliable (in some sense) even with nodes continuously joining, leaving, and failing. 3\u3001 Scalability : the system should function efficiently even with thousands or millions of nodes. NOTE: redis cluster\u4e5f\u5177\u6709\u4e0a\u8ff0\u7684property A key technique used to achieve these goals is that any one node needs to coordinate with only a few other nodes in the system \u2013 most commonly, O (log n ) of the $ n $ participants (see below) \u2013 so that only a limited amount of work needs to be done for each change in membership. Some DHT designs seek to be secure against malicious\uff08\u6076\u610f\u7684\uff09 participants[ 10] and to allow participants to remain anonymous , though this is less common than in many other peer-to-peer (especially file sharing ) systems; see anonymous P2P . Finally, DHTs must deal with more traditional distributed systems issues such as load balancing , data integrity , and performance (in particular, ensuring that operations such as routing and data storage or retrieval complete quickly). Structure The structure of a DHT can be decomposed into several main components.[ 11] [ 12] 1\u3001The foundation is an abstract keyspace , such as the set of 160-bit strings . 2\u3001A keyspace partitioning scheme splits ownership of this keyspace among the participating nodes. 3\u3001An overlay network then connects the nodes, allowing them to find the owner of any given key in the keyspace . \u4e24\u6b21hash/map Once these components are in place, a typical use of the DHT for storage and retrieval might proceed as follows. Suppose the keyspace is the set of 160-bit strings. To index a file with given filename and data in the DHT, the SHA-1 hash of filename is generated, producing a 160-bit key k , and a message put ( k, data ) is sent to any node participating in the DHT. The message is forwarded from node to node through the overlay network until it reaches the single node responsible for key k as specified by the keyspace partitioning . That node then stores the key and the data. Any other client can then retrieve the contents of the file by again hashing filename to produce k and asking any DHT node to find the data associated with k with a message get ( k ). The message will again be routed through the overlay to the node responsible for k , which will reply with the stored data . NOTE: 1\u3001\u4e0a\u8ff0\u662f\u4f7f\u7528 SHA-1 hash \u6765\u83b7\u5f97abstract keyspace\uff1b\u4f7f\u7528 Consistent hashing \u3001 Rendezvous hashing \u6765\u8fdb\u884ckeyspace partition\uff1b 2\u3001**redis cluster**\u662f\u5426\u5b58\u5728\u4e0a\u8ff0forward\u7684\u8fc7\u7a0b\uff1f The keyspace partitioning and overlay network components are described below with the goal of capturing the principal ideas common to most DHTs; many designs differ in the details. Keyspace partitioning Most DHTs use some variant of consistent hashing or rendezvous hashing to map keys to nodes. The two algorithms appear to have been devised independently and simultaneously to solve the distributed hash table problem. Both consistent hashing and rendezvous hashing have the essential property that removal or addition of one node changes only the set of keys owned by the nodes with adjacent IDs , and leaves all other nodes unaffected. Contrast this with a traditional hash table in which addition or removal of one bucket causes nearly the entire keyspace to be remapped. Since any change in ownership typically corresponds to bandwidth -intensive movement of objects stored in the DHT from one node to another, minimizing such reorganization is required to efficiently support high rates of churn (node arrival and failure). Consistent hashing Further information: Consistent hashing NOTE: \u53c2\u89c1 Consistent-hashing \u7ae0\u8282 Rendezvous hashing Further information: Rendezvous hashing NOTE: \u53c2\u89c1 Rendezvous-hashing \u7ae0\u8282 Overlay network Each node maintains a set of links to other nodes (its neighbors or routing table ). Together, these links form the overlay network . A node picks its neighbors according to a certain structure, called the network's topology . All DHT topologies share some variant of the most essential property: for any key k , each node either has a node ID that owns k or has a link to a node whose node ID is closer to k , in terms of the keyspace distance defined above. It is then easy to route a message to the owner of any key k using the following greedy algorithm (that is not necessarily globally optimal): at each step, forward the message to the neighbor whose ID is closest to k . When there is no such neighbor, then we must have arrived at the closest node, which is the owner of k as defined above. This style of routing is sometimes called key-based routing . NOTE: keyspace distance \u5728consistent hashing\u4e2d\u5b9a\u4e49\u7684\uff1b Beyond basic routing correctness, two important constraints on the topology are to guarantee that the maximum number of hops \uff08\u8df3\uff09 in any route (route length) is low, so that requests complete quickly; and that the maximum number of neighbors of any node (maximum node degree ) is low, so that maintenance overhead is not excessive. Of course, having shorter routes requires higher maximum degree . Some common choices for maximum degree and route length are as follows, where n is the number of nodes in the DHT, using Big O notation : Max. degree Max route length Used in Note $ O(1) $ $ O(n) $ Worst lookup lengths, with likely much slower lookups times $ O(\\log n) $ $ O(\\log n) $ Chord Kademlia Pastry Tapestry Most common, but not optimal (degree/route length). Chord is the most basic version, with Kademlia seeming the most popular optimized variant (should have improved average lookup) $ O(\\log n) $ $ O(\\log n/\\log(\\log n)) $ Koorde Likely would be more complex to implement, but lookups might be faster (have a lower worst case bound) $ O({\\sqrt {n}}) $ $ O(1) $ Worst local storage needs, with lots of communication after any node connects or disconnects Algorithms for overlay networks Aside from routing, there exist many algorithms that exploit the structure of the overlay network for sending a message to all nodes, or a subset of nodes, in a DHT.[ 16] These algorithms are used by applications to do overlay multicast , range queries, or to collect statistics. Two systems that are based on this approach are Structella,[ 17] which implements flooding and random walks on a Pastry overlay, and DQ-DHT,[ 18] which implements a dynamic querying search algorithm over a Chord network. DHT implementations Most notable differences encountered in practical instances of DHT implementations include at least the following: NOTE: \u672a\u9605\u8bfb","title":"Introduction"},{"location":"Distributed-data-store/Distributed-hash-table/#distributed#hash#table","text":"","title":"Distributed hash table"},{"location":"Distributed-data-store/Distributed-hash-table/#maphash","text":"1\u3001\u6839\u636ekey\u627e\u5230server\uff0c\u56e0\u6b64\u9700\u8981\u56e0\u6b64Keyspace partitioning\uff0c\u540e\u9762\u4f1a\u5bf9\u6b64\u8fdb\u884c\u8bf4\u660e 2\u3001\u5728\u5bf9\u5e94\u7684server\u4e2d\uff0c\u6839\u636ekey\u627e\u5230value","title":"\u5b58\u5728\u4e24\u6b21map/hash"},{"location":"Distributed-data-store/Distributed-hash-table/#rehash#of#keyspace","text":"\u5728 distributed hash table \u4e2d\uff0c\u5bf9 keyspace \u7684 rehash \u4e0d\u662f\u548c\u666e\u901a\u7684hash table\u90a3\u6837\uff0c\u7531\u4e8eload factor\u800c\u5f15\u8d77\u7684\uff0c\u5728 distributed hash table \u4e2d\uff0c\u5bf9 keyspace \u7684 rehash \u662f\u7531\u4e8e cluster \u4e2d node \u7684 \u589e\u52a0\u3001\u9000\u51fa \u800c\u5f15\u8d77\u7684\u3002","title":"Rehash of keyspace"},{"location":"Distributed-data-store/Distributed-hash-table/#wikipedia#distributed#hash#table","text":"NOTE : redis cluster\u53ef\u4ee5\u770b\u505a\u662f Distributed hash table \uff1b A distributed hash table ( DHT ) is a class of a decentralized distributed system that provides a lookup service similar to a hash table : ( key , value ) pairs are stored in a DHT, and any participating node can efficiently retrieve the value associated with a given key. Keys are unique identifiers which map to particular values , which in turn can be anything from addresses, to documents , to arbitrary data .[ 1] Responsibility for maintaining the mapping from keys to values is distributed among the nodes, in such a way that a change in the set of participants causes a minimal amount of disruption(\u4e2d\u65ad). This allows a DHT to scale to extremely large numbers of nodes and to handle continual node arrivals, departures, and failures. DHTs form an infrastructure that can be used to build more complex services, such as anycast , cooperative Web caching , distributed file systems , domain name services , instant messaging , multicast , and also peer-to-peer file sharing and content distribution systems. Notable distributed networks that use DHTs include BitTorrent 's distributed tracker, the Coral Content Distribution Network , the Kad network , the Storm botnet , the Tox instant messenger , Freenet , the YaCy search engine, and the InterPlanetary File System . Distributed hash tables","title":"wikipedia Distributed hash table"},{"location":"Distributed-data-store/Distributed-hash-table/#history","text":"DHT research was originally motivated, in part, by peer-to-peer systems such as Freenet , gnutella , BitTorrent and Napster , which took advantage of resources distributed across the Internet to provide a single useful application. In particular, they took advantage of increased bandwidth and hard disk capacity to provide a file-sharing service.[ 2] NOTE: DHT\u7814\u7a76\u6700\u521d\u7684\u52a8\u673a\u90e8\u5206\u662f\u7531Freenet\uff0cgnutella\uff0cBitTorrent\u548cNapster\u7b49\u5bf9\u7b49\u7cfb\u7edf\u63a8\u52a8\u7684\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u5229\u7528\u5206\u5e03\u5728Internet\u4e0a\u7684\u8d44\u6e90\u6765\u63d0\u4f9b\u5355\u4e00\u6709\u7528\u7684\u5e94\u7528\u7a0b\u5e8f\u3002\u7279\u522b\u662f\uff0c\u4ed6\u4eec\u5229\u7528\u589e\u52a0\u7684\u5e26\u5bbd\u548c\u786c\u76d8\u5bb9\u91cf\u6765\u63d0\u4f9b\u6587\u4ef6\u5171\u4eab\u670d\u52a1\u3002 These systems differed in how they located the data offered by their peers. Napster, the first large-scale P2P content delivery system, required a central index server : each node, upon joining, would send a list of locally held files to the server, which would perform searches and refer the queries to the nodes that held the results. This central component left the system vulnerable to attacks and lawsuits. NOTE: \u8fd9\u4e9b\u7cfb\u7edf\u5728\u5b9a\u4f4d\u540c\u884c\u63d0\u4f9b\u7684\u6570\u636e\u65b9\u9762\u5b58\u5728\u5dee\u5f02\u3002 Napster\u662f\u7b2c\u4e00\u4e2a\u5927\u89c4\u6a21P2P\u5185\u5bb9\u4f20\u9001\u7cfb\u7edf\uff0c\u9700\u8981\u4e00\u4e2a\u4e2d\u592e\u7d22\u5f15\u670d\u52a1\u5668\uff1a\u6bcf\u4e2a\u8282\u70b9\u5728\u52a0\u5165\u65f6\uff0c\u4f1a\u5411\u670d\u52a1\u5668\u53d1\u9001\u4e00\u4e2a\u672c\u5730\u4fdd\u5b58\u6587\u4ef6\u5217\u8868\uff0c\u8fd9\u5c06\u6267\u884c\u641c\u7d22\u5e76\u5c06\u67e5\u8be2\u5f15\u7528\u5230\u6301\u6709\u7ed3\u679c\u3002\u8fd9\u4e00\u6838\u5fc3\u7ec4\u4ef6\u4f7f\u7cfb\u7edf\u5bb9\u6613\u53d7\u5230\u653b\u51fb\u548c\u8bc9\u8bbc Gnutella and similar networks moved to a flooding query model \u2013 in essence, each search would result in a message being broadcast to every other machine in the network. While avoiding a single point of failure , this method was significantly less efficient than Napster. Later versions of Gnutella clients moved to a dynamic querying model which vastly improved efficiency.[ 3] NOTE: Gnutella\u548c\u7c7b\u4f3c\u7684\u7f51\u7edc\u8f6c\u79fb\u5230\u6d2a\u6cdb\u67e5\u8be2\u6a21\u578b - \u5b9e\u8d28\u4e0a\uff0c\u6bcf\u6b21\u641c\u7d22\u90fd\u4f1a\u5bfc\u81f4\u6d88\u606f\u88ab\u5e7f\u64ad\u5230\u7f51\u7edc\u4e2d\u7684\u6bcf\u53f0\u5176\u4ed6\u673a\u5668\u3002\u867d\u7136\u907f\u514d\u4e86\u5355\u70b9\u6545\u969c\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u7684\u6548\u7387\u660e\u663e\u4f4e\u4e8eNapster\u3002\u66f4\u9ad8\u7248\u672c\u7684Gnutella\u5ba2\u6237\u8f6c\u5411\u52a8\u6001\u67e5\u8be2\u6a21\u578b\uff0c\u5927\u5927\u63d0\u9ad8\u4e86\u6548\u7387\u3002 Freenet is fully distributed, but employs a heuristic key-based routing in which each file is associated with a key, and files with similar keys tend to cluster on a similar set of nodes. Queries are likely to be routed through the network to such a cluster without needing to visit many peers.[ 4] However, Freenet does not guarantee that data will be found. NOTE: Freenet\u662f\u5b8c\u5168\u5206\u5e03\u5f0f\u7684\uff0c\u4f46\u91c7\u7528\u57fa\u4e8e\u5bc6\u94a5\u7684\u542f\u53d1\u5f0f\u8def\u7531\uff0c\u5176\u4e2d\u6bcf\u4e2a\u6587\u4ef6\u4e0e\u5bc6\u94a5\u76f8\u5173\u8054\uff0c\u5177\u6709\u76f8\u4f3c\u5bc6\u94a5\u7684\u6587\u4ef6\u503e\u5411\u4e8e\u5728\u7c7b\u4f3c\u7684\u8282\u70b9\u96c6\u4e0a\u96c6\u7fa4\u3002\u67e5\u8be2\u5f88\u53ef\u80fd\u901a\u8fc7\u7f51\u7edc\u8def\u7531\u5230\u8fd9\u6837\u7684\u96c6\u7fa4\uff0c\u800c\u65e0\u9700\u8bbf\u95ee\u8bb8\u591a\u5bf9\u7b49\u4f53\u3002[4]\u4f46\u662f\uff0cFreenet\u4e0d\u4fdd\u8bc1\u4f1a\u627e\u5230\u6570\u636e\u3002 Distributed hash table**s use a more structured key-based routing in order to attain both the **decentralization of Freenet and gnutella, and the efficiency and guaranteed results of Napster. One drawback is that, like Freenet, DHTs only directly support exact-match search , rather than keyword search , although Freenet's routing algorithm can be generalized to any key type where a closeness operation can be defined.[ 5] NOTE: \u5206\u5e03\u5f0f\u54c8\u5e0c\u8868\u4f7f\u7528\u66f4\u52a0\u7ed3\u6784\u5316\u7684\u57fa\u4e8e\u5bc6\u94a5\u7684\u8def\u7531\uff0c\u4ee5\u5b9e\u73b0Freenet\u548cgnutella\u7684\u5206\u6563\u5316\uff0c\u4ee5\u53caNapster\u7684\u6548\u7387\u548c\u4fdd\u8bc1\u7ed3\u679c\u3002 \u4e00\u4e2a\u7f3a\u70b9\u662f\uff0c\u50cfFreenet\u4e00\u6837\uff0cDHT\u53ea\u76f4\u63a5\u652f\u6301\u7cbe\u786e\u5339\u914d\u641c\u7d22\uff0c\u800c\u4e0d\u662f\u5173\u952e\u5b57\u641c\u7d22\uff0c\u5c3d\u7ba1Freenet\u7684\u8def\u7531\u7b97\u6cd5\u53ef\u4ee5\u63a8\u5e7f\u5230\u4efb\u4f55\u53ef\u4ee5\u5b9a\u4e49\u63a5\u8fd1\u5ea6\u64cd\u4f5c\u7684\u5bc6\u94a5\u7c7b\u578b\u3002 In 2001, four systems\u2014 CAN ,[ 6] Chord ,[ 7] Pastry , and Tapestry \u2014ignited\uff08\u6fc0\u8d77\uff09 DHTs as a popular research topic. A project called the Infrastructure for Resilient Internet Systems (Iris) was funded by a $12 million grant from the US National Science Foundation in 2002.[ 8] Researchers included Sylvia Ratnasamy , Ion Stoica , Hari Balakrishnan and Scott Shenker .[ 9] Outside academia, DHT technology has been adopted as a component of BitTorrent and in the Coral Content Distribution Network . NOTE: 2001\u5e74\uff0c\u56db\u4e2a\u7cfb\u7edf-CAN\uff0c[6] Chord\uff0c[7] Pastry\u548cTapestry-\u70b9\u71c3\u4e86DHT\u4f5c\u4e3a\u4e00\u4e2a\u70ed\u95e8\u7684\u7814\u7a76\u8bfe\u9898\u3002 \u4e00\u4e2a\u540d\u4e3a\u201c\u5f39\u6027\u4e92\u8054\u7f51\u7cfb\u7edf\u57fa\u7840\u8bbe\u65bd\u201d\uff08Iris\uff09\u7684\u9879\u76ee\u75312002\u5e74\u7f8e\u56fd\u56fd\u5bb6\u79d1\u5b66\u57fa\u91d1\u4f1a\u62e8\u6b3e1200\u4e07\u7f8e\u5143\u8d44\u52a9\u3002[8] \u7814\u7a76\u4eba\u5458\u5305\u62ecSylvia Ratnasamy\uff0cIon Stoica\uff0cHari Balakrishnan\u548cScott Shenker\u3002[9] \u5728\u5b66\u672f\u754c\u4e4b\u5916\uff0cDHT\u6280\u672f\u5df2\u88ab\u91c7\u7eb3\u4e3aBitTorrent\u548cCoral\u5185\u5bb9\u5206\u53d1\u7f51\u7edc\u7684\u4e00\u4e2a\u7ec4\u6210\u90e8\u5206\u3002","title":"History"},{"location":"Distributed-data-store/Distributed-hash-table/#properties","text":"DHTs characteristically emphasize the following properties: 1\u3001 Autonomy\uff08\u81ea\u6cbb\u7684\uff09 and decentralization : the nodes collectively form the system without any central coordination . 2\u3001 Fault tolerance : the system should be reliable (in some sense) even with nodes continuously joining, leaving, and failing. 3\u3001 Scalability : the system should function efficiently even with thousands or millions of nodes. NOTE: redis cluster\u4e5f\u5177\u6709\u4e0a\u8ff0\u7684property A key technique used to achieve these goals is that any one node needs to coordinate with only a few other nodes in the system \u2013 most commonly, O (log n ) of the $ n $ participants (see below) \u2013 so that only a limited amount of work needs to be done for each change in membership. Some DHT designs seek to be secure against malicious\uff08\u6076\u610f\u7684\uff09 participants[ 10] and to allow participants to remain anonymous , though this is less common than in many other peer-to-peer (especially file sharing ) systems; see anonymous P2P . Finally, DHTs must deal with more traditional distributed systems issues such as load balancing , data integrity , and performance (in particular, ensuring that operations such as routing and data storage or retrieval complete quickly).","title":"Properties"},{"location":"Distributed-data-store/Distributed-hash-table/#structure","text":"The structure of a DHT can be decomposed into several main components.[ 11] [ 12] 1\u3001The foundation is an abstract keyspace , such as the set of 160-bit strings . 2\u3001A keyspace partitioning scheme splits ownership of this keyspace among the participating nodes. 3\u3001An overlay network then connects the nodes, allowing them to find the owner of any given key in the keyspace .","title":"Structure"},{"location":"Distributed-data-store/Distributed-hash-table/#hashmap","text":"Once these components are in place, a typical use of the DHT for storage and retrieval might proceed as follows. Suppose the keyspace is the set of 160-bit strings. To index a file with given filename and data in the DHT, the SHA-1 hash of filename is generated, producing a 160-bit key k , and a message put ( k, data ) is sent to any node participating in the DHT. The message is forwarded from node to node through the overlay network until it reaches the single node responsible for key k as specified by the keyspace partitioning . That node then stores the key and the data. Any other client can then retrieve the contents of the file by again hashing filename to produce k and asking any DHT node to find the data associated with k with a message get ( k ). The message will again be routed through the overlay to the node responsible for k , which will reply with the stored data . NOTE: 1\u3001\u4e0a\u8ff0\u662f\u4f7f\u7528 SHA-1 hash \u6765\u83b7\u5f97abstract keyspace\uff1b\u4f7f\u7528 Consistent hashing \u3001 Rendezvous hashing \u6765\u8fdb\u884ckeyspace partition\uff1b 2\u3001**redis cluster**\u662f\u5426\u5b58\u5728\u4e0a\u8ff0forward\u7684\u8fc7\u7a0b\uff1f The keyspace partitioning and overlay network components are described below with the goal of capturing the principal ideas common to most DHTs; many designs differ in the details.","title":"\u4e24\u6b21hash/map"},{"location":"Distributed-data-store/Distributed-hash-table/#keyspace#partitioning","text":"Most DHTs use some variant of consistent hashing or rendezvous hashing to map keys to nodes. The two algorithms appear to have been devised independently and simultaneously to solve the distributed hash table problem. Both consistent hashing and rendezvous hashing have the essential property that removal or addition of one node changes only the set of keys owned by the nodes with adjacent IDs , and leaves all other nodes unaffected. Contrast this with a traditional hash table in which addition or removal of one bucket causes nearly the entire keyspace to be remapped. Since any change in ownership typically corresponds to bandwidth -intensive movement of objects stored in the DHT from one node to another, minimizing such reorganization is required to efficiently support high rates of churn (node arrival and failure).","title":"Keyspace partitioning"},{"location":"Distributed-data-store/Distributed-hash-table/#consistent#hashing","text":"Further information: Consistent hashing NOTE: \u53c2\u89c1 Consistent-hashing \u7ae0\u8282","title":"Consistent hashing"},{"location":"Distributed-data-store/Distributed-hash-table/#rendezvous#hashing","text":"Further information: Rendezvous hashing NOTE: \u53c2\u89c1 Rendezvous-hashing \u7ae0\u8282","title":"Rendezvous hashing"},{"location":"Distributed-data-store/Distributed-hash-table/#overlay#network","text":"Each node maintains a set of links to other nodes (its neighbors or routing table ). Together, these links form the overlay network . A node picks its neighbors according to a certain structure, called the network's topology . All DHT topologies share some variant of the most essential property: for any key k , each node either has a node ID that owns k or has a link to a node whose node ID is closer to k , in terms of the keyspace distance defined above. It is then easy to route a message to the owner of any key k using the following greedy algorithm (that is not necessarily globally optimal): at each step, forward the message to the neighbor whose ID is closest to k . When there is no such neighbor, then we must have arrived at the closest node, which is the owner of k as defined above. This style of routing is sometimes called key-based routing . NOTE: keyspace distance \u5728consistent hashing\u4e2d\u5b9a\u4e49\u7684\uff1b Beyond basic routing correctness, two important constraints on the topology are to guarantee that the maximum number of hops \uff08\u8df3\uff09 in any route (route length) is low, so that requests complete quickly; and that the maximum number of neighbors of any node (maximum node degree ) is low, so that maintenance overhead is not excessive. Of course, having shorter routes requires higher maximum degree . Some common choices for maximum degree and route length are as follows, where n is the number of nodes in the DHT, using Big O notation : Max. degree Max route length Used in Note $ O(1) $ $ O(n) $ Worst lookup lengths, with likely much slower lookups times $ O(\\log n) $ $ O(\\log n) $ Chord Kademlia Pastry Tapestry Most common, but not optimal (degree/route length). Chord is the most basic version, with Kademlia seeming the most popular optimized variant (should have improved average lookup) $ O(\\log n) $ $ O(\\log n/\\log(\\log n)) $ Koorde Likely would be more complex to implement, but lookups might be faster (have a lower worst case bound) $ O({\\sqrt {n}}) $ $ O(1) $ Worst local storage needs, with lots of communication after any node connects or disconnects","title":"Overlay network"},{"location":"Distributed-data-store/Distributed-hash-table/#algorithms#for#overlay#networks","text":"Aside from routing, there exist many algorithms that exploit the structure of the overlay network for sending a message to all nodes, or a subset of nodes, in a DHT.[ 16] These algorithms are used by applications to do overlay multicast , range queries, or to collect statistics. Two systems that are based on this approach are Structella,[ 17] which implements flooding and random walks on a Pastry overlay, and DQ-DHT,[ 18] which implements a dynamic querying search algorithm over a Chord network.","title":"Algorithms for overlay networks"},{"location":"Distributed-data-store/Distributed-hash-table/#dht#implementations","text":"Most notable differences encountered in practical instances of DHT implementations include at least the following: NOTE: \u672a\u9605\u8bfb","title":"DHT implementations"},{"location":"Distributed-data-store/Distributed-hash-table/wikipedia-Distributed-cache/","text":"wikipedia Distributed cache In computing , a distributed cache is an extension of the traditional concept of cache used in a single locale . A distributed cache may span multiple servers so that it can grow in size and in transactional capacity. It is mainly used to store application data residing in database and web session data. The idea of distributed caching[ 1] has become feasible now because main memory has become very cheap and network cards have become very fast, with 1 Gbit now standard everywhere and 10 Gbit gaining traction. Also, a distributed cache works well on lower cost machines usually employed for web servers as opposed to database servers which require expensive hardware.[ 2] An emerging internet architecture known as Information-centric networking (ICN) is one of the best examples of a distributed cache network. The ICN is a network level solution hence the existing distributed network cache management schemes are not well suited for ICN.[ 3] In the supercomputer environment, distributed cache is typically implemented in the form of burst buffer .","title":"wikipedia-Distributed-cache"},{"location":"Distributed-data-store/Distributed-hash-table/wikipedia-Distributed-cache/#wikipedia#distributed#cache","text":"In computing , a distributed cache is an extension of the traditional concept of cache used in a single locale . A distributed cache may span multiple servers so that it can grow in size and in transactional capacity. It is mainly used to store application data residing in database and web session data. The idea of distributed caching[ 1] has become feasible now because main memory has become very cheap and network cards have become very fast, with 1 Gbit now standard everywhere and 10 Gbit gaining traction. Also, a distributed cache works well on lower cost machines usually employed for web servers as opposed to database servers which require expensive hardware.[ 2] An emerging internet architecture known as Information-centric networking (ICN) is one of the best examples of a distributed cache network. The ICN is a network level solution hence the existing distributed network cache management schemes are not well suited for ICN.[ 3] In the supercomputer environment, distributed cache is typically implemented in the form of burst buffer .","title":"wikipedia Distributed cache"},{"location":"Event-driven-concurrent-server/","text":"Event-driven model and concurrent server \u4e00\u3001\u5c06\u4e24\u8005\u653e\u5230\u540c\u4e00\u4e2a\u7ae0\u8282\u6765\u8fdb\u884c\u63cf\u8ff0\uff0c\u56e0\u4e3a\u4e24\u8005\u5b58\u5728\u7740\u4e00\u5b9a\u7684\u5173\u8054\u3002 \u4e8c\u3001\u7531\u4e8eevent-driven model\u975e\u5e38\u5f3a\u5927\uff0c\u56e0\u6b64concurrent server\u5176\u5b9e\u4e5f\u53ef\u4ee5\u4f7f\u7528 event-driven model \u6765\u8fdb\u884c\u63cf\u8ff0\uff0c\u56e0\u6b64 Concurrent-server \u7ae0\u8282\u7684\u5185\u5bb9\u548c Event-driven-model \u7ae0\u8282\u7684\u5185\u5bb9\u6709\u4e9b\u91cd\u590d\uff0c\u4f46\u662f\u4e24\u4e2a\u7ae0\u8282\u7684\u4fa7\u91cd\u70b9\u662f\u4e0d\u540c\u7684: 1\u3001 Concurrent-server \u7ae0\u8282\u7684\u91cd\u70b9\u5185\u5bb9\u5728concurrency \u4e09\u3001\u5f88\u591alibrary\uff0c\u90fd\u878d\u5408\u4e86\u4e24\u8005\uff0c\u6bd4\u5982 libuv \u3001 Netty time event && file event \u5927\u591a\u6570concurrent server\u9700\u8981\u5904\u7406: 1\u3001time event 2\u3001file event","title":"Introduction"},{"location":"Event-driven-concurrent-server/#event-driven#model#and#concurrent#server","text":"\u4e00\u3001\u5c06\u4e24\u8005\u653e\u5230\u540c\u4e00\u4e2a\u7ae0\u8282\u6765\u8fdb\u884c\u63cf\u8ff0\uff0c\u56e0\u4e3a\u4e24\u8005\u5b58\u5728\u7740\u4e00\u5b9a\u7684\u5173\u8054\u3002 \u4e8c\u3001\u7531\u4e8eevent-driven model\u975e\u5e38\u5f3a\u5927\uff0c\u56e0\u6b64concurrent server\u5176\u5b9e\u4e5f\u53ef\u4ee5\u4f7f\u7528 event-driven model \u6765\u8fdb\u884c\u63cf\u8ff0\uff0c\u56e0\u6b64 Concurrent-server \u7ae0\u8282\u7684\u5185\u5bb9\u548c Event-driven-model \u7ae0\u8282\u7684\u5185\u5bb9\u6709\u4e9b\u91cd\u590d\uff0c\u4f46\u662f\u4e24\u4e2a\u7ae0\u8282\u7684\u4fa7\u91cd\u70b9\u662f\u4e0d\u540c\u7684: 1\u3001 Concurrent-server \u7ae0\u8282\u7684\u91cd\u70b9\u5185\u5bb9\u5728concurrency \u4e09\u3001\u5f88\u591alibrary\uff0c\u90fd\u878d\u5408\u4e86\u4e24\u8005\uff0c\u6bd4\u5982 libuv \u3001 Netty","title":"Event-driven model and concurrent server"},{"location":"Event-driven-concurrent-server/#time#event#file#event","text":"\u5927\u591a\u6570concurrent server\u9700\u8981\u5904\u7406: 1\u3001time event 2\u3001file event","title":"time event &amp;&amp; file event"},{"location":"Event-driven-concurrent-server/Apache-Thrift/","text":"Apache Thrift \u5728\u9605\u8bfbcnblogs \u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u51e0\u79cd\u6570\u636e\u4ea4\u6362\u534f\u8bae \u65f6\uff0c\u5176\u4e2d\u6709\u5173\u4e8eThrift\u7684\u4ecb\u7ecd\u3002 wikipedia Apache Thrift Thrift is an interface definition language and binary communication protocol used for defining and creating services for numerous programming languages. It forms a remote procedure call framework (RPC) and was developed at Facebook for \" scalable cross-language services development \". It combines a software stack with a code generation engine to build cross-platform services which can connect applications written in a variety of languages and frameworks, including ActionScript, C, C++, C#, Cappuccino, Cocoa, Delphi, Erlang, Go, Haskell, Java, JavaScript, Objective-C, OCaml, Perl, PHP, Python, Ruby, Elixir, Rust, Smalltalk and Swift. It was developed at Facebook and it is now an open source project in the Apache Software Foundation. The implementation was described in an April 2007 technical paper released by Facebook, now hosted on Apache. NOTE: \u76f8\u6bd4\u4e8e Apache Thrift \u5b98\u7f51\u7684\u4ecb\u7ecd\uff0cWikipedia\u7684\u4ecb\u7ecd\u66f4\u52a0\u5168\u9762\u3002 Apache Thrift","title":"Introduction"},{"location":"Event-driven-concurrent-server/Apache-Thrift/#apache#thrift","text":"\u5728\u9605\u8bfbcnblogs \u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u51e0\u79cd\u6570\u636e\u4ea4\u6362\u534f\u8bae \u65f6\uff0c\u5176\u4e2d\u6709\u5173\u4e8eThrift\u7684\u4ecb\u7ecd\u3002","title":"Apache Thrift"},{"location":"Event-driven-concurrent-server/Apache-Thrift/#wikipedia#apache#thrift","text":"Thrift is an interface definition language and binary communication protocol used for defining and creating services for numerous programming languages. It forms a remote procedure call framework (RPC) and was developed at Facebook for \" scalable cross-language services development \". It combines a software stack with a code generation engine to build cross-platform services which can connect applications written in a variety of languages and frameworks, including ActionScript, C, C++, C#, Cappuccino, Cocoa, Delphi, Erlang, Go, Haskell, Java, JavaScript, Objective-C, OCaml, Perl, PHP, Python, Ruby, Elixir, Rust, Smalltalk and Swift. It was developed at Facebook and it is now an open source project in the Apache Software Foundation. The implementation was described in an April 2007 technical paper released by Facebook, now hosted on Apache. NOTE: \u76f8\u6bd4\u4e8e Apache Thrift \u5b98\u7f51\u7684\u4ecb\u7ecd\uff0cWikipedia\u7684\u4ecb\u7ecd\u66f4\u52a0\u5168\u9762\u3002","title":"wikipedia Apache Thrift"},{"location":"Event-driven-concurrent-server/Apache-Thrift/#apache#thrift_1","text":"","title":"Apache Thrift"},{"location":"Event-driven-concurrent-server/Apache-Thrift/cnblogs-thrift%E7%9A%84%E5%8E%9F%E7%90%86%E5%92%8C%E4%BD%BF%E7%94%A8/","text":"cnblogs Thrift \u7684\u539f\u7406\u548c\u4f7f\u7528","title":"Introduction"},{"location":"Event-driven-concurrent-server/Apache-Thrift/cnblogs-thrift%E7%9A%84%E5%8E%9F%E7%90%86%E5%92%8C%E4%BD%BF%E7%94%A8/#cnblogs#thrift","text":"","title":"cnblogs Thrift \u7684\u539f\u7406\u548c\u4f7f\u7528"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/","text":"apache Thrift: Scalable Cross-Language Services Implementation NOTE: \u672a\u9605\u8bfb\u5b8c\u6210 Abstract Thrift is a software library and set of code-generation tools developed at Facebook to expedite(\u4fc3\u8fdb) development and implementation of efficient and scalable backend services . Its primary goal is to enable efficient and reliable communication across programming languages by abstracting the portions of each language that tend to require the most customization into a common library that is implemented in each language. Specifically, Thrift allows developers to define datatypes and service interfaces in a single language-neutral file and generate all the necessary code to build RPC clients and servers. This paper details the motivations and design choices we made in Thrift, as well as some of the more interesting implementation details. It is not intended to be taken as research, but rather it is an exposition on what we did and why. 1 Introduction As Facebook\u2019s traffic and network structure have scaled, the resource demands of many operations on the site (i.e. search, ad selection and delivery, event logging) have presented technical requirements drastically outside the scope of the LAMP framework. \u968f\u7740Facebook\u6d41\u91cf\u548c\u7f51\u7edc\u7ed3\u6784\u7684\u6269\u5927\uff0c\u7f51\u7ad9\u4e0a\u8bb8\u591a\u64cd\u4f5c(\u5982\u641c\u7d22\u3001\u5e7f\u544a\u9009\u62e9\u548c\u53d1\u5e03\u3001\u4e8b\u4ef6\u8bb0\u5f55)\u7684\u8d44\u6e90\u9700\u6c42\u5df2\u7ecf\u5927\u5927\u8d85\u51fa\u4e86LAMP\u6846\u67b6\u7684\u8303\u56f4\u3002 In our implementation of these services, various programming languages have been selected to optimize for the right combination of performance, ease and speed of development, availability of existing libraries, etc. By and large, Facebook\u2019s engineering culture has tended towards choosing the best tools and implementations available over standardizing on any one programming language and begrudgingly(\u541d\u556c\u7684) accepting its inherent limitations. Given this design choice, we were presented with the challenge of building a transparent, high-performance bridge across many programming languages. We found that most available solutions were either too limited, did not offer sufficient datatype freedom, or suffered from subpar(\u4f4e\u4e8e\u6807\u51c6\u7684) performance. 1 The solution that we have implemented combines a language-neutral software stack implemented across numerous programming languages and an associated code generation engine that transforms a simple interface and data definition language into client and server remote procedure call libraries. Choosing static code generation over a dynamic system allows us to create validated code that can be run without the need for any advanced introspective(\u5185\u7701\u7684) run-time type checking. It is also designed to be as simple as possible for the developer, who can typically define all the necessary data structures and interfaces for a complex service in a single short file. Surprised that a robust open solution to these relatively common problems did not yet exist, we committed early on to making the Thrift implementation open source. In evaluating the challenges of cross-language interaction in a networked environment, some key components were identified: Types Types . A common type system must exist across programming languages without requiring that the application developer use custom Thrift datatypes or write their own serialization code. That is, a C++ programmer should be able to transparently exchange a strongly typed STL map for a dynamic Python dictionary. Neither programmer should be forced to write any code below the application layer to achieve this. Section 2 details the Thrift type system. Transport Transport . Each language must have a common interface to bidirectional raw data transport. The specifics of how a given transport is implemented should not matter to the service developer. The same application code should be able to run against TCP stream sockets, raw data in memory, or files on disk. Section 3 details the Thrift Transport layer. Protocol Protocol . Datatypes must have some way of using the Transport layer to encode and decode themselves. Again, the application developer need not be concerned by this layer. Whether the service uses an XML or binary protocol is immaterial(\u900f\u660e\u7684) to the application code. All that matters is that the data can be read and written in a consistent, deterministic matter. Section 4 details the Thrift Protocol layer . Versioning Versioning . For robust services, the involved datatypes must provide a mechanism for versioning themselves. Specifically, it should be possible to add or remove fields in an object or alter the argument list of a function without any interruption in service (or, worse yet, nasty segmentation faults). Section 5 details Thrift\u2019s versioning system. Processors Processors . Finally, we generate code capable of processing data streams to accomplish remote procedure calls. Section 6 details the generated code and TProcessor paradigm . Section 7 discusses implementation details, and Section 8 describes our conclusions. 2 Types 2.1 Base Types 2.2 Structs 2.3 Containers 2.4 Exceptions 2.5 Services 3 Transport The transport layer is used by the generated code to facilitate data transfer. 3.1 Interface 3.2 Implementation 3.2.1 TSocket 3.2.2 TFileTransport 3.2.3 Utilities 4 Protocol A second major abstraction in Thrift is the separation of data structure from transport representation. Thrift enforces a certain messaging structure when transporting data, but it is agnostic to the protocol encoding in use. That is, it does not matter whether data is encoded as XML, human-readable ASCII, or a dense binary format as long as the data supports a fixed set of operations that allow it to be deterministically read and written by generated code. 7 Implementation Details 7.1 Target Languages 7.2 Generated Structs 7.3 RPC Method Identification Method calls in RPC are implemented by sending the method name as a string. 7.4 Servers and Multithreading 7.5 Thread Primitive void run () { { Synchronized s ( manager -> monitor ); if ( manager -> state == TimerManager :: STARTING ) { manager -> state = TimerManager :: STARTED ; manager -> monitor . notifyAll (); } } } 7.6 Thread, Runnable, and shared_ptr 7.7 ThreadManager ThreadManager creates a pool of worker threads and allows applications to schedule tasks for execution as free worker threads become available. 7.8 TimerManager TimerManager allows applications to schedule Runnable objects for execution at some point in the future. 7.10 Compiler The Thrift compiler is implemented in C++ using standard lex/yacc lexing and parsing. Though it could have been implemented with fewer lines of code in another language (i.e. Python Lex-Yacc (PLY) or ocamlyacc), using C++ forces explicit definition of the language constructs. Strongly typing the parse tree elements (debatably) makes the code more approachable for new developers. 7.11 TFileTransport The TFileWriterTransport uses a system of swapping in-memory buffers to ensure good performance while logging large amounts of data. NOTE: \u8fd9\u6bb5\u8bdd\u8981\u5982\u4f55\u7406\u89e3\uff1f 8\u3001 Facebook Thrift Services Thrift has been employed in a large number of applications at Facebook, including search, logging, mobile, ads and the developer platform. Two specific usages are discussed below. 8.1 Search Thrift is used as the underlying protocol and transport layer for the Facebook Search service. The multi-language code generation is well suited for search because it allows for application development in an efficient server side language (C++) and allows the Facebook PHP-based web application to make calls to the search service using Thrift PHP libraries. There is also a large variety of search stats, deployment and testing functionality that is built on top of generated Python code. Additionally, the Thrift log file format is used as a redo log for providing real-time search index updates. Thrift has allowed the search team to leverage each language for its strengths and to develop code at a rapid pace. NOTE: \u4e00\u3001\"Apache-thrift-cross and use multiple language-leverage strength\" 8.2 Logging 9 Conclusions Thrift has enabled Facebook to build scalable backend services efficiently by enabling engineers to divide and conquer. Application developers can focus on application code without worrying about the sockets layer . We avoid duplicated work by writing buffering and I/O logic in one place, rather than interspersing it in each application. NOTE: thrift\u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u8ba9programmer\u4e13\u6ce8\u4e8e\u4e1a\u52a1\u5f00\u53d1 Thrift has been employed in a wide variety of applications at Facebook, including search, logging, mobile, ads, and the developer platform. We have found that the marginal performance cost incurred by an extra layer of software abstraction is far eclipsed by the gains in developer efficiency and systems reliability. NOTE: tradeoff","title":"Introduction"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#apache#thrift#scalable#cross-language#services#implementation","text":"NOTE: \u672a\u9605\u8bfb\u5b8c\u6210","title":"apache Thrift: Scalable Cross-Language Services Implementation"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#abstract","text":"Thrift is a software library and set of code-generation tools developed at Facebook to expedite(\u4fc3\u8fdb) development and implementation of efficient and scalable backend services . Its primary goal is to enable efficient and reliable communication across programming languages by abstracting the portions of each language that tend to require the most customization into a common library that is implemented in each language. Specifically, Thrift allows developers to define datatypes and service interfaces in a single language-neutral file and generate all the necessary code to build RPC clients and servers. This paper details the motivations and design choices we made in Thrift, as well as some of the more interesting implementation details. It is not intended to be taken as research, but rather it is an exposition on what we did and why.","title":"Abstract"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#1#introduction","text":"As Facebook\u2019s traffic and network structure have scaled, the resource demands of many operations on the site (i.e. search, ad selection and delivery, event logging) have presented technical requirements drastically outside the scope of the LAMP framework. \u968f\u7740Facebook\u6d41\u91cf\u548c\u7f51\u7edc\u7ed3\u6784\u7684\u6269\u5927\uff0c\u7f51\u7ad9\u4e0a\u8bb8\u591a\u64cd\u4f5c(\u5982\u641c\u7d22\u3001\u5e7f\u544a\u9009\u62e9\u548c\u53d1\u5e03\u3001\u4e8b\u4ef6\u8bb0\u5f55)\u7684\u8d44\u6e90\u9700\u6c42\u5df2\u7ecf\u5927\u5927\u8d85\u51fa\u4e86LAMP\u6846\u67b6\u7684\u8303\u56f4\u3002 In our implementation of these services, various programming languages have been selected to optimize for the right combination of performance, ease and speed of development, availability of existing libraries, etc. By and large, Facebook\u2019s engineering culture has tended towards choosing the best tools and implementations available over standardizing on any one programming language and begrudgingly(\u541d\u556c\u7684) accepting its inherent limitations. Given this design choice, we were presented with the challenge of building a transparent, high-performance bridge across many programming languages. We found that most available solutions were either too limited, did not offer sufficient datatype freedom, or suffered from subpar(\u4f4e\u4e8e\u6807\u51c6\u7684) performance. 1 The solution that we have implemented combines a language-neutral software stack implemented across numerous programming languages and an associated code generation engine that transforms a simple interface and data definition language into client and server remote procedure call libraries. Choosing static code generation over a dynamic system allows us to create validated code that can be run without the need for any advanced introspective(\u5185\u7701\u7684) run-time type checking. It is also designed to be as simple as possible for the developer, who can typically define all the necessary data structures and interfaces for a complex service in a single short file. Surprised that a robust open solution to these relatively common problems did not yet exist, we committed early on to making the Thrift implementation open source. In evaluating the challenges of cross-language interaction in a networked environment, some key components were identified:","title":"1 Introduction"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#types","text":"Types . A common type system must exist across programming languages without requiring that the application developer use custom Thrift datatypes or write their own serialization code. That is, a C++ programmer should be able to transparently exchange a strongly typed STL map for a dynamic Python dictionary. Neither programmer should be forced to write any code below the application layer to achieve this. Section 2 details the Thrift type system.","title":"Types"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#transport","text":"Transport . Each language must have a common interface to bidirectional raw data transport. The specifics of how a given transport is implemented should not matter to the service developer. The same application code should be able to run against TCP stream sockets, raw data in memory, or files on disk. Section 3 details the Thrift Transport layer.","title":"Transport"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#protocol","text":"Protocol . Datatypes must have some way of using the Transport layer to encode and decode themselves. Again, the application developer need not be concerned by this layer. Whether the service uses an XML or binary protocol is immaterial(\u900f\u660e\u7684) to the application code. All that matters is that the data can be read and written in a consistent, deterministic matter. Section 4 details the Thrift Protocol layer .","title":"Protocol"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#versioning","text":"Versioning . For robust services, the involved datatypes must provide a mechanism for versioning themselves. Specifically, it should be possible to add or remove fields in an object or alter the argument list of a function without any interruption in service (or, worse yet, nasty segmentation faults). Section 5 details Thrift\u2019s versioning system.","title":"Versioning"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#processors","text":"Processors . Finally, we generate code capable of processing data streams to accomplish remote procedure calls. Section 6 details the generated code and TProcessor paradigm . Section 7 discusses implementation details, and Section 8 describes our conclusions.","title":"Processors"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#2#types","text":"","title":"2 Types"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#21#base#types","text":"","title":"2.1 Base Types"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#22#structs","text":"","title":"2.2 Structs"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#23#containers","text":"","title":"2.3 Containers"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#24#exceptions","text":"","title":"2.4 Exceptions"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#25#services","text":"","title":"2.5 Services"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#3#transport","text":"The transport layer is used by the generated code to facilitate data transfer.","title":"3 Transport"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#31#interface","text":"","title":"3.1 Interface"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#32#implementation","text":"","title":"3.2 Implementation"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#321#tsocket","text":"","title":"3.2.1 TSocket"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#322#tfiletransport","text":"","title":"3.2.2 TFileTransport"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#323#utilities","text":"","title":"3.2.3 Utilities"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#4#protocol","text":"A second major abstraction in Thrift is the separation of data structure from transport representation. Thrift enforces a certain messaging structure when transporting data, but it is agnostic to the protocol encoding in use. That is, it does not matter whether data is encoded as XML, human-readable ASCII, or a dense binary format as long as the data supports a fixed set of operations that allow it to be deterministically read and written by generated code.","title":"4 Protocol"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#7#implementation#details","text":"","title":"7 Implementation Details"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#71#target#languages","text":"","title":"7.1 Target Languages"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#72#generated#structs","text":"","title":"7.2 Generated Structs"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#73#rpc#method#identification","text":"Method calls in RPC are implemented by sending the method name as a string.","title":"7.3 RPC Method Identification"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#74#servers#and#multithreading","text":"","title":"7.4 Servers and Multithreading"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#75#thread#primitive","text":"void run () { { Synchronized s ( manager -> monitor ); if ( manager -> state == TimerManager :: STARTING ) { manager -> state = TimerManager :: STARTED ; manager -> monitor . notifyAll (); } } }","title":"7.5 Thread Primitive"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#76#thread#runnable#and#shared_ptr","text":"","title":"7.6 Thread, Runnable, and shared_ptr"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#77#threadmanager","text":"ThreadManager creates a pool of worker threads and allows applications to schedule tasks for execution as free worker threads become available.","title":"7.7 ThreadManager"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#78#timermanager","text":"TimerManager allows applications to schedule Runnable objects for execution at some point in the future.","title":"7.8 TimerManager"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#710#compiler","text":"The Thrift compiler is implemented in C++ using standard lex/yacc lexing and parsing. Though it could have been implemented with fewer lines of code in another language (i.e. Python Lex-Yacc (PLY) or ocamlyacc), using C++ forces explicit definition of the language constructs. Strongly typing the parse tree elements (debatably) makes the code more approachable for new developers.","title":"7.10 Compiler"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#711#tfiletransport","text":"The TFileWriterTransport uses a system of swapping in-memory buffers to ensure good performance while logging large amounts of data. NOTE: \u8fd9\u6bb5\u8bdd\u8981\u5982\u4f55\u7406\u89e3\uff1f","title":"7.11 TFileTransport"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#8#facebook#thrift#services","text":"Thrift has been employed in a large number of applications at Facebook, including search, logging, mobile, ads and the developer platform. Two specific usages are discussed below.","title":"8\u3001 Facebook Thrift Services"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#81#search","text":"Thrift is used as the underlying protocol and transport layer for the Facebook Search service. The multi-language code generation is well suited for search because it allows for application development in an efficient server side language (C++) and allows the Facebook PHP-based web application to make calls to the search service using Thrift PHP libraries. There is also a large variety of search stats, deployment and testing functionality that is built on top of generated Python code. Additionally, the Thrift log file format is used as a redo log for providing real-time search index updates. Thrift has allowed the search team to leverage each language for its strengths and to develop code at a rapid pace. NOTE: \u4e00\u3001\"Apache-thrift-cross and use multiple language-leverage strength\"","title":"8.1 Search"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#82#logging","text":"","title":"8.2 Logging"},{"location":"Event-driven-concurrent-server/Apache-Thrift/paper-Thrift-Scalable-Cross-Language-Services-Implementation/#9#conclusions","text":"Thrift has enabled Facebook to build scalable backend services efficiently by enabling engineers to divide and conquer. Application developers can focus on application code without worrying about the sockets layer . We avoid duplicated work by writing buffering and I/O logic in one place, rather than interspersing it in each application. NOTE: thrift\u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u8ba9programmer\u4e13\u6ce8\u4e8e\u4e1a\u52a1\u5f00\u53d1 Thrift has been employed in a wide variety of applications at Facebook, including search, logging, mobile, ads, and the developer platform. We have found that the marginal performance cost incurred by an extra layer of software abstraction is far eclipsed by the gains in developer efficiency and systems reliability. NOTE: tradeoff","title":"9 Conclusions"},{"location":"Event-driven-concurrent-server/Apache-Thrift/zhihu-Thrift%E7%9A%84%E9%82%A3%E4%BA%9B%E6%9C%8D%E5%8A%A1%E6%A8%A1%E5%9E%8B/","text":"zhihu \u3010Thrift\u3011Thrift\u7684\u90a3\u4e9b\u670d\u52a1\u6a21\u578b","title":"Introduction"},{"location":"Event-driven-concurrent-server/Apache-Thrift/zhihu-Thrift%E7%9A%84%E9%82%A3%E4%BA%9B%E6%9C%8D%E5%8A%A1%E6%A8%A1%E5%9E%8B/#zhihu#thriftthrift","text":"","title":"zhihu \u3010Thrift\u3011Thrift\u7684\u90a3\u4e9b\u670d\u52a1\u6a21\u578b"},{"location":"Event-driven-concurrent-server/Concurrent-server-/","text":"\u5173\u4e8e\u672c\u7ae0 \u4e00\u3001\u672c\u7ae0\u8ba8\u8bbaconcurrent server\u76f8\u5173\u95ee\u9898\uff0c\u5305\u62ec: 1\u3001C10K\u95ee\u9898 2\u3001\u5982\u4f55\u5b9e\u73b0\u9ad8\u6548concurrent server 3\u3001process\u5185\u90e8\u5982\u4f55\u5bf9request\u3001message\u8fdb\u884c\u9ad8\u6548\u7684\u3001\u5e76\u53d1\u5730\u5904\u7406 4\u3001\u5982\u4f55\u8bbe\u8ba1thread model\uff0c\u4ee5\u5145\u5206\u53d1\u6325concurrency \u4e8c\u3001\u7531\u4e8eevent-driven model\u975e\u5e38\u5f3a\u5927\uff0c\u672c\u7ae0\u63cf\u8ff0\u7684concurrent server\u5176\u5b9e\u4e5f\u53ef\u4ee5\u4f7f\u7528 event-driven model \u6765\u8fdb\u884c\u63cf\u8ff0\uff0c\u5e76\u4e14\u672c\u7ae0\u7684\u5185\u5bb9\u548cevent-driven model\u7ae0\u8282\u7684\u5185\u5bb9\u6709\u4e9b\u91cd\u590d\uff0c\u4f46\u662f\u4e24\u4e2a\u7ae0\u8282\u7684\u4fa7\u91cd\u70b9\u662f\u4e0d\u540c\u7684: 1\u3001\u672c\u7ae0\u4fa7\u91cd\u70b9\u662f\u5728concurrency\u4e0a\uff0cevent-driven model\u7ae0\u8282\u7684\u4fa7\u91cd\u70b9\u662fmessage\u3001message passing\u4e0a\uff1b 2\u3001\u672c\u7ae0\u4fa7\u91cd\u70b9\u662fintra-process TODO \u4e0b\u9762\u662f\u6d89\u53ca\u8fd9\u4e2atopic\u7684\u975e\u5e38\u597d\u7684\u5185\u5bb9: https://eli.thegreenplace.net/tag/concurrency https://eli.thegreenplace.net/2017/concurrent-servers-part-1-introduction/ Nginx was created to solve the C10k problem.[ 1] [ 3] Load balancing (computing) Event-driven architecture Event-driven programming Reactor pattern","title":"Introduction"},{"location":"Event-driven-concurrent-server/Concurrent-server-/#_1","text":"\u4e00\u3001\u672c\u7ae0\u8ba8\u8bbaconcurrent server\u76f8\u5173\u95ee\u9898\uff0c\u5305\u62ec: 1\u3001C10K\u95ee\u9898 2\u3001\u5982\u4f55\u5b9e\u73b0\u9ad8\u6548concurrent server 3\u3001process\u5185\u90e8\u5982\u4f55\u5bf9request\u3001message\u8fdb\u884c\u9ad8\u6548\u7684\u3001\u5e76\u53d1\u5730\u5904\u7406 4\u3001\u5982\u4f55\u8bbe\u8ba1thread model\uff0c\u4ee5\u5145\u5206\u53d1\u6325concurrency \u4e8c\u3001\u7531\u4e8eevent-driven model\u975e\u5e38\u5f3a\u5927\uff0c\u672c\u7ae0\u63cf\u8ff0\u7684concurrent server\u5176\u5b9e\u4e5f\u53ef\u4ee5\u4f7f\u7528 event-driven model \u6765\u8fdb\u884c\u63cf\u8ff0\uff0c\u5e76\u4e14\u672c\u7ae0\u7684\u5185\u5bb9\u548cevent-driven model\u7ae0\u8282\u7684\u5185\u5bb9\u6709\u4e9b\u91cd\u590d\uff0c\u4f46\u662f\u4e24\u4e2a\u7ae0\u8282\u7684\u4fa7\u91cd\u70b9\u662f\u4e0d\u540c\u7684: 1\u3001\u672c\u7ae0\u4fa7\u91cd\u70b9\u662f\u5728concurrency\u4e0a\uff0cevent-driven model\u7ae0\u8282\u7684\u4fa7\u91cd\u70b9\u662fmessage\u3001message passing\u4e0a\uff1b 2\u3001\u672c\u7ae0\u4fa7\u91cd\u70b9\u662fintra-process","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Event-driven-concurrent-server/Concurrent-server-/#todo","text":"\u4e0b\u9762\u662f\u6d89\u53ca\u8fd9\u4e2atopic\u7684\u975e\u5e38\u597d\u7684\u5185\u5bb9: https://eli.thegreenplace.net/tag/concurrency https://eli.thegreenplace.net/2017/concurrent-servers-part-1-introduction/ Nginx was created to solve the C10k problem.[ 1] [ 3] Load balancing (computing) Event-driven architecture Event-driven programming Reactor pattern","title":"TODO"},{"location":"Event-driven-concurrent-server/Concurrent-server-/C10K-problem/","text":"C10k problem \u53c2\u8003\u6587\u7ae0: 1\u3001wikipedia C10k problem 2\u3001 libevent \u2013 an event notification library 3\u3001nick-black Fast UNIX Servers 4\u3001kegel The C10K problem \u8fd9\u662f\u6700\u65e9\u63d0\u51fa\u8fd9\u4e2a\u95ee\u9898\u7684\u3002 wikipedia C10k problem The C10k problem is the problem of optimising network sockets to handle a large number of clients at the same time.[ 1] The name C10k is a numeronym for concurrently handling ten thousand connections.[ 2] Note that concurrent connections are not the same as requests per second , though they are similar: handling many requests per second requires high throughput (processing them quickly), while high number of concurrent connections requires efficient scheduling of connections. In other words, handling many requests per second is concerned with the speed of handling requests, whereas a system capable of handling a high number of concurrent connections does not necessarily have to be a fast system, only one where each request will deterministically return a response within a (not necessarily fixed) finite amount of time. The problem of socket server optimisation has been studied because a number of factors must be considered to allow a web server to support many clients. This can involve a combination of operating system constraints and web server software limitations. According to the scope of services to be made available and the capabilities of the operating system as well as hardware considerations such as multi-processing capabilities, a multi-threading model or a single threading model can be preferred. Concurrently with this aspect, which involves considerations regarding memory management (usually operating system related), strategies implied relate to the very diverse aspects of the I/O management.[ 2]","title":"Introduction"},{"location":"Event-driven-concurrent-server/Concurrent-server-/C10K-problem/#c10k#problem","text":"\u53c2\u8003\u6587\u7ae0: 1\u3001wikipedia C10k problem 2\u3001 libevent \u2013 an event notification library 3\u3001nick-black Fast UNIX Servers 4\u3001kegel The C10K problem \u8fd9\u662f\u6700\u65e9\u63d0\u51fa\u8fd9\u4e2a\u95ee\u9898\u7684\u3002","title":"C10k problem"},{"location":"Event-driven-concurrent-server/Concurrent-server-/C10K-problem/#wikipedia#c10k#problem","text":"The C10k problem is the problem of optimising network sockets to handle a large number of clients at the same time.[ 1] The name C10k is a numeronym for concurrently handling ten thousand connections.[ 2] Note that concurrent connections are not the same as requests per second , though they are similar: handling many requests per second requires high throughput (processing them quickly), while high number of concurrent connections requires efficient scheduling of connections. In other words, handling many requests per second is concerned with the speed of handling requests, whereas a system capable of handling a high number of concurrent connections does not necessarily have to be a fast system, only one where each request will deterministically return a response within a (not necessarily fixed) finite amount of time. The problem of socket server optimisation has been studied because a number of factors must be considered to allow a web server to support many clients. This can involve a combination of operating system constraints and web server software limitations. According to the scope of services to be made available and the capabilities of the operating system as well as hardware considerations such as multi-processing capabilities, a multi-threading model or a single threading model can be preferred. Concurrently with this aspect, which involves considerations regarding memory management (usually operating system related), strategies implied relate to the very diverse aspects of the I/O management.[ 2]","title":"wikipedia C10k problem"},{"location":"Event-driven-concurrent-server/Concurrent-server-/C10K-problem/kegel-Unix-The-C10K-problem/","text":"kegel The C10K problem [ Help save the best Linux news source on the web -- subscribe to Linux Weekly News! ] It's time for web servers to handle ten thousand clients simultaneously, don't you think? After all, the web is a big place now. And computers are big, too. You can buy a 1000MHz machine with 2 gigabytes of RAM and an 1000Mbit/sec Ethernet card for $1200 or so. Let's see - at 20000 clients, that's 50KHz , 100Kbytes , and 50Kbits/sec per client. It shouldn't take any more horsepower\uff08\u9a6c\u529b\uff09 than that to take four kilobytes from the disk and send them to the network once a second for each of twenty thousand clients. (That works out to $0.08 per client, by the way. Those $100 /client licensing fees some operating systems charge are starting to look a little heavy!) So hardware is no longer the bottleneck. NOTE: \u7535\u8111\u4e5f\u5f88\u5927\u3002 \u60a8\u53ef\u4ee5\u4ee51200\u7f8e\u5143\u7684\u4ef7\u683c\u8d2d\u4e70\u5e26\u67092 GB RAM\u548c1000Mbit / sec\u4ee5\u592a\u7f51\u5361\u76841000MHz\u673a\u5668\u3002 \u8ba9\u6211\u4eec\u770b\u770b - \u572820000\u4e2a\u5ba2\u6237\u7aef\uff0c\u6bcf\u4e2a\u5ba2\u6237\u7aef\u768450KHz\uff0c100Kbytes\u548c50Kbits / sec\u3002 \u5b83\u4e0d\u5e94\u8be5\u91c7\u53d6\u4efb\u4f55\u66f4\u591a\u7684\u9a6c\u529b\uff08\u9a6c\u529b\uff09\u4ece\u76d8\u4e2d\u53d6\u51fa4\u5343\u5b57\u8282\u5e76\u5c06\u5b83\u4eec\u6bcf\u79d2\u53d1\u9001\u5230\u7f51\u7edc\u4e00\u6b21\uff0c\u6bcf2\u4e07\u4e2a\u5ba2\u6237\u7aef\u3002 \uff08\u987a\u4fbf\u63d0\u4e00\u4e0b\uff0c\u6bcf\u4e2a\u5ba2\u6237\u7684\u8d39\u7528\u4e3a0.08\u7f8e\u5143\u3002\u4e00\u4e9b\u64cd\u4f5c\u7cfb\u7edf\u6536\u8d39\u7684100\u7f8e\u5143/\u5ba2\u6237\u8bb8\u53ef\u8d39\u7528\u5f00\u59cb\u53d8\u5f97\u6709\u70b9\u6c89\u91cd\uff01\uff09\u56e0\u6b64\u786c\u4ef6\u4e0d\u518d\u662f\u74f6\u9888\u3002 In 1999 one of the busiest ftp sites, cdrom.com, actually handled 10000 clients simultaneously through a Gigabit Ethernet pipe. As of 2001, that same speed is now being offered by several ISPs , who expect it to become increasingly popular with large business customers. And the thin client model of computing appears to be coming back in style -- this time with the server out on the Internet, serving thousands of clients. With that in mind, here are a few notes on how to configure operating systems and write code to support thousands of clients. The discussion centers around Unix-like operating systems, as that's my personal area of interest, but Windows is also covered a bit. Related Sites See Nick Black's excellent Fast UNIX Servers In October 2003, Felix von Leitner put together an excellent web page and presentation about network scalability , complete with benchmarks comparing various networking system calls and operating systems. One of his observations is that the 2.6 Linux kernel really does beat the 2.4 kernel, but there are many, many good graphs that will give the OS developers food for thought for some time. (See also the Slashdot comments; it'll be interesting to see whether anyone does followup benchmarks improving on Felix's results.) NOTE: 2003\u5e7410\u6708\uff0cFelix von Leitner\u6574\u7406\u4e86\u4e00\u4e2a\u5173\u4e8e\u7f51\u7edc\u53ef\u6269\u5c55\u6027\u7684\u4f18\u79c0\u7f51\u9875\u548c\u6f14\u793a\u6587\u7a3f\uff0c\u5176\u4e2d\u5305\u62ec\u6bd4\u8f83\u5404\u79cd\u7f51\u7edc\u7cfb\u7edf\u8c03\u7528\u548c\u64cd\u4f5c\u7cfb\u7edf\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002 \u4ed6\u7684\u4e00\u4e2a\u89c2\u5bdf\u7ed3\u679c\u662f2.6 Linux\u5185\u6838\u786e\u5b9e\u51fb\u8d25\u4e862.4\u5185\u6838\uff0c\u4f46\u662f\u6709\u5f88\u591a\u5f88\u597d\u7684\u56fe\u8868\u53ef\u4ee5\u8ba9\u64cd\u4f5c\u7cfb\u7edf\u5f00\u53d1\u4eba\u5458\u5728\u4e00\u6bb5\u65f6\u95f4\u5185\u6df1\u601d\u719f\u8651\u3002 \uff08\u53e6\u8bf7\u53c2\u9605Slashdot\u7684\u8bc4\u8bba;\u770b\u770b\u662f\u5426\u6709\u4eba\u4f1a\u6839\u636eFelix\u7684\u7ed3\u679c\u6539\u8fdb\u540e\u7eed\u57fa\u51c6\uff0c\u8fd9\u5c06\u4f1a\u5f88\u6709\u8da3\u3002\uff09 Book to Read First If you haven't read it already, go out and get a copy of Unix Network Programming : Networking Apis: Sockets and Xti (Volume 1) by the late W. Richard Stevens. It describes many of the I/O strategies and pitfalls related to writing high-performance servers. It even talks about the 'thundering herd' problem. And while you're at it, go read Jeff Darcy's notes on high-performance server design . (Another book which might be more helpful for those who are using rather than writing a web server is Building Scalable Web Sites by Cal Henderson.) I/O frameworks Prepackaged libraries are available that abstract some of the techniques presented below, insulating your code from the operating system and making it more portable. 1\u3001 ACE , a heavyweight C++ I/O framework, contains object-oriented implementations of some of these I/O strategies and many other useful things. In particular, his Reactor is an OO way of doing nonblocking I/O, and Proactor is an OO way of doing asynchronous I/O. 2\u3001 ASIO is an C++ I/O framework which is becoming part of the Boost library. It's like ACE updated for the STL era. NOTE: ASIO\u662fasynchronous IO\uff0c\u5b83\u6240\u91c7\u7528\u7684\u662fProactor\u6a21\u5f0f\uff0c\u53c2\u770b\u5176\u6587\u6863\uff1a Proactor and Boost.Asio 3\u3001 libevent is a lightweight C I/O framework by Niels Provos. It supports kqueue and select , and soon will support poll and epoll . It's level-triggered only, I think, which has both good and bad sides. Niels has a nice graph of time to handle one event as a function of the number of connections. It shows kqueue and sys_epoll as clear winners. 4\u3001My own attempts at lightweight frameworks (sadly, not kept up to date): Poller is a lightweight C++ I/O framework that implements a level-triggered readiness API using whatever underlying readiness API you want ( poll , select , /dev/poll , kqueue , or sigio ). It's useful for benchmarks that compare the performance of the various APIs. This document links to Poller subclasses below to illustrate how each of the readiness APIs can be used. rn is a lightweight C I/O framework that was my second try after Poller. It's lgpl (so it's easier to use in commercial apps) and C (so it's easier to use in non-C++ apps). It was used in some commercial products. 6\u3001Matt Welsh wrote a paper in April 2000 about how to balance the use of worker thread and event-driven techniques when building scalable servers. The paper describes part of his Sandstorm I/O framework. 7\u3001 Cory Nelson's Scale! library - an async socket, file, and pipe I/O library for Windows I/O Strategies Designers of networking software have many options. Here are a few: 1\u3001Whether and how to issue multiple I/O calls from a single thread\uff08\u5355\u7ebf\u7a0b\u4e2d\u5982\u4f55issue\u591a\u4e2aI/O\u8c03\u7528\uff09 1.1\u3001Don't; use blocking/synchronous calls throughout, and possibly use multiple threads or processes to achieve concurrency NOTE : \u8fd9\u53e5\u8bdd\u7684\u610f\u601d\u662f\u4e0d\u4f7f\u7528single thread\uff0c\u800c\u662f\u4f7f\u7528multiple thread\u6216multiple process\u6765\u5b8c\u6210concurrency\u3002 1.2\u3001Use nonblocking calls (e.g. write() on a socket set to O_NONBLOCK ) to start I/O, and readiness notification (e.g. poll() or /dev/poll ) to know when it's OK to start the next I/O on that channel. Generally only usable with network I/O, not disk I/O. 1.3\u3001Use asynchronous calls (e.g. aio_write() ) to start I/O, and completion notification (e.g. signals or completion ports) to know when the I/O finishes. Good for both network and disk I/O. NOTE: \u5176\u5b9e\u4e0a\u9762\u4e09\u6bb5\u6240\u63cf\u8ff0\u7684\u662f\u4e09\u79cdIO strategy\uff0c\u5728APUE\u7684chapter 14 advanced I/O\u4e2d\u90fd\u6709\u63cf\u8ff0\uff1b 2\u3001How to control the code servicing each client 2.1\u3001one process for each client (classic Unix approach, used since 1980 or so) 2.2\u3001one OS-level thread handles many clients; each client is controlled by: a user-level thread (e.g. GNU state threads, classic Java with green threads) a state machine (a bit esoteric, but popular in some circles; my favorite) a continuation (a bit esoteric, but popular in some circles) 2.3\u3001one OS-level thread for each client (e.g. classic Java with native threads) 2.4\u3001one OS-level thread for each active client (e.g. Tomcat with apache front end; NT completion ports; thread pools) 2.5\u3001Whether to use standard O/S services, or put some code into the kernel (e.g. in a custom driver, kernel module, or VxD) The following five combinations seem to be popular: Serve many clients with each thread, and use nonblocking I/O and level-triggered readiness notification Serve many clients with each thread, and use nonblocking I/O and readiness change notification Serve many clients with each server thread, and use asynchronous I/O serve one client with each server thread, and use blocking I/O Build the server code into the kernel 1. Serve many clients with each thread, and use nonblocking I/O and level-triggered readiness notification Serve many clients with each thread\u7684\u610f\u601d\u662f\uff1a\u4e3a\u6bcf\u4e2a\u7ebf\u7a0b\u63d0\u4f9b\u8bb8\u591a\u5ba2\u6237\u7aef\uff0c\u5b83\u5b9e\u9645\u4e0a\u6240\u6307\u4e3a\u4e00\u4e2athread\u5904\u7406\u591a\u4e2aclient\uff1b ... set nonblocking mode on all network handles , and use select() or poll() to tell which network handle has data waiting. This is the traditional favorite. With this scheme, the kernel tells you whether a file descriptor is ready, whether or not you've done anything with that file descriptor since the last time the kernel told you about it. (The name 'level triggered' comes from computer hardware design; it's the opposite of 'edge triggered' . Jonathon Lemon introduced the terms in his BSDCON 2000 paper on kqueue() .) NOTE: : \u8fd9\u544a\u8bc9\u4e86\u6211\u4eec\uff0c select \u548c poll \u53ea\u80fd\u591f\u4f7f\u7528level-triggered readiness notification\uff0c\u800c\u4e0d\u652f\u6301 'edge triggered' \u3002 Note: it's particularly important to remember that readiness notification from the kernel is only a hint; the file descriptor might not be ready anymore when you try to read from it. That's why it's important to use nonblocking mode when using readiness notification . An important bottleneck in this method is that read() or sendfile() from disk blocks if the page is not in core at the moment; setting nonblocking mode on a disk file handle has no effect. Same thing goes for memory-mapped disk files . The first time a server needs disk I/O, its process blocks , all clients must wait, and that raw nonthreaded performance goes to waste. NOTE: : \u5f53\u4ecedisk\u5728\u5c06\u6587\u4ef6 read() or sendfile() \u5230core\uff08\u5185\u5b58\uff09\u4e2d\u65f6\uff0c\u5373\u4f7f\u5728file handle\u4e0a\u8bbe\u7f6e\u4e86**nonblocking mode**\uff0c\u4ecd\u7136\u4f1ablock\uff1b This is what asynchronous I/O is for, but on systems that lack AIO , worker threads or processes that do the disk I/O can also get around this bottleneck. One approach is to use memory-mapped files , and if mincore() indicates I/O is needed, ask a worker to do the I/O, and continue handling network traffic. Jef Poskanzer mentions that Pai, Druschel, and Zwaenepoel's 1999 Flash web server uses this trick; they gave a talk at Usenix '99 on it. It looks like mincore() is available in BSD-derived Unixes like FreeBSD and Solaris, but is not part of the Single Unix Specification . It's available as part of Linux as of kernel 2.3.51, thanks to Chuck Lever . NOTE: : This is what asynchronous I/O is for, but on systems that lack AIO , worker threads or processes that do the disk I/O can also get around this bottleneck.\u7ffb\u8bd1\u662f\uff1a\u8fd9\u5c31\u662f\u5f02\u6b65I / O\u7684\u7528\u9014\uff0c\u4f46\u5728\u7f3a\u5c11AIO\u7684\u7cfb\u7edf\u4e0a\uff0c\u6267\u884c\u78c1\u76d8I / O\u7684\u5de5\u4f5c\u7ebf\u7a0b\u6216\u8fdb\u7a0b\u4e5f\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e2a\u74f6\u9888\u95ee\u9898\u3002\u5b83\u7684\u610f\u601d\u662f\u4f7f\u7528\u4e00\u4e2athread\u6216\u8005process\u6765\u6267\u884cIO\uff0c\u800c\u4e0d\u963b\u585e\u4e3b\u7ebf\u7a0b\uff1b But in November 2003 on the freebsd-hackers list, Vivek Pei et al reported very good results using system-wide profiling of their Flash web server to attack bottlenecks. One bottleneck they found was mincore (guess that wasn't such a good idea after all) Another was the fact that sendfile blocks on disk access; they improved performance by introducing a modified sendfile() that return something like EWOULDBLOCK when the disk page it's fetching is not yet in core. (Not sure how you tell the user the page is now resident... seems to me what's really needed here is aio_sendfile() .) The end result of their optimizations is a SpecWeb99 score of about 800 on a 1GHZ/1GB FreeBSD box, which is better than anything on file at spec.org. 2. Serve many clients with each thread, and use nonblocking I/O and readiness change notification Readiness change notification (or edge-triggered readiness notification ) means you give the kernel a file descriptor , and later, when that descriptor transitions from not ready to ready \uff0cthe kernel notifies you somehow. It then assumes you know the file descriptor is ready, and will not send any more readiness notifications of that type for that file descriptor until you do something that causes the file descriptor to no longer be ready (e.g. until you receive the EWOULDBLOCK error on a send , recv , or accept call, or a send or recv transfers less than the requested number of bytes). When you use readiness change notification , you must be prepared for spurious events , since one common implementation is to signal readiness whenever any packets are received, regardless of whether the file descriptor was already ready. This is the opposite of \" level-triggered \" readiness notification. It's a bit less forgiving of programming mistakes, since if you miss just one event, the connection that event was for gets stuck forever. Nevertheless, I have found that edge-triggered readiness notification made programming nonblocking clients with OpenSSL easier, so it's worth trying. [ Banga, Mogul, Drusha '99] described this kind of scheme in 1999. 3. Serve many clients with each server thread, and use asynchronous I/O This has not yet become popular in Unix, probably because few operating systems support asynchronous I/O, also possibly because it (like nonblocking I/O) requires rethinking your application. Under standard Unix, asynchronous I/O is provided by the aio_ interface (scroll down from that link to \"Asynchronous input and output\"), which associates a signal and value with each I/O operation . Signals and their values are queued and delivered efficiently to the user process. This is from the POSIX 1003.1b realtime extensions, and is also in the Single Unix Specification, version 2. AIO is normally used with edge-triggered completion notification , i.e. a signal is queued when the operation is complete. (It can also be used with level triggered completion notification by calling aio_suspend() , though I suspect few people do this.) glibc 2.1 and later provide a generic implementation written for standards compliance rather than performance. Ben LaHaise's implementation for Linux AIO was merged into the main Linux kernel as of 2.5.32. It doesn't use kernel threads, and has a very efficient underlying api, but (as of 2.6.0-test2) doesn't yet support sockets. (There is also an AIO patch for the 2.4 kernels, but the 2.5/2.6 implementation is somewhat different.) More info: The page \" Kernel Asynchronous I/O (AIO) Support for Linux \" which tries to tie together all info about the 2.6 kernel's implementation of AIO (posted 16 Sept 2003) Round 3: aio vs /dev/epoll by Benjamin C.R. LaHaise (presented at 2002 OLS) Asynchronous I/O Suport in Linux 2.5 , by Bhattacharya, Pratt, Pulaverty, and Morgan, IBM; presented at OLS '2003 Design Notes on Asynchronous I/O (aio) for Linux by Suparna Bhattacharya -- compares Ben's AIO with SGI's KAIO and a few other AIO projects Linux AIO home page - Ben's preliminary patches, mailing list, etc. linux-aio mailing list archives libaio-oracle - library implementing standard Posix AIO on top of libaio. First mentioned by Joel Becker on 18 Apr 2003 . Suparna also suggests having a look at the the DAFS API's approach to AIO Red Hat AS and Suse SLES both provide a high-performance implementation on the 2.4 kernel; it is related to, but not completely identical to, the 2.6 kernel implementation. In February 2006, a new attempt is being made to provide network AIO; see the note above about Evgeniy Polyakov's kevent-based AIO . In 1999, SGI implemented high-speed AIO for Linux . As of version 1.1, it's said to work well with both disk I/O and sockets. It seems to use kernel threads. It is still useful for people who can't wait for Ben's AIO to support sockets. The O'Reilly book POSIX.4: Programming for the Real World is said to include a good introduction to aio. A tutorial for the earlier, nonstandard, aio implementation on Solaris is online at Sunsite . It's probably worth a look, but keep in mind you'll need to mentally convert \"aioread\" to \"aio_read\", etc. Note that AIO doesn't provide a way to open files without blocking for disk I/O; if you care about the sleep caused by opening a disk file, Linus suggests you should simply do the open() in a different thread rather than wishing for an aio_open() system call. Under Windows, asynchronous I/O is associated with the terms \"Overlapped I/O\" and IOCP or \"I/O Completion Port\". Microsoft's IOCP combines techniques from the prior art like asynchronous I/O (like aio_write) and queued completion notification (like when using the aio_sigevent field with aio_write) with a new idea of holding back some requests to try to keep the number of running threads associated with a single IOCP constant. For more information, see Inside I/O Completion Ports by Mark Russinovich at sysinternals.com, Jeffrey Richter's book \"Programming Server-Side Applications for Microsoft Windows 2000\" ( Amazon , MSPress ), U.S. patent #06223207 , or MSDN . 4. Serve one client with each server thread ... and let read() and write() block. Has the disadvantage of using a whole stack frame for each client, which costs memory. Many OS's also have trouble handling more than a few hundred threads. If each thread gets a 2MB stack (not an uncommon default value), you run out of virtual memory at (2^30 / 2^21) = 512 threads on a 32 bit machine with 1GB user-accessible VM (like, say, Linux as normally shipped on x86). You can work around this by giving each thread a smaller stack, but since most thread libraries don't allow growing thread stacks once created, doing this means designing your program to minimize stack use. You can also work around this by moving to a 64 bit processor. \u5177\u6709\u4e3a\u6bcf\u4e2a\u5ba2\u6237\u7aef\u4f7f\u7528\u6574\u4e2a\u5806\u6808\u5e27\u7684\u7f3a\u70b9\uff0c\u8fd9\u4f1a\u82b1\u8d39\u5185\u5b58\u3002\u8bb8\u591a\u64cd\u4f5c\u7cfb\u7edf\u4e5f\u96be\u4ee5\u5904\u7406\u8d85\u8fc7\u51e0\u767e\u4e2a\u7ebf\u7a0b\u3002\u5982\u679c\u6bcf\u4e2a\u7ebf\u7a0b\u83b7\u5f972MB\u5806\u6808\uff08\u4e0d\u662f\u975e\u5e38\u89c1\u7684\u9ed8\u8ba4\u503c\uff09\uff0c\u5219\u572832\u4f4d\u673a\u5668\u4e0a\u7684\uff082 ^ 30/2 ^ 21\uff09= 512\u4e2a\u7ebf\u7a0b\u4e0a\u8017\u5c3d\u865a\u62df\u5185\u5b58\uff0c\u5e76\u4e14\u5177\u67091GB\u7528\u6237\u53ef\u8bbf\u95ee\u7684VM\uff08\u4f8b\u5982\uff0c Linux\u901a\u5e38\u5728x86\u4e0a\u53d1\u5e03\uff09\u3002\u60a8\u53ef\u4ee5\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u7ebf\u7a0b\u63d0\u4f9b\u66f4\u5c0f\u7684\u5806\u6808\u6765\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u4f46\u7531\u4e8e\u5927\u591a\u6570\u7ebf\u7a0b\u5e93\u5728\u521b\u5efa\u540e\u4e0d\u5141\u8bb8\u589e\u52a0\u7ebf\u7a0b\u5806\u6808\uff0c\u56e0\u6b64\u8fd9\u6837\u505a\u610f\u5473\u7740\u8bbe\u8ba1\u7a0b\u5e8f\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u5806\u6808\u4f7f\u7528\u3002\u60a8\u4e5f\u53ef\u4ee5\u901a\u8fc7\u8f6c\u79fb\u523064\u4f4d\u5904\u7406\u5668\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002 The thread support in Linux, FreeBSD, and Solaris is improving, and 64 bit processors are just around the corner even for mainstream users. Perhaps in the not-too-distant future, those who prefer using one thread per client will be able to use that paradigm even for 10000 clients. Nevertheless, at the current time, if you actually want to support that many clients, you're probably better off using some other paradigm. Linux\uff0cFreeBSD\u548cSolaris\u4e2d\u7684\u7ebf\u7a0b\u652f\u6301\u6b63\u5728\u6539\u8fdb\uff0c\u5373\u4f7f\u5bf9\u4e8e\u4e3b\u6d41\u7528\u6237\uff0c64\u4f4d\u5904\u7406\u5668\u4e5f\u5373\u5c06\u5230\u6765\u3002\u4e5f\u8bb8\u5728\u4e0d\u592a\u9065\u8fdc\u7684\u672a\u6765\uff0c\u90a3\u4e9b\u559c\u6b22\u6bcf\u4e2a\u5ba2\u6237\u7aef\u4f7f\u7528\u4e00\u4e2a\u7ebf\u7a0b\u7684\u4eba\u5373\u4f7f\u5bf910000\u4e2a\u5ba2\u6237\u7aef\u4e5f\u80fd\u591f\u4f7f\u7528\u8fd9\u4e2a\u8303\u4f8b\u3002\u7136\u800c\uff0c\u5728\u76ee\u524d\u8fd9\u4e2a\u65f6\u5019\uff0c\u5982\u679c\u4f60\u771f\u7684\u60f3\u8981\u652f\u6301\u90a3\u4e48\u591a\u5ba2\u6237\uff0c\u4f60\u53ef\u80fd\u6700\u597d\u8fd8\u662f\u4f7f\u7528\u5176\u4ed6\u4e00\u4e9b\u8303\u4f8b\u3002 For an unabashedly pro-thread viewpoint, see Why Events Are A Bad Idea (for High-concurrency Servers) by von Behren, Condit, and Brewer, UCB, presented at HotOS IX. Anyone from the anti-thread camp care to point out a paper that rebuts this one? :-) \u5bf9\u4e8e\u4e00\u4e2a\u6beb\u4e0d\u63a9\u9970\u7684\u4eb2\u7ebf\u7a0b\u89c2\u70b9\uff0c\u8bf7\u53c2\u9605HotOS IX\u4e0a\u7684von Behren\uff0cCondit\u548cBrewer\uff0cUCB\uff0c\u4e3a\u4ec0\u4e48\u4e8b\u4ef6\u662f\u4e00\u4e2a\u574f\u4e3b\u610f\uff08\u5bf9\u4e8e\u9ad8\u5e76\u53d1\u670d\u52a1\u5668\uff09\u3002\u53cd\u7ebf\u8425\u5730\u7684\u4efb\u4f55\u4eba\u90fd\u5728\u6307\u51fa\u4e00\u7bc7\u53cd\u9a73\u8fd9\u7bc7\u8bba\u6587\u7684\u8bba\u6587\u5417\uff1f :-) 5. Build the server code into the kernel Novell and Microsoft are both said to have done this at various times, at least one NFS implementation does this, khttpd does this for Linux and static web pages, and \"TUX\" (Threaded linUX webserver) is a blindingly fast and flexible kernel-space HTTP server by Ingo Molnar for Linux. Ingo's September 1, 2000 announcement says an alpha version of TUX can be downloaded from ftp://ftp.redhat.com/pub/redhat/tux , and explains how to join a mailing list for more info. The linux-kernel list has been discussing the pros and cons of this approach, and the consensus seems to be instead of moving web servers into the kernel, the kernel should have the smallest possible hooks added to improve web server performance. That way, other kinds of servers can benefit. See e.g. Zach Brown's remarks about userland vs. kernel http servers. It appears that the 2.4 linux kernel provides sufficient power to user programs, as the X15 server runs about as fast as Tux, but doesn't use any kernel modifications. Bring the TCP stack into userspace See for instance the netmap packet I/O framework, and the Sandstorm proof-of-concept web server based on it. Measuring Server Performance Examples Nginx is a web server that uses whatever high-efficiency network event mechanism is available on the target OS. It's getting popular; there are even about it (and since this page was originally written, many more, including a of that book.) thttpd Very simple. Uses a single process. It has good performance, but doesn't scale with the number of CPU's. Can also use kqueue. mathopd . Similar to thttpd. fhttpd boa Roxen Zeus , a commercial server that tries to be the absolute fastest. See their tuning guide . The other non-Java servers listed at http://www.acme.com/software/thttpd/benchmarks.html BetaFTPd Flash-Lite - web server using IO-Lite. Flash: An efficient and portable Web server -- uses select(), mmap(), mincore() The Flash web server as of 2003 -- uses select(), modified sendfile(), async open() xitami - uses select() to implement its own thread abstraction for portability to systems without threads. Medusa - a server-writing toolkit in Python that tries to deliver very high performance. userver - a small http server that can use select, poll, epoll, or sigio Interesting /dev/poll-based servers N. Provos, C. Lever , \"Scalable Network I/O in Linux,\" May, 2000. [FREENIX track, Proc. USENIX 2000, San Diego, California (June, 2000).] Describes a version of thttpd modified to support /dev/poll. Performance is compared with phhttpd. Interesting epoll-based servers ribs2 cmogstored - uses epoll/kqueue for most networking, threads for disk and accept4","title":"Introduction"},{"location":"Event-driven-concurrent-server/Concurrent-server-/C10K-problem/kegel-Unix-The-C10K-problem/#kegel#the#c10k#problem","text":"[ Help save the best Linux news source on the web -- subscribe to Linux Weekly News! ] It's time for web servers to handle ten thousand clients simultaneously, don't you think? After all, the web is a big place now. And computers are big, too. You can buy a 1000MHz machine with 2 gigabytes of RAM and an 1000Mbit/sec Ethernet card for $1200 or so. Let's see - at 20000 clients, that's 50KHz , 100Kbytes , and 50Kbits/sec per client. It shouldn't take any more horsepower\uff08\u9a6c\u529b\uff09 than that to take four kilobytes from the disk and send them to the network once a second for each of twenty thousand clients. (That works out to $0.08 per client, by the way. Those $100 /client licensing fees some operating systems charge are starting to look a little heavy!) So hardware is no longer the bottleneck. NOTE: \u7535\u8111\u4e5f\u5f88\u5927\u3002 \u60a8\u53ef\u4ee5\u4ee51200\u7f8e\u5143\u7684\u4ef7\u683c\u8d2d\u4e70\u5e26\u67092 GB RAM\u548c1000Mbit / sec\u4ee5\u592a\u7f51\u5361\u76841000MHz\u673a\u5668\u3002 \u8ba9\u6211\u4eec\u770b\u770b - \u572820000\u4e2a\u5ba2\u6237\u7aef\uff0c\u6bcf\u4e2a\u5ba2\u6237\u7aef\u768450KHz\uff0c100Kbytes\u548c50Kbits / sec\u3002 \u5b83\u4e0d\u5e94\u8be5\u91c7\u53d6\u4efb\u4f55\u66f4\u591a\u7684\u9a6c\u529b\uff08\u9a6c\u529b\uff09\u4ece\u76d8\u4e2d\u53d6\u51fa4\u5343\u5b57\u8282\u5e76\u5c06\u5b83\u4eec\u6bcf\u79d2\u53d1\u9001\u5230\u7f51\u7edc\u4e00\u6b21\uff0c\u6bcf2\u4e07\u4e2a\u5ba2\u6237\u7aef\u3002 \uff08\u987a\u4fbf\u63d0\u4e00\u4e0b\uff0c\u6bcf\u4e2a\u5ba2\u6237\u7684\u8d39\u7528\u4e3a0.08\u7f8e\u5143\u3002\u4e00\u4e9b\u64cd\u4f5c\u7cfb\u7edf\u6536\u8d39\u7684100\u7f8e\u5143/\u5ba2\u6237\u8bb8\u53ef\u8d39\u7528\u5f00\u59cb\u53d8\u5f97\u6709\u70b9\u6c89\u91cd\uff01\uff09\u56e0\u6b64\u786c\u4ef6\u4e0d\u518d\u662f\u74f6\u9888\u3002 In 1999 one of the busiest ftp sites, cdrom.com, actually handled 10000 clients simultaneously through a Gigabit Ethernet pipe. As of 2001, that same speed is now being offered by several ISPs , who expect it to become increasingly popular with large business customers. And the thin client model of computing appears to be coming back in style -- this time with the server out on the Internet, serving thousands of clients. With that in mind, here are a few notes on how to configure operating systems and write code to support thousands of clients. The discussion centers around Unix-like operating systems, as that's my personal area of interest, but Windows is also covered a bit.","title":"kegel The C10K problem"},{"location":"Event-driven-concurrent-server/Concurrent-server-/C10K-problem/kegel-Unix-The-C10K-problem/#related#sites","text":"See Nick Black's excellent Fast UNIX Servers In October 2003, Felix von Leitner put together an excellent web page and presentation about network scalability , complete with benchmarks comparing various networking system calls and operating systems. One of his observations is that the 2.6 Linux kernel really does beat the 2.4 kernel, but there are many, many good graphs that will give the OS developers food for thought for some time. (See also the Slashdot comments; it'll be interesting to see whether anyone does followup benchmarks improving on Felix's results.) NOTE: 2003\u5e7410\u6708\uff0cFelix von Leitner\u6574\u7406\u4e86\u4e00\u4e2a\u5173\u4e8e\u7f51\u7edc\u53ef\u6269\u5c55\u6027\u7684\u4f18\u79c0\u7f51\u9875\u548c\u6f14\u793a\u6587\u7a3f\uff0c\u5176\u4e2d\u5305\u62ec\u6bd4\u8f83\u5404\u79cd\u7f51\u7edc\u7cfb\u7edf\u8c03\u7528\u548c\u64cd\u4f5c\u7cfb\u7edf\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002 \u4ed6\u7684\u4e00\u4e2a\u89c2\u5bdf\u7ed3\u679c\u662f2.6 Linux\u5185\u6838\u786e\u5b9e\u51fb\u8d25\u4e862.4\u5185\u6838\uff0c\u4f46\u662f\u6709\u5f88\u591a\u5f88\u597d\u7684\u56fe\u8868\u53ef\u4ee5\u8ba9\u64cd\u4f5c\u7cfb\u7edf\u5f00\u53d1\u4eba\u5458\u5728\u4e00\u6bb5\u65f6\u95f4\u5185\u6df1\u601d\u719f\u8651\u3002 \uff08\u53e6\u8bf7\u53c2\u9605Slashdot\u7684\u8bc4\u8bba;\u770b\u770b\u662f\u5426\u6709\u4eba\u4f1a\u6839\u636eFelix\u7684\u7ed3\u679c\u6539\u8fdb\u540e\u7eed\u57fa\u51c6\uff0c\u8fd9\u5c06\u4f1a\u5f88\u6709\u8da3\u3002\uff09","title":"Related Sites"},{"location":"Event-driven-concurrent-server/Concurrent-server-/C10K-problem/kegel-Unix-The-C10K-problem/#book#to#read#first","text":"If you haven't read it already, go out and get a copy of Unix Network Programming : Networking Apis: Sockets and Xti (Volume 1) by the late W. Richard Stevens. It describes many of the I/O strategies and pitfalls related to writing high-performance servers. It even talks about the 'thundering herd' problem. And while you're at it, go read Jeff Darcy's notes on high-performance server design . (Another book which might be more helpful for those who are using rather than writing a web server is Building Scalable Web Sites by Cal Henderson.)","title":"Book to Read First"},{"location":"Event-driven-concurrent-server/Concurrent-server-/C10K-problem/kegel-Unix-The-C10K-problem/#io#frameworks","text":"Prepackaged libraries are available that abstract some of the techniques presented below, insulating your code from the operating system and making it more portable. 1\u3001 ACE , a heavyweight C++ I/O framework, contains object-oriented implementations of some of these I/O strategies and many other useful things. In particular, his Reactor is an OO way of doing nonblocking I/O, and Proactor is an OO way of doing asynchronous I/O. 2\u3001 ASIO is an C++ I/O framework which is becoming part of the Boost library. It's like ACE updated for the STL era. NOTE: ASIO\u662fasynchronous IO\uff0c\u5b83\u6240\u91c7\u7528\u7684\u662fProactor\u6a21\u5f0f\uff0c\u53c2\u770b\u5176\u6587\u6863\uff1a Proactor and Boost.Asio 3\u3001 libevent is a lightweight C I/O framework by Niels Provos. It supports kqueue and select , and soon will support poll and epoll . It's level-triggered only, I think, which has both good and bad sides. Niels has a nice graph of time to handle one event as a function of the number of connections. It shows kqueue and sys_epoll as clear winners. 4\u3001My own attempts at lightweight frameworks (sadly, not kept up to date): Poller is a lightweight C++ I/O framework that implements a level-triggered readiness API using whatever underlying readiness API you want ( poll , select , /dev/poll , kqueue , or sigio ). It's useful for benchmarks that compare the performance of the various APIs. This document links to Poller subclasses below to illustrate how each of the readiness APIs can be used. rn is a lightweight C I/O framework that was my second try after Poller. It's lgpl (so it's easier to use in commercial apps) and C (so it's easier to use in non-C++ apps). It was used in some commercial products. 6\u3001Matt Welsh wrote a paper in April 2000 about how to balance the use of worker thread and event-driven techniques when building scalable servers. The paper describes part of his Sandstorm I/O framework. 7\u3001 Cory Nelson's Scale! library - an async socket, file, and pipe I/O library for Windows","title":"I/O frameworks"},{"location":"Event-driven-concurrent-server/Concurrent-server-/C10K-problem/kegel-Unix-The-C10K-problem/#io#strategies","text":"Designers of networking software have many options. Here are a few: 1\u3001Whether and how to issue multiple I/O calls from a single thread\uff08\u5355\u7ebf\u7a0b\u4e2d\u5982\u4f55issue\u591a\u4e2aI/O\u8c03\u7528\uff09 1.1\u3001Don't; use blocking/synchronous calls throughout, and possibly use multiple threads or processes to achieve concurrency NOTE : \u8fd9\u53e5\u8bdd\u7684\u610f\u601d\u662f\u4e0d\u4f7f\u7528single thread\uff0c\u800c\u662f\u4f7f\u7528multiple thread\u6216multiple process\u6765\u5b8c\u6210concurrency\u3002 1.2\u3001Use nonblocking calls (e.g. write() on a socket set to O_NONBLOCK ) to start I/O, and readiness notification (e.g. poll() or /dev/poll ) to know when it's OK to start the next I/O on that channel. Generally only usable with network I/O, not disk I/O. 1.3\u3001Use asynchronous calls (e.g. aio_write() ) to start I/O, and completion notification (e.g. signals or completion ports) to know when the I/O finishes. Good for both network and disk I/O. NOTE: \u5176\u5b9e\u4e0a\u9762\u4e09\u6bb5\u6240\u63cf\u8ff0\u7684\u662f\u4e09\u79cdIO strategy\uff0c\u5728APUE\u7684chapter 14 advanced I/O\u4e2d\u90fd\u6709\u63cf\u8ff0\uff1b 2\u3001How to control the code servicing each client 2.1\u3001one process for each client (classic Unix approach, used since 1980 or so) 2.2\u3001one OS-level thread handles many clients; each client is controlled by: a user-level thread (e.g. GNU state threads, classic Java with green threads) a state machine (a bit esoteric, but popular in some circles; my favorite) a continuation (a bit esoteric, but popular in some circles) 2.3\u3001one OS-level thread for each client (e.g. classic Java with native threads) 2.4\u3001one OS-level thread for each active client (e.g. Tomcat with apache front end; NT completion ports; thread pools) 2.5\u3001Whether to use standard O/S services, or put some code into the kernel (e.g. in a custom driver, kernel module, or VxD) The following five combinations seem to be popular: Serve many clients with each thread, and use nonblocking I/O and level-triggered readiness notification Serve many clients with each thread, and use nonblocking I/O and readiness change notification Serve many clients with each server thread, and use asynchronous I/O serve one client with each server thread, and use blocking I/O Build the server code into the kernel","title":"I/O Strategies"},{"location":"Event-driven-concurrent-server/Concurrent-server-/C10K-problem/kegel-Unix-The-C10K-problem/#1#serve#many#clients#with#each#thread#and#use#nonblocking#io#and#level-triggered#readiness#notification","text":"Serve many clients with each thread\u7684\u610f\u601d\u662f\uff1a\u4e3a\u6bcf\u4e2a\u7ebf\u7a0b\u63d0\u4f9b\u8bb8\u591a\u5ba2\u6237\u7aef\uff0c\u5b83\u5b9e\u9645\u4e0a\u6240\u6307\u4e3a\u4e00\u4e2athread\u5904\u7406\u591a\u4e2aclient\uff1b ... set nonblocking mode on all network handles , and use select() or poll() to tell which network handle has data waiting. This is the traditional favorite. With this scheme, the kernel tells you whether a file descriptor is ready, whether or not you've done anything with that file descriptor since the last time the kernel told you about it. (The name 'level triggered' comes from computer hardware design; it's the opposite of 'edge triggered' . Jonathon Lemon introduced the terms in his BSDCON 2000 paper on kqueue() .) NOTE: : \u8fd9\u544a\u8bc9\u4e86\u6211\u4eec\uff0c select \u548c poll \u53ea\u80fd\u591f\u4f7f\u7528level-triggered readiness notification\uff0c\u800c\u4e0d\u652f\u6301 'edge triggered' \u3002 Note: it's particularly important to remember that readiness notification from the kernel is only a hint; the file descriptor might not be ready anymore when you try to read from it. That's why it's important to use nonblocking mode when using readiness notification . An important bottleneck in this method is that read() or sendfile() from disk blocks if the page is not in core at the moment; setting nonblocking mode on a disk file handle has no effect. Same thing goes for memory-mapped disk files . The first time a server needs disk I/O, its process blocks , all clients must wait, and that raw nonthreaded performance goes to waste. NOTE: : \u5f53\u4ecedisk\u5728\u5c06\u6587\u4ef6 read() or sendfile() \u5230core\uff08\u5185\u5b58\uff09\u4e2d\u65f6\uff0c\u5373\u4f7f\u5728file handle\u4e0a\u8bbe\u7f6e\u4e86**nonblocking mode**\uff0c\u4ecd\u7136\u4f1ablock\uff1b This is what asynchronous I/O is for, but on systems that lack AIO , worker threads or processes that do the disk I/O can also get around this bottleneck. One approach is to use memory-mapped files , and if mincore() indicates I/O is needed, ask a worker to do the I/O, and continue handling network traffic. Jef Poskanzer mentions that Pai, Druschel, and Zwaenepoel's 1999 Flash web server uses this trick; they gave a talk at Usenix '99 on it. It looks like mincore() is available in BSD-derived Unixes like FreeBSD and Solaris, but is not part of the Single Unix Specification . It's available as part of Linux as of kernel 2.3.51, thanks to Chuck Lever . NOTE: : This is what asynchronous I/O is for, but on systems that lack AIO , worker threads or processes that do the disk I/O can also get around this bottleneck.\u7ffb\u8bd1\u662f\uff1a\u8fd9\u5c31\u662f\u5f02\u6b65I / O\u7684\u7528\u9014\uff0c\u4f46\u5728\u7f3a\u5c11AIO\u7684\u7cfb\u7edf\u4e0a\uff0c\u6267\u884c\u78c1\u76d8I / O\u7684\u5de5\u4f5c\u7ebf\u7a0b\u6216\u8fdb\u7a0b\u4e5f\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e2a\u74f6\u9888\u95ee\u9898\u3002\u5b83\u7684\u610f\u601d\u662f\u4f7f\u7528\u4e00\u4e2athread\u6216\u8005process\u6765\u6267\u884cIO\uff0c\u800c\u4e0d\u963b\u585e\u4e3b\u7ebf\u7a0b\uff1b But in November 2003 on the freebsd-hackers list, Vivek Pei et al reported very good results using system-wide profiling of their Flash web server to attack bottlenecks. One bottleneck they found was mincore (guess that wasn't such a good idea after all) Another was the fact that sendfile blocks on disk access; they improved performance by introducing a modified sendfile() that return something like EWOULDBLOCK when the disk page it's fetching is not yet in core. (Not sure how you tell the user the page is now resident... seems to me what's really needed here is aio_sendfile() .) The end result of their optimizations is a SpecWeb99 score of about 800 on a 1GHZ/1GB FreeBSD box, which is better than anything on file at spec.org.","title":"1. Serve many clients with each thread, and use nonblocking I/O and level-triggered readiness notification"},{"location":"Event-driven-concurrent-server/Concurrent-server-/C10K-problem/kegel-Unix-The-C10K-problem/#2#serve#many#clients#with#each#thread#and#use#nonblocking#io#and#readiness#change#notification","text":"Readiness change notification (or edge-triggered readiness notification ) means you give the kernel a file descriptor , and later, when that descriptor transitions from not ready to ready \uff0cthe kernel notifies you somehow. It then assumes you know the file descriptor is ready, and will not send any more readiness notifications of that type for that file descriptor until you do something that causes the file descriptor to no longer be ready (e.g. until you receive the EWOULDBLOCK error on a send , recv , or accept call, or a send or recv transfers less than the requested number of bytes). When you use readiness change notification , you must be prepared for spurious events , since one common implementation is to signal readiness whenever any packets are received, regardless of whether the file descriptor was already ready. This is the opposite of \" level-triggered \" readiness notification. It's a bit less forgiving of programming mistakes, since if you miss just one event, the connection that event was for gets stuck forever. Nevertheless, I have found that edge-triggered readiness notification made programming nonblocking clients with OpenSSL easier, so it's worth trying. [ Banga, Mogul, Drusha '99] described this kind of scheme in 1999.","title":"2. Serve many clients with each thread, and use nonblocking I/O and readiness change notification"},{"location":"Event-driven-concurrent-server/Concurrent-server-/C10K-problem/kegel-Unix-The-C10K-problem/#3#serve#many#clients#with#each#server#thread#and#use#asynchronous#io","text":"This has not yet become popular in Unix, probably because few operating systems support asynchronous I/O, also possibly because it (like nonblocking I/O) requires rethinking your application. Under standard Unix, asynchronous I/O is provided by the aio_ interface (scroll down from that link to \"Asynchronous input and output\"), which associates a signal and value with each I/O operation . Signals and their values are queued and delivered efficiently to the user process. This is from the POSIX 1003.1b realtime extensions, and is also in the Single Unix Specification, version 2. AIO is normally used with edge-triggered completion notification , i.e. a signal is queued when the operation is complete. (It can also be used with level triggered completion notification by calling aio_suspend() , though I suspect few people do this.) glibc 2.1 and later provide a generic implementation written for standards compliance rather than performance. Ben LaHaise's implementation for Linux AIO was merged into the main Linux kernel as of 2.5.32. It doesn't use kernel threads, and has a very efficient underlying api, but (as of 2.6.0-test2) doesn't yet support sockets. (There is also an AIO patch for the 2.4 kernels, but the 2.5/2.6 implementation is somewhat different.) More info: The page \" Kernel Asynchronous I/O (AIO) Support for Linux \" which tries to tie together all info about the 2.6 kernel's implementation of AIO (posted 16 Sept 2003) Round 3: aio vs /dev/epoll by Benjamin C.R. LaHaise (presented at 2002 OLS) Asynchronous I/O Suport in Linux 2.5 , by Bhattacharya, Pratt, Pulaverty, and Morgan, IBM; presented at OLS '2003 Design Notes on Asynchronous I/O (aio) for Linux by Suparna Bhattacharya -- compares Ben's AIO with SGI's KAIO and a few other AIO projects Linux AIO home page - Ben's preliminary patches, mailing list, etc. linux-aio mailing list archives libaio-oracle - library implementing standard Posix AIO on top of libaio. First mentioned by Joel Becker on 18 Apr 2003 . Suparna also suggests having a look at the the DAFS API's approach to AIO Red Hat AS and Suse SLES both provide a high-performance implementation on the 2.4 kernel; it is related to, but not completely identical to, the 2.6 kernel implementation. In February 2006, a new attempt is being made to provide network AIO; see the note above about Evgeniy Polyakov's kevent-based AIO . In 1999, SGI implemented high-speed AIO for Linux . As of version 1.1, it's said to work well with both disk I/O and sockets. It seems to use kernel threads. It is still useful for people who can't wait for Ben's AIO to support sockets. The O'Reilly book POSIX.4: Programming for the Real World is said to include a good introduction to aio. A tutorial for the earlier, nonstandard, aio implementation on Solaris is online at Sunsite . It's probably worth a look, but keep in mind you'll need to mentally convert \"aioread\" to \"aio_read\", etc. Note that AIO doesn't provide a way to open files without blocking for disk I/O; if you care about the sleep caused by opening a disk file, Linus suggests you should simply do the open() in a different thread rather than wishing for an aio_open() system call. Under Windows, asynchronous I/O is associated with the terms \"Overlapped I/O\" and IOCP or \"I/O Completion Port\". Microsoft's IOCP combines techniques from the prior art like asynchronous I/O (like aio_write) and queued completion notification (like when using the aio_sigevent field with aio_write) with a new idea of holding back some requests to try to keep the number of running threads associated with a single IOCP constant. For more information, see Inside I/O Completion Ports by Mark Russinovich at sysinternals.com, Jeffrey Richter's book \"Programming Server-Side Applications for Microsoft Windows 2000\" ( Amazon , MSPress ), U.S. patent #06223207 , or MSDN .","title":"3. Serve many clients with each server thread, and use asynchronous I/O"},{"location":"Event-driven-concurrent-server/Concurrent-server-/C10K-problem/kegel-Unix-The-C10K-problem/#4#serve#one#client#with#each#server#thread","text":"... and let read() and write() block. Has the disadvantage of using a whole stack frame for each client, which costs memory. Many OS's also have trouble handling more than a few hundred threads. If each thread gets a 2MB stack (not an uncommon default value), you run out of virtual memory at (2^30 / 2^21) = 512 threads on a 32 bit machine with 1GB user-accessible VM (like, say, Linux as normally shipped on x86). You can work around this by giving each thread a smaller stack, but since most thread libraries don't allow growing thread stacks once created, doing this means designing your program to minimize stack use. You can also work around this by moving to a 64 bit processor. \u5177\u6709\u4e3a\u6bcf\u4e2a\u5ba2\u6237\u7aef\u4f7f\u7528\u6574\u4e2a\u5806\u6808\u5e27\u7684\u7f3a\u70b9\uff0c\u8fd9\u4f1a\u82b1\u8d39\u5185\u5b58\u3002\u8bb8\u591a\u64cd\u4f5c\u7cfb\u7edf\u4e5f\u96be\u4ee5\u5904\u7406\u8d85\u8fc7\u51e0\u767e\u4e2a\u7ebf\u7a0b\u3002\u5982\u679c\u6bcf\u4e2a\u7ebf\u7a0b\u83b7\u5f972MB\u5806\u6808\uff08\u4e0d\u662f\u975e\u5e38\u89c1\u7684\u9ed8\u8ba4\u503c\uff09\uff0c\u5219\u572832\u4f4d\u673a\u5668\u4e0a\u7684\uff082 ^ 30/2 ^ 21\uff09= 512\u4e2a\u7ebf\u7a0b\u4e0a\u8017\u5c3d\u865a\u62df\u5185\u5b58\uff0c\u5e76\u4e14\u5177\u67091GB\u7528\u6237\u53ef\u8bbf\u95ee\u7684VM\uff08\u4f8b\u5982\uff0c Linux\u901a\u5e38\u5728x86\u4e0a\u53d1\u5e03\uff09\u3002\u60a8\u53ef\u4ee5\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u7ebf\u7a0b\u63d0\u4f9b\u66f4\u5c0f\u7684\u5806\u6808\u6765\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u4f46\u7531\u4e8e\u5927\u591a\u6570\u7ebf\u7a0b\u5e93\u5728\u521b\u5efa\u540e\u4e0d\u5141\u8bb8\u589e\u52a0\u7ebf\u7a0b\u5806\u6808\uff0c\u56e0\u6b64\u8fd9\u6837\u505a\u610f\u5473\u7740\u8bbe\u8ba1\u7a0b\u5e8f\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u5806\u6808\u4f7f\u7528\u3002\u60a8\u4e5f\u53ef\u4ee5\u901a\u8fc7\u8f6c\u79fb\u523064\u4f4d\u5904\u7406\u5668\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002 The thread support in Linux, FreeBSD, and Solaris is improving, and 64 bit processors are just around the corner even for mainstream users. Perhaps in the not-too-distant future, those who prefer using one thread per client will be able to use that paradigm even for 10000 clients. Nevertheless, at the current time, if you actually want to support that many clients, you're probably better off using some other paradigm. Linux\uff0cFreeBSD\u548cSolaris\u4e2d\u7684\u7ebf\u7a0b\u652f\u6301\u6b63\u5728\u6539\u8fdb\uff0c\u5373\u4f7f\u5bf9\u4e8e\u4e3b\u6d41\u7528\u6237\uff0c64\u4f4d\u5904\u7406\u5668\u4e5f\u5373\u5c06\u5230\u6765\u3002\u4e5f\u8bb8\u5728\u4e0d\u592a\u9065\u8fdc\u7684\u672a\u6765\uff0c\u90a3\u4e9b\u559c\u6b22\u6bcf\u4e2a\u5ba2\u6237\u7aef\u4f7f\u7528\u4e00\u4e2a\u7ebf\u7a0b\u7684\u4eba\u5373\u4f7f\u5bf910000\u4e2a\u5ba2\u6237\u7aef\u4e5f\u80fd\u591f\u4f7f\u7528\u8fd9\u4e2a\u8303\u4f8b\u3002\u7136\u800c\uff0c\u5728\u76ee\u524d\u8fd9\u4e2a\u65f6\u5019\uff0c\u5982\u679c\u4f60\u771f\u7684\u60f3\u8981\u652f\u6301\u90a3\u4e48\u591a\u5ba2\u6237\uff0c\u4f60\u53ef\u80fd\u6700\u597d\u8fd8\u662f\u4f7f\u7528\u5176\u4ed6\u4e00\u4e9b\u8303\u4f8b\u3002 For an unabashedly pro-thread viewpoint, see Why Events Are A Bad Idea (for High-concurrency Servers) by von Behren, Condit, and Brewer, UCB, presented at HotOS IX. Anyone from the anti-thread camp care to point out a paper that rebuts this one? :-) \u5bf9\u4e8e\u4e00\u4e2a\u6beb\u4e0d\u63a9\u9970\u7684\u4eb2\u7ebf\u7a0b\u89c2\u70b9\uff0c\u8bf7\u53c2\u9605HotOS IX\u4e0a\u7684von Behren\uff0cCondit\u548cBrewer\uff0cUCB\uff0c\u4e3a\u4ec0\u4e48\u4e8b\u4ef6\u662f\u4e00\u4e2a\u574f\u4e3b\u610f\uff08\u5bf9\u4e8e\u9ad8\u5e76\u53d1\u670d\u52a1\u5668\uff09\u3002\u53cd\u7ebf\u8425\u5730\u7684\u4efb\u4f55\u4eba\u90fd\u5728\u6307\u51fa\u4e00\u7bc7\u53cd\u9a73\u8fd9\u7bc7\u8bba\u6587\u7684\u8bba\u6587\u5417\uff1f :-)","title":"4. Serve one client with each server thread"},{"location":"Event-driven-concurrent-server/Concurrent-server-/C10K-problem/kegel-Unix-The-C10K-problem/#5#build#the#server#code#into#the#kernel","text":"Novell and Microsoft are both said to have done this at various times, at least one NFS implementation does this, khttpd does this for Linux and static web pages, and \"TUX\" (Threaded linUX webserver) is a blindingly fast and flexible kernel-space HTTP server by Ingo Molnar for Linux. Ingo's September 1, 2000 announcement says an alpha version of TUX can be downloaded from ftp://ftp.redhat.com/pub/redhat/tux , and explains how to join a mailing list for more info. The linux-kernel list has been discussing the pros and cons of this approach, and the consensus seems to be instead of moving web servers into the kernel, the kernel should have the smallest possible hooks added to improve web server performance. That way, other kinds of servers can benefit. See e.g. Zach Brown's remarks about userland vs. kernel http servers. It appears that the 2.4 linux kernel provides sufficient power to user programs, as the X15 server runs about as fast as Tux, but doesn't use any kernel modifications.","title":"5. Build the server code into the kernel"},{"location":"Event-driven-concurrent-server/Concurrent-server-/C10K-problem/kegel-Unix-The-C10K-problem/#bring#the#tcp#stack#into#userspace","text":"See for instance the netmap packet I/O framework, and the Sandstorm proof-of-concept web server based on it.","title":"Bring the TCP stack into userspace"},{"location":"Event-driven-concurrent-server/Concurrent-server-/C10K-problem/kegel-Unix-The-C10K-problem/#measuring#server#performance","text":"","title":"Measuring Server Performance"},{"location":"Event-driven-concurrent-server/Concurrent-server-/C10K-problem/kegel-Unix-The-C10K-problem/#examples","text":"Nginx is a web server that uses whatever high-efficiency network event mechanism is available on the target OS. It's getting popular; there are even about it (and since this page was originally written, many more, including a of that book.) thttpd Very simple. Uses a single process. It has good performance, but doesn't scale with the number of CPU's. Can also use kqueue. mathopd . Similar to thttpd. fhttpd boa Roxen Zeus , a commercial server that tries to be the absolute fastest. See their tuning guide . The other non-Java servers listed at http://www.acme.com/software/thttpd/benchmarks.html BetaFTPd Flash-Lite - web server using IO-Lite. Flash: An efficient and portable Web server -- uses select(), mmap(), mincore() The Flash web server as of 2003 -- uses select(), modified sendfile(), async open() xitami - uses select() to implement its own thread abstraction for portability to systems without threads. Medusa - a server-writing toolkit in Python that tries to deliver very high performance. userver - a small http server that can use select, poll, epoll, or sigio","title":"Examples"},{"location":"Event-driven-concurrent-server/Concurrent-server-/C10K-problem/kegel-Unix-The-C10K-problem/#interesting#devpoll-based#servers","text":"N. Provos, C. Lever , \"Scalable Network I/O in Linux,\" May, 2000. [FREENIX track, Proc. USENIX 2000, San Diego, California (June, 2000).] Describes a version of thttpd modified to support /dev/poll. Performance is compared with phhttpd.","title":"Interesting /dev/poll-based servers"},{"location":"Event-driven-concurrent-server/Concurrent-server-/C10K-problem/kegel-Unix-The-C10K-problem/#interesting#epoll-based#servers","text":"ribs2 cmogstored - uses epoll/kqueue for most networking, threads for disk and accept4","title":"Interesting epoll-based servers"},{"location":"Event-driven-concurrent-server/Concurrent-server-/Case/","text":"\u6848\u4f8b \u4e0b\u9762\u662f\u6211\u9047\u5230\u7684\uff0c\u975e\u5e38\u597d\u7684concurrent server\uff0c\u5b83\u4eec\u90fd\u975e\u5e38\u5178\u578b\uff0c\u5e76\u4e14\u6709\u7684\u63d0\u4f9b\u4e86\u591a\u79cdconcurrency\u65b9\u5f0f\u3002 \u5e76\u53d1\u6a21\u578b \u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\uff0c\u5bf9\u6b64\u8fdb\u884c\u4e86\u8ba8\u8bba: \u4e00\u3001kegel The C10K problem \u4e8c\u3001lwn The SO_REUSEPORT socket option \u4e09\u3001zhihu \u5982\u4f55\u6df1\u523b\u7406\u89e3reactor\u548cproactor\uff1f # \u5c0f\u6797coding\u7684\u56de\u7b54 \u975e\u5e38\u597d\u7684\u6587\u7ae0\uff0c\u5c06\u53ef\u80fd\u7684\u5e76\u53d1\u6a21\u578b\u603b\u7ed3\u5f97\u975e\u5e38\u597d\u3002 \u5927\u591a\u6570\u90fd\u662f\u4f7f\u7528: 1\u3001reactor\u3001handler\u3001acceptor 2\u3001master worker multiple thread multiple process Nginx nginx has one master process and several worker processes. The main purpose of the master process is to read and evaluate configuration, and maintain worker processes. Worker processes do actual processing of requests. nginx employs event-based model and OS-dependent mechanisms to efficiently distribute requests among worker processes. The number of worker processes is defined in the configuration file and may be fixed for a given configuration or automatically adjusted to the number of available CPU cores (see worker_processes ). \u5e76\u53d1\u6a21\u578b \u591a Reactor \u591a\u8fdb\u7a0b\u65b9\u6848 Redis \u5e76\u53d1\u6a21\u578b \u8bfb\u53d6\u89e3\u6790\u547d\u4ee4-\u591a\u7ebf\u7a0b \u6267\u884c\u547d\u4ee4-\u5355\u7ebf\u7a0b \u8fd4\u56de\u54cd\u5e94-\u591a\u7ebf\u7a0b Memcached \u5e76\u53d1\u6a21\u578b \u591a Reactor \u591a\u7ebf\u7a0b\u65b9\u6848 inetd TODO Python Gunicorn Gunicorn is based on the pre-fork worker model. This means that there is a central master process that manages a set of worker processes. The master never knows anything about individual clients. All requests and responses are handled completely by worker processes. Celery Tornado","title":"Introduction"},{"location":"Event-driven-concurrent-server/Concurrent-server-/Case/#_1","text":"\u4e0b\u9762\u662f\u6211\u9047\u5230\u7684\uff0c\u975e\u5e38\u597d\u7684concurrent server\uff0c\u5b83\u4eec\u90fd\u975e\u5e38\u5178\u578b\uff0c\u5e76\u4e14\u6709\u7684\u63d0\u4f9b\u4e86\u591a\u79cdconcurrency\u65b9\u5f0f\u3002","title":"\u6848\u4f8b"},{"location":"Event-driven-concurrent-server/Concurrent-server-/Case/#_2","text":"\u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\uff0c\u5bf9\u6b64\u8fdb\u884c\u4e86\u8ba8\u8bba: \u4e00\u3001kegel The C10K problem \u4e8c\u3001lwn The SO_REUSEPORT socket option \u4e09\u3001zhihu \u5982\u4f55\u6df1\u523b\u7406\u89e3reactor\u548cproactor\uff1f # \u5c0f\u6797coding\u7684\u56de\u7b54 \u975e\u5e38\u597d\u7684\u6587\u7ae0\uff0c\u5c06\u53ef\u80fd\u7684\u5e76\u53d1\u6a21\u578b\u603b\u7ed3\u5f97\u975e\u5e38\u597d\u3002 \u5927\u591a\u6570\u90fd\u662f\u4f7f\u7528: 1\u3001reactor\u3001handler\u3001acceptor 2\u3001master worker multiple thread multiple process","title":"\u5e76\u53d1\u6a21\u578b"},{"location":"Event-driven-concurrent-server/Concurrent-server-/Case/#nginx","text":"nginx has one master process and several worker processes. The main purpose of the master process is to read and evaluate configuration, and maintain worker processes. Worker processes do actual processing of requests. nginx employs event-based model and OS-dependent mechanisms to efficiently distribute requests among worker processes. The number of worker processes is defined in the configuration file and may be fixed for a given configuration or automatically adjusted to the number of available CPU cores (see worker_processes ).","title":"Nginx"},{"location":"Event-driven-concurrent-server/Concurrent-server-/Case/#_3","text":"\u591a Reactor \u591a\u8fdb\u7a0b\u65b9\u6848","title":"\u5e76\u53d1\u6a21\u578b"},{"location":"Event-driven-concurrent-server/Concurrent-server-/Case/#redis","text":"","title":"Redis"},{"location":"Event-driven-concurrent-server/Concurrent-server-/Case/#_4","text":"\u8bfb\u53d6\u89e3\u6790\u547d\u4ee4-\u591a\u7ebf\u7a0b \u6267\u884c\u547d\u4ee4-\u5355\u7ebf\u7a0b \u8fd4\u56de\u54cd\u5e94-\u591a\u7ebf\u7a0b","title":"\u5e76\u53d1\u6a21\u578b"},{"location":"Event-driven-concurrent-server/Concurrent-server-/Case/#memcached","text":"","title":"Memcached"},{"location":"Event-driven-concurrent-server/Concurrent-server-/Case/#_5","text":"\u591a Reactor \u591a\u7ebf\u7a0b\u65b9\u6848","title":"\u5e76\u53d1\u6a21\u578b"},{"location":"Event-driven-concurrent-server/Concurrent-server-/Case/#inetd","text":"TODO","title":"inetd"},{"location":"Event-driven-concurrent-server/Concurrent-server-/Case/#python","text":"","title":"Python"},{"location":"Event-driven-concurrent-server/Concurrent-server-/Case/#gunicorn","text":"Gunicorn is based on the pre-fork worker model. This means that there is a central master process that manages a set of worker processes. The master never knows anything about individual clients. All requests and responses are handled completely by worker processes.","title":"Gunicorn"},{"location":"Event-driven-concurrent-server/Concurrent-server-/Case/#celery","text":"","title":"Celery"},{"location":"Event-driven-concurrent-server/Concurrent-server-/Case/#tornado","text":"","title":"Tornado"},{"location":"Event-driven-concurrent-server/Concurrent-server-/Case/HTTP-server/","text":"HTTP server IO multiplex https://github.com/jeremycw/httpserver.h \u591a\u8fdb\u7a0b https://github.com/pizhi/HttpServer https://github.com/ethereum/lahja","title":"Introduction"},{"location":"Event-driven-concurrent-server/Concurrent-server-/Case/HTTP-server/#http#server","text":"","title":"HTTP server"},{"location":"Event-driven-concurrent-server/Concurrent-server-/Case/HTTP-server/#io#multiplex","text":"https://github.com/jeremycw/httpserver.h","title":"IO multiplex"},{"location":"Event-driven-concurrent-server/Concurrent-server-/Case/HTTP-server/#_1","text":"https://github.com/pizhi/HttpServer https://github.com/ethereum/lahja","title":"\u591a\u8fdb\u7a0b"},{"location":"Event-driven-concurrent-server/Concurrent-server-/Load-balance/","text":"Load balancing wikipedia Load balancing (computing) In computing , load balancing refers to the process of distributing a set of tasks over a set of resources (computing units), with the aim of making their overall processing more efficient. Load balancing techniques can optimize the response time for each task, avoiding unevenly overloading compute nodes while other compute nodes are left idle. developer.51cto \u6253\u5de5\u4eba\uff0c\u652f\u6491\u4ebf\u7ea7\u9ad8\u5e76\u53d1\u7684\u7cfb\u7edf\u957f\u5565\u6837? # Dubbo \u8d1f\u8f7d\u5747\u8861\u7b56\u7565 Dubbo \u8d1f\u8f7d\u5747\u8861\u7b56\u7565\uff1a 1\u3001\u52a0\u6743\u968f\u673a\uff1a\u5047\u8bbe\u6211\u4eec\u6709\u4e00\u7ec4\u670d\u52a1\u5668 servers=[A, B, C]\uff0c\u4ed6\u4eec\u5bf9\u5e94\u7684\u6743\u91cd\u4e3a weights=[5, 3, 2]\uff0c\u6743\u91cd\u603b\u548c\u4e3a 10\u3002 \u73b0\u5728\u628a\u8fd9\u4e9b\u6743\u91cd\u503c\u5e73\u94fa\u5728\u4e00\u7ef4\u5750\u6807\u503c\u4e0a\uff0c[0, 5) \u533a\u95f4\u5c5e\u4e8e\u670d\u52a1\u5668 A\uff0c[5, 8) \u533a\u95f4\u5c5e\u4e8e\u670d\u52a1\u5668 B\uff0c[8, 10) \u533a\u95f4\u5c5e\u4e8e\u670d\u52a1\u5668 C\u3002 \u63a5\u4e0b\u6765\u901a\u8fc7\u968f\u673a\u6570\u751f\u6210\u5668\u751f\u6210\u4e00\u4e2a\u8303\u56f4\u5728 [0, 10) \u4e4b\u95f4\u7684\u968f\u673a\u6570\uff0c\u7136\u540e\u8ba1\u7b97\u8fd9\u4e2a\u968f\u673a\u6570\u4f1a\u843d\u5230\u54ea\u4e2a\u533a\u95f4\u4e0a\u5c31\u53ef\u4ee5\u4e86\u3002 2\u3001\u6700\u5c0f\u6d3b\u8dc3\u6570\uff1a\u6bcf\u4e2a\u670d\u52a1\u63d0\u4f9b\u8005\u5bf9\u5e94\u4e00\u4e2a\u6d3b\u8dc3\u6570 active\uff0c\u521d\u59cb\u60c5\u51b5\u4e0b\uff0c\u6240\u6709\u670d\u52a1\u63d0\u4f9b\u8005\u6d3b\u8dc3\u6570\u5747\u4e3a 0\u3002\u6bcf\u6536\u5230\u4e00\u4e2a\u8bf7\u6c42\uff0c\u6d3b\u8dc3\u6570\u52a0 1\uff0c\u5b8c\u6210\u8bf7\u6c42\u540e\u5219\u5c06\u6d3b\u8dc3\u6570\u51cf 1\u3002 \u5728\u670d\u52a1\u8fd0\u884c\u4e00\u6bb5\u65f6\u95f4\u540e\uff0c\u6027\u80fd\u597d\u7684\u670d\u52a1\u63d0\u4f9b\u8005\u5904\u7406\u8bf7\u6c42\u7684\u901f\u5ea6\u66f4\u5feb\uff0c\u56e0\u6b64\u6d3b\u8dc3\u6570\u4e0b\u964d\u7684\u4e5f\u8d8a\u5feb\uff0c\u6b64\u65f6\u8fd9\u6837\u7684\u670d\u52a1\u63d0\u4f9b\u8005\u80fd\u591f\u4f18\u5148\u83b7\u53d6\u5230\u65b0\u7684\u670d\u52a1\u8bf7\u6c42\u3002 3\u3001\u4e00\u81f4\u6027 hash\uff1a\u901a\u8fc7 hash \u7b97\u6cd5\uff0c\u628a provider \u7684 invoke \u548c\u968f\u673a\u8282\u70b9\u751f\u6210 hash\uff0c\u5e76\u5c06\u8fd9\u4e2a hash \u6295\u5c04\u5230 [0, 2^32 - 1] \u7684\u5706\u73af\u4e0a\uff0c\u67e5\u8be2\u7684\u65f6\u5019\u6839\u636e key \u8fdb\u884c md5 \u7136\u540e\u8fdb\u884c hash\uff0c\u5f97\u5230\u7b2c\u4e00\u4e2a\u8282\u70b9\u7684\u503c\u5927\u4e8e\u7b49\u4e8e\u5f53\u524d hash \u7684 invoker\u3002 4\u3001\u52a0\u6743\u8f6e\u8be2\uff1a\u6bd4\u5982\u670d\u52a1\u5668 A\u3001B\u3001C \u6743\u91cd\u6bd4\u4e3a 5:2:1\uff0c\u90a3\u4e48\u5728 8 \u6b21\u8bf7\u6c42\u4e2d\uff0c\u670d\u52a1\u5668 A \u5c06\u6536\u5230\u5176\u4e2d\u7684 5 \u6b21\u8bf7\u6c42\uff0c\u670d\u52a1\u5668 B \u4f1a\u6536\u5230\u5176\u4e2d\u7684 2 \u6b21\u8bf7\u6c42\uff0c\u670d\u52a1\u5668 C \u5219\u6536\u5230\u5176\u4e2d\u7684 1 \u6b21\u8bf7\u6c42\u3002 dubbo Dubbo#Load Balance TODO serverfault what-kind-of-load-balancing-algorithms-are-there /// \u5f80\u961f\u5217\u4e2d\u63d2\u5165\u6d88\u606f void CQuoteOrderWorker :: enqueue () { /// \u7b97\u6cd5:\u7b97\u6cd5\u7c7b\u4f3c\u4e8e\u5c06\u4e00\u4e2a\u957f\u7ef3\u6298\u53e0\u653e\u5165\u4e00\u4e2a\u56fa\u5b9a\u5bbd\u5ea6\u7684\u69fd\u4e2d\uff0c\u9700\u8981\u5c06\u957f\u7ef3\u4e0d\u65ad\u8fdb\u884c\u6298\u53e0\uff0c\u663e\u7136\u9664\u53bb\u6700\u540e\u4e00\u6298\uff0c\u6bcf\u4e00\u6298\u7684\u957f\u5ea6\u7b49\u4e8e\u69fd\u7684\u5bbd\u5ea6 /// \u6700\u540e\u4e00\u6298\u7684\u957f\u5ea6\u4e0d\u4e00\u5b9a\u7b49\u4e8e\u69fd\u7684\u5bbd\u5ea6\uff1b /// \u5238\u7684\u6570\u91cf int stock_num = stock_reader_p_ -> stocks (). size (); /// fold\u53ef\u4ee5\u7406\u89e3\u4e3a\u201c\u6298\u201d\uff0c\u6216\u201c\u8d9f\u201d int fold_num = ceil ( stock_num / quote_thread_num ); /// \u7d22\u5f15 int stock_index = 0 ; /// \u5bf9[0, fold_num-2]\u6298\uff0c\u6bcf\u4e00\u6298\u7684\u957f\u5ea6\u7b49\u4e8e\u69fd\u7684\u5bbd\u5ea6\uff0c\u53ef\u4ee5\u5b8c\u5168\u653e\u5165 for ( int fold_index = 0 ; fold_index < fold_num - 1 ; ++ fold_index ) { for ( int queue_index = 0 ; queue_index < quote_thread_num ; queue_index ++ ) { stock_index = fold_index * quote_thread_num + queue_index ; Stock * stock_p = stock_reader_p_ -> stocks ()[ stock_index ]; QuoteMessage * message_p = new QuoteMessage ( stock_p -> market_ , stock_p -> stock_code_ ); mtx_Quote [ queue_index ] -> Acquire (); m_GQuoteMessage [ queue_index ] -> push ( message_p ); mtx_Quote [ queue_index ] -> Release (); } } /// \u5bf9\u7b2cfold-1\u6298 for ( int queue_index = 0 ; queue_index < quote_thread_num ; queue_index ++ ) { stock_index = stock_num - ( fold_num - 2 ) * quote_thread_num - 1 + queue_index ; if ( stock_index <= stock_num - 1 ) { Stock * stock_p = stock_reader_p_ -> stocks ()[ stock_index ]; QuoteMessage * message_p = new QuoteMessage ( stock_p -> market_ , stock_p -> stock_code_ ); m_GQuoteMessage [ queue_index ] -> push ( message_p ); } else { /// \u5df2\u7ecf\u653e\u5b8c\u4e86\uff0c\u5219\u53ef\u4ee5\u9000\u51fa break ; } } } void CQuoteInfoWorker :: enqueue ( QuoteMessage * quote_message_p ) { /// \u884c\u60c5\u7ebf\u7a0b\u548c\u7279\u5f81\u5904\u7406\u7ebf\u7a0b\u4e2a\u6570\u4e0d\u4e00\u5b9a\u4e00\u6837\uff0c\u6210\u5458\u53d8\u91cffeature_thread_nos\u7684\u957f\u5ea6\u7b49\u4e8e\u7279\u5f81\u7ebf\u7a0b\u6570\uff0c /// \u6bcf\u6b21pop\u51fa\u6765\u4e00\u4e2a\u540e\u5728push\u8fdb\u961f\u5c3e\uff0c\u53ef\u4ee5\u5b9e\u73b0\u4e0b\u6e38\u7ebf\u7a0b\u7684\u8d1f\u8f7d\u5747\u8861 /// FeatureMessage * feature_message_p = new FeatureMessage ( quote_message_p -> market_ , quote_message_p -> stock_ ); /// feature_message\u5c06\u63d2\u5165\u5230\u4e0b\u9762\u8fd9\u4e2a\u961f\u5217\u4e2d int feature_thread_no = feature_thread_nos -> front (); mtx_Feature [ feature_thread_no ] -> Acquire (); m_GFeatureMessage [ feature_thread_no ] -> push ( feature_message_p ); mtx_Feature [ feature_thread_no ] -> Release (); /// \u8fdb\u5165\u5230\u961f\u5c3e feature_thread_nos -> pop (); feature_thread_nos -> push ( feature_thread_no ); } udaparts Tutoral 8 \u2013 Treat SocketPro Server as a Router for Load Balancing csdn \u8d1f\u8f7d\u5747\u8861\u7b97\u6cd5--\u8f6e\u8be2\u6cd5\uff08Round Robin\uff09","title":"Introduction"},{"location":"Event-driven-concurrent-server/Concurrent-server-/Load-balance/#load#balancing","text":"","title":"Load balancing"},{"location":"Event-driven-concurrent-server/Concurrent-server-/Load-balance/#wikipedia#load#balancing#computing","text":"In computing , load balancing refers to the process of distributing a set of tasks over a set of resources (computing units), with the aim of making their overall processing more efficient. Load balancing techniques can optimize the response time for each task, avoiding unevenly overloading compute nodes while other compute nodes are left idle.","title":"wikipedia Load balancing (computing)"},{"location":"Event-driven-concurrent-server/Concurrent-server-/Load-balance/#developer51cto#dubbo","text":"Dubbo \u8d1f\u8f7d\u5747\u8861\u7b56\u7565\uff1a 1\u3001\u52a0\u6743\u968f\u673a\uff1a\u5047\u8bbe\u6211\u4eec\u6709\u4e00\u7ec4\u670d\u52a1\u5668 servers=[A, B, C]\uff0c\u4ed6\u4eec\u5bf9\u5e94\u7684\u6743\u91cd\u4e3a weights=[5, 3, 2]\uff0c\u6743\u91cd\u603b\u548c\u4e3a 10\u3002 \u73b0\u5728\u628a\u8fd9\u4e9b\u6743\u91cd\u503c\u5e73\u94fa\u5728\u4e00\u7ef4\u5750\u6807\u503c\u4e0a\uff0c[0, 5) \u533a\u95f4\u5c5e\u4e8e\u670d\u52a1\u5668 A\uff0c[5, 8) \u533a\u95f4\u5c5e\u4e8e\u670d\u52a1\u5668 B\uff0c[8, 10) \u533a\u95f4\u5c5e\u4e8e\u670d\u52a1\u5668 C\u3002 \u63a5\u4e0b\u6765\u901a\u8fc7\u968f\u673a\u6570\u751f\u6210\u5668\u751f\u6210\u4e00\u4e2a\u8303\u56f4\u5728 [0, 10) \u4e4b\u95f4\u7684\u968f\u673a\u6570\uff0c\u7136\u540e\u8ba1\u7b97\u8fd9\u4e2a\u968f\u673a\u6570\u4f1a\u843d\u5230\u54ea\u4e2a\u533a\u95f4\u4e0a\u5c31\u53ef\u4ee5\u4e86\u3002 2\u3001\u6700\u5c0f\u6d3b\u8dc3\u6570\uff1a\u6bcf\u4e2a\u670d\u52a1\u63d0\u4f9b\u8005\u5bf9\u5e94\u4e00\u4e2a\u6d3b\u8dc3\u6570 active\uff0c\u521d\u59cb\u60c5\u51b5\u4e0b\uff0c\u6240\u6709\u670d\u52a1\u63d0\u4f9b\u8005\u6d3b\u8dc3\u6570\u5747\u4e3a 0\u3002\u6bcf\u6536\u5230\u4e00\u4e2a\u8bf7\u6c42\uff0c\u6d3b\u8dc3\u6570\u52a0 1\uff0c\u5b8c\u6210\u8bf7\u6c42\u540e\u5219\u5c06\u6d3b\u8dc3\u6570\u51cf 1\u3002 \u5728\u670d\u52a1\u8fd0\u884c\u4e00\u6bb5\u65f6\u95f4\u540e\uff0c\u6027\u80fd\u597d\u7684\u670d\u52a1\u63d0\u4f9b\u8005\u5904\u7406\u8bf7\u6c42\u7684\u901f\u5ea6\u66f4\u5feb\uff0c\u56e0\u6b64\u6d3b\u8dc3\u6570\u4e0b\u964d\u7684\u4e5f\u8d8a\u5feb\uff0c\u6b64\u65f6\u8fd9\u6837\u7684\u670d\u52a1\u63d0\u4f9b\u8005\u80fd\u591f\u4f18\u5148\u83b7\u53d6\u5230\u65b0\u7684\u670d\u52a1\u8bf7\u6c42\u3002 3\u3001\u4e00\u81f4\u6027 hash\uff1a\u901a\u8fc7 hash \u7b97\u6cd5\uff0c\u628a provider \u7684 invoke \u548c\u968f\u673a\u8282\u70b9\u751f\u6210 hash\uff0c\u5e76\u5c06\u8fd9\u4e2a hash \u6295\u5c04\u5230 [0, 2^32 - 1] \u7684\u5706\u73af\u4e0a\uff0c\u67e5\u8be2\u7684\u65f6\u5019\u6839\u636e key \u8fdb\u884c md5 \u7136\u540e\u8fdb\u884c hash\uff0c\u5f97\u5230\u7b2c\u4e00\u4e2a\u8282\u70b9\u7684\u503c\u5927\u4e8e\u7b49\u4e8e\u5f53\u524d hash \u7684 invoker\u3002 4\u3001\u52a0\u6743\u8f6e\u8be2\uff1a\u6bd4\u5982\u670d\u52a1\u5668 A\u3001B\u3001C \u6743\u91cd\u6bd4\u4e3a 5:2:1\uff0c\u90a3\u4e48\u5728 8 \u6b21\u8bf7\u6c42\u4e2d\uff0c\u670d\u52a1\u5668 A \u5c06\u6536\u5230\u5176\u4e2d\u7684 5 \u6b21\u8bf7\u6c42\uff0c\u670d\u52a1\u5668 B \u4f1a\u6536\u5230\u5176\u4e2d\u7684 2 \u6b21\u8bf7\u6c42\uff0c\u670d\u52a1\u5668 C \u5219\u6536\u5230\u5176\u4e2d\u7684 1 \u6b21\u8bf7\u6c42\u3002","title":"developer.51cto \u6253\u5de5\u4eba\uff0c\u652f\u6491\u4ebf\u7ea7\u9ad8\u5e76\u53d1\u7684\u7cfb\u7edf\u957f\u5565\u6837? # Dubbo \u8d1f\u8f7d\u5747\u8861\u7b56\u7565"},{"location":"Event-driven-concurrent-server/Concurrent-server-/Load-balance/#dubbo#dubboload#balance","text":"","title":"dubbo Dubbo#Load Balance"},{"location":"Event-driven-concurrent-server/Concurrent-server-/Load-balance/#todo","text":"","title":"TODO"},{"location":"Event-driven-concurrent-server/Concurrent-server-/Load-balance/#serverfault#what-kind-of-load-balancing-algorithms-are-there","text":"/// \u5f80\u961f\u5217\u4e2d\u63d2\u5165\u6d88\u606f void CQuoteOrderWorker :: enqueue () { /// \u7b97\u6cd5:\u7b97\u6cd5\u7c7b\u4f3c\u4e8e\u5c06\u4e00\u4e2a\u957f\u7ef3\u6298\u53e0\u653e\u5165\u4e00\u4e2a\u56fa\u5b9a\u5bbd\u5ea6\u7684\u69fd\u4e2d\uff0c\u9700\u8981\u5c06\u957f\u7ef3\u4e0d\u65ad\u8fdb\u884c\u6298\u53e0\uff0c\u663e\u7136\u9664\u53bb\u6700\u540e\u4e00\u6298\uff0c\u6bcf\u4e00\u6298\u7684\u957f\u5ea6\u7b49\u4e8e\u69fd\u7684\u5bbd\u5ea6 /// \u6700\u540e\u4e00\u6298\u7684\u957f\u5ea6\u4e0d\u4e00\u5b9a\u7b49\u4e8e\u69fd\u7684\u5bbd\u5ea6\uff1b /// \u5238\u7684\u6570\u91cf int stock_num = stock_reader_p_ -> stocks (). size (); /// fold\u53ef\u4ee5\u7406\u89e3\u4e3a\u201c\u6298\u201d\uff0c\u6216\u201c\u8d9f\u201d int fold_num = ceil ( stock_num / quote_thread_num ); /// \u7d22\u5f15 int stock_index = 0 ; /// \u5bf9[0, fold_num-2]\u6298\uff0c\u6bcf\u4e00\u6298\u7684\u957f\u5ea6\u7b49\u4e8e\u69fd\u7684\u5bbd\u5ea6\uff0c\u53ef\u4ee5\u5b8c\u5168\u653e\u5165 for ( int fold_index = 0 ; fold_index < fold_num - 1 ; ++ fold_index ) { for ( int queue_index = 0 ; queue_index < quote_thread_num ; queue_index ++ ) { stock_index = fold_index * quote_thread_num + queue_index ; Stock * stock_p = stock_reader_p_ -> stocks ()[ stock_index ]; QuoteMessage * message_p = new QuoteMessage ( stock_p -> market_ , stock_p -> stock_code_ ); mtx_Quote [ queue_index ] -> Acquire (); m_GQuoteMessage [ queue_index ] -> push ( message_p ); mtx_Quote [ queue_index ] -> Release (); } } /// \u5bf9\u7b2cfold-1\u6298 for ( int queue_index = 0 ; queue_index < quote_thread_num ; queue_index ++ ) { stock_index = stock_num - ( fold_num - 2 ) * quote_thread_num - 1 + queue_index ; if ( stock_index <= stock_num - 1 ) { Stock * stock_p = stock_reader_p_ -> stocks ()[ stock_index ]; QuoteMessage * message_p = new QuoteMessage ( stock_p -> market_ , stock_p -> stock_code_ ); m_GQuoteMessage [ queue_index ] -> push ( message_p ); } else { /// \u5df2\u7ecf\u653e\u5b8c\u4e86\uff0c\u5219\u53ef\u4ee5\u9000\u51fa break ; } } } void CQuoteInfoWorker :: enqueue ( QuoteMessage * quote_message_p ) { /// \u884c\u60c5\u7ebf\u7a0b\u548c\u7279\u5f81\u5904\u7406\u7ebf\u7a0b\u4e2a\u6570\u4e0d\u4e00\u5b9a\u4e00\u6837\uff0c\u6210\u5458\u53d8\u91cffeature_thread_nos\u7684\u957f\u5ea6\u7b49\u4e8e\u7279\u5f81\u7ebf\u7a0b\u6570\uff0c /// \u6bcf\u6b21pop\u51fa\u6765\u4e00\u4e2a\u540e\u5728push\u8fdb\u961f\u5c3e\uff0c\u53ef\u4ee5\u5b9e\u73b0\u4e0b\u6e38\u7ebf\u7a0b\u7684\u8d1f\u8f7d\u5747\u8861 /// FeatureMessage * feature_message_p = new FeatureMessage ( quote_message_p -> market_ , quote_message_p -> stock_ ); /// feature_message\u5c06\u63d2\u5165\u5230\u4e0b\u9762\u8fd9\u4e2a\u961f\u5217\u4e2d int feature_thread_no = feature_thread_nos -> front (); mtx_Feature [ feature_thread_no ] -> Acquire (); m_GFeatureMessage [ feature_thread_no ] -> push ( feature_message_p ); mtx_Feature [ feature_thread_no ] -> Release (); /// \u8fdb\u5165\u5230\u961f\u5c3e feature_thread_nos -> pop (); feature_thread_nos -> push ( feature_thread_no ); }","title":"serverfault what-kind-of-load-balancing-algorithms-are-there"},{"location":"Event-driven-concurrent-server/Concurrent-server-/Load-balance/#udaparts#tutoral#8#treat#socketpro#server#as#a#router#for#load#balancing","text":"","title":"udaparts Tutoral 8 \u2013 Treat SocketPro Server as a Router for Load Balancing"},{"location":"Event-driven-concurrent-server/Concurrent-server-/Load-balance/#csdn#--round#robin","text":"","title":"csdn \u8d1f\u8f7d\u5747\u8861\u7b97\u6cd5--\u8f6e\u8be2\u6cd5\uff08Round Robin\uff09"},{"location":"Event-driven-concurrent-server/Concurrent-server-/Load-balance/Round-robin/","text":"Round-robin scheduling 1\u3001\u5728 stlab / libraries / stlab / concurrency / channel.hpp \u4e2d\uff0c\u6709 class round_robin_queue_strategy zhuanlan.zhihu \u4e3a\u4ec0\u4e48\u8f6e\u8be2\u8c03\u5ea6\u7b97\u6cd5\u79f0\u4e3a Round Robin \uff1f \u6700\u8fd1\u91cd\u6e29\u4e86\u4e0bnginx\uff0c\u770b\u5230\u8d1f\u8f7d\u5747\u8861\u8c03\u5ea6\u7b97\u6cd5\u9ed8\u8ba4\u662f round robin\uff0c\u4e5f\u5c31\u662f\u8f6e\u8be2\u8c03\u5ea6\u7b97\u6cd5\u3002 \u7b97\u6cd5\u672c\u8eab\u5f88\u7b80\u5355\uff0c\u8f6e\u7740\u4e00\u4e2a\u4e00\u4e2a\u6765\uff0c\u975e\u5e38\u7b80\u5355\u9ad8\u6548\u516c\u5e73\u7684\u8c03\u5ea6\u7b97\u6cd5\u3002 wikipedia Round-robin scheduling","title":"Introduction"},{"location":"Event-driven-concurrent-server/Concurrent-server-/Load-balance/Round-robin/#round-robin#scheduling","text":"1\u3001\u5728 stlab / libraries / stlab / concurrency / channel.hpp \u4e2d\uff0c\u6709 class round_robin_queue_strategy","title":"Round-robin scheduling"},{"location":"Event-driven-concurrent-server/Concurrent-server-/Load-balance/Round-robin/#zhuanlanzhihu#round#robin","text":"\u6700\u8fd1\u91cd\u6e29\u4e86\u4e0bnginx\uff0c\u770b\u5230\u8d1f\u8f7d\u5747\u8861\u8c03\u5ea6\u7b97\u6cd5\u9ed8\u8ba4\u662f round robin\uff0c\u4e5f\u5c31\u662f\u8f6e\u8be2\u8c03\u5ea6\u7b97\u6cd5\u3002 \u7b97\u6cd5\u672c\u8eab\u5f88\u7b80\u5355\uff0c\u8f6e\u7740\u4e00\u4e2a\u4e00\u4e2a\u6765\uff0c\u975e\u5e38\u7b80\u5355\u9ad8\u6548\u516c\u5e73\u7684\u8c03\u5ea6\u7b97\u6cd5\u3002","title":"zhuanlan.zhihu \u4e3a\u4ec0\u4e48\u8f6e\u8be2\u8c03\u5ea6\u7b97\u6cd5\u79f0\u4e3a Round Robin \uff1f"},{"location":"Event-driven-concurrent-server/Concurrent-server-/Load-balance/Round-robin/#wikipedia#round-robin#scheduling","text":"","title":"wikipedia Round-robin scheduling"},{"location":"Event-driven-concurrent-server/Concurrent-server-/developer.51cto-%E6%94%AF%E6%92%91%E4%BA%BF%E7%BA%A7%E9%AB%98%E5%B9%B6%E5%8F%91%E7%9A%84%E7%B3%BB%E7%BB%9F%E9%95%BF%E5%95%A5%E6%A0%B7/","text":"developer.51cto \u6253\u5de5\u4eba\uff0c\u652f\u6491\u4ebf\u7ea7\u9ad8\u5e76\u53d1\u7684\u7cfb\u7edf\u957f\u5565\u6837? \u5fae\u670d\u52a1\u67b6\u6784\u6f14\u5316 RPC \u6d88\u606f\u961f\u5217 \u6570\u636e\u5e93 \u7f13\u5b58","title":"Introduction"},{"location":"Event-driven-concurrent-server/Concurrent-server-/developer.51cto-%E6%94%AF%E6%92%91%E4%BA%BF%E7%BA%A7%E9%AB%98%E5%B9%B6%E5%8F%91%E7%9A%84%E7%B3%BB%E7%BB%9F%E9%95%BF%E5%95%A5%E6%A0%B7/#developer51cto","text":"","title":"developer.51cto \u6253\u5de5\u4eba\uff0c\u652f\u6491\u4ebf\u7ea7\u9ad8\u5e76\u53d1\u7684\u7cfb\u7edf\u957f\u5565\u6837?"},{"location":"Event-driven-concurrent-server/Concurrent-server-/developer.51cto-%E6%94%AF%E6%92%91%E4%BA%BF%E7%BA%A7%E9%AB%98%E5%B9%B6%E5%8F%91%E7%9A%84%E7%B3%BB%E7%BB%9F%E9%95%BF%E5%95%A5%E6%A0%B7/#_1","text":"","title":"\u5fae\u670d\u52a1\u67b6\u6784\u6f14\u5316"},{"location":"Event-driven-concurrent-server/Concurrent-server-/developer.51cto-%E6%94%AF%E6%92%91%E4%BA%BF%E7%BA%A7%E9%AB%98%E5%B9%B6%E5%8F%91%E7%9A%84%E7%B3%BB%E7%BB%9F%E9%95%BF%E5%95%A5%E6%A0%B7/#rpc","text":"","title":"RPC"},{"location":"Event-driven-concurrent-server/Concurrent-server-/developer.51cto-%E6%94%AF%E6%92%91%E4%BA%BF%E7%BA%A7%E9%AB%98%E5%B9%B6%E5%8F%91%E7%9A%84%E7%B3%BB%E7%BB%9F%E9%95%BF%E5%95%A5%E6%A0%B7/#_2","text":"","title":"\u6d88\u606f\u961f\u5217"},{"location":"Event-driven-concurrent-server/Concurrent-server-/developer.51cto-%E6%94%AF%E6%92%91%E4%BA%BF%E7%BA%A7%E9%AB%98%E5%B9%B6%E5%8F%91%E7%9A%84%E7%B3%BB%E7%BB%9F%E9%95%BF%E5%95%A5%E6%A0%B7/#_3","text":"","title":"\u6570\u636e\u5e93"},{"location":"Event-driven-concurrent-server/Concurrent-server-/developer.51cto-%E6%94%AF%E6%92%91%E4%BA%BF%E7%BA%A7%E9%AB%98%E5%B9%B6%E5%8F%91%E7%9A%84%E7%B3%BB%E7%BB%9F%E9%95%BF%E5%95%A5%E6%A0%B7/#_4","text":"","title":"\u7f13\u5b58"},{"location":"Event-driven-concurrent-server/Design-pattern/","text":"\u5173\u4e8e\u672c\u7ae0 1\u3001Concurrent computing\u6709\u7740\u975e\u5e38\u5e7f\u6cdb\u7684application\uff0c\u56e0\u6b64\u5b83\u7684design pattern\u5f88\u591a\u90fd\u662f\u548c\u5177\u4f53\u7684application\u5bc6\u5207\u76f8\u5173\u7684\uff0c\u6240\u4ee5\u6211\u5c06\u90e8\u5206\u5185\u5bb9\u5206\u6563\u5230\u4e86\u5177\u4f53\u7684application\u4e2d 2\u3001\u7531\u4e8eevent-driven model\u975e\u5e38\u5f3a\u5927\uff0c\u80fd\u591f\u63cf\u8ff0\u975e\u5e38\u591a\u7684\u95ee\u9898\uff0c\u5305\u62ec\u672c\u7ae0\u63cf\u8ff0\u7684\u4e00\u4e9bdesign pattern\u4e5f\u662f\u53ef\u4ee5\u5f52\u5165\u5230event-driven model\u7684\uff0c\u4e3a\u4e86\u533a\u5206\uff0c\u672c\u7ae0\u6240\u63cf\u8ff0\u7684patter\uff0c\u4e3b\u8981\u662fintra-process\uff0c\u5373\u5728\u540c\u4e00\u4e2aprocess\u5185\uff1b\u8fd9\u4e9bpattern\u4e2d\uff0c\u4e5f\u4f1a\u6d89\u53ca\u5230message\uff0c\u5f53\u8c08\u53camessage\u7684\u65f6\u5019\uff0c\u5b83\u4eec\u7684passing\u90fd\u662f\u4e0d\u7ecf\u8fc7network\u7684\uff0c\u5927\u591a\u6570\u90fd\u662f\u5728thread\u4e4b\u95f4\u8fdb\u884c\u4f20\u8f93 wikipedia Concurrency pattern \u5b9e\u73b0event-driven model\u7684\u4e00\u4e9bpattern\u3002 Pattern \u8bf4\u660e Observer pattern Actor model Reactor pattern Proactor pattern Message queue Messaging pattern Publish\u2013subscribe pattern \u4e00\u4e2a\u5178\u578b\u7684\u4f8b\u5b50\u5c31\u662fredis\u7684pub/sub\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0cpub\u9700\u8981\u6ce8\u518c\u56de\u8c03\u51fd\u6570\uff0c\u7528\u4e8e\u6307\u5b9a\u5f53\u6536\u5230\u4fe1\u606f\u65f6\uff0c\u9700\u8981\u6267\u884c\u7684\u52a8\u4f5c\u3002\u8fd9\u975e\u5e38\u7c7b\u4f3c\u4e8esignal handler\u3002","title":"Introduction"},{"location":"Event-driven-concurrent-server/Design-pattern/#_1","text":"1\u3001Concurrent computing\u6709\u7740\u975e\u5e38\u5e7f\u6cdb\u7684application\uff0c\u56e0\u6b64\u5b83\u7684design pattern\u5f88\u591a\u90fd\u662f\u548c\u5177\u4f53\u7684application\u5bc6\u5207\u76f8\u5173\u7684\uff0c\u6240\u4ee5\u6211\u5c06\u90e8\u5206\u5185\u5bb9\u5206\u6563\u5230\u4e86\u5177\u4f53\u7684application\u4e2d 2\u3001\u7531\u4e8eevent-driven model\u975e\u5e38\u5f3a\u5927\uff0c\u80fd\u591f\u63cf\u8ff0\u975e\u5e38\u591a\u7684\u95ee\u9898\uff0c\u5305\u62ec\u672c\u7ae0\u63cf\u8ff0\u7684\u4e00\u4e9bdesign pattern\u4e5f\u662f\u53ef\u4ee5\u5f52\u5165\u5230event-driven model\u7684\uff0c\u4e3a\u4e86\u533a\u5206\uff0c\u672c\u7ae0\u6240\u63cf\u8ff0\u7684patter\uff0c\u4e3b\u8981\u662fintra-process\uff0c\u5373\u5728\u540c\u4e00\u4e2aprocess\u5185\uff1b\u8fd9\u4e9bpattern\u4e2d\uff0c\u4e5f\u4f1a\u6d89\u53ca\u5230message\uff0c\u5f53\u8c08\u53camessage\u7684\u65f6\u5019\uff0c\u5b83\u4eec\u7684passing\u90fd\u662f\u4e0d\u7ecf\u8fc7network\u7684\uff0c\u5927\u591a\u6570\u90fd\u662f\u5728thread\u4e4b\u95f4\u8fdb\u884c\u4f20\u8f93","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Event-driven-concurrent-server/Design-pattern/#wikipedia#concurrency#pattern","text":"\u5b9e\u73b0event-driven model\u7684\u4e00\u4e9bpattern\u3002 Pattern \u8bf4\u660e Observer pattern Actor model Reactor pattern Proactor pattern Message queue Messaging pattern Publish\u2013subscribe pattern \u4e00\u4e2a\u5178\u578b\u7684\u4f8b\u5b50\u5c31\u662fredis\u7684pub/sub\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0cpub\u9700\u8981\u6ce8\u518c\u56de\u8c03\u51fd\u6570\uff0c\u7528\u4e8e\u6307\u5b9a\u5f53\u6536\u5230\u4fe1\u606f\u65f6\uff0c\u9700\u8981\u6267\u884c\u7684\u52a8\u4f5c\u3002\u8fd9\u975e\u5e38\u7c7b\u4f3c\u4e8esignal handler\u3002","title":"wikipedia Concurrency pattern"},{"location":"Event-driven-concurrent-server/Design-pattern/Asynchronous-method-dispatch/","text":"Asynchronous method dispatch Wikipedia Asynchronous method dispatch Asynchronous method dispatch (AMD) is a data communication method used when there is a need for the server side to handle a large number of long lasting client requests. Using synchronous method dispatch (SMD), this scenario may turn the server into an unavailable busy state resulting in a connection failure response caused by a network connection request timeout . Mechanism The servicing of a client request is immediately dispatched to an available thread from a pool of threads and the client is put in a blocking state. Upon the completion of the task, the server is notified by a callback. The server unblocks the client and transmits the response back to the client. In case of thread starvation, clients are blocked waiting for threads to become available.","title":"Asynchronous-method-dispatch"},{"location":"Event-driven-concurrent-server/Design-pattern/Asynchronous-method-dispatch/#asynchronous#method#dispatch","text":"","title":"Asynchronous method dispatch"},{"location":"Event-driven-concurrent-server/Design-pattern/Asynchronous-method-dispatch/#wikipedia#asynchronous#method#dispatch","text":"Asynchronous method dispatch (AMD) is a data communication method used when there is a need for the server side to handle a large number of long lasting client requests. Using synchronous method dispatch (SMD), this scenario may turn the server into an unavailable busy state resulting in a connection failure response caused by a network connection request timeout .","title":"Wikipedia Asynchronous method dispatch"},{"location":"Event-driven-concurrent-server/Design-pattern/Asynchronous-method-dispatch/#mechanism","text":"The servicing of a client request is immediately dispatched to an available thread from a pool of threads and the client is put in a blocking state. Upon the completion of the task, the server is notified by a callback. The server unblocks the client and transmits the response back to the client. In case of thread starvation, clients are blocked waiting for threads to become available.","title":"Mechanism"},{"location":"Event-driven-concurrent-server/Design-pattern/Active-object%2Bobserver%2Bvisitor/","text":"Active-object+observer+visitor 1\u3001\u63a5\u6536\u6d88\u606f\uff0c\u6d88\u606f\u6e90\u6e90\u4e0d\u65ad\u5730\u4ea7\u751f 2\u3001\u5e76\u53d1 3\u3001\u6d88\u606f\u7684\u79cd\u7c7b\u975e\u5e38\u591a\uff0c\u4e0d\u540c\u7684observer\u5bf9\u4e0d\u540c\u7c7b\u578b\u7684\u6d88\u606f\u7684\u5904\u7406\u662f\u4e0d\u540c\u7684 \u5f02\u6b65\u5904\u7406\u6d88\u606f \u5982\u679c\u9700\u8981\u5f02\u6b65\u5904\u7406\u6d88\u606f\uff0c\u5219\u9700\u8981\u4f7f\u7528message queue\u4e86\u3002 4\u3001performance: a\u3001\u907f\u514d\u9891\u7e41\u7684new\uff0c\u4f7f\u7528inplacement new\u3001message queue(memory pool)\u3001\u4f7f\u7528copy\u66ff\u4ee3new \u53c2\u89c1\u5de5\u7a0b Linux-OS Kernel\\Guide\\Multitasking\\Process-model\\Process-resource\\Process-memory-model\\Virtual-address-space\\Segment\\Stack-VS-heap \u7ae0\u8282 b\u3001\u4e00\u65e6\u4f7f\u7528\u4e86message queue(memory pool)\uff0c\u5c31\u9700\u8981\u8ba9\u6240\u6709\u7684message\u90fd\u4fdd\u6301\u4e00\u4e2a\u7c7b\u578b c\u3001\u90a3\u5982\u4f55\u5bf9\u4e0d\u540c\u79cd\u7c7b\u7684message\u8fdb\u884cdispatch\u5462\uff1f \u901a\u8fc7if-else\u5b9e\u73b0\u4e0d\u540c\u7c7b\u578b\u7684\u6d88\u606f\u8fdb\u884c\u4e0d\u540c\u7684\u5904\u7406 \u4f7f\u7528visitor pattern\u5b9e\u73b0double dispatch \u540c\u6b65\u5904\u7406 \u76f8\u5bf9\u6bd4\u8f83\u7b80\u5355 draft Observer pattern and visitor pattern event source\u5c31\u662f\u662f\u4e00\u4e2aabstract structure\uff0cevent source\u4f1a\u4ea7\u751f\u5404\u79cdevent\uff0c\u5bf9\u4e8e\u6bcf\u79cdevent\uff0c\u4e0d\u540c\u7684listener\u9700\u8981\u6267\u884c\u4e0d\u540c\u7684algorithm\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u5176\u5b9e\u975e\u5e38\u7c7b\u4f3c\u4e8evisitor: 1) \u5c06event source\u770b\u505a\u662f\u7531\u8fd9\u4e9bevent\u7ec4\u6210\u7684abstract structure\uff0c\u5219\u6574\u4e2a\u8fc7\u7a0b\u5c31\u662f\u5bf9\u8fd9\u4e2aabstract structure\u7684visit\uff1b 2) call back Observer pattern + Visitor pattern for message system https://stackoverflow.com/questions/32079697/observer-pattern-visitor-pattern-for-message-system","title":"Introduction"},{"location":"Event-driven-concurrent-server/Design-pattern/Active-object%2Bobserver%2Bvisitor/#active-objectobservervisitor","text":"1\u3001\u63a5\u6536\u6d88\u606f\uff0c\u6d88\u606f\u6e90\u6e90\u4e0d\u65ad\u5730\u4ea7\u751f 2\u3001\u5e76\u53d1 3\u3001\u6d88\u606f\u7684\u79cd\u7c7b\u975e\u5e38\u591a\uff0c\u4e0d\u540c\u7684observer\u5bf9\u4e0d\u540c\u7c7b\u578b\u7684\u6d88\u606f\u7684\u5904\u7406\u662f\u4e0d\u540c\u7684","title":"Active-object+observer+visitor"},{"location":"Event-driven-concurrent-server/Design-pattern/Active-object%2Bobserver%2Bvisitor/#_1","text":"\u5982\u679c\u9700\u8981\u5f02\u6b65\u5904\u7406\u6d88\u606f\uff0c\u5219\u9700\u8981\u4f7f\u7528message queue\u4e86\u3002 4\u3001performance: a\u3001\u907f\u514d\u9891\u7e41\u7684new\uff0c\u4f7f\u7528inplacement new\u3001message queue(memory pool)\u3001\u4f7f\u7528copy\u66ff\u4ee3new \u53c2\u89c1\u5de5\u7a0b Linux-OS Kernel\\Guide\\Multitasking\\Process-model\\Process-resource\\Process-memory-model\\Virtual-address-space\\Segment\\Stack-VS-heap \u7ae0\u8282 b\u3001\u4e00\u65e6\u4f7f\u7528\u4e86message queue(memory pool)\uff0c\u5c31\u9700\u8981\u8ba9\u6240\u6709\u7684message\u90fd\u4fdd\u6301\u4e00\u4e2a\u7c7b\u578b c\u3001\u90a3\u5982\u4f55\u5bf9\u4e0d\u540c\u79cd\u7c7b\u7684message\u8fdb\u884cdispatch\u5462\uff1f \u901a\u8fc7if-else\u5b9e\u73b0\u4e0d\u540c\u7c7b\u578b\u7684\u6d88\u606f\u8fdb\u884c\u4e0d\u540c\u7684\u5904\u7406 \u4f7f\u7528visitor pattern\u5b9e\u73b0double dispatch","title":"\u5f02\u6b65\u5904\u7406\u6d88\u606f"},{"location":"Event-driven-concurrent-server/Design-pattern/Active-object%2Bobserver%2Bvisitor/#_2","text":"\u76f8\u5bf9\u6bd4\u8f83\u7b80\u5355","title":"\u540c\u6b65\u5904\u7406"},{"location":"Event-driven-concurrent-server/Design-pattern/Active-object%2Bobserver%2Bvisitor/#draft","text":"","title":"draft"},{"location":"Event-driven-concurrent-server/Design-pattern/Active-object%2Bobserver%2Bvisitor/#observer#pattern#and#visitor#pattern","text":"event source\u5c31\u662f\u662f\u4e00\u4e2aabstract structure\uff0cevent source\u4f1a\u4ea7\u751f\u5404\u79cdevent\uff0c\u5bf9\u4e8e\u6bcf\u79cdevent\uff0c\u4e0d\u540c\u7684listener\u9700\u8981\u6267\u884c\u4e0d\u540c\u7684algorithm\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u5176\u5b9e\u975e\u5e38\u7c7b\u4f3c\u4e8evisitor: 1) \u5c06event source\u770b\u505a\u662f\u7531\u8fd9\u4e9bevent\u7ec4\u6210\u7684abstract structure\uff0c\u5219\u6574\u4e2a\u8fc7\u7a0b\u5c31\u662f\u5bf9\u8fd9\u4e2aabstract structure\u7684visit\uff1b 2) call back","title":"Observer pattern and visitor pattern"},{"location":"Event-driven-concurrent-server/Design-pattern/Active-object%2Bobserver%2Bvisitor/#observer#pattern#visitor#pattern#for#message#system","text":"https://stackoverflow.com/questions/32079697/observer-pattern-visitor-pattern-for-message-system","title":"Observer pattern + Visitor pattern for message system"},{"location":"Event-driven-concurrent-server/Design-pattern/Active-object%2Bobserver%2Bvisitor/drdobbs-Message-Handling-Without-Dependencies/","text":"","title":"Introduction"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/","text":"Actor model and active object model VS \u4e24\u79cd\u4e4b\u95f4\u662f\u5426\u5b58\u5728\u5173\u8054\uff1f\u5728\u672c\u8282\u8fdb\u884c\u8ba8\u8bba\u3002 citeseerx Comparison of Active Objects and the Actor Model Abstract\u2014This paper compares two patterns of concurrency: the Actor Model and the Active Object Model . The Actor Model is a pattern that decouples function invocation from execution and offers both inherent thread-safety and scalability. The Active Object Model inherits from the Actor Model and as such presents the same major properties. As the Active Object Model inherits from the Actor Model one may think they are equivalent. Throughout this paper we show that both patterns differ in terms of structure design and communication protocols. These differences affect the choice of pattern for specific applications as each pattern has strengths and weaknesses. carlgibbs Contrasting Active Objects vs Tasks vs Actors","title":"Introduction"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/#actor#model#and#active#object#model","text":"","title":"Actor model  and active object model"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/#vs","text":"\u4e24\u79cd\u4e4b\u95f4\u662f\u5426\u5b58\u5728\u5173\u8054\uff1f\u5728\u672c\u8282\u8fdb\u884c\u8ba8\u8bba\u3002","title":"VS"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/#citeseerx#comparison#of#active#objects#and#the#actor#model","text":"Abstract\u2014This paper compares two patterns of concurrency: the Actor Model and the Active Object Model . The Actor Model is a pattern that decouples function invocation from execution and offers both inherent thread-safety and scalability. The Active Object Model inherits from the Actor Model and as such presents the same major properties. As the Active Object Model inherits from the Actor Model one may think they are equivalent. Throughout this paper we show that both patterns differ in terms of structure design and communication protocols. These differences affect the choice of pattern for specific applications as each pattern has strengths and weaknesses.","title":"citeseerx Comparison of Active Objects and the Actor Model"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/#carlgibbs#contrasting#active#objects#vs#tasks#vs#actors","text":"","title":"carlgibbs Contrasting Active Objects vs Tasks vs Actors"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Active-object/","text":"Active object pattern Herb Sutter Effective Concurrency: Prefer Using Active Objects Instead of Naked Threads \u975e\u5e38\u597d\u7684\u6587\u7ae0\uff0c\u5df2\u7ecf\u6536\u5f55\u4e86\u3002 wikipedia Active object The active object design pattern decouples method execution from method invocation for objects that each reside in their own thread of control.[ 1] The goal is to introduce concurrency , by using asynchronous method invocation and a scheduler for handling requests.[ 2] NOTE: 1\u3001\u7ed3\u5408\u4e0b\u9762\u7684\u4f8b\u5b50\u6765\u770b\uff0c\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u610f\u601d\u5176\u5b9e\u662fmethod invocation\u53ea\u662f\u5f80queue\u4e2d\u6dfb\u52a0\u4e86\u4e00\u4e2arequest/message\uff0cmethod execution\u5728\u53e6\u5916\u4e00\u4e2athread\u4e2d\u8fdb\u884c\uff1b 2\u3001 \u8fd9\u4e2a\u540d\u79f0\u7684\u7531\u6765\u662f\u4ec0\u4e48\uff1f\u4f55\u4e3aactive\uff1f\u5728 rosettacode Active object \u4e2d\u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u4e86\u89e3\u91ca 3\u3001\u5176\u5b9e\u6211\u7f16\u5199\u7684\u5f88\u591amultithread application\u90fd\u53ef\u4ee5\u5f52\u5165\u5230active object pattern\u7684\u8303\u7574 The pattern consists of six elements: A proxy , which provides an interface towards clients with publicly accessible methods. An interface which defines the method request on an active object. A list of pending requests from clients. A scheduler , which decides which request to execute next. The implementation of the active object method. A callback or variable for the client to receive the result. Example NOTE: 1\u3001\u539f\u6587\u7ed9\u51fa\u7684\u662fJava\u7684\u4f8b\u5b50 stackoverflow Explain \u201cActive-object\u201d pattern comments Active-object is the Command Pattern , implemented concurrently. A NOTE: 1\u3001\u975e\u5e38\u5f62\u8c61\u751f\u52a8\u7684\u6bd4\u55bb 2\u3001\u7ed3\u5408\u4e0b\u9762\u7684\u6bd4\u55bb\u6765\u7406\u89e3wikipedia Active object \u4e2d\u7684\u5185\u5bb9 The Active object pattern's goal is to separate the method calling from method execution. It is like a waiter in a restaurant, who just hands the orders from the customers to the chef. When a customer orders some food from the waiter the customer is the Client , the waiter is the Proxy he writes it up on a paper (obviously doesn't start to cook it), the paper is the MethodRequest , the table's number on the paper is the Future object gives the paper to the chef, who decides which cook should prepare it (who has time). the chef is the Scheduler who has a list of papers ( ActivationList ) and the cooks are the Servant -s NOTE: 1\u3001**ActivationList**\u5176\u5b9e\u5bf9\u5e94\u7684\u5c31\u662fqueue When the meal is ready, the cook places it on the serve bar and the waiter brings it to the customers table. Client reads the method's result, from the Result object. Implementation 1\u3001\u9700\u8981\u4e00\u4e2amessage queue\u3002 TODO 1\u3001wikipedia Active object 2\u3001stackoverflow To use Active object or not? 3\u3001 https://sites.google.com/site/kjellhedstrom2/active-object-with-cpp0x","title":"Introduction"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Active-object/#active#object#pattern","text":"","title":"Active object pattern"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Active-object/#herb#sutter#effective#concurrency#prefer#using#active#objects#instead#of#naked#threads","text":"\u975e\u5e38\u597d\u7684\u6587\u7ae0\uff0c\u5df2\u7ecf\u6536\u5f55\u4e86\u3002","title":"Herb Sutter Effective Concurrency: Prefer Using Active Objects Instead of Naked Threads"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Active-object/#wikipedia#active#object","text":"The active object design pattern decouples method execution from method invocation for objects that each reside in their own thread of control.[ 1] The goal is to introduce concurrency , by using asynchronous method invocation and a scheduler for handling requests.[ 2] NOTE: 1\u3001\u7ed3\u5408\u4e0b\u9762\u7684\u4f8b\u5b50\u6765\u770b\uff0c\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u610f\u601d\u5176\u5b9e\u662fmethod invocation\u53ea\u662f\u5f80queue\u4e2d\u6dfb\u52a0\u4e86\u4e00\u4e2arequest/message\uff0cmethod execution\u5728\u53e6\u5916\u4e00\u4e2athread\u4e2d\u8fdb\u884c\uff1b 2\u3001 \u8fd9\u4e2a\u540d\u79f0\u7684\u7531\u6765\u662f\u4ec0\u4e48\uff1f\u4f55\u4e3aactive\uff1f\u5728 rosettacode Active object \u4e2d\u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u4e86\u89e3\u91ca 3\u3001\u5176\u5b9e\u6211\u7f16\u5199\u7684\u5f88\u591amultithread application\u90fd\u53ef\u4ee5\u5f52\u5165\u5230active object pattern\u7684\u8303\u7574 The pattern consists of six elements: A proxy , which provides an interface towards clients with publicly accessible methods. An interface which defines the method request on an active object. A list of pending requests from clients. A scheduler , which decides which request to execute next. The implementation of the active object method. A callback or variable for the client to receive the result.","title":"wikipedia Active object"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Active-object/#example","text":"NOTE: 1\u3001\u539f\u6587\u7ed9\u51fa\u7684\u662fJava\u7684\u4f8b\u5b50","title":"Example"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Active-object/#stackoverflow#explain#active-object#pattern","text":"","title":"stackoverflow Explain \u201cActive-object\u201d pattern"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Active-object/#comments","text":"Active-object is the Command Pattern , implemented concurrently.","title":"comments"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Active-object/#a","text":"NOTE: 1\u3001\u975e\u5e38\u5f62\u8c61\u751f\u52a8\u7684\u6bd4\u55bb 2\u3001\u7ed3\u5408\u4e0b\u9762\u7684\u6bd4\u55bb\u6765\u7406\u89e3wikipedia Active object \u4e2d\u7684\u5185\u5bb9 The Active object pattern's goal is to separate the method calling from method execution. It is like a waiter in a restaurant, who just hands the orders from the customers to the chef. When a customer orders some food from the waiter the customer is the Client , the waiter is the Proxy he writes it up on a paper (obviously doesn't start to cook it), the paper is the MethodRequest , the table's number on the paper is the Future object gives the paper to the chef, who decides which cook should prepare it (who has time). the chef is the Scheduler who has a list of papers ( ActivationList ) and the cooks are the Servant -s NOTE: 1\u3001**ActivationList**\u5176\u5b9e\u5bf9\u5e94\u7684\u5c31\u662fqueue When the meal is ready, the cook places it on the serve bar and the waiter brings it to the customers table. Client reads the method's result, from the Result object.","title":"A"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Active-object/#implementation","text":"1\u3001\u9700\u8981\u4e00\u4e2amessage queue\u3002","title":"Implementation"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Active-object/#todo","text":"1\u3001wikipedia Active object 2\u3001stackoverflow To use Active object or not? 3\u3001 https://sites.google.com/site/kjellhedstrom2/active-object-with-cpp0x","title":"TODO"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Active-object/Asynchronous-method-invocation/","text":"Asynchronous method invocation","title":"Asynchronous-method-invocation"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Active-object/Asynchronous-method-invocation/#asynchronous#method#invocation","text":"","title":"Asynchronous method invocation"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Active-object/Impl/","text":"Active object pattern implementation \u9664\u4e86\u672c\u6587\u6536\u5f55\u7684\uff0c\u4e0b\u6765\u4e5f\u4f7f\u7528\u4e86active object : 1\u3001spdlog \u5b83\u7684implementation\u548c lightful /** syscpp **\u6709\u4e9b\u7c7b\u4f3c","title":"Introduction"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Active-object/Impl/#active#object#pattern#implementation","text":"\u9664\u4e86\u672c\u6587\u6536\u5f55\u7684\uff0c\u4e0b\u6765\u4e5f\u4f7f\u7528\u4e86active object : 1\u3001spdlog \u5b83\u7684implementation\u548c lightful /** syscpp **\u6709\u4e9b\u7c7b\u4f3c","title":"Active object pattern implementation"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Active-object/Impl/TODO-codeproject-A%20Technique%20for%20Active%20Object%20Aggregation%20in%20C%2B%2B/","text":"codeproject A Technique for Active Object Aggregation in C++ NOTE: 1\u3001\u6d4f\u89c8\u4e86\u4e00\u4e9b\u539f\u6587\uff0c\u53d1\u73b0\u5b83\u7684\u91cd\u70b9\u662f\u63cf\u8ff0: \u4f7f\u7528reference counting\u6765\u7ba1\u7406object","title":"codeproject [A Technique for Active Object Aggregation in C++](https://www.codeproject.com/articles/9474/a-technique-for-active-object-aggregation-in-c)"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Active-object/Impl/TODO-codeproject-A%20Technique%20for%20Active%20Object%20Aggregation%20in%20C%2B%2B/#codeproject#a#technique#for#active#object#aggregation#in#c","text":"NOTE: 1\u3001\u6d4f\u89c8\u4e86\u4e00\u4e9b\u539f\u6587\uff0c\u53d1\u73b0\u5b83\u7684\u91cd\u70b9\u662f\u63cf\u8ff0: \u4f7f\u7528reference counting\u6765\u7ba1\u7406object","title":"codeproject A Technique for Active Object Aggregation in C++"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Active-object/Impl/rosettacode-Active-object/","text":"rosettacode Active object NOTE: 1\u3001\u611f\u89c9\u8fd9\u7bc7\u6587\u7ae0\u6240\u63cf\u8ff0\u7684\u4e0d\u662factive object pattern In object-oriented programming an object is active when its state depends on clock\uff08\u968f\u7740\u65f6\u95f4\u7684\u6d41\u901d\uff0c\u5b83\u7684state\u53ef\u80fd\u4f1a\u53d8\u66f4\uff0c\u8fd9\u5c31\u662factive\u7684\uff09. Usually an active object encapsulates a task that updates the object's state. To the outer world the object looks like a normal object with methods that can be called from outside. Implementation of such methods must have a certain synchronization mechanism with the encapsulated task in order to prevent object's state corruption. A typical instance of an active object is an animation widget. The widget state changes with the time, while as an object it has all properties of a normal widget. The task Implement an active integrator(\u79ef\u5206\u5668) object. The object has an input and output . The input can be set using the method Input . The input is a function of time\uff08\u5173\u4e8e\u65f6\u95f4\u7684\u51fd\u6570\uff09. The output can be queried using the method Output . The object integrates its input over the time and the result becomes the object's output. So if the input is K ( t ) and the output is S , the object state S is changed to S + ( K ( t*1) + *K ( t*0)) * (*t*1 - *t*0) / 2, i.e. it integrates *K using the trapeze method. Initially K is constant 0 and S is 0. In order to test the object: set its input to sin (2\u03c0 f t ), where the frequency f =0.5Hz. The phase is irrelevant. wait 2s set the input to constant 0 wait 0.5s Verify that now the object's output is approximately 0 (the sine has the period of 2s). The accuracy of the result will depend on the OS scheduler time slicing and the accuracy of the clock. C Uses POSIX threads. Library: pthread #include <stdio.h> #include <stdlib.h> #include <unistd.h> #include <math.h> #include <sys/time.h> #include <pthread.h> /* no need to lock the object: at worst the readout would be 1 tick off, which is no worse than integrator's inate inaccuracy */ typedef struct { double ( * func )( double ); struct timeval start ; double v , last_v , last_t ; pthread_t id ; } integ_t , * integ ; void update ( integ x ) { struct timeval tv ; double t , v , ( * f )( double ); f = x -> func ; gettimeofday ( & tv , 0 ); t = (( tv . tv_sec - x -> start . tv_sec ) * 1000000 + tv . tv_usec - x -> start . tv_usec ) * 1e-6 ; v = f ? f ( t ) : 0 ; x -> v += ( x -> last_v + v ) * ( t - x -> last_t ) / 2 ; x -> last_t = t ; } void * tick ( void * a ) { integ x = a ; while ( 1 ) { usleep ( 100000 ); /* update every .1 sec */ update ( x ); } } void set_input ( integ x , double ( * func )( double )) { update ( x ); x -> func = func ; x -> last_t = 0 ; x -> last_v = func ? func ( 0 ) : 0 ; } integ new_integ ( double ( * func )( double )) { integ x = malloc ( sizeof ( integ_t )); x -> v = x -> last_v = 0 ; x -> func = 0 ; gettimeofday ( & x -> start , 0 ); set_input ( x , func ); pthread_create ( & x -> id , 0 , tick , x ); return x ; } double sine ( double t ) { return sin ( 4 * atan2 ( 1 , 1 ) * t ); } int main () { integ x = new_integ ( sine ); sleep ( 2 ); set_input ( x , 0 ); usleep ( 500000 ); printf ( \"%g \\n \" , x -> v ); return 0 ; } C++ Works with : C++14 #include <atomic> #include <chrono> #include <cmath> #include <iostream> #include <mutex> #include <thread> using namespace std :: chrono_literals ; class Integrator { public : using clock_type = std :: chrono :: high_resolution_clock ; using dur_t = std :: chrono :: duration < double > ; using func_t = double ( * )( double ); explicit Integrator ( func_t f = nullptr ); ~ Integrator (); void input ( func_t new_input ); double output () { return integrate (); } private : std :: atomic_flag continue_ ; std :: mutex mutex ; std :: thread worker ; func_t func ; double state = 0 ; //Improves precision by reducing sin result error on large values clock_type :: time_point const beginning = clock_type :: now (); clock_type :: time_point t_prev = beginning ; void do_work (); double integrate (); }; Integrator :: Integrator ( func_t f ) : func ( f ) { continue_ . test_and_set (); worker = std :: thread ( & Integrator :: do_work , this ); } Integrator ::~ Integrator () { continue_ . clear (); worker . join (); } void Integrator :: input ( func_t new_input ) { integrate (); std :: lock_guard < std :: mutex > lock ( mutex ); func = new_input ; } void Integrator :: do_work () { while ( continue_ . test_and_set ()) { integrate (); std :: this_thread :: sleep_for ( 1 ms ); } } double Integrator :: integrate () { std :: lock_guard < std :: mutex > lock ( mutex ); auto now = clock_type :: now (); dur_t start = t_prev - beginning ; dur_t fin = now - beginning ; if ( func ) state += ( func ( start . count ()) + func ( fin . count ())) * ( fin - start ). count () / 2 ; t_prev = now ; return state ; } double sine ( double time ) { constexpr double PI = 3.1415926535897932 ; return std :: sin ( 2 * PI * 0.5 * time ); } int main () { Integrator foo ( sine ); std :: this_thread :: sleep_for ( 2 s ); foo . input ( nullptr ); std :: this_thread :: sleep_for ( 500 ms ); std :: cout << foo . output (); } Python Assignment is thread-safe in Python, so no extra locks are needed in this case. from time import time , sleep from threading import Thread class Integrator ( Thread ): 'continuously integrate a function `K`, at each `interval` seconds' def __init__ ( self , K = lambda t : 0 , interval = 1e-4 ): Thread . __init__ ( self ) self . interval = interval self . K = K self . S = 0.0 self . __run = True self . start () def run ( self ): \"entry point for the thread\" interval = self . interval start = time () t0 , k0 = 0 , self . K ( 0 ) while self . __run : sleep ( interval ) t1 = time () - start k1 = self . K ( t1 ) self . S += ( k1 + k0 ) * ( t1 - t0 ) / 2.0 t0 , k0 = t1 , k1 def join ( self ): self . __run = False Thread . join ( self ) if __name__ == \"__main__\" : from math import sin , pi ai = Integrator ( lambda t : sin ( pi * t )) sleep ( 2 ) print ai . S ai . K = lambda t : 0 sleep ( 0.5 ) print ai . S","title":"rosettacode-Active-object"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Active-object/Impl/rosettacode-Active-object/#rosettacode#active#object","text":"NOTE: 1\u3001\u611f\u89c9\u8fd9\u7bc7\u6587\u7ae0\u6240\u63cf\u8ff0\u7684\u4e0d\u662factive object pattern In object-oriented programming an object is active when its state depends on clock\uff08\u968f\u7740\u65f6\u95f4\u7684\u6d41\u901d\uff0c\u5b83\u7684state\u53ef\u80fd\u4f1a\u53d8\u66f4\uff0c\u8fd9\u5c31\u662factive\u7684\uff09. Usually an active object encapsulates a task that updates the object's state. To the outer world the object looks like a normal object with methods that can be called from outside. Implementation of such methods must have a certain synchronization mechanism with the encapsulated task in order to prevent object's state corruption. A typical instance of an active object is an animation widget. The widget state changes with the time, while as an object it has all properties of a normal widget. The task Implement an active integrator(\u79ef\u5206\u5668) object. The object has an input and output . The input can be set using the method Input . The input is a function of time\uff08\u5173\u4e8e\u65f6\u95f4\u7684\u51fd\u6570\uff09. The output can be queried using the method Output . The object integrates its input over the time and the result becomes the object's output. So if the input is K ( t ) and the output is S , the object state S is changed to S + ( K ( t*1) + *K ( t*0)) * (*t*1 - *t*0) / 2, i.e. it integrates *K using the trapeze method. Initially K is constant 0 and S is 0. In order to test the object: set its input to sin (2\u03c0 f t ), where the frequency f =0.5Hz. The phase is irrelevant. wait 2s set the input to constant 0 wait 0.5s Verify that now the object's output is approximately 0 (the sine has the period of 2s). The accuracy of the result will depend on the OS scheduler time slicing and the accuracy of the clock.","title":"rosettacode Active object"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Active-object/Impl/rosettacode-Active-object/#c","text":"Uses POSIX threads. Library: pthread #include <stdio.h> #include <stdlib.h> #include <unistd.h> #include <math.h> #include <sys/time.h> #include <pthread.h> /* no need to lock the object: at worst the readout would be 1 tick off, which is no worse than integrator's inate inaccuracy */ typedef struct { double ( * func )( double ); struct timeval start ; double v , last_v , last_t ; pthread_t id ; } integ_t , * integ ; void update ( integ x ) { struct timeval tv ; double t , v , ( * f )( double ); f = x -> func ; gettimeofday ( & tv , 0 ); t = (( tv . tv_sec - x -> start . tv_sec ) * 1000000 + tv . tv_usec - x -> start . tv_usec ) * 1e-6 ; v = f ? f ( t ) : 0 ; x -> v += ( x -> last_v + v ) * ( t - x -> last_t ) / 2 ; x -> last_t = t ; } void * tick ( void * a ) { integ x = a ; while ( 1 ) { usleep ( 100000 ); /* update every .1 sec */ update ( x ); } } void set_input ( integ x , double ( * func )( double )) { update ( x ); x -> func = func ; x -> last_t = 0 ; x -> last_v = func ? func ( 0 ) : 0 ; } integ new_integ ( double ( * func )( double )) { integ x = malloc ( sizeof ( integ_t )); x -> v = x -> last_v = 0 ; x -> func = 0 ; gettimeofday ( & x -> start , 0 ); set_input ( x , func ); pthread_create ( & x -> id , 0 , tick , x ); return x ; } double sine ( double t ) { return sin ( 4 * atan2 ( 1 , 1 ) * t ); } int main () { integ x = new_integ ( sine ); sleep ( 2 ); set_input ( x , 0 ); usleep ( 500000 ); printf ( \"%g \\n \" , x -> v ); return 0 ; }","title":"C"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Active-object/Impl/rosettacode-Active-object/#c_1","text":"Works with : C++14 #include <atomic> #include <chrono> #include <cmath> #include <iostream> #include <mutex> #include <thread> using namespace std :: chrono_literals ; class Integrator { public : using clock_type = std :: chrono :: high_resolution_clock ; using dur_t = std :: chrono :: duration < double > ; using func_t = double ( * )( double ); explicit Integrator ( func_t f = nullptr ); ~ Integrator (); void input ( func_t new_input ); double output () { return integrate (); } private : std :: atomic_flag continue_ ; std :: mutex mutex ; std :: thread worker ; func_t func ; double state = 0 ; //Improves precision by reducing sin result error on large values clock_type :: time_point const beginning = clock_type :: now (); clock_type :: time_point t_prev = beginning ; void do_work (); double integrate (); }; Integrator :: Integrator ( func_t f ) : func ( f ) { continue_ . test_and_set (); worker = std :: thread ( & Integrator :: do_work , this ); } Integrator ::~ Integrator () { continue_ . clear (); worker . join (); } void Integrator :: input ( func_t new_input ) { integrate (); std :: lock_guard < std :: mutex > lock ( mutex ); func = new_input ; } void Integrator :: do_work () { while ( continue_ . test_and_set ()) { integrate (); std :: this_thread :: sleep_for ( 1 ms ); } } double Integrator :: integrate () { std :: lock_guard < std :: mutex > lock ( mutex ); auto now = clock_type :: now (); dur_t start = t_prev - beginning ; dur_t fin = now - beginning ; if ( func ) state += ( func ( start . count ()) + func ( fin . count ())) * ( fin - start ). count () / 2 ; t_prev = now ; return state ; } double sine ( double time ) { constexpr double PI = 3.1415926535897932 ; return std :: sin ( 2 * PI * 0.5 * time ); } int main () { Integrator foo ( sine ); std :: this_thread :: sleep_for ( 2 s ); foo . input ( nullptr ); std :: this_thread :: sleep_for ( 500 ms ); std :: cout << foo . output (); }","title":"C++"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Active-object/Impl/rosettacode-Active-object/#python","text":"Assignment is thread-safe in Python, so no extra locks are needed in this case. from time import time , sleep from threading import Thread class Integrator ( Thread ): 'continuously integrate a function `K`, at each `interval` seconds' def __init__ ( self , K = lambda t : 0 , interval = 1e-4 ): Thread . __init__ ( self ) self . interval = interval self . K = K self . S = 0.0 self . __run = True self . start () def run ( self ): \"entry point for the thread\" interval = self . interval start = time () t0 , k0 = 0 , self . K ( 0 ) while self . __run : sleep ( interval ) t1 = time () - start k1 = self . K ( t1 ) self . S += ( k1 + k0 ) * ( t1 - t0 ) / 2.0 t0 , k0 = t1 , k1 def join ( self ): self . __run = False Thread . join ( self ) if __name__ == \"__main__\" : from math import sin , pi ai = Integrator ( lambda t : sin ( pi * t )) sleep ( 2 ) print ai . S ai . K = lambda t : 0 sleep ( 0.5 ) print ai . S","title":"Python"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Active-object/Impl/kjellhedstrom2-Active-Object-the-C%2B%2B11-way/","text":"kjellhedstrom2 Active Object the C++11 way Introduction When Herb Sutter wrote about Active Objects in this Effective Concurrency blog series [blog entry: Prefer Using Active Objects Instead of Naked Threads ] I was thrilled since I was waiting for that particular blog post since his Effective Concurrency seminar \uff08\u7814\u8ba8\u4f1a\uff09 in Kista, Stockholm back in 2009. Herb explained then and later in his blog post a cookbook for getting your Active Object right. This could be done either with standard C++ using an object oriented approach, by passing message objects to the thread queue, or by using C++11 with lambda support and std::thread and other niceties. NOTE: \u7ffb\u8bd1\u5982\u4e0b: \"\u5f53Herb Sutter\u5728\u8fd9\u4e2a\u6709\u6548\u5e76\u53d1\u535a\u5ba2\u7cfb\u5217(\u535a\u5ba2\u6761\u76ee:\u66f4\u559c\u6b22\u4f7f\u7528\u6d3b\u52a8\u5bf9\u8c61\u800c\u4e0d\u662f\u88f8\u7ebf\u7a0b)\u4e2d\u5199\u5173\u4e8e\u6d3b\u52a8\u5bf9\u8c61\u7684\u6587\u7ae0\u65f6\uff0c\u6211\u5f88\u6fc0\u52a8\uff0c\u56e0\u4e3a\u81ea\u4ece2009\u5e74\u4ed6\u5728\u65af\u5fb7\u54e5\u5c14\u6469Kista\u7684\u6709\u6548\u5e76\u53d1\u7814\u8ba8\u4f1a\u4e4b\u540e\uff0c\u6211\u5c31\u4e00\u76f4\u5728\u7b49\u5f85\u90a3\u4e2a\u7279\u522b\u7684\u535a\u5ba2\u5e16\u5b50\u3002Herb\u540e\u6765\u5728\u4ed6\u7684\u535a\u5ba2\u4e2d\u89e3\u91ca\u4e86\u5982\u4f55\u6b63\u786e\u4f7f\u7528\u4f60\u7684\u6d3b\u52a8\u5bf9\u8c61\u3002\u8fd9\u53ef\u4ee5\u901a\u8fc7\u6807\u51c6\u7684c++\u4f7f\u7528\u9762\u5411\u5bf9\u8c61\u7684\u65b9\u6cd5\u6765\u5b9e\u73b0\uff0c\u901a\u8fc7\u5c06\u6d88\u606f\u5bf9\u8c61\u4f20\u9012\u5230\u7ebf\u7a0b\u961f\u5217\uff0c\u6216\u8005\u901a\u8fc7\u4f7f\u7528c++ 11\u4f7f\u7528lambda\u652f\u6301\u548cstd::thread\u548c\u5176\u4ed6\u7ec6\u8282\u3002\" Since I thought the \u201c object oriented \u201d approach was clumsy and would lead to lots of boilerplate \uff08\u6837\u677f\uff09code I worked on a a more clean approach that would to use the features similar to C++11 but using template magic for ( Gotw83 ) generic callbacks and normal standard C++ (without using std::tr1::function ). I showed this in my old blog with standard c++ code that can be downloaded from kjellkod.cc/active_object . However, just recently I made a job move from HiQ to Prevas and with that came a week or so between assignments. This gave me a great opportunity to learn more of the new C++11 with the help of just::thread implementation of the new C++11 standard library. The result was much cleaner, more readable code with guaranteed cross-platform behavior using a mutex protected message queue for thread communication. This is a huge improvement over the sketchy \u201cl ock-free circular fifo \u201d I used in my first Active Object example . For the new and improved Active Object I used standard library components and with the just::thread portability bundle I soon had a cross platform Active Object that works like a charm on both Windows (Visual Studio 2010 Express) and Linux Ubuntu (g++) . Using Git you can download the latest snapshot of my Active Object example at git://github.com/KjellKod/active-object.git or you can download (top right) a possibly old snapshot. The Active Object is also used in the popular and highly efficient asynchronous logger g2log. You can read about it here , and download its latest snapshot from BitBucket. NOTE: 1\u3001\u7a0d\u5fae\u6d4f\u89c8\u4e86\u4e0b\u9762\u7684code\uff0c\u5199\u5f97\u4e00\u822c Active Object .h Below is the updated Active Object class . The createActive () function is a helper factory function to ensure that the object is fully created before running a thread on it ' s run () function . typedef std :: function < void () > Callback ; class Active { private : Active ( const Active & ) = delete ; Active & operator = ( const Active & ) = delete ; Active (); // see: createActive() void doDone (){ done = true ;} void run (); shared_queue < Callback > mq ; std :: thread thd ; bool done ; // flag for finishing public : virtual ~ Active (); void send ( Callback msg_ ); // Factory: safe construction & thread start static std :: unique_ptr < Active > createActive (); }; The main difference from my first example apart from the queue is that here I use std::function to represent the jobs in the work queue instread of a home baked encapsuled generic callback Active Object .cpp The implementation is very straight forward. The background thread will sleep until a producing thread puts more work onto the thread with send(...). When a new item is put onto the queue the thread is notified, wakes up and starts to process the new item(s). At destruction a quit message is given from the destructor. This effectively makes sure that all messages are flushed and at the quit message the thread will exit. Active ::~ Active () { Callback quit_token = std :: bind ( & Active :: doDone , this ); send ( quit_token ); // tell thread to exit thd_ . join (); } // Add asynchronously to queue void Active :: send ( Callback msg_ ){ mq . push ( msg_ ); } // Will wait for msgs if queue is empty void Active :: run () { while ( ! done_ ) { // wait till job is available, then retrieve it and // executes the retrieved job in this thread (background) Callback func ; mq_ . wait_and_pop ( func ); func (); } } // Factory: safe construction of object before thread start std :: unique_ptr < Active > Active :: createActive (){ std :: unique_ptr < Active > aPtr ( new Active ()); aPtr -> thd = std :: thread ( & Active :: run , aPtr . get ()); return aPtr ; }","title":"Introduction"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Active-object/Impl/kjellhedstrom2-Active-Object-the-C%2B%2B11-way/#kjellhedstrom2#active#object#the#c11#way","text":"","title":"kjellhedstrom2 Active Object the C++11 way"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Active-object/Impl/kjellhedstrom2-Active-Object-the-C%2B%2B11-way/#introduction","text":"When Herb Sutter wrote about Active Objects in this Effective Concurrency blog series [blog entry: Prefer Using Active Objects Instead of Naked Threads ] I was thrilled since I was waiting for that particular blog post since his Effective Concurrency seminar \uff08\u7814\u8ba8\u4f1a\uff09 in Kista, Stockholm back in 2009. Herb explained then and later in his blog post a cookbook for getting your Active Object right. This could be done either with standard C++ using an object oriented approach, by passing message objects to the thread queue, or by using C++11 with lambda support and std::thread and other niceties. NOTE: \u7ffb\u8bd1\u5982\u4e0b: \"\u5f53Herb Sutter\u5728\u8fd9\u4e2a\u6709\u6548\u5e76\u53d1\u535a\u5ba2\u7cfb\u5217(\u535a\u5ba2\u6761\u76ee:\u66f4\u559c\u6b22\u4f7f\u7528\u6d3b\u52a8\u5bf9\u8c61\u800c\u4e0d\u662f\u88f8\u7ebf\u7a0b)\u4e2d\u5199\u5173\u4e8e\u6d3b\u52a8\u5bf9\u8c61\u7684\u6587\u7ae0\u65f6\uff0c\u6211\u5f88\u6fc0\u52a8\uff0c\u56e0\u4e3a\u81ea\u4ece2009\u5e74\u4ed6\u5728\u65af\u5fb7\u54e5\u5c14\u6469Kista\u7684\u6709\u6548\u5e76\u53d1\u7814\u8ba8\u4f1a\u4e4b\u540e\uff0c\u6211\u5c31\u4e00\u76f4\u5728\u7b49\u5f85\u90a3\u4e2a\u7279\u522b\u7684\u535a\u5ba2\u5e16\u5b50\u3002Herb\u540e\u6765\u5728\u4ed6\u7684\u535a\u5ba2\u4e2d\u89e3\u91ca\u4e86\u5982\u4f55\u6b63\u786e\u4f7f\u7528\u4f60\u7684\u6d3b\u52a8\u5bf9\u8c61\u3002\u8fd9\u53ef\u4ee5\u901a\u8fc7\u6807\u51c6\u7684c++\u4f7f\u7528\u9762\u5411\u5bf9\u8c61\u7684\u65b9\u6cd5\u6765\u5b9e\u73b0\uff0c\u901a\u8fc7\u5c06\u6d88\u606f\u5bf9\u8c61\u4f20\u9012\u5230\u7ebf\u7a0b\u961f\u5217\uff0c\u6216\u8005\u901a\u8fc7\u4f7f\u7528c++ 11\u4f7f\u7528lambda\u652f\u6301\u548cstd::thread\u548c\u5176\u4ed6\u7ec6\u8282\u3002\" Since I thought the \u201c object oriented \u201d approach was clumsy and would lead to lots of boilerplate \uff08\u6837\u677f\uff09code I worked on a a more clean approach that would to use the features similar to C++11 but using template magic for ( Gotw83 ) generic callbacks and normal standard C++ (without using std::tr1::function ). I showed this in my old blog with standard c++ code that can be downloaded from kjellkod.cc/active_object . However, just recently I made a job move from HiQ to Prevas and with that came a week or so between assignments. This gave me a great opportunity to learn more of the new C++11 with the help of just::thread implementation of the new C++11 standard library. The result was much cleaner, more readable code with guaranteed cross-platform behavior using a mutex protected message queue for thread communication. This is a huge improvement over the sketchy \u201cl ock-free circular fifo \u201d I used in my first Active Object example . For the new and improved Active Object I used standard library components and with the just::thread portability bundle I soon had a cross platform Active Object that works like a charm on both Windows (Visual Studio 2010 Express) and Linux Ubuntu (g++) . Using Git you can download the latest snapshot of my Active Object example at git://github.com/KjellKod/active-object.git or you can download (top right) a possibly old snapshot. The Active Object is also used in the popular and highly efficient asynchronous logger g2log. You can read about it here , and download its latest snapshot from BitBucket. NOTE: 1\u3001\u7a0d\u5fae\u6d4f\u89c8\u4e86\u4e0b\u9762\u7684code\uff0c\u5199\u5f97\u4e00\u822c","title":"Introduction"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Active-object/Impl/kjellhedstrom2-Active-Object-the-C%2B%2B11-way/#active#object#h","text":"Below is the updated Active Object class . The createActive () function is a helper factory function to ensure that the object is fully created before running a thread on it ' s run () function . typedef std :: function < void () > Callback ; class Active { private : Active ( const Active & ) = delete ; Active & operator = ( const Active & ) = delete ; Active (); // see: createActive() void doDone (){ done = true ;} void run (); shared_queue < Callback > mq ; std :: thread thd ; bool done ; // flag for finishing public : virtual ~ Active (); void send ( Callback msg_ ); // Factory: safe construction & thread start static std :: unique_ptr < Active > createActive (); }; The main difference from my first example apart from the queue is that here I use std::function to represent the jobs in the work queue instread of a home baked encapsuled generic callback","title":"Active Object .h"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Active-object/Impl/kjellhedstrom2-Active-Object-the-C%2B%2B11-way/#active#object#cpp","text":"The implementation is very straight forward. The background thread will sleep until a producing thread puts more work onto the thread with send(...). When a new item is put onto the queue the thread is notified, wakes up and starts to process the new item(s). At destruction a quit message is given from the destructor. This effectively makes sure that all messages are flushed and at the quit message the thread will exit. Active ::~ Active () { Callback quit_token = std :: bind ( & Active :: doDone , this ); send ( quit_token ); // tell thread to exit thd_ . join (); } // Add asynchronously to queue void Active :: send ( Callback msg_ ){ mq . push ( msg_ ); } // Will wait for msgs if queue is empty void Active :: run () { while ( ! done_ ) { // wait till job is available, then retrieve it and // executes the retrieved job in this thread (background) Callback func ; mq_ . wait_and_pop ( func ); func (); } } // Factory: safe construction of object before thread start std :: unique_ptr < Active > Active :: createActive (){ std :: unique_ptr < Active > aPtr ( new Active ()); aPtr -> thd = std :: thread ( & Active :: run , aPtr . get ()); return aPtr ; }","title":"Active Object .cpp"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Active-object/Impl/library-syscpp/","text":"lightful / syscpp ActorThread: Active Object pattern in C++ Implementation of the Active Object pattern wrapping a standard C++11 thread. Simple 1\u3001Whole implementation contained in a single header file! 2\u3001Inherit from a template and you are done. See the tiny example bellow. NOTE: 1\u3001CRTP mixin from above Performance 1\u3001Internal lock-free MPSC messages queue NOTE: \u9700\u8981\u8003\u8651\u662f\u5982\u4f55\u5b9e\u73b0\u7684 2\u3001Extensive internal use of move semantics supporting delivery of non-copiable objects NOTE: 1\u3001prefer move to copy\uff0c std::vector \u4e5f\u662f\u8fd9\u6837\u505a\u7684\uff0c\u53c2\u89c1 a\u3001 \u5982\u4f55\u8bc4\u4ef7 C++11 \u7684\u53f3\u503c\u5f15\u7528\uff08Rvalue reference\uff09\u7279\u6027\uff1f - Tinro\u7684\u56de\u7b54 - \u77e5\u4e4e 3\u3001Several million msg/sec between each two threads (both Linux and Windows) in ordinary hardware Robustness 1\u3001The wrapped thread lifecycle overlaps and is driven by the object existence 2\u3001The object is kept alive by smart pointers (whoever has a reference can safely send messages) 3\u3001No internal strong references (only the final users determine the destruction/end) 4\u3001Nonetheless, callbacks onto already deleted active objects do not crash the application NOTE: 1\u3001\u9605\u8bfb\u4e86\u4e0a\u8ff0\u5185\u5bb9\uff0c\u53d1\u73b0\u5b83\u7684\u5b9e\u73b0\u5176\u5b9e\u548cspdlog\u662f\u975e\u5e38\u7c7b\u4f3c\u7684: a\u3001spdlog\u4e5f\u662f\u4f7f\u7528\u7684active object pattern b\u3001spdlog\u4e2d\u4e5f\u4f7f\u7528\u4e86 std::shared_ptr Example // Linux: g++ -std=c++11 -lpthread demo.cpp -o demo // Windows: cl.exe demo.cpp #include <string> #include <iostream> #include <sstream> #include \"ActorThread.hpp\" struct Message { std :: string description ; }; struct OtherMessage { double beautifulness ; }; class Consumer : public ActorThread < Consumer > { friend ActorThread < Consumer > ; void onMessage ( Message & p ) { std :: cout << \"thread \" << std :: this_thread :: get_id () << \" receiving \" << p . description << std :: endl ; } void onMessage ( OtherMessage & p ) { std :: cout << \"thread \" << std :: this_thread :: get_id () << \" receiving \" << p . beautifulness << std :: endl ; } }; class Producer : public ActorThread < Producer > { friend ActorThread < Producer > ; std :: shared_ptr < Consumer > consumer ; void onMessage ( std :: shared_ptr < Consumer >& c ) { consumer = c ; timerStart ( true , std :: chrono :: milliseconds ( 250 ), TimerCycle :: Periodic ); timerStart ( 3.14 , std :: chrono :: milliseconds ( 333 ), TimerCycle :: Periodic ); } void onTimer ( const bool & ) { std :: ostringstream report ; report << \"test from thread \" << std :: this_thread :: get_id (); consumer -> send ( Message { report . str () }); } void onTimer ( const double & value ) { consumer -> send ( OtherMessage { value }); } }; class Application : public ActorThread < Application > { friend ActorThread < Application > ; void onStart () { auto consumer = Consumer :: create (); // spawn new threads auto producer = Producer :: create (); producer -> send ( consumer ); std :: this_thread :: sleep_for ( std :: chrono :: seconds ( 3 )); stop (); } }; int main () { return Application :: run (); // re-use existing thread } NOTE: 1\u3001\u4ece\u4e0a\u8ff0code\u6765\u770b\uff0c\u5b83\u662f\u975e\u5e38\u7b80\u6d01\u7684 2\u3001\u5b83\u4f7f\u7528\u4e86CRTP\u3001mixin-from-above\u3001friend base class 3\u3001\u4e0a\u8ff0 onMessage \uff0c\u8ba9\u6211\u60f3\u5230\u4e86: visitor message pattern","title":"Introduction"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Active-object/Impl/library-syscpp/#lightfulsyscpp","text":"","title":"lightful/syscpp"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Active-object/Impl/library-syscpp/#actorthread#active#object#pattern#in#c","text":"Implementation of the Active Object pattern wrapping a standard C++11 thread.","title":"ActorThread: Active Object pattern in C++"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Active-object/Impl/library-syscpp/#simple","text":"1\u3001Whole implementation contained in a single header file! 2\u3001Inherit from a template and you are done. See the tiny example bellow. NOTE: 1\u3001CRTP mixin from above","title":"Simple"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Active-object/Impl/library-syscpp/#performance","text":"1\u3001Internal lock-free MPSC messages queue NOTE: \u9700\u8981\u8003\u8651\u662f\u5982\u4f55\u5b9e\u73b0\u7684 2\u3001Extensive internal use of move semantics supporting delivery of non-copiable objects NOTE: 1\u3001prefer move to copy\uff0c std::vector \u4e5f\u662f\u8fd9\u6837\u505a\u7684\uff0c\u53c2\u89c1 a\u3001 \u5982\u4f55\u8bc4\u4ef7 C++11 \u7684\u53f3\u503c\u5f15\u7528\uff08Rvalue reference\uff09\u7279\u6027\uff1f - Tinro\u7684\u56de\u7b54 - \u77e5\u4e4e 3\u3001Several million msg/sec between each two threads (both Linux and Windows) in ordinary hardware","title":"Performance"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Active-object/Impl/library-syscpp/#robustness","text":"1\u3001The wrapped thread lifecycle overlaps and is driven by the object existence 2\u3001The object is kept alive by smart pointers (whoever has a reference can safely send messages) 3\u3001No internal strong references (only the final users determine the destruction/end) 4\u3001Nonetheless, callbacks onto already deleted active objects do not crash the application NOTE: 1\u3001\u9605\u8bfb\u4e86\u4e0a\u8ff0\u5185\u5bb9\uff0c\u53d1\u73b0\u5b83\u7684\u5b9e\u73b0\u5176\u5b9e\u548cspdlog\u662f\u975e\u5e38\u7c7b\u4f3c\u7684: a\u3001spdlog\u4e5f\u662f\u4f7f\u7528\u7684active object pattern b\u3001spdlog\u4e2d\u4e5f\u4f7f\u7528\u4e86 std::shared_ptr","title":"Robustness"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Active-object/Impl/library-syscpp/#example","text":"// Linux: g++ -std=c++11 -lpthread demo.cpp -o demo // Windows: cl.exe demo.cpp #include <string> #include <iostream> #include <sstream> #include \"ActorThread.hpp\" struct Message { std :: string description ; }; struct OtherMessage { double beautifulness ; }; class Consumer : public ActorThread < Consumer > { friend ActorThread < Consumer > ; void onMessage ( Message & p ) { std :: cout << \"thread \" << std :: this_thread :: get_id () << \" receiving \" << p . description << std :: endl ; } void onMessage ( OtherMessage & p ) { std :: cout << \"thread \" << std :: this_thread :: get_id () << \" receiving \" << p . beautifulness << std :: endl ; } }; class Producer : public ActorThread < Producer > { friend ActorThread < Producer > ; std :: shared_ptr < Consumer > consumer ; void onMessage ( std :: shared_ptr < Consumer >& c ) { consumer = c ; timerStart ( true , std :: chrono :: milliseconds ( 250 ), TimerCycle :: Periodic ); timerStart ( 3.14 , std :: chrono :: milliseconds ( 333 ), TimerCycle :: Periodic ); } void onTimer ( const bool & ) { std :: ostringstream report ; report << \"test from thread \" << std :: this_thread :: get_id (); consumer -> send ( Message { report . str () }); } void onTimer ( const double & value ) { consumer -> send ( OtherMessage { value }); } }; class Application : public ActorThread < Application > { friend ActorThread < Application > ; void onStart () { auto consumer = Consumer :: create (); // spawn new threads auto producer = Producer :: create (); producer -> send ( consumer ); std :: this_thread :: sleep_for ( std :: chrono :: seconds ( 3 )); stop (); } }; int main () { return Application :: run (); // re-use existing thread } NOTE: 1\u3001\u4ece\u4e0a\u8ff0code\u6765\u770b\uff0c\u5b83\u662f\u975e\u5e38\u7b80\u6d01\u7684 2\u3001\u5b83\u4f7f\u7528\u4e86CRTP\u3001mixin-from-above\u3001friend base class 3\u3001\u4e0a\u8ff0 onMessage \uff0c\u8ba9\u6211\u60f3\u5230\u4e86: visitor message pattern","title":"Example"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Actor/","text":"Actor model \u4e0b\u9762\u662f\u89e3\u51b3Actor model\u975e\u5e38\u597d\u7684\u6587\u7ae0: 1\u3001codemag Writing Concurrent Programs Using F# Mailbox Processors 2\u3001brianstorti The actor model in 10 minutes 3\u3001TODO rocketeer Concurrency in Erlang & Scala: The Actor Model \u4f7f\u7528\u4e86Actor model\u7684 Erlang programming language and actor model \u5173\u4e8e Erlang programming language and actor model\uff0c\u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\u8fdb\u884c\u63cf\u8ff0: 1\u3001codemag Writing Concurrent Programs Using F# Mailbox Processors 2\u3001brianstorti The actor model in 10 minutes 3\u3001TODO rocketeer Concurrency in Erlang & Scala: The Actor Model Python celery Instantiation \u00b6 A task is not instantiated for every request, but is registered in the task registry as a global instance. This means that the __init__ constructor will only be called once per process , and that the task class is semantically closer to an Actor . wikipedia Actor model NOTE: \u53c2\u4e0e\u8005\u6a21\u578b The actor model in computer science is a mathematical model of concurrent computation that treats \" actors \" as the universal primitives of concurrent computation(\u5176\u5c06\u201cactors\u201d\u89c6\u4e3a\u5e76\u53d1\u8ba1\u7b97\u7684\u901a\u7528\u539f\u8bed). In response to a message that it receives, an actor can: 1\u3001make local decisions 2\u3001create more actors 3\u3001send more messages 4\u3001determine how to respond to the next message received. Actors may modify their own private state , but can only affect each other through messages (avoiding the need for any locks ). The actor model originated in 1973.[ 1] It has been used both as a framework for a theoretical understanding (\u7406\u8bba\u4e0a\u7406\u89e3) of computation and as the theoretical basis for several practical implementations (\u5b9e\u9645\u5b9e\u73b0) of concurrent systems . The relationship of the model to other work is discussed in Actor model and process calculi . Fundamental concepts The actor model adopts the philosophy(\u54f2\u5b66) that everything is an actor . This is similar to the everything is an object philosophy used by some object-oriented programming languages . NOTE: 1\u3001\u8ba9\u6211\u60f3\u5230\u4e86Unix \"everything is a file philosophy\" An actor is a computational entity that, in response to a message it receives, can concurrently: send a finite number of messages to other actors; create a finite number of new actors; designate the behavior to be used for the next message it receives. There is no assumed sequence to the above actions and they could be carried out in parallel. Decoupling the sender from communications sent was a fundamental advance of the Actor model enabling asynchronous communication and control structures as patterns of passing messages .[ 8] Recipients(\u63a5\u6536) of messages are identified by address, sometimes called \"mailing address\". Thus an actor can only communicate with actors whose addresses it has. It can obtain those from a message it receives, or if the address is for an actor it has itself created. The actor model is characterized by inherent(\u56fa\u6709\u7684\uff0c\u5185\u5728\u7684\uff0c\u5929\u751f\u7684) concurrency of computation within(\u5185\u90e8) and among(\u4e4b\u95f4) actors, dynamic creation of actors, inclusion of actor addresses in messages, and interaction only through direct asynchronous message passing with no restriction on message arrival order. actor\u6a21\u578b\u7684\u7279\u5f81\u5728\u4e8eactor\u5185\u90e8\u548c\u4e4b\u95f4\u7684\u8ba1\u7b97\u7684\u5185\u5728\u5e76\u53d1\u6027\uff0cactor\u7684\u52a8\u6001\u521b\u5efa\uff0c\u5728\u6d88\u606f\u4e2d\u5305\u542bactor\u5730\u5740\uff0c\u4ee5\u53ca\u4ec5\u901a\u8fc7\u76f4\u63a5\u5f02\u6b65\u6d88\u606f\u4f20\u9012\u7684\u4ea4\u4e92\uff0c\u800c\u4e0d\u9650\u5236\u6d88\u606f\u5230\u8fbe\u987a\u5e8f\u3002","title":"Introduction"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Actor/#actor#model","text":"\u4e0b\u9762\u662f\u89e3\u51b3Actor model\u975e\u5e38\u597d\u7684\u6587\u7ae0: 1\u3001codemag Writing Concurrent Programs Using F# Mailbox Processors 2\u3001brianstorti The actor model in 10 minutes 3\u3001TODO rocketeer Concurrency in Erlang & Scala: The Actor Model","title":"Actor model"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Actor/#actor#model_1","text":"","title":"\u4f7f\u7528\u4e86Actor model\u7684"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Actor/#erlang#programming#language#and#actor#model","text":"\u5173\u4e8e Erlang programming language and actor model\uff0c\u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\u8fdb\u884c\u63cf\u8ff0: 1\u3001codemag Writing Concurrent Programs Using F# Mailbox Processors 2\u3001brianstorti The actor model in 10 minutes 3\u3001TODO rocketeer Concurrency in Erlang & Scala: The Actor Model","title":"Erlang programming language and actor model"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Actor/#python#celery","text":"Instantiation \u00b6 A task is not instantiated for every request, but is registered in the task registry as a global instance. This means that the __init__ constructor will only be called once per process , and that the task class is semantically closer to an Actor .","title":"Python celery"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Actor/#wikipedia#actor#model","text":"NOTE: \u53c2\u4e0e\u8005\u6a21\u578b The actor model in computer science is a mathematical model of concurrent computation that treats \" actors \" as the universal primitives of concurrent computation(\u5176\u5c06\u201cactors\u201d\u89c6\u4e3a\u5e76\u53d1\u8ba1\u7b97\u7684\u901a\u7528\u539f\u8bed). In response to a message that it receives, an actor can: 1\u3001make local decisions 2\u3001create more actors 3\u3001send more messages 4\u3001determine how to respond to the next message received. Actors may modify their own private state , but can only affect each other through messages (avoiding the need for any locks ). The actor model originated in 1973.[ 1] It has been used both as a framework for a theoretical understanding (\u7406\u8bba\u4e0a\u7406\u89e3) of computation and as the theoretical basis for several practical implementations (\u5b9e\u9645\u5b9e\u73b0) of concurrent systems . The relationship of the model to other work is discussed in Actor model and process calculi .","title":"wikipedia Actor model"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Actor/#fundamental#concepts","text":"The actor model adopts the philosophy(\u54f2\u5b66) that everything is an actor . This is similar to the everything is an object philosophy used by some object-oriented programming languages . NOTE: 1\u3001\u8ba9\u6211\u60f3\u5230\u4e86Unix \"everything is a file philosophy\" An actor is a computational entity that, in response to a message it receives, can concurrently: send a finite number of messages to other actors; create a finite number of new actors; designate the behavior to be used for the next message it receives. There is no assumed sequence to the above actions and they could be carried out in parallel. Decoupling the sender from communications sent was a fundamental advance of the Actor model enabling asynchronous communication and control structures as patterns of passing messages .[ 8] Recipients(\u63a5\u6536) of messages are identified by address, sometimes called \"mailing address\". Thus an actor can only communicate with actors whose addresses it has. It can obtain those from a message it receives, or if the address is for an actor it has itself created. The actor model is characterized by inherent(\u56fa\u6709\u7684\uff0c\u5185\u5728\u7684\uff0c\u5929\u751f\u7684) concurrency of computation within(\u5185\u90e8) and among(\u4e4b\u95f4) actors, dynamic creation of actors, inclusion of actor addresses in messages, and interaction only through direct asynchronous message passing with no restriction on message arrival order. actor\u6a21\u578b\u7684\u7279\u5f81\u5728\u4e8eactor\u5185\u90e8\u548c\u4e4b\u95f4\u7684\u8ba1\u7b97\u7684\u5185\u5728\u5e76\u53d1\u6027\uff0cactor\u7684\u52a8\u6001\u521b\u5efa\uff0c\u5728\u6d88\u606f\u4e2d\u5305\u542bactor\u5730\u5740\uff0c\u4ee5\u53ca\u4ec5\u901a\u8fc7\u76f4\u63a5\u5f02\u6b65\u6d88\u606f\u4f20\u9012\u7684\u4ea4\u4e92\uff0c\u800c\u4e0d\u9650\u5236\u6d88\u606f\u5230\u8fbe\u987a\u5e8f\u3002","title":"Fundamental concepts"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Actor/Writing-Concurrent-Programs-Using-F%23-Mailbox-Processors/","text":"Writing Concurrent Programs Using F# Mailbox Processors NOTE: 1\u3001\u8fd9\u7bc7\u6587\u7ae0\u5173\u4e8eactor model\u7684\u89e3\u91ca\u662f\u975e\u5e38\u597d\u7684\uff0c\u6b63\u662f\u8fd9\u4e2a\u539f\u56e0\uff0c\u624d\u6536\u5f55\u4e86\u8fd9\u7bc7\u6587\u7ae0 2\u3001 F# Mailbox Processors \u5176\u5b9e\u5c31\u662factor model\u4e2d\u7684actor There are many approaches to concurrency, but in this issue, I\u2019d like to talk about the built-in way that F# can handle message-based concurrency : using F# mailbox processors. The Actor Model To understand F# mailbox processors better, let\u2019s look a little closer at the theory they\u2019re based on: the actor model, which is also a prominent part of the Erlang programming language . There are a few key parts of the theory that can help you know when and why you should be using mailbox processors. The Actor Model is a model of concurrent programming that uses actors as the base unit. These actors are lightweight constructs that contain a queue, and can receive and process messages. NOTE: 1\u3001\u5230\u5904\u90fd\u53ef\u4ee5\u770b\u5230message queue 2\u3001\u65e2\u7136\u4f7f\u7528\u4e86message queue\uff0c\u90a3\u4e48\u5b83\u80af\u5b9a\u662f asynchronous When I say lightweight, I mean lightweight\u2014they\u2019re not like threads. I can easily spin up 100,000 on my aging laptop without a hitch. An actor\u2019s behavior is very limited. They can only: 1\u3001Create more actors 2\u3001Determine what to do with the next incoming message 3\u3001Send messages to another actor There are also a few rules for the actor model itself: 1\u3001Actors can be created dynamically. 2\u3001The only interaction between actors is through direct asynchronous message passing. 3\u3001Messages must include the actor\u2019s address. 4\u3001There\u2019s no restriction on message arrival order. A mailbox processor is based on a single actor. However, in general, you\u2019ll want to create several actors as part of a system. Even one of the creators of the actor model, Carl Hewitt, has famously said, \"One actor is no actor. They come in systems.\" Erlang actor analogy NOTE: 1\u3001\u4e0b\u9762\u7684\u7c7b\u6bd4\u662f\u975e\u5e38\u597d\u7684 I mentioned Erlang above as the first, and most faithful, representation of the actor model, because this gives you a good idea how to think about a whole system of actors. Erlang was developed for telephony applications, and you can think about a telephone as a single actor that can send messages (or make calls) to another telephone, as long as you have an address (telephone number). This analogy gives you a good working idea of how to build systems of actors, or mailbox processors. Actor, mailbox processor, and agent NOTE: 1\u3001\u4e09\u8005\u5176\u5b9e\u662f\u76f8\u540c\u7684\u542b\u4e49\u3002 Here\u2019s one more note before I jump into code. The original model is called the actor model and uses actors. F#\u2019s implementation of actors is referred to as mailbox processors, as I\u2019ve mentioned. Actors and mailbox processors are also occasionally referred to as agents. For most uses, these terms are interchangeable. For most purposes, the terms actor, mailbox processor, and agent are interchangeable.","title":"Introduction"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Actor/Writing-Concurrent-Programs-Using-F%23-Mailbox-Processors/#writing#concurrent#programs#using#f#mailbox#processors","text":"NOTE: 1\u3001\u8fd9\u7bc7\u6587\u7ae0\u5173\u4e8eactor model\u7684\u89e3\u91ca\u662f\u975e\u5e38\u597d\u7684\uff0c\u6b63\u662f\u8fd9\u4e2a\u539f\u56e0\uff0c\u624d\u6536\u5f55\u4e86\u8fd9\u7bc7\u6587\u7ae0 2\u3001 F# Mailbox Processors \u5176\u5b9e\u5c31\u662factor model\u4e2d\u7684actor There are many approaches to concurrency, but in this issue, I\u2019d like to talk about the built-in way that F# can handle message-based concurrency : using F# mailbox processors.","title":"Writing Concurrent Programs Using F# Mailbox Processors"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Actor/Writing-Concurrent-Programs-Using-F%23-Mailbox-Processors/#the#actor#model","text":"To understand F# mailbox processors better, let\u2019s look a little closer at the theory they\u2019re based on: the actor model, which is also a prominent part of the Erlang programming language . There are a few key parts of the theory that can help you know when and why you should be using mailbox processors. The Actor Model is a model of concurrent programming that uses actors as the base unit. These actors are lightweight constructs that contain a queue, and can receive and process messages. NOTE: 1\u3001\u5230\u5904\u90fd\u53ef\u4ee5\u770b\u5230message queue 2\u3001\u65e2\u7136\u4f7f\u7528\u4e86message queue\uff0c\u90a3\u4e48\u5b83\u80af\u5b9a\u662f asynchronous When I say lightweight, I mean lightweight\u2014they\u2019re not like threads. I can easily spin up 100,000 on my aging laptop without a hitch. An actor\u2019s behavior is very limited. They can only: 1\u3001Create more actors 2\u3001Determine what to do with the next incoming message 3\u3001Send messages to another actor There are also a few rules for the actor model itself: 1\u3001Actors can be created dynamically. 2\u3001The only interaction between actors is through direct asynchronous message passing. 3\u3001Messages must include the actor\u2019s address. 4\u3001There\u2019s no restriction on message arrival order. A mailbox processor is based on a single actor. However, in general, you\u2019ll want to create several actors as part of a system. Even one of the creators of the actor model, Carl Hewitt, has famously said, \"One actor is no actor. They come in systems.\"","title":"The Actor Model"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Actor/Writing-Concurrent-Programs-Using-F%23-Mailbox-Processors/#erlang#actor#analogy","text":"NOTE: 1\u3001\u4e0b\u9762\u7684\u7c7b\u6bd4\u662f\u975e\u5e38\u597d\u7684 I mentioned Erlang above as the first, and most faithful, representation of the actor model, because this gives you a good idea how to think about a whole system of actors. Erlang was developed for telephony applications, and you can think about a telephone as a single actor that can send messages (or make calls) to another telephone, as long as you have an address (telephone number). This analogy gives you a good working idea of how to build systems of actors, or mailbox processors.","title":"Erlang actor analogy"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Actor/Writing-Concurrent-Programs-Using-F%23-Mailbox-Processors/#actor#mailbox#processor#and#agent","text":"NOTE: 1\u3001\u4e09\u8005\u5176\u5b9e\u662f\u76f8\u540c\u7684\u542b\u4e49\u3002 Here\u2019s one more note before I jump into code. The original model is called the actor model and uses actors. F#\u2019s implementation of actors is referred to as mailbox processors, as I\u2019ve mentioned. Actors and mailbox processors are also occasionally referred to as agents. For most uses, these terms are interchangeable. For most purposes, the terms actor, mailbox processor, and agent are interchangeable.","title":"Actor, mailbox processor, and agent"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Actor/brianstorti-The-actor-model-in-10-minutes/","text":"brianstorti The actor model in 10 minutes The model The actor model is a conceptual model to deal with concurrent computation. It defines some general rules for how the system\u2019s components should behave and interact with each other. The most famous language that uses this model is probably Erlang . I\u2019ll try to focus more on the model itself and not in how it\u2019s implemented in different languages or libraries. Actors An actor is the primitive unit of computation. It\u2019s the thing that receives a message and do some kind of computation based on it. The idea is very similar to what we have in object-oriented languages: An object receives a message (a method call) and does something depending on which message it receives (which method we are calling). The main difference is that actors are completely isolated from each other and they will never share memory. It\u2019s also worth noting that an actor can maintain a private state that can never be changed directly by another actor. Actors have mailboxes It\u2019s important to understand that, although multiple actors can run at the same time, an actor will process a given message sequentially. This means that if you send 3 messages to the same actor, it will just execute one at a time. To have these 3 messages being executed concurrently, you need to create 3 actors and send one message to each. Messages are sent asynchronously to an actor, that needs to store them somewhere while it\u2019s processing another message. The mailbox is the place where these messages are stored. Actors communicate with each other by sending asynchronous messages. Those messages are stored in other actors' mailboxes until they're processed.","title":"Introduction"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Actor/brianstorti-The-actor-model-in-10-minutes/#brianstorti#the#actor#model#in#10#minutes","text":"","title":"brianstorti The actor model in 10 minutes"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Actor/brianstorti-The-actor-model-in-10-minutes/#the#model","text":"The actor model is a conceptual model to deal with concurrent computation. It defines some general rules for how the system\u2019s components should behave and interact with each other. The most famous language that uses this model is probably Erlang . I\u2019ll try to focus more on the model itself and not in how it\u2019s implemented in different languages or libraries.","title":"The model"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Actor/brianstorti-The-actor-model-in-10-minutes/#actors","text":"An actor is the primitive unit of computation. It\u2019s the thing that receives a message and do some kind of computation based on it. The idea is very similar to what we have in object-oriented languages: An object receives a message (a method call) and does something depending on which message it receives (which method we are calling). The main difference is that actors are completely isolated from each other and they will never share memory. It\u2019s also worth noting that an actor can maintain a private state that can never be changed directly by another actor.","title":"Actors"},{"location":"Event-driven-concurrent-server/Design-pattern/Actor%26%26active-object/Actor/brianstorti-The-actor-model-in-10-minutes/#actors#have#mailboxes","text":"It\u2019s important to understand that, although multiple actors can run at the same time, an actor will process a given message sequentially. This means that if you send 3 messages to the same actor, it will just execute one at a time. To have these 3 messages being executed concurrently, you need to create 3 actors and send one message to each. Messages are sent asynchronously to an actor, that needs to store them somewhere while it\u2019s processing another message. The mailbox is the place where these messages are stored. Actors communicate with each other by sending asynchronous messages. Those messages are stored in other actors' mailboxes until they're processed.","title":"Actors have mailboxes"},{"location":"Event-driven-concurrent-server/Design-pattern/Master-worker/","text":"Master-worker model \u5728 csdn \u5355\u4e2a\u8fdb\u7a0b\u76d1\u542c\u591a\u4e2a\u7aef\u53e3\u53ca\u591a\u4e2a\u8fdb\u7a0b\u76d1\u542c\u540c\u4e00\u4e2a\u7aef\u53e3 \u4e2d\uff0c\u63d0\u53ca\u4e86\"master/worker\": NGINX \u7684 master/work \u5904\u7406\u65b9\u6cd5\uff1a Flow of an NGINX worker process TODO cnblogs Master-Worker\u8bbe\u8ba1\u6a21\u5f0f\u4ecb\u7ecd mcs.anl.gov Master-worker pattern","title":"Introduction"},{"location":"Event-driven-concurrent-server/Design-pattern/Master-worker/#master-worker#model","text":"\u5728 csdn \u5355\u4e2a\u8fdb\u7a0b\u76d1\u542c\u591a\u4e2a\u7aef\u53e3\u53ca\u591a\u4e2a\u8fdb\u7a0b\u76d1\u542c\u540c\u4e00\u4e2a\u7aef\u53e3 \u4e2d\uff0c\u63d0\u53ca\u4e86\"master/worker\": NGINX \u7684 master/work \u5904\u7406\u65b9\u6cd5\uff1a Flow of an NGINX worker process","title":"Master-worker model"},{"location":"Event-driven-concurrent-server/Design-pattern/Master-worker/#todo","text":"cnblogs Master-Worker\u8bbe\u8ba1\u6a21\u5f0f\u4ecb\u7ecd mcs.anl.gov Master-worker pattern","title":"TODO"},{"location":"Event-driven-concurrent-server/Design-pattern/Observer-pattern/","text":"Observer pattern \u4e00\u3001Observer pattern\u4f7f\u7528OOP\u7684\u8bed\u8a00\u6765\u63cf\u8ff0event-driven model \u4e8c\u3001observer pattern\u548c publish-subscribe pattern \u6bd4\u8f83\u76f8\u5173 \u4e09\u3001\u5728\u4e0b\u9762\u7ae0\u8282\u4e2d\uff0c\u4e5f\u5bf9observer pattern\u8fdb\u884c\u4e86\u8bf4\u660e 1\u3001\u5de5\u7a0b programming-language \u7684 Multi-thread&&observer-pattern \u7ae0\u8282 2\u3001nextptr Using weak_ptr for circular references \u56db\u3001\u5f00\u6e90\u8f6f\u4ef6\u4e2d\u4f7f\u7528observer pattern\u7684 1\u3001spdlog async_logger wikipedia Observer pattern The observer pattern is a software design pattern in which an object , called the subject , maintains a list of its dependents, called observers , and notifies them automatically of any state changes, usually by calling one of their methods . NOTE: subject\u5bf9\u5e94\u7684\u662fevent-driven model\u7684**monitor**\u7684\u89d2\u8272\uff0cobserver\u5bf9\u5e94\u7684\u662fevent-driven model\u7684**executor**\u89d2\u8272\u3002subject\u901a\u8fc7\u8c03\u7528observer\u7684method\u6765\u5b9e\u73b0message passing\u3002 It is mainly used to implement distributed event handling systems, in \"event driven\" software. Most modern languages such as C# have built in \"event\" constructs which implement the observer pattern components. The observer pattern is also a key part in the familiar model\u2013view\u2013controller (MVC) architectural pattern.[ 1] The observer pattern is implemented in numerous programming libraries and systems, including almost all GUI toolkits. Strong vs. Weak reference The observer pattern can cause memory leaks , known as the lapsed listener problem , because in basic implementation it requires both explicit registration and explicit deregistration, as in the dispose pattern , because the subject holds strong references to the observers, keeping them alive. This can be prevented by the subject holding weak references to the observers. NOTE: \u4e00\u3001\u5982\u4f55\u6765\u89e3\u51b3\u5462\uff1f 1\u3001C++\uff0c\u53c2\u89c1: nextptr Using weak_ptr for circular references Coupling and typical pub-sub implementations Typically the observer pattern is implemented with the \" subject \" (which is being \"observed\") being part of the object, whose state change is being observed , to be communicated to the observers upon occurrence. This type of implementation is considered \" tightly coupled \", forcing both the observers and the subject to be aware of each other and have access to their internal parts, creating possible issues of scalability , speed, message recovery and maintenance (also called event or notification loss), the lack of flexibility in conditional dispersion(\u5206\u6563) and possible hindrance\uff08\u59a8\u788d\uff09 to desired security measures. In some ( non-polling ) implementations of the publish-subscribe pattern (also called the pub-sub pattern ), this is solved by creating a dedicated\uff08\u4e13\u95e8\u7684\uff09 \"message queue\" server and at times an extra \"message handler\" object , as added stages between the observer and the observed object whose state is being checked, thus \"decoupling\" the software components. In these cases, the message queue server is accessed by the observers with the observer pattern, \"subscribing to certain messages\" knowing only about the expected message (or not, in some cases), but knowing nothing about the message sender itself, and the sender may know nothing about the receivers. Other implementations of the publish-subscribe pattern, which achieve a similar effect of notification and communication to interested parties, do not use the observer pattern altogether.[ 4] [ 5] NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u5bf9\u6bd4\u4e86MQ\u548cobserver pattern\u3002 Still, in early implementations of multi-window operating systems like OS2 and Windows, the terms \"publish-subscribe pattern\" and \"event driven software development\" were used as a synonym for the observer pattern.[ 6] The observer pattern, as described in the GOF book , is a very basic concept and does not deal with observance removal or with any conditional or complex logic handling to be done by the observed \"subject\" before or after notifying the observers. The pattern also does not deal with recording the \"events\", the asynchronous passing of the notifications or guaranteeing they are being received. These concerns are typically dealt with in message queueing systems of which the observer pattern is only a small part. Related patterns: Publish\u2013subscribe pattern , mediator , singleton . Example Python class Observable ( object ): def __init__ ( self ) -> None : self . _observers = [] def register_observer ( self , observer ) -> None : self . _observers . append ( observer ) def notify_observers ( self , * args , ** kwargs ) -> None : for observer in self . _observers : observer . notify ( self , * args , ** kwargs ) class Observer ( object ): def __init__ ( self , observable ) -> None : observable . register_observer ( self ) def notify ( self , observable , * args , ** kwargs ) -> None : print ( 'Got' , args , kwargs , 'From' , observable ) subject = Observable () observer = Observer ( subject ) subject . notify_observers ( 'test' ) w3sdesign Observer design pattern Intent The Observer design pattern solves problems like: 1\u3001How can a one-to-many dependency between objects be defined without making the objects tightly coupled? 2\u3001How can an object notify an open-ended-number of other objects? NOTE: \u201copen-ended-number of other objects\u201d\u610f\u5473\u7740\uff0c\u5728runtime\uff0c\u53ef\u4ee5\u589e\u52a0observer\uff0c\u4e5f\u53ef\u4ee5\u5220\u9664observer\u3002 The Observer pattern describes how to solve such problems: 1\u3001Define a one-to-many dependency between objects so that when one object changes state, all its dependents are notified and updated automatically. 2\u3001The key idea in this pattern is to establish a flexible notification-registration mechanism that notifies all registered objects automatically when an event of interest occurs. NOTE: notification-registration mechanism\u662fobserver pattern\u7684\u6838\u5fc3\u601d\u60f3\uff0c\u51ed\u501f\u5b83\uff0cobserver pattern\u89e3\u51b3\u4e86\u524d\u9762\u63d0\u51fa\u7684\u95ee\u9898\u3002*notification-registration*\u4e5f\u53eb\u505apublish-subscribe\u3002 NOTE: \u5728\u539f\u6587\u7684intent\u8282\u7ed9\u51fa\u7684sample class diagram\u548csample sequence diagram\u5176\u5b9e\u5c31\u5df2\u7ecf\u5c55\u793a\u51fa\u4e86observer design pattern\u7684\u5b9e\u73b0\u4e86\u3002 Problem NOTE: \u5728\u539f\u6587\u7684\u8fd9\u4e00\u8282\u7ed9\u51fa\u4e86\u4e00\u4e2a\u53cd\u4f8b\uff0c\u6211\u4eec\u4e5f\u5e94\u8be5\u8981\u6ce8\u610f\u53cd\u4f8b\u3002\u5728\u539f\u6587\u7684motivation\u7ae0\u8282\uff0c\u7ed9\u51fa\u4e86\u66f4\u52a0\u8be6\u7ec6\u7684\u5bf9\u6bd4\u3002 Implementation NOTE: \u539f\u6587\u7684\u8fd9\u4e00\u8282\u7ed9\u51fa\u7684**Push Data**\u3001**Pull Data**\u5b9e\u73b0\u65b9\u5f0f Sample code NOTE: \u539f\u6587\u7ed9\u51fa\u7684sample code\u975e\u5e38\u597d\u3002 TODO observer pattern VS publish subscribe pattern https://hackernoon.com/observer-vs-pub-sub-pattern-50d3b27f838c microsoft Observer Design Pattern Events and routed events overview Handling and raising events oodesign Observer Pattern refactoring Observer NOTE: \u524d\u9762\u7ed9\u51fa\u7684\u793a\u4f8b\u90fd\u6ca1\u6709\u51c6\u786e\u63cf\u8ff0\u5bf9\u4e0d\u540c\u7c7b\u578b\u7684\u4e8b\u4ef6\uff0c\u6267\u884c\u4e0d\u540c\u7684\u51fd\u6570\uff0c\u800c\u672c\u6587\u7684\u793a\u4f8b\u5219\u5c55\u793a\u4e86\u8fd9\u4e00\u70b9\u3002\u5b83\u7684\u4ee3\u7801\u4e5f\u662f\u503c\u5f97\u9605\u8bfb\u7684https://refactoring.guru/design-patterns cpppatterns Observer","title":"Introduction"},{"location":"Event-driven-concurrent-server/Design-pattern/Observer-pattern/#observer#pattern","text":"\u4e00\u3001Observer pattern\u4f7f\u7528OOP\u7684\u8bed\u8a00\u6765\u63cf\u8ff0event-driven model \u4e8c\u3001observer pattern\u548c publish-subscribe pattern \u6bd4\u8f83\u76f8\u5173 \u4e09\u3001\u5728\u4e0b\u9762\u7ae0\u8282\u4e2d\uff0c\u4e5f\u5bf9observer pattern\u8fdb\u884c\u4e86\u8bf4\u660e 1\u3001\u5de5\u7a0b programming-language \u7684 Multi-thread&&observer-pattern \u7ae0\u8282 2\u3001nextptr Using weak_ptr for circular references \u56db\u3001\u5f00\u6e90\u8f6f\u4ef6\u4e2d\u4f7f\u7528observer pattern\u7684 1\u3001spdlog async_logger","title":"Observer pattern"},{"location":"Event-driven-concurrent-server/Design-pattern/Observer-pattern/#wikipedia#observer#pattern","text":"The observer pattern is a software design pattern in which an object , called the subject , maintains a list of its dependents, called observers , and notifies them automatically of any state changes, usually by calling one of their methods . NOTE: subject\u5bf9\u5e94\u7684\u662fevent-driven model\u7684**monitor**\u7684\u89d2\u8272\uff0cobserver\u5bf9\u5e94\u7684\u662fevent-driven model\u7684**executor**\u89d2\u8272\u3002subject\u901a\u8fc7\u8c03\u7528observer\u7684method\u6765\u5b9e\u73b0message passing\u3002 It is mainly used to implement distributed event handling systems, in \"event driven\" software. Most modern languages such as C# have built in \"event\" constructs which implement the observer pattern components. The observer pattern is also a key part in the familiar model\u2013view\u2013controller (MVC) architectural pattern.[ 1] The observer pattern is implemented in numerous programming libraries and systems, including almost all GUI toolkits.","title":"wikipedia Observer pattern"},{"location":"Event-driven-concurrent-server/Design-pattern/Observer-pattern/#strong#vs#weak#reference","text":"The observer pattern can cause memory leaks , known as the lapsed listener problem , because in basic implementation it requires both explicit registration and explicit deregistration, as in the dispose pattern , because the subject holds strong references to the observers, keeping them alive. This can be prevented by the subject holding weak references to the observers. NOTE: \u4e00\u3001\u5982\u4f55\u6765\u89e3\u51b3\u5462\uff1f 1\u3001C++\uff0c\u53c2\u89c1: nextptr Using weak_ptr for circular references","title":"Strong vs. Weak reference"},{"location":"Event-driven-concurrent-server/Design-pattern/Observer-pattern/#coupling#and#typical#pub-sub#implementations","text":"Typically the observer pattern is implemented with the \" subject \" (which is being \"observed\") being part of the object, whose state change is being observed , to be communicated to the observers upon occurrence. This type of implementation is considered \" tightly coupled \", forcing both the observers and the subject to be aware of each other and have access to their internal parts, creating possible issues of scalability , speed, message recovery and maintenance (also called event or notification loss), the lack of flexibility in conditional dispersion(\u5206\u6563) and possible hindrance\uff08\u59a8\u788d\uff09 to desired security measures. In some ( non-polling ) implementations of the publish-subscribe pattern (also called the pub-sub pattern ), this is solved by creating a dedicated\uff08\u4e13\u95e8\u7684\uff09 \"message queue\" server and at times an extra \"message handler\" object , as added stages between the observer and the observed object whose state is being checked, thus \"decoupling\" the software components. In these cases, the message queue server is accessed by the observers with the observer pattern, \"subscribing to certain messages\" knowing only about the expected message (or not, in some cases), but knowing nothing about the message sender itself, and the sender may know nothing about the receivers. Other implementations of the publish-subscribe pattern, which achieve a similar effect of notification and communication to interested parties, do not use the observer pattern altogether.[ 4] [ 5] NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u5bf9\u6bd4\u4e86MQ\u548cobserver pattern\u3002 Still, in early implementations of multi-window operating systems like OS2 and Windows, the terms \"publish-subscribe pattern\" and \"event driven software development\" were used as a synonym for the observer pattern.[ 6] The observer pattern, as described in the GOF book , is a very basic concept and does not deal with observance removal or with any conditional or complex logic handling to be done by the observed \"subject\" before or after notifying the observers. The pattern also does not deal with recording the \"events\", the asynchronous passing of the notifications or guaranteeing they are being received. These concerns are typically dealt with in message queueing systems of which the observer pattern is only a small part. Related patterns: Publish\u2013subscribe pattern , mediator , singleton .","title":"Coupling and typical pub-sub implementations"},{"location":"Event-driven-concurrent-server/Design-pattern/Observer-pattern/#example","text":"","title":"Example"},{"location":"Event-driven-concurrent-server/Design-pattern/Observer-pattern/#python","text":"class Observable ( object ): def __init__ ( self ) -> None : self . _observers = [] def register_observer ( self , observer ) -> None : self . _observers . append ( observer ) def notify_observers ( self , * args , ** kwargs ) -> None : for observer in self . _observers : observer . notify ( self , * args , ** kwargs ) class Observer ( object ): def __init__ ( self , observable ) -> None : observable . register_observer ( self ) def notify ( self , observable , * args , ** kwargs ) -> None : print ( 'Got' , args , kwargs , 'From' , observable ) subject = Observable () observer = Observer ( subject ) subject . notify_observers ( 'test' )","title":"Python"},{"location":"Event-driven-concurrent-server/Design-pattern/Observer-pattern/#w3sdesign#observer#design#pattern","text":"","title":"w3sdesign Observer design pattern"},{"location":"Event-driven-concurrent-server/Design-pattern/Observer-pattern/#intent","text":"The Observer design pattern solves problems like: 1\u3001How can a one-to-many dependency between objects be defined without making the objects tightly coupled? 2\u3001How can an object notify an open-ended-number of other objects? NOTE: \u201copen-ended-number of other objects\u201d\u610f\u5473\u7740\uff0c\u5728runtime\uff0c\u53ef\u4ee5\u589e\u52a0observer\uff0c\u4e5f\u53ef\u4ee5\u5220\u9664observer\u3002 The Observer pattern describes how to solve such problems: 1\u3001Define a one-to-many dependency between objects so that when one object changes state, all its dependents are notified and updated automatically. 2\u3001The key idea in this pattern is to establish a flexible notification-registration mechanism that notifies all registered objects automatically when an event of interest occurs. NOTE: notification-registration mechanism\u662fobserver pattern\u7684\u6838\u5fc3\u601d\u60f3\uff0c\u51ed\u501f\u5b83\uff0cobserver pattern\u89e3\u51b3\u4e86\u524d\u9762\u63d0\u51fa\u7684\u95ee\u9898\u3002*notification-registration*\u4e5f\u53eb\u505apublish-subscribe\u3002 NOTE: \u5728\u539f\u6587\u7684intent\u8282\u7ed9\u51fa\u7684sample class diagram\u548csample sequence diagram\u5176\u5b9e\u5c31\u5df2\u7ecf\u5c55\u793a\u51fa\u4e86observer design pattern\u7684\u5b9e\u73b0\u4e86\u3002","title":"Intent"},{"location":"Event-driven-concurrent-server/Design-pattern/Observer-pattern/#problem","text":"NOTE: \u5728\u539f\u6587\u7684\u8fd9\u4e00\u8282\u7ed9\u51fa\u4e86\u4e00\u4e2a\u53cd\u4f8b\uff0c\u6211\u4eec\u4e5f\u5e94\u8be5\u8981\u6ce8\u610f\u53cd\u4f8b\u3002\u5728\u539f\u6587\u7684motivation\u7ae0\u8282\uff0c\u7ed9\u51fa\u4e86\u66f4\u52a0\u8be6\u7ec6\u7684\u5bf9\u6bd4\u3002","title":"Problem"},{"location":"Event-driven-concurrent-server/Design-pattern/Observer-pattern/#implementation","text":"NOTE: \u539f\u6587\u7684\u8fd9\u4e00\u8282\u7ed9\u51fa\u7684**Push Data**\u3001**Pull Data**\u5b9e\u73b0\u65b9\u5f0f","title":"Implementation"},{"location":"Event-driven-concurrent-server/Design-pattern/Observer-pattern/#sample#code","text":"NOTE: \u539f\u6587\u7ed9\u51fa\u7684sample code\u975e\u5e38\u597d\u3002","title":"Sample code"},{"location":"Event-driven-concurrent-server/Design-pattern/Observer-pattern/#todo","text":"observer pattern VS publish subscribe pattern https://hackernoon.com/observer-vs-pub-sub-pattern-50d3b27f838c","title":"TODO"},{"location":"Event-driven-concurrent-server/Design-pattern/Observer-pattern/#microsoft#observer#design#pattern","text":"Events and routed events overview Handling and raising events","title":"microsoft Observer Design Pattern"},{"location":"Event-driven-concurrent-server/Design-pattern/Observer-pattern/#oodesign#observer#pattern","text":"","title":"oodesign Observer Pattern"},{"location":"Event-driven-concurrent-server/Design-pattern/Observer-pattern/#refactoring#observer","text":"NOTE: \u524d\u9762\u7ed9\u51fa\u7684\u793a\u4f8b\u90fd\u6ca1\u6709\u51c6\u786e\u63cf\u8ff0\u5bf9\u4e0d\u540c\u7c7b\u578b\u7684\u4e8b\u4ef6\uff0c\u6267\u884c\u4e0d\u540c\u7684\u51fd\u6570\uff0c\u800c\u672c\u6587\u7684\u793a\u4f8b\u5219\u5c55\u793a\u4e86\u8fd9\u4e00\u70b9\u3002\u5b83\u7684\u4ee3\u7801\u4e5f\u662f\u503c\u5f97\u9605\u8bfb\u7684https://refactoring.guru/design-patterns","title":"refactoring Observer"},{"location":"Event-driven-concurrent-server/Design-pattern/Observer-pattern/#cpppatterns#observer","text":"","title":"cpppatterns Observer"},{"location":"Event-driven-concurrent-server/Design-pattern/Observer-pattern/Code/","text":"Code w3sdesign \u5728 w3sdesign \u7684source code\u7ae0\u8282\u7ed9\u51fa\u4e86\u4f8b\u5b50\u3002 wikipedia \u7ef4\u57fa\u767e\u79d1 Observer pattern \u7ed9\u51fa\u7684 Example \u3002 Github TheLartians / Event martinmoene / observer-ptr-lite fnz / ObserverManager","title":"Introduction"},{"location":"Event-driven-concurrent-server/Design-pattern/Observer-pattern/Code/#code","text":"","title":"Code"},{"location":"Event-driven-concurrent-server/Design-pattern/Observer-pattern/Code/#w3sdesign","text":"\u5728 w3sdesign \u7684source code\u7ae0\u8282\u7ed9\u51fa\u4e86\u4f8b\u5b50\u3002","title":"w3sdesign"},{"location":"Event-driven-concurrent-server/Design-pattern/Observer-pattern/Code/#wikipedia","text":"\u7ef4\u57fa\u767e\u79d1 Observer pattern \u7ed9\u51fa\u7684 Example \u3002","title":"wikipedia"},{"location":"Event-driven-concurrent-server/Design-pattern/Observer-pattern/Code/#github","text":"","title":"Github"},{"location":"Event-driven-concurrent-server/Design-pattern/Observer-pattern/Code/#thelartiansevent","text":"","title":"TheLartians/Event"},{"location":"Event-driven-concurrent-server/Design-pattern/Observer-pattern/Code/#martinmoeneobserver-ptr-lite","text":"","title":"martinmoene/observer-ptr-lite"},{"location":"Event-driven-concurrent-server/Design-pattern/Observer-pattern/Code/#fnzobservermanager","text":"","title":"fnz/ObserverManager"},{"location":"Event-driven-concurrent-server/Design-pattern/Observer-pattern/Code/w3sdesign/Example1/","text":"Example 1 Basic Java code for implementing the sample UML diagrams.","title":"Example 1"},{"location":"Event-driven-concurrent-server/Design-pattern/Observer-pattern/Code/w3sdesign/Example1/#example#1","text":"Basic Java code for implementing the sample UML diagrams.","title":"Example 1"},{"location":"Event-driven-concurrent-server/Design-pattern/Observer-pattern/Code/w3sdesign/Example2/","text":"Example 2 Synchronizing state between a timer object (time of day) and a clock object.","title":"Example 2"},{"location":"Event-driven-concurrent-server/Design-pattern/Observer-pattern/Code/w3sdesign/Example2/#example#2","text":"Synchronizing state between a timer object (time of day) and a clock object.","title":"Example 2"},{"location":"Event-driven-concurrent-server/Design-pattern/Observer-pattern/Code/w3sdesign/Example3/","text":"Example 3 Event handling in a GUI application (Java Swing).","title":"Example 3"},{"location":"Event-driven-concurrent-server/Design-pattern/Observer-pattern/Code/w3sdesign/Example3/#example#3","text":"Event handling in a GUI application (Java Swing).","title":"Example 3"},{"location":"Event-driven-concurrent-server/Design-pattern/Observer-pattern/Lapsed-listener-problem/","text":"Lapsed listener problem \u53c2\u8003\u6587\u7ae0: 1\u3001wanweibaike Lapsed listener problem 2\u3001nextptr Using weak_ptr for circular references wanweibaike Lapsed listener problem","title":"Introduction"},{"location":"Event-driven-concurrent-server/Design-pattern/Observer-pattern/Lapsed-listener-problem/#lapsed#listener#problem","text":"\u53c2\u8003\u6587\u7ae0: 1\u3001wanweibaike Lapsed listener problem 2\u3001nextptr Using weak_ptr for circular references","title":"Lapsed listener problem"},{"location":"Event-driven-concurrent-server/Design-pattern/Observer-pattern/Lapsed-listener-problem/#wanweibaike#lapsed#listener#problem","text":"","title":"wanweibaike Lapsed listener problem"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/","text":"Proactor and reactor Reactor VS Proactor \u53c2\u8003\u6587\u7ae0: \u4e00\u3001zhihu \u5982\u4f55\u6df1\u523b\u7406\u89e3reactor\u548cproactor\uff1f \u4e0b\u9762\u662f\u6211\u7684\u603b\u7ed3: \u76f8\u540c\u70b9: 1\u3001\u4e24\u79cd\u90fd\u662f\u57fa\u4e8eIO multiplexing \u4e0d\u540c\u70b9: IO model: reactor sync\u540c\u6b65IO-proactor async\u5f02\u6b65IO reactor\u975e\u963b\u585e\u540c\u6b65IO-proactor\u975e\u963b\u585e\u5f02\u6b65IO reactor\u901a\u77e5notify IO\u5c31\u7eea\u72b6\u6001-proactor\u901a\u77e5notify IO\u5b8c\u6210\u72b6\u6001 stackoverflow What is the difference between event driven model and reactor pattern? From the wikipedia Reactor Pattern article: The reactor design pattern is an event handling pattern for handling service requests delivered concurrently to a service handler by one or more inputs. It named a few examples, e.g. nodejs , twisted , eventmachine But what I understand that above is popular event driven framework, so make them also a reactor pattern framework? How to differentiate between these two? Or they are the same? A The reactor pattern is more specific than \"event driven programming\". It is a specific implementation technique used when doing event driven programming . However, the term is not used with much accuracy in typical conversation, so you should be careful about using it and expecting your audience to understand you, and you should be careful in how you interpret the term when you encounter its use. One way to look at the reactor pattern is to consider it closely related to the idea of \"non-blocking\" operations. The reactor sends out notifications when certain operations can be completed without blocking. For example, select(2) can be used to implement the reactor pattern for reading from and writing to sockets using the standard BSD socket APIs ( recv(2) , send(2) , etc). select will tell you when you can receive bytes from a socket instantly - because the bytes are present in the kernel receiver buffer for that socket, for example. Another pattern you might want to consider while thinking about these ideas is the ***proactor* pattern**. In contrast to the reactor pattern , the proactor pattern has operations start regardless of whether they can finish immediately or not, has them performed asynchronously , and then arranges to deliver notification about their completion . The Windows I/O Completion Ports (IOCP) API is one example where the proactor pattern can be seen. When performing a send on a socket with IOCP, the send operation is started regardless of whether there is any room in the kernel send buffer for that socket. The send operation continues (in another thread, perhaps a thread in the kernel) while the WSASend call completes immediately. When the send actually completes (meaning only that the bytes being sent have been copied into the kernel send buffer for that socket), a callback function supplied to the WSASend call is invoked (in a new thread in the application). This approach of starting operations and then being notified when they are complete is central to the idea of asynchronous operations. Compare it to non-blocking operations where you wait until an operation can complete immediately before attempting to perform it. Either approach can be used for event driven programming. Using the reactor pattern, a program waits for the event of (for example) a socket being readable and then reads from it. Using the proactor pattern, the program instead waits for the event of a socket read completing. Strictly speaking, Twisted misuses the term reactor . The Twisted reactor which is based on select(2) ( twisted.internet.selectreactor ) is implemented using non-blocking I/O, which is very reactor-like. However, the interface it exposes to application code is asynchronous , making it more proactor-like. Twisted also has a reactor based on IOCP. This reactor exposes the same asynchronous application-facing API and uses the proactor-like IOCP APIs. This hybrid approach, varying from platform to platform in its details, makes neither the term \"reactor\" nor \"proactor\" particularly accurate, but since the API exposed by twisted.internet.reactor is basically entirely asynchronous instead of non-blocking, proactor would probably have been a better choice of name. A I think that this separation \"non-blocking\" and \"asynchronous\" is wrong, as the main implication of \"asynchronous\" is \"non-blocking\". Reactor pattern is about asynchronous (so non-blocking) calls, but synchronous (blocking) processing of those calls. Proactor is about asynchronous (non-blocking) calls and asynchronous (non-blocking) processing of those calls.","title":"Introduction"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/#proactor#and#reactor","text":"","title":"Proactor and reactor"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/#reactor#vs#proactor","text":"\u53c2\u8003\u6587\u7ae0: \u4e00\u3001zhihu \u5982\u4f55\u6df1\u523b\u7406\u89e3reactor\u548cproactor\uff1f \u4e0b\u9762\u662f\u6211\u7684\u603b\u7ed3: \u76f8\u540c\u70b9: 1\u3001\u4e24\u79cd\u90fd\u662f\u57fa\u4e8eIO multiplexing \u4e0d\u540c\u70b9: IO model: reactor sync\u540c\u6b65IO-proactor async\u5f02\u6b65IO reactor\u975e\u963b\u585e\u540c\u6b65IO-proactor\u975e\u963b\u585e\u5f02\u6b65IO reactor\u901a\u77e5notify IO\u5c31\u7eea\u72b6\u6001-proactor\u901a\u77e5notify IO\u5b8c\u6210\u72b6\u6001","title":"Reactor VS Proactor"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/#stackoverflow#what#is#the#difference#between#event#driven#model#and#reactor#pattern","text":"From the wikipedia Reactor Pattern article: The reactor design pattern is an event handling pattern for handling service requests delivered concurrently to a service handler by one or more inputs. It named a few examples, e.g. nodejs , twisted , eventmachine But what I understand that above is popular event driven framework, so make them also a reactor pattern framework? How to differentiate between these two? Or they are the same?","title":"stackoverflow What is the difference between event driven model and reactor pattern?"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/#a","text":"The reactor pattern is more specific than \"event driven programming\". It is a specific implementation technique used when doing event driven programming . However, the term is not used with much accuracy in typical conversation, so you should be careful about using it and expecting your audience to understand you, and you should be careful in how you interpret the term when you encounter its use. One way to look at the reactor pattern is to consider it closely related to the idea of \"non-blocking\" operations. The reactor sends out notifications when certain operations can be completed without blocking. For example, select(2) can be used to implement the reactor pattern for reading from and writing to sockets using the standard BSD socket APIs ( recv(2) , send(2) , etc). select will tell you when you can receive bytes from a socket instantly - because the bytes are present in the kernel receiver buffer for that socket, for example. Another pattern you might want to consider while thinking about these ideas is the ***proactor* pattern**. In contrast to the reactor pattern , the proactor pattern has operations start regardless of whether they can finish immediately or not, has them performed asynchronously , and then arranges to deliver notification about their completion . The Windows I/O Completion Ports (IOCP) API is one example where the proactor pattern can be seen. When performing a send on a socket with IOCP, the send operation is started regardless of whether there is any room in the kernel send buffer for that socket. The send operation continues (in another thread, perhaps a thread in the kernel) while the WSASend call completes immediately. When the send actually completes (meaning only that the bytes being sent have been copied into the kernel send buffer for that socket), a callback function supplied to the WSASend call is invoked (in a new thread in the application). This approach of starting operations and then being notified when they are complete is central to the idea of asynchronous operations. Compare it to non-blocking operations where you wait until an operation can complete immediately before attempting to perform it. Either approach can be used for event driven programming. Using the reactor pattern, a program waits for the event of (for example) a socket being readable and then reads from it. Using the proactor pattern, the program instead waits for the event of a socket read completing. Strictly speaking, Twisted misuses the term reactor . The Twisted reactor which is based on select(2) ( twisted.internet.selectreactor ) is implemented using non-blocking I/O, which is very reactor-like. However, the interface it exposes to application code is asynchronous , making it more proactor-like. Twisted also has a reactor based on IOCP. This reactor exposes the same asynchronous application-facing API and uses the proactor-like IOCP APIs. This hybrid approach, varying from platform to platform in its details, makes neither the term \"reactor\" nor \"proactor\" particularly accurate, but since the API exposed by twisted.internet.reactor is basically entirely asynchronous instead of non-blocking, proactor would probably have been a better choice of name.","title":"A"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/#a_1","text":"I think that this separation \"non-blocking\" and \"asynchronous\" is wrong, as the main implication of \"asynchronous\" is \"non-blocking\". Reactor pattern is about asynchronous (so non-blocking) calls, but synchronous (blocking) processing of those calls. Proactor is about asynchronous (non-blocking) calls and asynchronous (non-blocking) processing of those calls.","title":"A"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/Proactor/","text":"Proactor pattern \u5728 Boost.Asio # Overview \u4e2d\u63d0\u53ca\u5b83\u4f7f\u7528\u7684\u662fProactor pattern\u3002 wikipedia Proactor pattern Proactor is a software design pattern for event handling in which long running activities are running in an asynchronous part. A completion handler is called after the asynchronous part has terminated. The proactor pattern can be considered to be an asynchronous variant of the synchronous reactor pattern .[ 1] vanderbilt Proactor NOTE: \u6bd4\u8f83\u8be6\u7ec6\u7684\u8bba\u6587","title":"Introduction"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/Proactor/#proactor#pattern","text":"\u5728 Boost.Asio # Overview \u4e2d\u63d0\u53ca\u5b83\u4f7f\u7528\u7684\u662fProactor pattern\u3002","title":"Proactor pattern"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/Proactor/#wikipedia#proactor#pattern","text":"Proactor is a software design pattern for event handling in which long running activities are running in an asynchronous part. A completion handler is called after the asynchronous part has terminated. The proactor pattern can be considered to be an asynchronous variant of the synchronous reactor pattern .[ 1]","title":"wikipedia Proactor pattern"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/Proactor/#vanderbilt#proactor","text":"NOTE: \u6bd4\u8f83\u8be6\u7ec6\u7684\u8bba\u6587","title":"vanderbilt Proactor"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/Reactor/","text":"Reactor pattern 1\u3001wikipedia Reactor pattern \u5bf9 Reactor pattern\u7684\u63cf\u8ff0\u662f\u975e\u5e38\u6b63\u5f0f\u7684\uff0c \u521d\u6b21\u9605\u8bfb\u662f\u96be\u4ee5\u7406\u89e3\u7684 2\u3001Linux IO multiplexing\u5c31\u662f\u7528\u4e8e\u5b9e\u73b0Reactor pattern\u7684 3\u3001\u4f7f\u7528Reactor pattern\u7684\u4f8b\u5b50: a\u3001zhihu \u5982\u4f55\u6df1\u523b\u7406\u89e3reactor\u548cproactor\uff1f # A b\u3001zhihu I/O\u591a\u8def\u590d\u7528\u6280\u672f\uff08multiplexing\uff09\u662f\u4ec0\u4e48\uff1f # A \u5178\u578b\u7684\u4f8b\u5b50\u662f\u5c31\u662fRedis ae\uff0c\u5b83\u7684\u5b9e\u73b0\u6bd4\u8f83\u7b80\u5355\uff0c\u7ed3\u5408\u5b83\u7684\u5b9e\u73b0\u6765\u7406\u89e3\u4f1a\u975e\u5e38\u5feb\u3002 wikipedia Reactor pattern The reactor design pattern is an event handling pattern for handling service requests delivered concurrently to a service handler by one or more inputs. The service handler then demultiplexes the incoming requests and dispatches them synchronously to the associated request handlers.[ 1] NOTE: 1\u3001\u5982\u679c\u4f7f\u7528\u4e2d\u6587\u6765\u8fdb\u884c\u7406\u89e3\u7684\u8bdd\uff0creactor\u5c31\u53ef\u4ee5\u7ffb\u8bd1\u4e3a\u53cd\u5e94\u5668\uff0c\u5b83\u662f\u5bf9\u53d1\u751f\u7684event\u8fdb\u884creact\uff0c\u5373\u53cd\u6620\uff1b\u7531\u4e8e\u8fd9\u7bc7\u6587\u7ae0\u5e76\u6ca1\u6709\u5217\u4e3e\u793a\u4f8b\uff0c\u6240\u4ee5\u6211\u5728\u7b2c\u4e00\u6b21\u9605\u8bfb\u7684\u65f6\u5019\u5e76\u6ca1\u6709\u5b8c\u5168\u7406\u89e3\uff0c\u4eca\u5929\u5728\u9605\u8bfb\u300aredis\u8bbe\u8ba1\u4e0e\u5b9e\u73b0\u300b\u7684\u7b2c12\u7ae0\u4e8b\u4ef6\u7684\u65f6\u5019\uff0c\u5176\u4e2d\u5bf9\u6b64\u7684\u89e3\u91ca\u662f\u975e\u5e38\u5230\u4f4d\u7684\uff0c\u6240\u4ee5\u8981\u60f3\u5b8c\u5168\u7406\u89e3reactor pattern\uff0c\u9700\u8981\u7ed3\u5408\u8fd9\u672c\u4e66\u7684\u5185\u5bb9\u4e00\u8d77\u6765\u8fdb\u884c\u7406\u89e3\u3002 2\u3001\u4e0a\u6587\u4e2d\u7684**synchronously**\u975e\u5e38\u91cd\u8981\uff0c\u5b83\u662freactor\u548c Proactor pattern \u4e4b\u95f4\u7684\u5dee\u5f02\u6240\u5728\uff1b 3\u3001: \u53c2\u89c1 Redis \u8bbe\u8ba1\u4e0e\u5b9e\u73b0 \uff0c\u5176\u4e2dchapter 12\u6587\u4ef6\u4e8b\u4ef6 \u00b6 \u5bf9reactor pattern\u8fdb\u884c\u4e86\u4ecb\u7ecd\uff1b Structure Resources Any resource that can provide input to or consume output from the system. NOTE: resource\u53ef\u4ee5\u662fsocket\uff0cfile\u7b49\uff1b Synchronous Event Demultiplexer Uses an event loop to block on all resources. The demultiplexer sends the resource to the dispatcher when it is possible to start a synchronous operation on a resource without blocking ( Example: a synchronous call to read() will block if there is no data to read. The demultiplexer uses select() on the resource, which blocks until the resource is available for reading. In this case, a synchronous call to read() won't block, and the demultiplexer can send the resource to the dispatcher.) NOTE: 1\u3001\u4e0a\u9762\u8fd9\u4e00\u6bb5\u4e2d\u7684 Demultiplexer \u5176\u5b9e\u662f\u6bd4\u8f83\u5bb9\u6613\u8ba9\u4eba\u6df7\u6dc6\u7684\uff0c\u5b83\u7684\u5b9e\u73b0\u5f80\u5f80\u662f\u4f7f\u7528 I/O multiplex \u6280\u672f\uff0c\u5982 select \u7b49\u3002 2\u3001\u5176\u5b9e\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u662f\u975e\u5e38\u4efb\u610f\u8ba9\u4eba\u6df7\u6dc6\u7684\uff0c\u7ed3\u5408 Redis \u8bbe\u8ba1\u4e0e\u5b9e\u73b0 \uff0c\u5176\u4e2dchapter 12\u6587\u4ef6\u4e8b\u4ef6 \u00b6 \u5bf9reactor pattern\u7684\u4ecb\u7ecd\uff0c\u5df2\u7ecfredis\u7684\u5b9e\u73b0\u662f\u975e\u5e38\u4efb\u610f\u7684\uff1b Dispatcher Handles registering and unregistering of request handlers . Dispatches resources from the demultiplexer to the associated request handler. Request Handler An application defined request handler and its associated resource. Properties All reactor systems are single-threaded by definition, but can exist in a multithreaded environment. NOTE: redis\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u5c31\u662fsingle-thread Benefits The reactor pattern completely separates application-specific code from the reactor implementation, which means that application components can be divided into modular, reusable parts. Limitations The reactor pattern can be more difficult to debug [ 2] than a procedural pattern due to the inverted flow of control. Also, by only calling request handlers synchronously , the reactor pattern limits maximum concurrency, especially on symmetric multiprocessing hardware. The scalability\uff08\u53ef\u6269\u5c55\u6027\uff09 of the reactor pattern is limited not only by calling request handlers synchronously, but also by the demultiplexer .[ 3] TODO https://www.zhihu.com/question/26943938 https://www.codeproject.com/Articles/33011/Proactor-Pattern http://didawiki.cli.di.unipi.it/lib/exe/fetch.php/magistraleinformatica/tdp/tpd_reactor_proactor.pdf https://www.boost.org/doc/libs/1_47_0/doc/html/boost_asio/overview/core/async.html","title":"Introduction"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/Reactor/#reactor#pattern","text":"1\u3001wikipedia Reactor pattern \u5bf9 Reactor pattern\u7684\u63cf\u8ff0\u662f\u975e\u5e38\u6b63\u5f0f\u7684\uff0c \u521d\u6b21\u9605\u8bfb\u662f\u96be\u4ee5\u7406\u89e3\u7684 2\u3001Linux IO multiplexing\u5c31\u662f\u7528\u4e8e\u5b9e\u73b0Reactor pattern\u7684 3\u3001\u4f7f\u7528Reactor pattern\u7684\u4f8b\u5b50: a\u3001zhihu \u5982\u4f55\u6df1\u523b\u7406\u89e3reactor\u548cproactor\uff1f # A b\u3001zhihu I/O\u591a\u8def\u590d\u7528\u6280\u672f\uff08multiplexing\uff09\u662f\u4ec0\u4e48\uff1f # A \u5178\u578b\u7684\u4f8b\u5b50\u662f\u5c31\u662fRedis ae\uff0c\u5b83\u7684\u5b9e\u73b0\u6bd4\u8f83\u7b80\u5355\uff0c\u7ed3\u5408\u5b83\u7684\u5b9e\u73b0\u6765\u7406\u89e3\u4f1a\u975e\u5e38\u5feb\u3002","title":"Reactor pattern"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/Reactor/#wikipedia#reactor#pattern","text":"The reactor design pattern is an event handling pattern for handling service requests delivered concurrently to a service handler by one or more inputs. The service handler then demultiplexes the incoming requests and dispatches them synchronously to the associated request handlers.[ 1] NOTE: 1\u3001\u5982\u679c\u4f7f\u7528\u4e2d\u6587\u6765\u8fdb\u884c\u7406\u89e3\u7684\u8bdd\uff0creactor\u5c31\u53ef\u4ee5\u7ffb\u8bd1\u4e3a\u53cd\u5e94\u5668\uff0c\u5b83\u662f\u5bf9\u53d1\u751f\u7684event\u8fdb\u884creact\uff0c\u5373\u53cd\u6620\uff1b\u7531\u4e8e\u8fd9\u7bc7\u6587\u7ae0\u5e76\u6ca1\u6709\u5217\u4e3e\u793a\u4f8b\uff0c\u6240\u4ee5\u6211\u5728\u7b2c\u4e00\u6b21\u9605\u8bfb\u7684\u65f6\u5019\u5e76\u6ca1\u6709\u5b8c\u5168\u7406\u89e3\uff0c\u4eca\u5929\u5728\u9605\u8bfb\u300aredis\u8bbe\u8ba1\u4e0e\u5b9e\u73b0\u300b\u7684\u7b2c12\u7ae0\u4e8b\u4ef6\u7684\u65f6\u5019\uff0c\u5176\u4e2d\u5bf9\u6b64\u7684\u89e3\u91ca\u662f\u975e\u5e38\u5230\u4f4d\u7684\uff0c\u6240\u4ee5\u8981\u60f3\u5b8c\u5168\u7406\u89e3reactor pattern\uff0c\u9700\u8981\u7ed3\u5408\u8fd9\u672c\u4e66\u7684\u5185\u5bb9\u4e00\u8d77\u6765\u8fdb\u884c\u7406\u89e3\u3002 2\u3001\u4e0a\u6587\u4e2d\u7684**synchronously**\u975e\u5e38\u91cd\u8981\uff0c\u5b83\u662freactor\u548c Proactor pattern \u4e4b\u95f4\u7684\u5dee\u5f02\u6240\u5728\uff1b 3\u3001: \u53c2\u89c1 Redis \u8bbe\u8ba1\u4e0e\u5b9e\u73b0 \uff0c\u5176\u4e2dchapter 12\u6587\u4ef6\u4e8b\u4ef6 \u00b6 \u5bf9reactor pattern\u8fdb\u884c\u4e86\u4ecb\u7ecd\uff1b","title":"wikipedia Reactor pattern"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/Reactor/#structure","text":"","title":"Structure"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/Reactor/#resources","text":"Any resource that can provide input to or consume output from the system. NOTE: resource\u53ef\u4ee5\u662fsocket\uff0cfile\u7b49\uff1b","title":"Resources"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/Reactor/#synchronous#event#demultiplexer","text":"Uses an event loop to block on all resources. The demultiplexer sends the resource to the dispatcher when it is possible to start a synchronous operation on a resource without blocking ( Example: a synchronous call to read() will block if there is no data to read. The demultiplexer uses select() on the resource, which blocks until the resource is available for reading. In this case, a synchronous call to read() won't block, and the demultiplexer can send the resource to the dispatcher.) NOTE: 1\u3001\u4e0a\u9762\u8fd9\u4e00\u6bb5\u4e2d\u7684 Demultiplexer \u5176\u5b9e\u662f\u6bd4\u8f83\u5bb9\u6613\u8ba9\u4eba\u6df7\u6dc6\u7684\uff0c\u5b83\u7684\u5b9e\u73b0\u5f80\u5f80\u662f\u4f7f\u7528 I/O multiplex \u6280\u672f\uff0c\u5982 select \u7b49\u3002 2\u3001\u5176\u5b9e\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u662f\u975e\u5e38\u4efb\u610f\u8ba9\u4eba\u6df7\u6dc6\u7684\uff0c\u7ed3\u5408 Redis \u8bbe\u8ba1\u4e0e\u5b9e\u73b0 \uff0c\u5176\u4e2dchapter 12\u6587\u4ef6\u4e8b\u4ef6 \u00b6 \u5bf9reactor pattern\u7684\u4ecb\u7ecd\uff0c\u5df2\u7ecfredis\u7684\u5b9e\u73b0\u662f\u975e\u5e38\u4efb\u610f\u7684\uff1b","title":"Synchronous Event Demultiplexer"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/Reactor/#dispatcher","text":"Handles registering and unregistering of request handlers . Dispatches resources from the demultiplexer to the associated request handler.","title":"Dispatcher"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/Reactor/#request#handler","text":"An application defined request handler and its associated resource.","title":"Request Handler"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/Reactor/#properties","text":"All reactor systems are single-threaded by definition, but can exist in a multithreaded environment. NOTE: redis\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u5c31\u662fsingle-thread","title":"Properties"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/Reactor/#benefits","text":"The reactor pattern completely separates application-specific code from the reactor implementation, which means that application components can be divided into modular, reusable parts.","title":"Benefits"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/Reactor/#limitations","text":"The reactor pattern can be more difficult to debug [ 2] than a procedural pattern due to the inverted flow of control. Also, by only calling request handlers synchronously , the reactor pattern limits maximum concurrency, especially on symmetric multiprocessing hardware. The scalability\uff08\u53ef\u6269\u5c55\u6027\uff09 of the reactor pattern is limited not only by calling request handlers synchronously, but also by the demultiplexer .[ 3]","title":"Limitations"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/Reactor/#todo","text":"https://www.zhihu.com/question/26943938 https://www.codeproject.com/Articles/33011/Proactor-Pattern http://didawiki.cli.di.unipi.it/lib/exe/fetch.php/magistraleinformatica/tdp/tpd_reactor_proactor.pdf https://www.boost.org/doc/libs/1_47_0/doc/html/boost_asio/overview/core/async.html","title":"TODO"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/zhihu-%E5%A6%82%E4%BD%95%E6%B7%B1%E5%88%BB%E7%90%86%E8%A7%A3reactor%E5%92%8Cproactor/","text":"zhihu \u5982\u4f55\u6df1\u523b\u7406\u89e3reactor\u548cproactor\uff1f A 1\u3001\u6807\u51c6\u5b9a\u4e49 \u4e24\u79cdI/O\u591a\u8def\u590d\u7528\u6a21\u5f0f\uff1aReactor\u548cProactor Event Demultiplexer and Event Handler \u4e00\u822c\u5730,I/O\u591a\u8def\u590d\u7528\u673a\u5236\u90fd\u4f9d\u8d56\u4e8e\u4e00\u4e2a\u4e8b\u4ef6**\u591a\u8def\u5206\u79bb\u5668(Event Demultiplexer) \u3002**\u5206\u79bb\u5668\u5bf9\u8c61**\u53ef\u5c06\u6765\u81ea\u4e8b\u4ef6\u6e90\u7684I/O\u4e8b\u4ef6\u5206\u79bb\u51fa\u6765\uff0c\u5e76\u5206\u53d1\u5230\u5bf9\u5e94\u7684**read/write\u4e8b\u4ef6\u5904\u7406\u5668(Event Handler) (\u6216\u56de\u8c03\u51fd\u6570)\u3002\u5f00\u53d1\u4eba\u5458\u9884\u5148\u6ce8\u518c\u9700\u8981\u5904\u7406\u7684\u4e8b\u4ef6\u53ca\u5176\u4e8b\u4ef6\u5904\u7406\u5668\uff08\u6216\u56de\u8c03\u51fd\u6570\uff09\uff1b\u4e8b\u4ef6\u5206\u79bb\u5668\u8d1f\u8d23\u5c06\u8bf7\u6c42\u4e8b\u4ef6\u4f20\u9012\u7ed9\u4e8b\u4ef6\u5904\u7406\u5668\u3002 \u540c\u6b65IO\u6216\u5f02\u6b65IO \u4e24\u4e2a\u4e0e\u4e8b\u4ef6\u5206\u79bb\u5668\u6709\u5173\u7684\u6a21\u5f0f\u662fReactor\u548cProactor\u3002Reactor\u6a21\u5f0f\u91c7\u7528\u540c\u6b65IO\uff0c\u800cProactor\u91c7\u7528\u5f02\u6b65IO\u3002 NOTE: \u4e00\u3001\u5bf9\u6bd4 1\u3001reactor\u4e2d\uff0cEvent Demultiplexer\u901a\u77e5\u7684\u662f\"\u8bfb\u5199\u64cd\u4f5c\u51c6\u5907\u5c31\u7eea\"\u4e8b\u4ef6\uff0c\u6700\u540e\u7531**\u4e8b\u4ef6\u5904\u7406\u5668(\u56de\u8c03\u51fd\u6570)**\u8d1f\u8d23\u5b8c\u6210\u5b9e\u9645\u7684\u8bfb\u5199\u5de5\u4f5c 2\u3001proactor\u4e2d\uff0cEvent Demultiplexer\u901a\u77e5\u7684\u662f\"\u8bfb\u5199\u64cd\u4f5c\u5b8c\u6210\"\u4e8b\u4ef6\uff0c\u662f\u7531**OS**\u8d1f\u8d23\u5b8c\u6210\u5b9e\u9645\u7684\u8bfb\u5199\u5de5\u4f5c \u4e8c\u3001Proactor pattern\u5c31\u662f\u5178\u578b\u7684Asynchronous I/O Model\uff0c\u5173\u4e8e\"Asynchronous I/O Model\"\uff0c\u53c2\u89c1\u5de5\u7a0bLinux-OS\u7684 IO-model \u7ae0\u8282\u3002 \u5728**Reactor**\u4e2d\uff0c**\u4e8b\u4ef6\u5206\u79bb\u5668**\u8d1f\u8d23\u7b49\u5f85\u6587\u4ef6\u63cf\u8ff0\u7b26\u6216socket\u4e3a\u8bfb\u5199\u64cd\u4f5c\u51c6\u5907\u5c31\u7eea\uff0c\u7136\u540e\u5c06\u5c31\u7eea\u4e8b\u4ef6\u4f20\u9012\u7ed9\u5bf9\u5e94\u7684\u5904\u7406\u5668\uff0c\u6700\u540e\u7531**\u5904\u7406\u5668**\u8d1f\u8d23\u5b8c\u6210\u5b9e\u9645\u7684\u8bfb\u5199\u5de5\u4f5c\u3002 \u5728**Proactor**\u6a21\u5f0f\u4e2d\uff0c\u5904\u7406\u5668--\u6216\u8005\u517c\u4efb\u5904\u7406\u5668\u7684**\u4e8b\u4ef6\u5206\u79bb\u5668**\uff0c\u53ea\u8d1f\u8d23\u53d1\u8d77\u5f02\u6b65\u8bfb\u5199\u64cd\u4f5c\u3002IO\u64cd\u4f5c\u672c\u8eab\u7531**\u64cd\u4f5c\u7cfb\u7edf**\u6765\u5b8c\u6210\u3002\u4f20\u9012\u7ed9\u64cd\u4f5c\u7cfb\u7edf\u7684\u53c2\u6570\u9700\u8981\u5305\u62ec\u7528\u6237\u5b9a\u4e49\u7684**\u6570\u636e\u7f13\u51b2\u533a\u5730\u5740**\u548c**\u6570\u636e\u5927\u5c0f**\uff0c\u64cd\u4f5c\u7cfb\u7edf\u624d\u80fd\u4ece\u4e2d\u5f97\u5230\u5199\u51fa\u64cd\u4f5c\u6240\u9700\u6570\u636e\uff0c\u6216\u5199\u5165\u4ecesocket\u8bfb\u5230\u7684\u6570\u636e\u3002 \u4e8b\u4ef6\u5206\u79bb\u5668**\u6355\u83b7**IO\u64cd\u4f5c\u5b8c\u6210\u4e8b\u4ef6 \uff0c\u7136\u540e\u5c06\u4e8b\u4ef6\u4f20\u9012\u7ed9\u5bf9\u5e94\u5904\u7406\u5668\u3002\u6bd4\u5982\uff0c\u5728windows\u4e0a\uff0c\u5904\u7406\u5668\u53d1\u8d77\u4e00\u4e2a\u5f02\u6b65IO\u64cd\u4f5c\uff0c\u518d\u7531\u4e8b\u4ef6\u5206\u79bb\u5668\u7b49\u5f85IOCompletion\u4e8b\u4ef6\u3002\u5178\u578b\u7684\u5f02\u6b65\u6a21\u5f0f\u5b9e\u73b0\uff0c\u90fd\u5efa\u7acb\u5728\u64cd\u4f5c\u7cfb\u7edf\u652f\u6301\u5f02\u6b65API\u7684\u57fa\u7840\u4e4b\u4e0a\uff0c\u6211\u4eec\u5c06\u8fd9\u79cd\u5b9e\u73b0\u79f0\u4e3a\u201c\u7cfb\u7edf\u7ea7\u201d\u5f02\u6b65\u6216\u201c\u771f\u201d\u5f02\u6b65\uff0c\u56e0\u4e3a\u5e94\u7528\u7a0b\u5e8f\u5b8c\u5168\u4f9d\u8d56\u64cd\u4f5c\u7cfb\u7edf\u6267\u884c\u771f\u6b63\u7684IO\u5de5\u4f5c\u3002 Example \u4e3e\u4e2a\u4f8b\u5b50\uff0c\u5c06\u6709\u52a9\u4e8e\u7406\u89e3Reactor\u4e0eProactor\u4e8c\u8005\u7684\u5dee\u5f02\uff0c\u4ee5\u8bfb\u64cd\u4f5c\u4e3a\u4f8b\uff08\u7c7b\u64cd\u4f5c\u7c7b\u4f3c\uff09\u3002 \u5728Reactor\u4e2d\u5b9e\u73b0\u8bfb\uff1a - \u6ce8\u518c\u8bfb\u5c31\u7eea\u4e8b\u4ef6\u548c\u76f8\u5e94\u7684\u4e8b\u4ef6\u5904\u7406\u5668 - \u4e8b\u4ef6\u5206\u79bb\u5668\u7b49\u5f85\u4e8b\u4ef6 - \u4e8b\u4ef6\u5230\u6765\uff0c\u6fc0\u6d3b\u5206\u79bb\u5668\uff0c\u5206\u79bb\u5668\u8c03\u7528\u4e8b\u4ef6\u5bf9\u5e94\u7684\u5904\u7406\u5668\u3002 - \u4e8b\u4ef6\u5904\u7406\u5668\u5b8c\u6210\u5b9e\u9645\u7684\u8bfb\u64cd\u4f5c\uff0c\u5904\u7406\u8bfb\u5230\u7684\u6570\u636e\uff0c\u6ce8\u518c\u65b0\u7684\u4e8b\u4ef6\uff0c\u7136\u540e\u8fd4\u8fd8\u63a7\u5236\u6743\u3002 \u5728Proactor\u4e2d\u5b9e\u73b0\u8bfb\uff1a - **\u5904\u7406\u5668**\u53d1\u8d77\u5f02\u6b65\u8bfb\u64cd\u4f5c\uff08\u6ce8\u610f\uff1a\u64cd\u4f5c\u7cfb\u7edf\u5fc5\u987b\u652f\u6301\u5f02\u6b65IO\uff09\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5904\u7406\u5668\u65e0\u89c6IO\u5c31\u7eea\u4e8b\u4ef6\uff0c\u5b83\u5173\u6ce8\u7684\u662f\u5b8c\u6210\u4e8b\u4ef6\u3002 - \u4e8b\u4ef6\u5206\u79bb\u5668\u7b49\u5f85\u64cd\u4f5c\u5b8c\u6210\u4e8b\u4ef6 - \u5728\u5206\u79bb\u5668\u7b49\u5f85\u8fc7\u7a0b\u4e2d\uff0c\u64cd\u4f5c\u7cfb\u7edf\u5229\u7528\u5e76\u884c\u7684\u5185\u6838\u7ebf\u7a0b\u6267\u884c\u5b9e\u9645\u7684\u8bfb\u64cd\u4f5c\uff0c\u5e76\u5c06\u7ed3\u679c\u6570\u636e\u5b58\u5165\u7528\u6237\u81ea\u5b9a\u4e49\u7f13\u51b2\u533a\uff0c\u6700\u540e\u901a\u77e5\u4e8b\u4ef6\u5206\u79bb\u5668\u8bfb\u64cd\u4f5c\u5b8c\u6210\u3002 - \u4e8b\u4ef6\u5206\u79bb\u5668\u547c\u5524**\u5904\u7406\u5668**\u3002 - \u4e8b\u4ef6\u5904\u7406\u5668\u5904\u7406\u7528\u6237\u81ea\u5b9a\u4e49\u7f13\u51b2\u533a\u4e2d\u7684\u6570\u636e\uff0c\u7136\u540e\u542f\u52a8\u4e00\u4e2a\u65b0\u7684\u5f02\u6b65\u64cd\u4f5c\uff0c\u5e76\u5c06\u63a7\u5236\u6743\u8fd4\u56de\u4e8b\u4ef6\u5206\u79bb\u5668\u3002 \u4e24\u4e2a\u6a21\u5f0f\u7684\u76f8\u540c\u70b9\uff0c\u90fd\u662f\u5bf9\u67d0\u4e2aIO\u4e8b\u4ef6\u7684\u4e8b\u4ef6\u901a\u77e5(\u5373\u544a\u8bc9\u67d0\u4e2a\u6a21\u5757\uff0c\u8fd9\u4e2aIO\u64cd\u4f5c\u53ef\u4ee5\u8fdb\u884c\u6216\u5df2\u7ecf\u5b8c\u6210)\u3002\u5728\u7ed3\u6784\u4e0a\uff0c\u4e24\u8005\u4e5f\u6709\u76f8\u540c\u70b9\uff1ademultiplexor\u8d1f\u8d23\u63d0\u4ea4IO\u64cd\u4f5c(\u5f02\u6b65)\u3001\u67e5\u8be2\u8bbe\u5907\u662f\u5426\u53ef\u64cd\u4f5c(\u540c\u6b65)\uff0c\u7136\u540e\u5f53\u6761\u4ef6\u6ee1\u8db3\u65f6\uff0c\u5c31\u56de\u8c03handler\uff1b \u4e0d\u540c\u70b9\u5728\u4e8e\uff0c\u5f02\u6b65\u60c5\u51b5\u4e0b(Proactor)\uff0c\u5f53\u56de\u8c03handler\u65f6\uff0c\u8868\u793aIO\u64cd\u4f5c\u5df2\u7ecf\u5b8c\u6210\uff1b\u540c\u6b65\u60c5\u51b5\u4e0b(Reactor)\uff0c\u56de\u8c03handler\u65f6\uff0c\u8868\u793aIO\u8bbe\u5907\u53ef\u4ee5\u8fdb\u884c\u67d0\u4e2a\u64cd\u4f5c(can read or can write)\u3002 2\u3001\u901a\u4fd7\u7406\u89e3 \u4f7f\u7528Proactor\u6846\u67b6\u548cReactor\u6846\u67b6\u90fd\u53ef\u4ee5\u6781\u5927\u7684\u7b80\u5316\u7f51\u7edc\u5e94\u7528\u7684\u5f00\u53d1\uff0c\u4f46\u5b83\u4eec\u7684\u91cd\u70b9\u5374\u4e0d\u540c\u3002 Reactor\u6846\u67b6\u4e2d\u7528\u6237\u5b9a\u4e49\u7684\u64cd\u4f5c\u662f\u5728\u5b9e\u9645\u64cd\u4f5c\u4e4b\u524d\u8c03\u7528\u7684\u3002\u6bd4\u5982\u4f60\u5b9a\u4e49\u4e86\u64cd\u4f5c\u662f\u8981\u5411\u4e00\u4e2aSOCKET\u5199\u6570\u636e\uff0c\u90a3\u4e48\u5f53\u8be5SOCKET\u53ef\u4ee5\u63a5\u6536\u6570\u636e\u7684\u65f6\u5019\uff0c\u4f60\u7684\u64cd\u4f5c\u5c31\u4f1a\u88ab\u8c03\u7528\uff1b \u800cProactor\u6846\u67b6\u4e2d\u7528\u6237\u5b9a\u4e49\u7684\u64cd\u4f5c\u662f\u5728\u5b9e\u9645\u64cd\u4f5c\u4e4b\u540e\u8c03\u7528\u7684\u3002\u6bd4\u5982\u4f60\u5b9a\u4e49\u4e86\u4e00\u4e2a\u64cd\u4f5c\u8981\u663e\u793a\u4eceSOCKET\u4e2d\u8bfb\u5165\u7684\u6570\u636e\uff0c\u90a3\u4e48\u5f53\u8bfb\u64cd\u4f5c\u5b8c\u6210\u4ee5\u540e\uff0c\u4f60\u7684\u64cd\u4f5c\u624d\u4f1a\u88ab\u8c03\u7528\u3002 Proactor\u548cReactor\u90fd\u662f\u5e76\u53d1\u7f16\u7a0b\u4e2d\u7684\u8bbe\u8ba1\u6a21\u5f0f\u3002**\u5728\u6211\u770b\u6765\uff0c\u4ed6\u4eec\u90fd\u662f\u7528\u4e8e\u6d3e\u53d1/\u5206\u79bbIO\u64cd\u4f5c\u4e8b\u4ef6\u7684\u3002\u8fd9\u91cc\u6240\u8c13\u7684IO\u4e8b\u4ef6\u4e5f\u5c31\u662f\u8bf8\u5982read/write\u7684IO\u64cd\u4f5c\u3002\"\u6d3e\u53d1/\u5206\u79bb\"\u5c31\u662f\u5c06\u5355\u72ec\u7684IO\u4e8b\u4ef6\u901a\u77e5\u5230\u4e0a\u5c42\u6a21\u5757\u3002\u4e24\u4e2a\u6a21\u5f0f\u4e0d\u540c\u7684\u5730\u65b9\u5728\u4e8e\uff0c**Proactor\u7528\u4e8e\u5f02\u6b65IO\uff0c\u800cReactor\u7528\u4e8e\u540c\u6b65IO\u3002 \u90e8\u5206\u53c2\u8003\u81ea http://www.cnblogs.com/dawen/archive/2011/05/18/2050358.html A reactor\uff1a\u80fd\u6536\u4e86\u4f60\u8ddf\u4ffa\u8bf4\u4e00\u58f0\u3002 proactor: \u4f60\u7ed9\u6211\u6536\u5341\u4e2a\u5b57\u8282\uff0c\u6536\u597d\u4e86\u8ddf\u4ffa\u8bf4\u4e00\u58f0\u3002 A Reactor: libevent/libev/libuv/ZeroMQ/Event Library in Redis Proactor IOCP/Boost.Asio linux\u4e0b\u8fd8\u662fReactor\u628a, \u6ca1\u6709os\u652f\u6301, Proactor\u73a9\u4e0d\u8f6c. \u5c0f\u6797coding\u7684\u56de\u7b54 \u522b\u5c0f\u770b\u8fd9\u4e24\u4e2a\u4e1c\u897f\uff0c\u7279\u522b\u662f Reactor \u6a21\u5f0f\uff0c\u5e02\u9762\u4e0a\u5e38\u89c1\u7684\u5f00\u6e90\u8f6f\u4ef6\u5f88\u591a\u90fd\u91c7\u7528\u4e86\u8fd9\u4e2a\u65b9\u6848\uff0c\u6bd4\u5982 Redis\u3001Nginx\u3001Netty \u7b49\u7b49\uff0c\u6240\u4ee5\u5b66\u597d\u8fd9\u4e2a\u6a21\u5f0f\u8bbe\u8ba1\u7684\u601d\u60f3\uff0c\u4e0d\u4ec5\u6709\u52a9\u4e8e\u6211\u4eec\u7406\u89e3\u5f88\u591a\u5f00\u6e90\u8f6f\u4ef6\uff0c\u800c\u4e14\u4e5f\u80fd\u5728\u9762\u8bd5\u65f6\u5439\u903c\u3002 \u53d1\u8f66\uff01 ![img](https://pic4.zhimg.com/80/v2-e7c8ec8a75fc13c602f394bb3c8e45b5_1440w.jpg?source=1940ef5c) \u6f14\u8fdb NOTE: C10K\u95ee\u9898 Concurrent server: \u6bcf\u4e2aclient\u4e00\u4e2athread\u3001process \u5982\u679c\u8981\u8ba9\u670d\u52a1\u5668\u670d\u52a1\u591a\u4e2a\u5ba2\u6237\u7aef\uff0c\u90a3\u4e48\u6700\u76f4\u63a5\u7684\u65b9\u5f0f\u5c31\u662f\u4e3a\u6bcf\u4e00\u6761\u8fde\u63a5\u521b\u5efa\u7ebf\u7a0b\u3002 \u5176\u5b9e\u521b\u5efa\u8fdb\u7a0b\u4e5f\u662f\u53ef\u4ee5\u7684\uff0c\u539f\u7406\u662f\u4e00\u6837\u7684\uff0c\u8fdb\u7a0b\u548c\u7ebf\u7a0b\u7684\u533a\u522b\u5728\u4e8e\u7ebf\u7a0b\u6bd4\u8f83\u8f7b\u91cf\u7ea7\u4e9b\uff0c\u7ebf\u7a0b\u7684\u521b\u5efa\u548c\u7ebf\u7a0b\u95f4\u5207\u6362\u7684\u6210\u672c\u8981\u5c0f\u4e9b\uff0c\u4e3a\u4e86\u63cf\u8ff0\u7b80\u8ff0\uff0c\u540e\u9762\u90fd\u4ee5\u7ebf\u7a0b\u4e3a\u4f8b\u3002 \u5904\u7406\u5b8c\u4e1a\u52a1\u903b\u8f91\u540e\uff0c\u968f\u7740\u8fde\u63a5\u5173\u95ed\u540e\u7ebf\u7a0b\u4e5f\u540c\u6837\u8981\u9500\u6bc1\u4e86\uff0c\u4f46\u662f\u8fd9\u6837\u4e0d\u505c\u5730\u521b\u5efa\u548c\u9500\u6bc1\u7ebf\u7a0b\uff0c\u4e0d\u4ec5\u4f1a\u5e26\u6765\u6027\u80fd\u5f00\u9500\uff0c\u4e5f\u4f1a\u9020\u6210\u6d6a\u8d39\u8d44\u6e90\uff0c\u800c\u4e14\u5982\u679c\u8981\u8fde\u63a5\u51e0\u4e07\u6761\u8fde\u63a5\uff0c\u521b\u5efa\u51e0\u4e07\u4e2a\u7ebf\u7a0b\u53bb\u5e94\u5bf9\u4e5f\u662f\u4e0d\u73b0\u5b9e\u7684\u3002 Concurrent server: thread pool \u8981\u8fd9\u4e48\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u5462\uff1f\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u300c\u8d44\u6e90\u590d\u7528\u300d\u7684\u65b9\u5f0f\u3002 \u4e5f\u5c31\u662f\u4e0d\u7528\u518d\u4e3a\u6bcf\u4e2a\u8fde\u63a5\u521b\u5efa\u7ebf\u7a0b\uff0c\u800c\u662f\u521b\u5efa\u4e00\u4e2a\u300c\u7ebf\u7a0b\u6c60\u300d\uff0c\u5c06\u8fde\u63a5\u5206\u914d\u7ed9\u7ebf\u7a0b\uff0c\u7136\u540e\u4e00\u4e2a\u7ebf\u7a0b\u53ef\u4ee5\u5904\u7406\u591a\u4e2a\u8fde\u63a5\u7684\u4e1a\u52a1\u3002 \u4e0d\u8fc7\uff0c\u8fd9\u6837\u53c8\u5f15\u6765\u4e00\u4e2a\u65b0\u7684\u95ee\u9898\uff0c\u7ebf\u7a0b\u600e\u6837\u624d\u80fd\u9ad8\u6548\u5730\u5904\u7406\u591a\u4e2a\u8fde\u63a5\u7684\u4e1a\u52a1\uff1f \u5f53\u4e00\u4e2a\u8fde\u63a5\u5bf9\u5e94\u4e00\u4e2a\u7ebf\u7a0b\u65f6\uff0c\u7ebf\u7a0b\u4e00\u822c\u91c7\u7528\u300cread -> \u4e1a\u52a1\u5904\u7406 -> send\u300d\u7684\u5904\u7406\u6d41\u7a0b\uff0c\u5982\u679c\u5f53\u524d\u8fde\u63a5\u6ca1\u6709\u6570\u636e\u53ef\u8bfb\uff0c\u90a3\u4e48\u7ebf\u7a0b\u4f1a\u963b\u585e\u5728 read \u64cd\u4f5c\u4e0a\uff08 socket \u9ed8\u8ba4\u60c5\u51b5\u662f\u963b\u585e I/O\uff09\uff0c\u4e0d\u8fc7\u8fd9\u79cd\u963b\u585e\u65b9\u5f0f\u5e76\u4e0d\u5f71\u54cd\u5176\u4ed6\u7ebf\u7a0b\u3002 \u4f46\u662f\u5f15\u5165\u4e86\u7ebf\u7a0b\u6c60\uff0c\u90a3\u4e48\u4e00\u4e2a\u7ebf\u7a0b\u8981\u5904\u7406\u591a\u4e2a\u8fde\u63a5\u7684\u4e1a\u52a1\uff0c\u7ebf\u7a0b\u5728\u5904\u7406\u67d0\u4e2a\u8fde\u63a5\u7684 read \u64cd\u4f5c\u65f6\uff0c\u5982\u679c\u9047\u5230\u6ca1\u6709\u6570\u636e\u53ef\u8bfb\uff0c\u5c31\u4f1a\u53d1\u751f\u963b\u585e\uff0c\u90a3\u4e48\u7ebf\u7a0b\u5c31\u6ca1\u529e\u6cd5\u7ee7\u7eed\u5904\u7406\u5176\u4ed6\u8fde\u63a5\u7684\u4e1a\u52a1\u3002 \u8981\u89e3\u51b3\u8fd9\u4e00\u4e2a\u95ee\u9898\uff0c\u6700\u7b80\u5355\u7684\u65b9\u5f0f\u5c31\u662f\u5c06 socket \u6539\u6210\u975e\u963b\u585e\uff0c\u7136\u540e\u7ebf\u7a0b\u4e0d\u65ad\u5730\u8f6e\u8be2\u8c03\u7528 read \u64cd\u4f5c\u6765\u5224\u65ad\u662f\u5426\u6709\u6570\u636e\uff0c\u8fd9\u79cd\u65b9\u5f0f\u867d\u7136\u8be5\u80fd\u591f\u89e3\u51b3\u963b\u585e\u7684\u95ee\u9898\uff0c\u4f46\u662f\u89e3\u51b3\u7684\u65b9\u5f0f\u6bd4\u8f83\u7c97\u66b4\uff0c\u56e0\u4e3a\u8f6e\u8be2\u662f\u8981\u6d88\u8017 CPU \u7684\uff0c\u800c\u4e14\u968f\u7740\u4e00\u4e2a \u7ebf\u7a0b\u5904\u7406\u7684\u8fde\u63a5\u8d8a\u591a\uff0c\u8f6e\u8be2\u7684\u6548\u7387\u5c31\u4f1a\u8d8a\u4f4e\u3002 \u4e0a\u9762\u7684\u95ee\u9898\u5728\u4e8e\uff0c\u7ebf\u7a0b\u5e76\u4e0d\u77e5\u9053\u5f53\u524d\u8fde\u63a5\u662f\u5426\u6709\u6570\u636e\u53ef\u8bfb\uff0c\u4ece\u800c\u9700\u8981\u6bcf\u6b21\u901a\u8fc7 read \u53bb\u8bd5\u63a2\u3002 Concurrent server: event driven IO multiplexing \u90a3\u6709\u6ca1\u6709\u529e\u6cd5\u5728\u53ea\u6709\u5f53\u8fde\u63a5\u4e0a\u6709\u6570\u636e\u7684\u65f6\u5019\uff0c\u7ebf\u7a0b\u624d\u53bb\u53d1\u8d77\u8bfb\u8bf7\u6c42\u5462\uff1f\u7b54\u6848\u662f\u6709\u7684\uff0c\u5b9e\u73b0\u8fd9\u4e00\u6280\u672f\u7684\u5c31\u662f I/O \u591a\u8def\u590d\u7528\u3002 I/O \u591a\u8def\u590d\u7528\u6280\u672f\u4f1a\u7528\u4e00\u4e2a\u7cfb\u7edf\u8c03\u7528\u51fd\u6570\u6765\u76d1\u542c\u6211\u4eec\u6240\u6709\u5173\u5fc3\u7684\u8fde\u63a5\uff0c\u4e5f\u5c31\u8bf4\u53ef\u4ee5\u5728\u4e00\u4e2a\u76d1\u63a7\u7ebf\u7a0b\u91cc\u9762\u76d1\u63a7\u5f88\u591a\u7684\u8fde\u63a5\u3002 ![img](https://pic1.zhimg.com/50/v2-0a86ab90d8167860dec5c695064648f3_hd.jpg) \u6211\u4eec\u719f\u6089\u7684 select/poll/epoll \u5c31\u662f\u5185\u6838\u63d0\u4f9b\u7ed9\u7528\u6237\u6001\u7684\u591a\u8def\u590d\u7528\u7cfb\u7edf\u8c03\u7528\uff0c\u7ebf\u7a0b\u53ef\u4ee5\u901a\u8fc7\u4e00\u4e2a\u7cfb\u7edf\u8c03\u7528\u51fd\u6570\u4ece\u5185\u6838\u4e2d\u83b7\u53d6\u591a\u4e2a\u4e8b\u4ef6\u3002 PS\uff1a\u5982\u679c\u60f3\u77e5\u9053 select/poll/epoll \u7684\u533a\u522b\uff0c\u53ef\u4ee5\u770b\u770b\u5c0f\u6797\u4e4b\u524d\u5199\u7684\u8fd9\u7bc7\u6587\u7ae0\uff1a \u8fd9\u6b21\u7b54\u5e94\u6211\uff0c\u4e00\u4e3e\u62ff\u4e0b I/O \u591a\u8def\u590d\u7528\uff01 select/poll/epoll \u662f\u5982\u4f55\u83b7\u53d6\u7f51\u7edc\u4e8b\u4ef6\u7684\u5462\uff1f \u5728\u83b7\u53d6\u4e8b\u4ef6\u65f6\uff0c\u5148\u628a\u6211\u4eec\u8981\u5173\u5fc3\u7684\u8fde\u63a5\u4f20\u7ed9\u5185\u6838\uff0c\u518d\u7531\u5185\u6838\u68c0\u6d4b\uff1a 1\u3001\u5982\u679c\u6ca1\u6709\u4e8b\u4ef6\u53d1\u751f\uff0c\u7ebf\u7a0b\u53ea\u9700\u963b\u585e\u5728\u8fd9\u4e2a\u7cfb\u7edf\u8c03\u7528\uff0c\u800c\u65e0\u9700\u50cf\u524d\u9762\u7684\u7ebf\u7a0b\u6c60\u65b9\u6848\u90a3\u6837\u8f6e\u8bad\u8c03\u7528 read \u64cd\u4f5c\u6765\u5224\u65ad\u662f\u5426\u6709\u6570\u636e\u3002 2\u3001\u5982\u679c\u6709\u4e8b\u4ef6\u53d1\u751f\uff0c\u5185\u6838\u4f1a\u8fd4\u56de\u4ea7\u751f\u4e86\u4e8b\u4ef6\u7684\u8fde\u63a5\uff0c\u7ebf\u7a0b\u5c31\u4f1a\u4ece\u963b\u585e\u72b6\u6001\u8fd4\u56de\uff0c\u7136\u540e\u5728\u7528\u6237\u6001\u4e2d\u518d\u5904\u7406\u8fd9\u4e9b\u8fde\u63a5\u5bf9\u5e94\u7684\u4e1a\u52a1\u5373\u53ef\u3002 Reactor \u6a21\u5f0f \u5f53\u4e0b\u5f00\u6e90\u8f6f\u4ef6\u80fd\u505a\u5230\u7f51\u7edc\u9ad8\u6027\u80fd\u7684\u539f\u56e0\u5c31\u662f I/O \u591a\u8def\u590d\u7528\u5417\uff1f \u662f\u7684\uff0c\u57fa\u672c\u662f\u57fa\u4e8e I/O \u591a\u8def\u590d\u7528\uff0c\u7528\u8fc7 I/O \u591a\u8def\u590d\u7528\u63a5\u53e3\u5199\u7f51\u7edc\u7a0b\u5e8f\u7684\u540c\u5b66\uff0c\u80af\u5b9a\u77e5\u9053\u662f\u9762\u5411\u8fc7\u7a0b\u7684\u65b9\u5f0f\u5199\u4ee3\u7801\u7684\uff0c\u8fd9\u6837\u7684\u5f00\u53d1\u7684\u6548\u7387\u4e0d\u9ad8\u3002 \u4e8e\u662f\uff0c\u5927\u4f6c\u4eec\u57fa\u4e8e\u9762\u5411\u5bf9\u8c61\u7684\u601d\u60f3\uff0c\u5bf9 I/O \u591a\u8def\u590d\u7528\u4f5c\u4e86\u4e00\u5c42\u5c01\u88c5\uff0c\u8ba9\u4f7f\u7528\u8005\u4e0d\u7528\u8003\u8651\u5e95\u5c42\u7f51\u7edc API \u7684\u7ec6\u8282\uff0c\u53ea\u9700\u8981\u5173\u6ce8\u5e94\u7528\u4ee3\u7801\u7684\u7f16\u5199\u3002 \u5927\u4f6c\u4eec\u8fd8\u4e3a\u8fd9\u79cd\u6a21\u5f0f\u53d6\u4e86\u4e2a\u8ba9\u4eba\u7b2c\u4e00\u65f6\u95f4\u96be\u4ee5\u7406\u89e3\u7684\u540d\u5b57\uff1a Reactor \u6a21\u5f0f \u3002 Reactor \u7ffb\u8bd1\u8fc7\u6765\u7684\u610f\u601d\u662f\u300c\u53cd\u5e94\u5806\u300d\uff0c\u53ef\u80fd\u5927\u5bb6\u4f1a\u8054\u60f3\u5230\u7269\u7406\u5b66\u91cc\u7684\u6838\u53cd\u5e94\u5806\uff0c\u5b9e\u9645\u4e0a\u5e76\u4e0d\u662f\u7684\u8fd9\u4e2a\u610f\u601d\u3002 \u8fd9\u91cc\u7684\u53cd\u5e94\u6307\u7684\u662f\u300c \u5bf9\u4e8b\u4ef6\u53cd\u5e94 \u300d\uff0c\u4e5f\u5c31\u662f**\u6765\u4e86\u4e00\u4e2a\u4e8b\u4ef6\uff0cReactor \u5c31\u6709\u76f8\u5bf9\u5e94\u7684\u53cd\u5e94/\u54cd\u5e94**\u3002 \u4e8b\u5b9e\u4e0a\uff0cReactor \u6a21\u5f0f\u4e5f\u53eb Dispatcher \u6a21\u5f0f\uff0c\u6211\u89c9\u5f97\u8fd9\u4e2a\u540d\u5b57\u66f4\u8d34\u5408\u8be5\u6a21\u5f0f\u7684\u542b\u4e49\uff0c\u5373 I/O \u591a\u8def\u590d\u7528\u76d1\u542c\u4e8b\u4ef6\uff0c\u6536\u5230\u4e8b\u4ef6\u540e\uff0c\u6839\u636e\u4e8b\u4ef6\u7c7b\u578b\u5206\u914d\uff08Dispatch\uff09\u7ed9\u67d0\u4e2a\u8fdb\u7a0b / \u7ebf\u7a0b \u3002 Reactor \u6a21\u5f0f**\u4e3b\u8981\u7531 **Reactor \u548c**\u5904\u7406\u8d44\u6e90\u6c60**\u8fd9\u4e24\u4e2a\u6838\u5fc3\u90e8\u5206\u7ec4\u6210\uff0c\u5b83\u4fe9\u8d1f\u8d23\u7684\u4e8b\u60c5\u5982\u4e0b\uff1a 1\u3001Reactor \u8d1f\u8d23\u76d1\u542c\u548c\u5206\u53d1\u4e8b\u4ef6\uff0c\u4e8b\u4ef6\u7c7b\u578b\u5305\u542b\u8fde\u63a5\u4e8b\u4ef6\u3001\u8bfb\u5199\u4e8b\u4ef6\uff1b 2\u3001\u5904\u7406\u8d44\u6e90\u6c60\u8d1f\u8d23\u5904\u7406\u4e8b\u4ef6\uff0c\u5982 read -> \u4e1a\u52a1\u903b\u8f91 -> send\uff1b Reactor \u6a21\u5f0f\u662f\u7075\u6d3b\u591a\u53d8\u7684\uff0c\u53ef\u4ee5\u5e94\u5bf9\u4e0d\u540c\u7684\u4e1a\u52a1\u573a\u666f\uff0c\u7075\u6d3b\u5728\u4e8e\uff1a 1\u3001Reactor \u7684\u6570\u91cf\u53ef\u4ee5\u53ea\u6709\u4e00\u4e2a\uff0c\u4e5f\u53ef\u4ee5\u6709\u591a\u4e2a\uff1b 2\u3001\u5904\u7406\u8d44\u6e90\u6c60\u53ef\u4ee5\u662f\u5355\u4e2a\u8fdb\u7a0b / \u7ebf\u7a0b\uff0c\u4e5f\u53ef\u4ee5\u662f\u591a\u4e2a\u8fdb\u7a0b /\u7ebf\u7a0b\uff1b \u5c06\u4e0a\u9762\u7684\u4e24\u4e2a\u56e0\u7d20\u6392\u5217\u7ec4\u8bbe\u4e00\u4e0b\uff0c\u7406\u8bba\u4e0a\u5c31\u53ef\u4ee5\u6709 4 \u79cd\u65b9\u6848\u9009\u62e9\uff1a 1\u3001\u5355 Reactor \u5355\u8fdb\u7a0b / \u7ebf\u7a0b\uff1b 2\u3001\u5355 Reactor \u591a\u8fdb\u7a0b / \u7ebf\u7a0b\uff1b NOTE: Redis\u662f\u91c7\u7528\u7684\u8fd9\u79cd\u65b9\u6848 3\u3001\u591a Reactor \u5355\u8fdb\u7a0b / \u7ebf\u7a0b\uff1b NOTE: \u65e0\u5b9e\u9645\u7528\u9014 4\u3001\u591a Reactor \u591a\u8fdb\u7a0b / \u7ebf\u7a0b\uff1b \u5176\u4e2d\uff0c\u300c\u591a Reactor \u5355\u8fdb\u7a0b / \u7ebf\u7a0b\u300d\u5b9e\u73b0\u65b9\u6848\u76f8\u6bd4\u300c\u5355 Reactor \u5355\u8fdb\u7a0b / \u7ebf\u7a0b\u300d\u65b9\u6848\uff0c\u4e0d\u4ec5\u590d\u6742\u800c\u4e14\u4e5f\u6ca1\u6709\u6027\u80fd\u4f18\u52bf\uff0c\u56e0\u6b64\u5b9e\u9645\u4e2d\u5e76\u6ca1\u6709\u5e94\u7528\u3002 \u5269\u4e0b\u7684 3 \u4e2a\u65b9\u6848\u90fd\u662f\u6bd4\u8f83\u7ecf\u5178\u7684\uff0c\u4e14\u90fd\u6709\u5e94\u7528\u5728\u5b9e\u9645\u7684\u9879\u76ee\u4e2d\uff1a 1\u3001\u5355 Reactor \u5355\u8fdb\u7a0b / \u7ebf\u7a0b\uff1b 2\u3001\u5355 Reactor \u591a\u7ebf\u7a0b / \u8fdb\u7a0b\uff1b 4\u3001\u591a Reactor \u591a\u8fdb\u7a0b / \u7ebf\u7a0b\uff1b \u65b9\u6848\u5177\u4f53\u4f7f\u7528\u8fdb\u7a0b\u8fd8\u662f\u7ebf\u7a0b\uff0c\u8981\u770b\u4f7f\u7528\u7684\u7f16\u7a0b\u8bed\u8a00\u4ee5\u53ca\u5e73\u53f0\u6709\u5173\uff1a 2\u3001Java \u8bed\u8a00\u4e00\u822c\u4f7f\u7528\u7ebf\u7a0b\uff0c\u6bd4\u5982 Netty; 3\u3001C \u8bed\u8a00\u4f7f\u7528\u8fdb\u7a0b\u548c\u7ebf\u7a0b\u90fd\u53ef\u4ee5\uff0c\u4f8b\u5982 Nginx \u4f7f\u7528\u7684\u662f\u8fdb\u7a0b\uff0cMemcache \u4f7f\u7528\u7684\u662f\u7ebf\u7a0b\u3002 \u63a5\u4e0b\u6765\uff0c\u5206\u522b\u4ecb\u7ecd\u8fd9\u4e09\u4e2a\u7ecf\u5178\u7684 Reactor \u65b9\u6848\u3002 Reactor \u5355 Reactor \u5355\u8fdb\u7a0b / \u7ebf\u7a0b \u4e00\u822c\u6765\u8bf4\uff0cC \u8bed\u8a00\u5b9e\u73b0\u7684\u662f\u300c \u5355 Reactor \u5355\u8fdb\u7a0b \u300d\u7684\u65b9\u6848\uff0c\u56e0\u4e3a C \u8bed\u7f16\u5199\u5b8c\u7684\u7a0b\u5e8f\uff0c\u8fd0\u884c\u540e\u5c31\u662f\u4e00\u4e2a\u72ec\u7acb\u7684\u8fdb\u7a0b\uff0c\u4e0d\u9700\u8981\u5728\u8fdb\u7a0b\u4e2d\u518d\u521b\u5efa\u7ebf\u7a0b\u3002 \u800c Java \u8bed\u8a00\u5b9e\u73b0\u7684\u662f\u300c \u5355 Reactor \u5355\u7ebf\u7a0b \u300d\u7684\u65b9\u6848\uff0c\u56e0\u4e3a Java \u7a0b\u5e8f\u662f\u8dd1\u5728 Java \u865a\u62df\u673a\u8fd9\u4e2a\u8fdb\u7a0b\u4e0a\u9762\u7684\uff0c\u865a\u62df\u673a\u4e2d\u6709\u5f88\u591a\u7ebf\u7a0b\uff0c\u6211\u4eec\u5199\u7684 Java \u7a0b\u5e8f\u53ea\u662f\u5176\u4e2d\u7684\u4e00\u4e2a\u7ebf\u7a0b\u800c\u5df2\u3002 \u6211\u4eec\u6765\u770b\u770b\u300c \u5355 Reactor \u5355\u8fdb\u7a0b \u300d\u7684\u65b9\u6848\u793a\u610f\u56fe\uff1a ![img](https://pic4.zhimg.com/50/v2-614eb69d0186c32de123115b10c3c682_hd.jpg?source=1940ef5c) \u53ef\u4ee5\u770b\u5230\u8fdb\u7a0b\u91cc\u6709 Reactor\u3001Acceptor\u3001Handler \u8fd9\u4e09\u4e2a\u5bf9\u8c61\uff1a Reactor \u5bf9\u8c61\u7684\u4f5c\u7528\u662f\u76d1\u542c\u548c\u5206\u53d1\u4e8b\u4ef6\uff1b Acceptor \u5bf9\u8c61\u7684\u4f5c\u7528\u662f\u83b7\u53d6\u8fde\u63a5\uff1b Handler \u5bf9\u8c61\u7684\u4f5c\u7528\u662f\u5904\u7406\u4e1a\u52a1\uff1b \u5bf9\u8c61\u91cc\u7684 select\u3001accept\u3001read\u3001send \u662f\u7cfb\u7edf\u8c03\u7528\u51fd\u6570\uff0cdispatch \u548c \u300c\u4e1a\u52a1\u5904\u7406\u300d\u662f\u9700\u8981\u5b8c\u6210\u7684\u64cd\u4f5c\uff0c\u5176\u4e2d dispatch \u662f\u5206\u53d1\u4e8b\u4ef6\u64cd\u4f5c\u3002 \u63a5\u4e0b\u6765\uff0c\u4ecb\u7ecd\u4e0b\u300c\u5355 Reactor \u5355\u8fdb\u7a0b\u300d\u8fd9\u4e2a\u65b9\u6848\uff1a 1\u3001Reactor \u5bf9\u8c61\u901a\u8fc7 select \uff08IO \u591a\u8def\u590d\u7528\u63a5\u53e3\uff09 \u76d1\u542c\u4e8b\u4ef6\uff0c\u6536\u5230\u4e8b\u4ef6\u540e\u901a\u8fc7 dispatch \u8fdb\u884c\u5206\u53d1\uff0c\u5177\u4f53\u5206\u53d1\u7ed9 Acceptor \u5bf9\u8c61\u8fd8\u662f Handler \u5bf9\u8c61\uff0c\u8fd8\u8981\u770b\u6536\u5230\u7684\u4e8b\u4ef6\u7c7b\u578b\uff1b 2\u3001\u5982\u679c\u662f\u8fde\u63a5\u5efa\u7acb\u7684\u4e8b\u4ef6\uff0c\u5219\u4ea4\u7531 Acceptor \u5bf9\u8c61\u8fdb\u884c\u5904\u7406\uff0cAcceptor \u5bf9\u8c61\u4f1a\u901a\u8fc7 accept \u65b9\u6cd5 \u83b7\u53d6\u8fde\u63a5\uff0c\u5e76\u521b\u5efa\u4e00\u4e2a Handler \u5bf9\u8c61\u6765\u5904\u7406\u540e\u7eed\u7684\u54cd\u5e94\u4e8b\u4ef6\uff1b 3\u3001\u5982\u679c\u4e0d\u662f\u8fde\u63a5\u5efa\u7acb\u4e8b\u4ef6\uff0c \u5219\u4ea4\u7531\u5f53\u524d\u8fde\u63a5\u5bf9\u5e94\u7684 Handler \u5bf9\u8c61\u6765\u8fdb\u884c\u54cd\u5e94\uff1b 4\u3001Handler \u5bf9\u8c61\u901a\u8fc7 read -> \u4e1a\u52a1\u5904\u7406 -> send \u7684\u6d41\u7a0b\u6765\u5b8c\u6210\u5b8c\u6574\u7684\u4e1a\u52a1\u6d41\u7a0b\u3002 \u5355 Reactor \u5355\u8fdb\u7a0b\u7684\u65b9\u6848\u56e0\u4e3a\u5168\u90e8\u5de5\u4f5c\u90fd\u5728\u540c\u4e00\u4e2a\u8fdb\u7a0b\u5185\u5b8c\u6210\uff0c\u6240\u4ee5\u5b9e\u73b0\u8d77\u6765\u6bd4\u8f83\u7b80\u5355\uff0c\u4e0d\u9700\u8981\u8003\u8651\u8fdb\u7a0b\u95f4\u901a\u4fe1\uff0c\u4e5f\u4e0d\u7528\u62c5\u5fc3\u591a\u8fdb\u7a0b\u7ade\u4e89\u3002 \u4f46\u662f\uff0c\u8fd9\u79cd\u65b9\u6848\u5b58\u5728 2 \u4e2a\u7f3a\u70b9\uff1a 1\u3001\u7b2c\u4e00\u4e2a\u7f3a\u70b9\uff0c\u56e0\u4e3a\u53ea\u6709\u4e00\u4e2a\u8fdb\u7a0b\uff0c \u65e0\u6cd5\u5145\u5206\u5229\u7528 \u591a\u6838 CPU \u7684\u6027\u80fd \uff1b 2\u3001\u7b2c\u4e8c\u4e2a\u7f3a\u70b9\uff0cHandler \u5bf9\u8c61\u5728\u4e1a\u52a1\u5904\u7406\u65f6\uff0c\u6574\u4e2a\u8fdb\u7a0b\u662f\u65e0\u6cd5\u5904\u7406\u5176\u4ed6\u8fde\u63a5\u7684\u4e8b\u4ef6\u7684\uff0c \u5982\u679c\u4e1a\u52a1\u5904\u7406\u8017\u65f6\u6bd4\u8f83\u957f\uff0c\u90a3\u4e48\u5c31\u9020\u6210\u54cd\u5e94\u7684\u5ef6\u8fdf \uff1b \u6240\u4ee5\uff0c\u5355 Reactor \u5355\u8fdb\u7a0b\u7684\u65b9\u6848**\u4e0d\u9002\u7528\u8ba1\u7b97\u673a\u5bc6\u96c6\u578b\u7684\u573a\u666f\uff0c\u53ea\u9002\u7528\u4e8e\u4e1a\u52a1\u5904\u7406\u975e\u5e38\u5feb\u901f\u7684\u573a\u666f**\u3002 Redis \u662f\u7531 C \u8bed\u8a00\u5b9e\u73b0\u7684\uff0c\u5b83\u91c7\u7528\u7684\u6b63\u662f\u300c\u5355 Reactor \u5355\u8fdb\u7a0b\u300d\u7684\u65b9\u6848\uff0c\u56e0\u4e3a Redis \u4e1a\u52a1\u5904\u7406\u4e3b\u8981\u662f\u5728\u5185\u5b58\u4e2d\u5b8c\u6210\uff0c\u64cd\u4f5c\u7684\u901f\u5ea6\u662f\u5f88\u5feb\u7684\uff0c\u6027\u80fd\u74f6\u9888\u4e0d\u5728 CPU \u4e0a\uff0c\u6240\u4ee5 Redis \u5bf9\u4e8e\u547d\u4ee4\u7684\u5904\u7406\u662f\u5355\u8fdb\u7a0b\u7684\u65b9\u6848\u3002 NOTE: Redis\u7684\u65b0\u7248\u672c\u5df2\u7ecf\u66ff\u6362\u4e3amultiple thread\u4e86 \u5355 Reactor \u591a\u7ebf\u7a0b / \u591a\u8fdb\u7a0b \u5982\u679c\u8981\u514b\u670d\u300c\u5355 Reactor \u5355\u7ebf\u7a0b / \u8fdb\u7a0b\u300d\u65b9\u6848\u7684\u7f3a\u70b9\uff0c\u90a3\u4e48\u5c31\u9700\u8981\u5f15\u5165\u591a\u7ebf\u7a0b / \u591a\u8fdb\u7a0b\uff0c\u8fd9\u6837\u5c31\u4ea7\u751f\u4e86**\u5355 Reactor \u591a\u7ebf\u7a0b / \u591a\u8fdb\u7a0b**\u7684\u65b9\u6848\u3002 \u95fb\u5176\u540d\u4e0d\u5982\u770b\u5176\u56fe\uff0c\u5148\u6765\u770b\u770b\u300c\u5355 Reactor \u591a\u7ebf\u7a0b\u300d\u65b9\u6848\u7684\u793a\u610f\u56fe\u5982\u4e0b\uff1a \u8be6\u7ec6\u8bf4\u4e00\u4e0b\u8fd9\u4e2a\u65b9\u6848\uff1a Reactor \u5bf9\u8c61\u901a\u8fc7 select \uff08IO \u591a\u8def\u590d\u7528\u63a5\u53e3\uff09 \u76d1\u542c\u4e8b\u4ef6\uff0c\u6536\u5230\u4e8b\u4ef6\u540e\u901a\u8fc7 dispatch \u8fdb\u884c\u5206\u53d1\uff0c\u5177\u4f53\u5206\u53d1\u7ed9 Acceptor \u5bf9\u8c61\u8fd8\u662f Handler \u5bf9\u8c61\uff0c\u8fd8\u8981\u770b\u6536\u5230\u7684\u4e8b\u4ef6\u7c7b\u578b\uff1b \u5982\u679c\u662f\u8fde\u63a5\u5efa\u7acb\u7684\u4e8b\u4ef6\uff0c\u5219\u4ea4\u7531 Acceptor \u5bf9\u8c61\u8fdb\u884c\u5904\u7406\uff0cAcceptor \u5bf9\u8c61\u4f1a\u901a\u8fc7 accept \u65b9\u6cd5 \u83b7\u53d6\u8fde\u63a5\uff0c\u5e76\u521b\u5efa\u4e00\u4e2a Handler \u5bf9\u8c61\u6765\u5904\u7406\u540e\u7eed\u7684\u54cd\u5e94\u4e8b\u4ef6\uff1b \u5982\u679c\u4e0d\u662f\u8fde\u63a5\u5efa\u7acb\u4e8b\u4ef6\uff0c \u5219\u4ea4\u7531\u5f53\u524d\u8fde\u63a5\u5bf9\u5e94\u7684 Handler \u5bf9\u8c61\u6765\u8fdb\u884c\u54cd\u5e94\uff1b \u4e0a\u9762\u7684\u4e09\u4e2a\u6b65\u9aa4\u548c\u5355 Reactor \u5355\u7ebf\u7a0b\u65b9\u6848\u662f\u4e00\u6837\u7684\uff0c\u63a5\u4e0b\u6765\u7684\u6b65\u9aa4\u5c31\u5f00\u59cb\u4e0d\u4e00\u6837\u4e86\uff1a Handler \u5bf9\u8c61\u4e0d\u518d\u8d1f\u8d23\u4e1a\u52a1\u5904\u7406\uff0c\u53ea\u8d1f\u8d23\u6570\u636e\u7684\u63a5\u6536\u548c\u53d1\u9001\uff0cHandler \u5bf9\u8c61\u901a\u8fc7 read \u8bfb\u53d6\u5230\u6570\u636e\u540e\uff0c\u4f1a\u5c06\u6570\u636e\u53d1\u7ed9\u5b50\u7ebf\u7a0b\u91cc\u7684 Processor \u5bf9\u8c61\u8fdb\u884c\u4e1a\u52a1\u5904\u7406\uff1b \u5b50\u7ebf\u7a0b\u91cc\u7684 Processor \u5bf9\u8c61\u5c31\u8fdb\u884c\u4e1a\u52a1\u5904\u7406\uff0c\u5904\u7406\u5b8c\u540e\uff0c\u5c06\u7ed3\u679c\u53d1\u7ed9\u4e3b\u7ebf\u7a0b\u4e2d\u7684 Handler \u5bf9\u8c61\uff0c\u63a5\u7740\u7531 Handler \u901a\u8fc7 send \u65b9\u6cd5\u5c06\u54cd\u5e94\u7ed3\u679c\u53d1\u9001\u7ed9 client\uff1b \u5355 Reator \u591a\u7ebf\u7a0b\u7684\u65b9\u6848\u4f18\u52bf\u5728\u4e8e**\u80fd\u591f\u5145\u5206\u5229\u7528\u591a\u6838 CPU \u7684\u80fd**\uff0c\u90a3\u65e2\u7136\u5f15\u5165\u591a\u7ebf\u7a0b\uff0c\u90a3\u4e48\u81ea\u7136\u5c31\u5e26\u6765\u4e86\u591a\u7ebf\u7a0b\u7ade\u4e89\u8d44\u6e90\u7684\u95ee\u9898\u3002 \u4f8b\u5982\uff0c\u5b50\u7ebf\u7a0b\u5b8c\u6210\u4e1a\u52a1\u5904\u7406\u540e\uff0c\u8981\u628a\u7ed3\u679c\u4f20\u9012\u7ed9\u4e3b\u7ebf\u7a0b\u7684 Reactor \u8fdb\u884c\u53d1\u9001\uff0c\u8fd9\u91cc\u6d89\u53ca\u5171\u4eab\u6570\u636e\u7684\u7ade\u4e89\u3002 \u8981\u907f\u514d\u591a\u7ebf\u7a0b\u7531\u4e8e\u7ade\u4e89\u5171\u4eab\u8d44\u6e90\u800c\u5bfc\u81f4\u6570\u636e\u9519\u4e71\u7684\u95ee\u9898\uff0c\u5c31\u9700\u8981\u5728\u64cd\u4f5c\u5171\u4eab\u8d44\u6e90\u524d\u52a0\u4e0a\u4e92\u65a5\u9501\uff0c\u4ee5\u4fdd\u8bc1\u4efb\u610f\u65f6\u95f4\u91cc\u53ea\u6709\u4e00\u4e2a\u7ebf\u7a0b\u5728\u64cd\u4f5c\u5171\u4eab\u8d44\u6e90\uff0c\u5f85\u8be5\u7ebf\u7a0b\u64cd\u4f5c\u5b8c\u91ca\u653e\u4e92\u65a5\u9501\u540e\uff0c\u5176\u4ed6\u7ebf\u7a0b\u624d\u6709\u673a\u4f1a\u64cd\u4f5c\u5171\u4eab\u6570\u636e\u3002 \u804a\u5b8c\u5355 Reactor \u591a\u7ebf\u7a0b\u7684\u65b9\u6848\uff0c\u63a5\u7740\u6765\u770b\u770b\u5355 Reactor \u591a\u8fdb\u7a0b\u7684\u65b9\u6848\u3002 \u4e8b\u5b9e\u4e0a\uff0c\u5355 Reactor \u591a\u8fdb\u7a0b\u76f8\u6bd4\u5355 Reactor \u591a\u7ebf\u7a0b\u5b9e\u73b0\u8d77\u6765\u5f88\u9ebb\u70e6\uff0c\u4e3b\u8981\u56e0\u4e3a\u8981\u8003\u8651\u5b50\u8fdb\u7a0b <-> \u7236\u8fdb\u7a0b\u7684\u53cc\u5411\u901a\u4fe1\uff0c\u5e76\u4e14\u7236\u8fdb\u7a0b\u8fd8\u5f97\u77e5\u9053\u5b50\u8fdb\u7a0b\u8981\u5c06\u6570\u636e\u53d1\u9001\u7ed9\u54ea\u4e2a\u5ba2\u6237\u7aef\u3002 \u800c\u591a\u7ebf\u7a0b\u95f4\u53ef\u4ee5\u5171\u4eab\u6570\u636e\uff0c\u867d\u7136\u8981\u989d\u5916\u8003\u8651\u5e76\u53d1\u95ee\u9898\uff0c\u4f46\u662f\u8fd9\u8fdc\u6bd4\u8fdb\u7a0b\u95f4\u901a\u4fe1\u7684\u590d\u6742\u5ea6\u4f4e\u5f97\u591a\uff0c\u56e0\u6b64\u5b9e\u9645\u5e94\u7528\u4e2d\u4e5f\u770b\u4e0d\u5230\u5355 Reactor \u591a\u8fdb\u7a0b\u7684\u6a21\u5f0f\u3002 \u53e6\u5916\uff0c\u300c\u5355 Reactor\u300d\u7684\u6a21\u5f0f\u8fd8\u6709\u4e2a\u95ee\u9898\uff0c \u56e0\u4e3a\u4e00\u4e2a Reactor \u5bf9\u8c61\u627f\u62c5\u6240\u6709\u4e8b\u4ef6\u7684\u76d1\u542c\u548c\u54cd\u5e94\uff0c\u800c\u4e14\u53ea\u5728\u4e3b\u7ebf\u7a0b\u4e2d\u8fd0\u884c\uff0c\u5728\u9762\u5bf9\u77ac\u95f4\u9ad8\u5e76\u53d1\u7684\u573a\u666f\u65f6\uff0c\u5bb9\u6613\u6210\u4e3a\u6027\u80fd\u7684\u74f6\u9888\u7684\u5730\u65b9 \u3002 \u591a Reactor \u591a\u8fdb\u7a0b / \u7ebf\u7a0b \u8981\u89e3\u51b3\u300c\u5355 Reactor\u300d\u7684\u95ee\u9898\uff0c\u5c31\u662f\u5c06\u300c\u5355 Reactor\u300d\u5b9e\u73b0\u6210\u300c\u591a Reactor\u300d\uff0c\u8fd9\u6837\u5c31\u4ea7\u751f\u4e86\u7b2c **\u591a Reactor \u591a\u8fdb\u7a0b / \u7ebf\u7a0b**\u7684\u65b9\u6848\u3002 \u8001\u89c4\u77e9\uff0c\u95fb\u5176\u540d\u4e0d\u5982\u770b\u5176\u56fe\u3002\u591a Reactor \u591a\u8fdb\u7a0b / \u7ebf\u7a0b\u65b9\u6848\u7684\u793a\u610f\u56fe\u5982\u4e0b\uff08\u4ee5\u7ebf\u7a0b\u4e3a\u4f8b\uff09\uff1a ![img](https://pic1.zhimg.com/50/v2-4da008d8b7f55a0c18bef0e87c5c5bb1_hd.jpg?source=1940ef5c) \u65b9\u6848\u8be6\u7ec6\u8bf4\u660e\u5982\u4e0b\uff1a \u4e3b\u7ebf\u7a0b\u4e2d\u7684 MainReactor \u5bf9\u8c61\u901a\u8fc7 select \u76d1\u63a7\u8fde\u63a5\u5efa\u7acb\u4e8b\u4ef6\uff0c\u6536\u5230\u4e8b\u4ef6\u540e\u901a\u8fc7 Acceptor \u5bf9\u8c61\u4e2d\u7684 accept \u83b7\u53d6\u8fde\u63a5\uff0c\u5c06\u65b0\u7684\u8fde\u63a5\u5206\u914d\u7ed9\u67d0\u4e2a\u5b50\u7ebf\u7a0b\uff1b \u5b50\u7ebf\u7a0b\u4e2d\u7684 SubReactor \u5bf9\u8c61\u5c06 MainReactor \u5bf9\u8c61\u5206\u914d\u7684\u8fde\u63a5\u52a0\u5165 select \u7ee7\u7eed\u8fdb\u884c\u76d1\u542c\uff0c\u5e76\u521b\u5efa\u4e00\u4e2a Handler \u7528\u4e8e\u5904\u7406\u8fde\u63a5\u7684\u54cd\u5e94\u4e8b\u4ef6\u3002 \u5982\u679c\u6709\u65b0\u7684\u4e8b\u4ef6\u53d1\u751f\u65f6\uff0cSubReactor \u5bf9\u8c61\u4f1a\u8c03\u7528\u5f53\u524d\u8fde\u63a5\u5bf9\u5e94\u7684 Handler \u5bf9\u8c61\u6765\u8fdb\u884c\u54cd\u5e94\u3002 Handler \u5bf9\u8c61\u901a\u8fc7 read -> \u4e1a\u52a1\u5904\u7406 -> send \u7684\u6d41\u7a0b\u6765\u5b8c\u6210\u5b8c\u6574\u7684\u4e1a\u52a1\u6d41\u7a0b\u3002 \u591a Reactor \u591a\u7ebf\u7a0b\u7684\u65b9\u6848\u867d\u7136\u770b\u8d77\u6765\u590d\u6742\u7684\uff0c\u4f46\u662f\u5b9e\u9645\u5b9e\u73b0\u65f6\u6bd4\u5355 Reactor \u591a\u7ebf\u7a0b\u7684\u65b9\u6848\u8981\u7b80\u5355\u7684\u591a\uff0c\u539f\u56e0\u5982\u4e0b\uff1a \u4e3b\u7ebf\u7a0b\u548c\u5b50\u7ebf\u7a0b\u5206\u5de5\u660e\u786e\uff0c\u4e3b\u7ebf\u7a0b\u53ea\u8d1f\u8d23\u63a5\u6536\u65b0\u8fde\u63a5\uff0c\u5b50\u7ebf\u7a0b\u8d1f\u8d23\u5b8c\u6210\u540e\u7eed\u7684\u4e1a\u52a1\u5904\u7406\u3002 \u4e3b\u7ebf\u7a0b\u548c\u5b50\u7ebf\u7a0b\u7684\u4ea4\u4e92\u5f88\u7b80\u5355\uff0c\u4e3b\u7ebf\u7a0b\u53ea\u9700\u8981\u628a\u65b0\u8fde\u63a5\u4f20\u7ed9\u5b50\u7ebf\u7a0b\uff0c\u5b50\u7ebf\u7a0b\u65e0\u987b\u8fd4\u56de\u6570\u636e\uff0c\u76f4\u63a5\u5c31\u53ef\u4ee5\u5728\u5b50\u7ebf\u7a0b\u5c06\u5904\u7406\u7ed3\u679c\u53d1\u9001\u7ed9\u5ba2\u6237\u7aef\u3002 \u5927\u540d\u9f0e\u9f0e\u7684\u4e24\u4e2a\u5f00\u6e90\u8f6f\u4ef6 Netty \u548c Memcache \u90fd\u91c7\u7528\u4e86\u300c\u591a Reactor \u591a\u7ebf\u7a0b\u300d\u7684\u65b9\u6848\u3002 NOTE: \u8fd9\u79cd\u65b9\u5f0f\uff0c\u4e0d\u9700\u8981\u7ebf\u7a0b\u5b9a\u4f4d \u91c7\u7528\u4e86\u300c\u591a Reactor \u591a\u8fdb\u7a0b\u300d\u65b9\u6848\u7684\u5f00\u6e90\u8f6f\u4ef6\u662f Nginx\uff0c\u4e0d\u8fc7\u65b9\u6848\u4e0e\u6807\u51c6\u7684\u591a Reactor \u591a\u8fdb\u7a0b\u6709\u4e9b\u5dee\u5f02\u3002 \u5177\u4f53\u5dee\u5f02\u8868\u73b0\u5728\u4e3b\u8fdb\u7a0b\u4e2d\u4ec5\u4ec5\u7528\u6765\u521d\u59cb\u5316 socket\uff0c\u5e76\u6ca1\u6709\u521b\u5efa mainReactor \u6765 accept \u8fde\u63a5\uff0c\u800c\u662f\u7531\u5b50\u8fdb\u7a0b\u7684 Reactor \u6765 accept \u8fde\u63a5\uff0c\u901a\u8fc7\u9501\u6765\u63a7\u5236\u4e00\u6b21\u53ea\u6709\u4e00\u4e2a\u5b50\u8fdb\u7a0b\u8fdb\u884c accept\uff08\u9632\u6b62\u51fa\u73b0\u60ca\u7fa4\u73b0\u8c61\uff09\uff0c\u5b50\u8fdb\u7a0b accept \u65b0\u8fde\u63a5\u540e\u5c31\u653e\u5230\u81ea\u5df1\u7684 Reactor \u8fdb\u884c\u5904\u7406\uff0c\u4e0d\u4f1a\u518d\u5206\u914d\u7ed9\u5176\u4ed6\u5b50\u8fdb\u7a0b\u3002 Proactor \u524d\u9762\u63d0\u5230\u7684 Reactor \u662f\u975e\u963b\u585e\u540c\u6b65\u7f51\u7edc\u6a21\u5f0f \uff0c\u800c Proactor \u662f\u5f02\u6b65\u7f51\u7edc\u6a21\u5f0f \u3002 \u8fd9\u91cc\u5148\u7ed9\u5927\u5bb6\u590d\u4e60\u4e0b\u963b\u585e\u3001\u975e\u963b\u585e\u3001\u540c\u6b65\u3001\u5f02\u6b65 I/O \u7684\u6982\u5ff5\u3002 NOTE: \u4e0b\u9762\u7684\u5185\u5bb9\u662f\u57fa\u4e8e \"UNP 6.1 I/O Multiplexing: The select and poll Functions \u00b6 \"\u7684 \u963b\u585e I/O \u5148\u6765\u770b\u770b**\u963b\u585e I/O**\uff0c\u5f53\u7528\u6237\u7a0b\u5e8f\u6267\u884c read \uff0c\u7ebf\u7a0b\u4f1a\u88ab\u963b\u585e\uff0c\u4e00\u76f4\u7b49\u5230\u5185\u6838\u6570\u636e\u51c6\u5907\u597d\uff0c\u5e76\u628a\u6570\u636e\u4ece\u5185\u6838\u7f13\u51b2\u533a\u62f7\u8d1d\u5230\u5e94\u7528\u7a0b\u5e8f\u7684\u7f13\u51b2\u533a\u4e2d\uff0c\u5f53\u62f7\u8d1d\u8fc7\u7a0b\u5b8c\u6210\uff0c read \u624d\u4f1a\u8fd4\u56de\u3002 \u6ce8\u610f\uff0c \u963b\u585e\u7b49\u5f85\u7684\u662f\u300c\u5185\u6838\u6570\u636e\u51c6\u5907\u597d\u300d\u548c\u300c\u6570\u636e\u4ece\u5185\u6838\u6001\u62f7\u8d1d\u5230\u7528\u6237\u6001\u300d\u8fd9\u4e24\u4e2a\u8fc7\u7a0b \u3002\u8fc7\u7a0b\u5982\u4e0b\u56fe\uff1a \u975e\u963b\u585e I/O \u77e5\u9053\u4e86\u963b\u585e I/O \uff0c\u6765\u770b\u770b**\u975e\u963b\u585e I/O**\uff0c\u975e\u963b\u585e\u7684 read \u8bf7\u6c42\u5728\u6570\u636e\u672a\u51c6\u5907\u597d\u7684\u60c5\u51b5\u4e0b\u7acb\u5373\u8fd4\u56de\uff0c\u53ef\u4ee5\u7ee7\u7eed\u5f80\u4e0b\u6267\u884c\uff0c\u6b64\u65f6\u5e94\u7528\u7a0b\u5e8f\u4e0d\u65ad\u8f6e\u8be2\u5185\u6838\uff0c\u76f4\u5230\u6570\u636e\u51c6\u5907\u597d\uff0c\u5185\u6838\u5c06\u6570\u636e\u62f7\u8d1d\u5230\u5e94\u7528\u7a0b\u5e8f\u7f13\u51b2\u533a\uff0c read \u8c03\u7528\u624d\u53ef\u4ee5\u83b7\u53d6\u5230\u7ed3\u679c\u3002\u8fc7\u7a0b\u5982\u4e0b\u56fe\uff1a ![img](https://pic4.zhimg.com/50/v2-51e052e2beecef41da3aed3ebc2b80bd_hd.jpg?source=1940ef5c) \u6ce8\u610f\uff0c \u8fd9\u91cc\u6700\u540e\u4e00\u6b21 read \u8c03\u7528\uff0c\u83b7\u53d6\u6570\u636e\u7684\u8fc7\u7a0b\uff0c\u662f\u4e00\u4e2a\u540c\u6b65\u7684\u8fc7\u7a0b\uff0c\u662f\u9700\u8981\u7b49\u5f85\u7684\u8fc7\u7a0b\u3002\u8fd9\u91cc\u7684\u540c\u6b65\u6307\u7684\u662f\u5185\u6838\u6001\u7684\u6570\u636e\u62f7\u8d1d\u5230\u7528\u6237\u7a0b\u5e8f\u7684\u7f13\u5b58\u533a\u8fd9\u4e2a\u8fc7\u7a0b\u3002 \u4e3e\u4e2a\u4f8b\u5b50\uff0c\u5982\u679c socket \u8bbe\u7f6e\u4e86 O_NONBLOCK \u6807\u5fd7\uff0c\u90a3\u4e48\u5c31\u8868\u793a\u4f7f\u7528\u7684\u662f\u975e\u963b\u585e I/O \u7684\u65b9\u5f0f\u8bbf\u95ee\uff0c\u800c\u4e0d\u505a\u4efb\u4f55\u8bbe\u7f6e\u7684\u8bdd\uff0c\u9ed8\u8ba4\u662f\u963b\u585e I/O\u3002 \u56e0\u6b64\uff0c\u65e0\u8bba read \u548c send \u662f\u963b\u585e I/O\uff0c\u8fd8\u662f\u975e\u963b\u585e I/O \u90fd\u662f\u540c\u6b65\u8c03\u7528\u3002\u56e0\u4e3a\u5728 read \u8c03\u7528\u65f6\uff0c\u5185\u6838\u5c06\u6570\u636e\u4ece\u5185\u6838\u7a7a\u95f4\u62f7\u8d1d\u5230\u7528\u6237\u7a7a\u95f4\u7684\u8fc7\u7a0b\u90fd\u662f\u9700\u8981\u7b49\u5f85\u7684\uff0c\u4e5f\u5c31\u662f\u8bf4\u8fd9\u4e2a\u8fc7\u7a0b\u662f\u540c\u6b65\u7684\uff0c\u5982\u679c\u5185\u6838\u5b9e\u73b0\u7684\u62f7\u8d1d\u6548\u7387\u4e0d\u9ad8\uff0cread \u8c03\u7528\u5c31\u4f1a\u5728\u8fd9\u4e2a\u540c\u6b65\u8fc7\u7a0b\u4e2d\u7b49\u5f85\u6bd4\u8f83\u957f\u7684\u65f6\u95f4\u3002 \u5f02\u6b65 I/O \u800c\u771f\u6b63\u7684**\u5f02\u6b65 I/O** \u662f\u300c\u5185\u6838\u6570\u636e\u51c6\u5907\u597d\u300d\u548c\u300c\u6570\u636e\u4ece\u5185\u6838\u6001\u62f7\u8d1d\u5230\u7528\u6237\u6001\u300d\u8fd9**\u4e24\u4e2a\u8fc7\u7a0b\u90fd\u4e0d\u7528\u7b49\u5f85**\u3002 \u5f53\u6211\u4eec\u53d1\u8d77 aio_read \uff08\u5f02\u6b65 I/O\uff09 \u4e4b\u540e\uff0c\u5c31\u7acb\u5373\u8fd4\u56de\uff0c\u5185\u6838\u81ea\u52a8\u5c06\u6570\u636e\u4ece\u5185\u6838\u7a7a\u95f4\u62f7\u8d1d\u5230\u7528\u6237\u7a7a\u95f4\uff0c\u8fd9\u4e2a\u62f7\u8d1d\u8fc7\u7a0b\u540c\u6837\u662f\u5f02\u6b65\u7684\uff0c\u5185\u6838\u81ea\u52a8\u5b8c\u6210\u7684\uff0c\u548c\u524d\u9762\u7684\u540c\u6b65\u64cd\u4f5c\u4e0d\u4e00\u6837\uff0c \u5e94\u7528\u7a0b\u5e8f\u5e76\u4e0d\u9700\u8981\u4e3b\u52a8\u53d1\u8d77\u62f7\u8d1d\u52a8\u4f5c \u3002\u8fc7\u7a0b\u5982\u4e0b\u56fe\uff1a \u5f62\u8c61\u7684\u4f8b\u5b50 \u4e3e\u4e2a\u4f60\u53bb\u996d\u5802\u5403\u996d\u7684\u4f8b\u5b50\uff0c\u4f60\u597d\u6bd4\u5e94\u7528\u7a0b\u5e8f\uff0c\u996d\u5802\u597d\u6bd4\u64cd\u4f5c\u7cfb\u7edf\u3002 \u963b\u585e I/O \u597d\u6bd4\uff0c\u4f60\u53bb\u996d\u5802\u5403\u996d\uff0c\u4f46\u662f\u996d\u5802\u7684\u83dc\u8fd8\u6ca1\u505a\u597d\uff0c\u7136\u540e\u4f60\u5c31\u4e00\u76f4\u5728\u90a3\u91cc\u7b49\u554a\u7b49\uff0c\u7b49\u4e86\u597d\u957f\u4e00\u6bb5\u65f6\u95f4\u7ec8\u4e8e\u7b49\u5230\u996d\u5802\u963f\u59e8\u628a\u83dc\u7aef\u4e86\u51fa\u6765\uff08\u6570\u636e\u51c6\u5907\u7684\u8fc7\u7a0b\uff09\uff0c\u4f46\u662f\u4f60\u8fd8\u5f97\u7ee7\u7eed\u7b49\u963f\u59e8\u628a\u83dc\uff08\u5185\u6838\u7a7a\u95f4\uff09\u6253\u5230\u4f60\u7684\u996d\u76d2\u91cc\uff08\u7528\u6237\u7a7a\u95f4\uff09\uff0c\u7ecf\u5386\u5b8c\u8fd9\u4e24\u4e2a\u8fc7\u7a0b\uff0c\u4f60\u624d\u53ef\u4ee5\u79bb\u5f00\u3002 \u975e\u963b\u585e I/O \u597d\u6bd4\uff0c\u4f60\u53bb\u4e86\u996d\u5802\uff0c\u95ee\u963f\u59e8\u83dc\u505a\u597d\u4e86\u6ca1\u6709\uff0c\u963f\u59e8\u544a\u8bc9\u4f60\u6ca1\uff0c\u4f60\u5c31\u79bb\u5f00\u4e86\uff0c\u8fc7\u51e0\u5341\u5206\u949f\uff0c\u4f60\u53c8\u6765\u996d\u5802\u95ee\u963f\u59e8\uff0c\u963f\u59e8\u8bf4\u505a\u597d\u4e86\uff0c\u4e8e\u662f\u963f\u59e8\u5e2e\u4f60\u628a\u83dc\u6253\u5230\u4f60\u7684\u996d\u76d2\u91cc\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u4f60\u662f\u5f97\u7b49\u5f85\u7684\u3002 \u5f02\u6b65 I/O \u597d\u6bd4\uff0c\u4f60\u8ba9\u996d\u5802\u963f\u59e8\u5c06\u83dc\u505a\u597d\u5e76\u628a\u83dc\u6253\u5230\u996d\u76d2\u91cc\u540e\uff0c\u628a\u996d\u76d2\u9001\u5230\u4f60\u9762\u524d\uff0c\u6574\u4e2a\u8fc7\u7a0b\u4f60\u90fd\u4e0d\u9700\u8981\u4efb\u4f55\u7b49\u5f85\u3002 \u5f88\u660e\u663e\uff0c\u5f02\u6b65 I/O \u6bd4\u540c\u6b65 I/O \u6027\u80fd\u66f4\u597d\uff0c\u56e0\u4e3a\u5f02\u6b65 I/O \u5728\u300c\u5185\u6838\u6570\u636e\u51c6\u5907\u597d\u300d\u548c\u300c\u6570\u636e\u4ece\u5185\u6838\u7a7a\u95f4\u62f7\u8d1d\u5230\u7528\u6237\u7a7a\u95f4\u300d\u8fd9\u4e24\u4e2a\u8fc7\u7a0b\u90fd\u4e0d\u7528\u7b49\u5f85\u3002 Proactor \u6b63\u662f\u91c7\u7528\u4e86\u5f02\u6b65 I/O \u6280\u672f\uff0c\u6240\u4ee5\u88ab\u79f0\u4e3a\u5f02\u6b65\u7f51\u7edc\u6a21\u578b\u3002 \u73b0\u5728\u6211\u4eec\u518d\u6765\u7406\u89e3 Reactor \u548c Proactor \u7684\u533a\u522b\uff0c\u5c31\u6bd4\u8f83\u6e05\u6670\u4e86\u3002 1\u3001 Reactor \u662f\u975e\u963b\u585e\u540c\u6b65\u7f51\u7edc\u6a21\u5f0f\uff0c\u611f\u77e5\u7684\u662f\u5c31\u7eea\u53ef\u8bfb\u5199\u4e8b\u4ef6 \u3002\u5728\u6bcf\u6b21\u611f\u77e5\u5230\u6709\u4e8b\u4ef6\u53d1\u751f\uff08\u6bd4\u5982\u53ef\u8bfb\u5c31\u7eea\u4e8b\u4ef6\uff09\u540e\uff0c\u5c31\u9700\u8981\u5e94\u7528\u8fdb\u7a0b\u4e3b\u52a8\u8c03\u7528 read \u65b9\u6cd5\u6765\u5b8c\u6210\u6570\u636e\u7684\u8bfb\u53d6\uff0c\u4e5f\u5c31\u662f\u8981\u5e94\u7528\u8fdb\u7a0b\u4e3b\u52a8\u5c06 socket \u63a5\u6536\u7f13\u5b58\u4e2d\u7684\u6570\u636e\u8bfb\u5230\u5e94\u7528\u8fdb\u7a0b\u5185\u5b58\u4e2d\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u662f\u540c\u6b65\u7684\uff0c\u8bfb\u53d6\u5b8c\u6570\u636e\u540e\u5e94\u7528\u8fdb\u7a0b\u624d\u80fd\u5904\u7406\u6570\u636e\u3002 2\u3001 Proactor \u662f\u5f02\u6b65\u7f51\u7edc\u6a21\u5f0f\uff0c \u611f\u77e5\u7684\u662f\u5df2\u5b8c\u6210\u7684\u8bfb\u5199\u4e8b\u4ef6 \u3002\u5728\u53d1\u8d77\u5f02\u6b65\u8bfb\u5199\u8bf7\u6c42\u65f6\uff0c\u9700\u8981\u4f20\u5165\u6570\u636e\u7f13\u51b2\u533a\u7684\u5730\u5740\uff08\u7528\u6765\u5b58\u653e\u7ed3\u679c\u6570\u636e\uff09\u7b49\u4fe1\u606f\uff0c\u8fd9\u6837\u7cfb\u7edf\u5185\u6838\u624d\u53ef\u4ee5\u81ea\u52a8\u5e2e\u6211\u4eec\u628a\u6570\u636e\u7684\u8bfb\u5199\u5de5\u4f5c\u5b8c\u6210\uff0c\u8fd9\u91cc\u7684\u8bfb\u5199\u5de5\u4f5c\u5168\u7a0b\u7531\u64cd\u4f5c\u7cfb\u7edf\u6765\u505a\uff0c\u5e76\u4e0d\u9700\u8981\u50cf Reactor \u90a3\u6837\u8fd8\u9700\u8981\u5e94\u7528\u8fdb\u7a0b\u4e3b\u52a8\u53d1\u8d77 read/write \u6765\u8bfb\u5199\u6570\u636e\uff0c\u64cd\u4f5c\u7cfb\u7edf\u5b8c\u6210\u8bfb\u5199\u5de5\u4f5c\u540e\uff0c\u5c31\u4f1a\u901a\u77e5\u5e94\u7528\u8fdb\u7a0b\u76f4\u63a5\u5904\u7406\u6570\u636e\u3002 \u56e0\u6b64\uff0c Reactor \u53ef\u4ee5\u7406\u89e3\u4e3a\u300c\u6765\u4e86\u4e8b\u4ef6\u64cd\u4f5c\u7cfb\u7edf\u901a\u77e5\u5e94\u7528\u8fdb\u7a0b\uff0c\u8ba9\u5e94\u7528\u8fdb\u7a0b\u6765\u5904\u7406\u300d \uff0c\u800c Proactor \u53ef\u4ee5\u7406\u89e3\u4e3a\u300c\u6765\u4e86\u4e8b\u4ef6\u64cd\u4f5c\u7cfb\u7edf\u6765\u5904\u7406\uff0c\u5904\u7406\u5b8c\u518d\u901a\u77e5\u5e94\u7528\u8fdb\u7a0b\u300d \u3002\u8fd9\u91cc\u7684\u300c\u4e8b\u4ef6\u300d\u5c31\u662f\u6709\u65b0\u8fde\u63a5\u3001\u6709\u6570\u636e\u53ef\u8bfb\u3001\u6709\u6570\u636e\u53ef\u5199\u7684\u8fd9\u4e9b I/O \u4e8b\u4ef6\u8fd9\u91cc\u7684\u300c\u5904\u7406\u300d\u5305\u542b\u4ece\u9a71\u52a8\u8bfb\u53d6\u5230\u5185\u6838\u4ee5\u53ca\u4ece\u5185\u6838\u8bfb\u53d6\u5230\u7528\u6237\u7a7a\u95f4\u3002 \u4e3e\u4e2a\u5b9e\u9645\u751f\u6d3b\u4e2d\u7684\u4f8b\u5b50\uff0cReactor \u6a21\u5f0f\u5c31\u662f\u5feb\u9012\u5458\u5728\u697c\u4e0b\uff0c\u7ed9\u4f60\u6253\u7535\u8bdd\u544a\u8bc9\u4f60\u5feb\u9012\u5230\u4f60\u5bb6\u5c0f\u533a\u4e86\uff0c\u4f60\u9700\u8981\u81ea\u5df1\u4e0b\u697c\u6765\u62ff\u5feb\u9012\u3002\u800c\u5728 Proactor \u6a21\u5f0f\u4e0b\uff0c\u5feb\u9012\u5458\u76f4\u63a5\u5c06\u5feb\u9012\u9001\u5230\u4f60\u5bb6\u95e8\u53e3\uff0c\u7136\u540e\u901a\u77e5\u4f60\u3002 \u65e0\u8bba\u662f Reactor\uff0c\u8fd8\u662f Proactor\uff0c\u90fd\u662f\u4e00\u79cd\u57fa\u4e8e\u300c\u4e8b\u4ef6\u5206\u53d1\u300d\u7684\u7f51\u7edc\u7f16\u7a0b\u6a21\u5f0f\uff0c\u533a\u522b\u5728\u4e8e Reactor \u6a21\u5f0f\u662f\u57fa\u4e8e\u300c\u5f85\u5b8c\u6210\u300d\u7684 I/O \u4e8b\u4ef6\uff0c\u800c Proactor \u6a21\u5f0f\u5219\u662f\u57fa\u4e8e\u300c\u5df2\u5b8c\u6210\u300d\u7684 I/O \u4e8b\u4ef6 \u3002 \u63a5\u4e0b\u6765\uff0c\u4e00\u8d77\u770b\u770b Proactor \u6a21\u5f0f\u7684\u793a\u610f\u56fe\uff1a \u4ecb\u7ecd\u4e00\u4e0b Proactor \u6a21\u5f0f\u7684\u5de5\u4f5c\u6d41\u7a0b\uff1a Proactor Initiator \u8d1f\u8d23\u521b\u5efa Proactor \u548c Handler \u5bf9\u8c61\uff0c\u5e76\u5c06 Proactor \u548c Handler \u90fd\u901a\u8fc7 Asynchronous Operation Processor \u6ce8\u518c\u5230\u5185\u6838\uff1b Asynchronous Operation Processor \u8d1f\u8d23\u5904\u7406\u6ce8\u518c\u8bf7\u6c42\uff0c\u5e76\u5904\u7406 I/O \u64cd\u4f5c\uff1b Asynchronous Operation Processor \u5b8c\u6210 I/O \u64cd\u4f5c\u540e\u901a\u77e5 Proactor\uff1b Proactor \u6839\u636e\u4e0d\u540c\u7684\u4e8b\u4ef6\u7c7b\u578b\u56de\u8c03\u4e0d\u540c\u7684 Handler \u8fdb\u884c\u4e1a\u52a1\u5904\u7406\uff1b Handler \u5b8c\u6210\u4e1a\u52a1\u5904\u7406\uff1b \u53ef\u60dc\u7684\u662f\uff0c\u5728 Linux \u4e0b\u7684\u5f02\u6b65 I/O \u662f\u4e0d\u5b8c\u5584\u7684\uff0c aio \u7cfb\u5217\u51fd\u6570\u662f\u7531 POSIX \u5b9a\u4e49\u7684\u5f02\u6b65\u64cd\u4f5c\u63a5\u53e3\uff0c\u4e0d\u662f\u771f\u6b63\u7684\u64cd\u4f5c\u7cfb\u7edf\u7ea7\u522b\u652f\u6301\u7684\uff0c\u800c\u662f\u5728\u7528\u6237\u7a7a\u95f4\u6a21\u62df\u51fa\u6765\u7684\u5f02\u6b65\uff0c\u5e76\u4e14\u4ec5\u4ec5\u652f\u6301\u57fa\u4e8e\u672c\u5730\u6587\u4ef6\u7684 aio \u5f02\u6b65\u64cd\u4f5c\uff0c\u7f51\u7edc\u7f16\u7a0b\u4e2d\u7684 socket \u662f\u4e0d\u652f\u6301\u7684\uff0c\u8fd9\u4e5f\u4f7f\u5f97\u57fa\u4e8e Linux \u7684\u9ad8\u6027\u80fd\u7f51\u7edc\u7a0b\u5e8f\u90fd\u662f\u4f7f\u7528 Reactor \u65b9\u6848\u3002 \u800c Windows \u91cc\u5b9e\u73b0\u4e86\u4e00\u5957\u5b8c\u6574\u7684\u652f\u6301 socket \u7684\u5f02\u6b65\u7f16\u7a0b\u63a5\u53e3\uff0c\u8fd9\u5957\u63a5\u53e3\u5c31\u662f IOCP \uff0c\u662f\u7531\u64cd\u4f5c\u7cfb\u7edf\u7ea7\u522b\u5b9e\u73b0\u7684\u5f02\u6b65 I/O\uff0c\u771f\u6b63\u610f\u4e49\u4e0a\u5f02\u6b65 I/O\uff0c\u56e0\u6b64\u5728 Windows \u91cc\u5b9e\u73b0\u9ad8\u6027\u80fd\u7f51\u7edc\u7a0b\u5e8f\u53ef\u4ee5\u4f7f\u7528\u6548\u7387\u66f4\u9ad8\u7684 Proactor \u65b9\u6848\u3002 \u603b\u7ed3 \u5e38\u89c1\u7684 Reactor \u5b9e\u73b0\u65b9\u6848\u6709\u4e09\u79cd\u3002 \u7b2c\u4e00\u79cd\u65b9\u6848\u5355 Reactor \u5355\u8fdb\u7a0b / \u7ebf\u7a0b\uff0c\u4e0d\u7528\u8003\u8651\u8fdb\u7a0b\u95f4\u901a\u4fe1\u4ee5\u53ca\u6570\u636e\u540c\u6b65\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u5b9e\u73b0\u8d77\u6765\u6bd4\u8f83\u7b80\u5355\uff0c\u8fd9\u79cd\u65b9\u6848\u7684\u7f3a\u9677\u5728\u4e8e\u65e0\u6cd5\u5145\u5206\u5229\u7528\u591a\u6838 CPU\uff0c\u800c\u4e14\u5904\u7406\u4e1a\u52a1\u903b\u8f91\u7684\u65f6\u95f4\u4e0d\u80fd\u592a\u957f\uff0c\u5426\u5219\u4f1a\u5ef6\u8fdf\u54cd\u5e94\uff0c\u6240\u4ee5\u4e0d\u9002\u7528\u4e8e\u8ba1\u7b97\u673a\u5bc6\u96c6\u578b\u7684\u573a\u666f\uff0c\u9002\u7528\u4e8e\u4e1a\u52a1\u5904\u7406\u5feb\u901f\u7684\u573a\u666f\uff0c\u6bd4\u5982 Redis \u91c7\u7528\u7684\u662f\u5355 Reactor \u5355\u8fdb\u7a0b\u7684\u65b9\u6848\u3002 \u7b2c\u4e8c\u79cd\u65b9\u6848\u5355 Reactor \u591a\u7ebf\u7a0b\uff0c\u901a\u8fc7\u591a\u7ebf\u7a0b\u7684\u65b9\u5f0f\u89e3\u51b3\u4e86\u65b9\u6848\u4e00\u7684\u7f3a\u9677\uff0c\u4f46\u5b83\u79bb\u9ad8\u5e76\u53d1\u8fd8\u5dee\u4e00\u70b9\u8ddd\u79bb\uff0c\u5dee\u5728\u53ea\u6709\u4e00\u4e2a Reactor \u5bf9\u8c61\u6765\u627f\u62c5\u6240\u6709\u4e8b\u4ef6\u7684\u76d1\u542c\u548c\u54cd\u5e94\uff0c\u800c\u4e14\u53ea\u5728\u4e3b\u7ebf\u7a0b\u4e2d\u8fd0\u884c\uff0c\u5728\u9762\u5bf9\u77ac\u95f4\u9ad8\u5e76\u53d1\u7684\u573a\u666f\u65f6\uff0c\u5bb9\u6613\u6210\u4e3a\u6027\u80fd\u7684\u74f6\u9888\u7684\u5730\u65b9\u3002 \u7b2c\u4e09\u79cd\u65b9\u6848\u591a Reactor \u591a\u8fdb\u7a0b / \u7ebf\u7a0b\uff0c\u901a\u8fc7\u591a\u4e2a Reactor \u6765\u89e3\u51b3\u4e86\u65b9\u6848\u4e8c\u7684\u7f3a\u9677\uff0c\u4e3b Reactor \u53ea\u8d1f\u8d23\u76d1\u542c\u4e8b\u4ef6\uff0c\u54cd\u5e94\u4e8b\u4ef6\u7684\u5de5\u4f5c\u4ea4\u7ed9\u4e86\u4ece Reactor\uff0cNetty \u548c Memcache \u90fd\u91c7\u7528\u4e86\u300c\u591a Reactor \u591a\u7ebf\u7a0b\u300d\u7684\u65b9\u6848\uff0cNginx \u5219\u91c7\u7528\u4e86\u7c7b\u4f3c\u4e8e \u300c\u591a Reactor \u591a\u8fdb\u7a0b\u300d\u7684\u65b9\u6848\u3002 Reactor \u53ef\u4ee5\u7406\u89e3\u4e3a\u300c\u6765\u4e86\u4e8b\u4ef6\u64cd\u4f5c\u7cfb\u7edf\u901a\u77e5\u5e94\u7528\u8fdb\u7a0b\uff0c\u8ba9\u5e94\u7528\u8fdb\u7a0b\u6765\u5904\u7406\u300d\uff0c\u800c Proactor \u53ef\u4ee5\u7406\u89e3\u4e3a\u300c\u6765\u4e86\u4e8b\u4ef6\u64cd\u4f5c\u7cfb\u7edf\u6765\u5904\u7406\uff0c\u5904\u7406\u5b8c\u518d\u901a\u77e5\u5e94\u7528\u8fdb\u7a0b\u300d\u3002 \u56e0\u6b64\uff0c\u771f\u6b63\u7684\u5927\u6740\u5668\u8fd8\u662f Proactor\uff0c\u5b83\u662f\u91c7\u7528\u5f02\u6b65 I/O \u5b9e\u73b0\u7684\u5f02\u6b65\u7f51\u7edc\u6a21\u578b\uff0c\u611f\u77e5\u7684\u662f\u5df2\u5b8c\u6210\u7684\u8bfb\u5199\u4e8b\u4ef6\uff0c\u800c\u4e0d\u9700\u8981\u50cf Reactor \u611f\u77e5\u5230\u4e8b\u4ef6\u540e\uff0c\u8fd8\u9700\u8981\u8c03\u7528 read \u6765\u4ece\u5185\u6838\u4e2d\u83b7\u53d6\u6570\u636e\u3002 \u4e0d\u8fc7\uff0c\u65e0\u8bba\u662f Reactor\uff0c\u8fd8\u662f Proactor\uff0c\u90fd\u662f\u4e00\u79cd\u57fa\u4e8e\u300c\u4e8b\u4ef6\u5206\u53d1\u300d\u7684\u7f51\u7edc\u7f16\u7a0b\u6a21\u5f0f\uff0c\u533a\u522b\u5728\u4e8e Reactor \u6a21\u5f0f\u662f\u57fa\u4e8e\u300c\u5f85\u5b8c\u6210\u300d\u7684 I/O \u4e8b\u4ef6\uff0c\u800c Proactor \u6a21\u5f0f\u5219\u662f\u57fa\u4e8e\u300c\u5df2\u5b8c\u6210\u300d\u7684 I/O \u4e8b\u4ef6\u3002","title":"Introduction"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/zhihu-%E5%A6%82%E4%BD%95%E6%B7%B1%E5%88%BB%E7%90%86%E8%A7%A3reactor%E5%92%8Cproactor/#zhihu#reactorproactor","text":"","title":"zhihu \u5982\u4f55\u6df1\u523b\u7406\u89e3reactor\u548cproactor\uff1f"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/zhihu-%E5%A6%82%E4%BD%95%E6%B7%B1%E5%88%BB%E7%90%86%E8%A7%A3reactor%E5%92%8Cproactor/#a","text":"","title":"A"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/zhihu-%E5%A6%82%E4%BD%95%E6%B7%B1%E5%88%BB%E7%90%86%E8%A7%A3reactor%E5%92%8Cproactor/#1","text":"\u4e24\u79cdI/O\u591a\u8def\u590d\u7528\u6a21\u5f0f\uff1aReactor\u548cProactor","title":"1\u3001\u6807\u51c6\u5b9a\u4e49"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/zhihu-%E5%A6%82%E4%BD%95%E6%B7%B1%E5%88%BB%E7%90%86%E8%A7%A3reactor%E5%92%8Cproactor/#event#demultiplexer#and#event#handler","text":"\u4e00\u822c\u5730,I/O\u591a\u8def\u590d\u7528\u673a\u5236\u90fd\u4f9d\u8d56\u4e8e\u4e00\u4e2a\u4e8b\u4ef6**\u591a\u8def\u5206\u79bb\u5668(Event Demultiplexer) \u3002**\u5206\u79bb\u5668\u5bf9\u8c61**\u53ef\u5c06\u6765\u81ea\u4e8b\u4ef6\u6e90\u7684I/O\u4e8b\u4ef6\u5206\u79bb\u51fa\u6765\uff0c\u5e76\u5206\u53d1\u5230\u5bf9\u5e94\u7684**read/write\u4e8b\u4ef6\u5904\u7406\u5668(Event Handler) (\u6216\u56de\u8c03\u51fd\u6570)\u3002\u5f00\u53d1\u4eba\u5458\u9884\u5148\u6ce8\u518c\u9700\u8981\u5904\u7406\u7684\u4e8b\u4ef6\u53ca\u5176\u4e8b\u4ef6\u5904\u7406\u5668\uff08\u6216\u56de\u8c03\u51fd\u6570\uff09\uff1b\u4e8b\u4ef6\u5206\u79bb\u5668\u8d1f\u8d23\u5c06\u8bf7\u6c42\u4e8b\u4ef6\u4f20\u9012\u7ed9\u4e8b\u4ef6\u5904\u7406\u5668\u3002","title":"Event Demultiplexer and Event Handler"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/zhihu-%E5%A6%82%E4%BD%95%E6%B7%B1%E5%88%BB%E7%90%86%E8%A7%A3reactor%E5%92%8Cproactor/#ioio","text":"\u4e24\u4e2a\u4e0e\u4e8b\u4ef6\u5206\u79bb\u5668\u6709\u5173\u7684\u6a21\u5f0f\u662fReactor\u548cProactor\u3002Reactor\u6a21\u5f0f\u91c7\u7528\u540c\u6b65IO\uff0c\u800cProactor\u91c7\u7528\u5f02\u6b65IO\u3002 NOTE: \u4e00\u3001\u5bf9\u6bd4 1\u3001reactor\u4e2d\uff0cEvent Demultiplexer\u901a\u77e5\u7684\u662f\"\u8bfb\u5199\u64cd\u4f5c\u51c6\u5907\u5c31\u7eea\"\u4e8b\u4ef6\uff0c\u6700\u540e\u7531**\u4e8b\u4ef6\u5904\u7406\u5668(\u56de\u8c03\u51fd\u6570)**\u8d1f\u8d23\u5b8c\u6210\u5b9e\u9645\u7684\u8bfb\u5199\u5de5\u4f5c 2\u3001proactor\u4e2d\uff0cEvent Demultiplexer\u901a\u77e5\u7684\u662f\"\u8bfb\u5199\u64cd\u4f5c\u5b8c\u6210\"\u4e8b\u4ef6\uff0c\u662f\u7531**OS**\u8d1f\u8d23\u5b8c\u6210\u5b9e\u9645\u7684\u8bfb\u5199\u5de5\u4f5c \u4e8c\u3001Proactor pattern\u5c31\u662f\u5178\u578b\u7684Asynchronous I/O Model\uff0c\u5173\u4e8e\"Asynchronous I/O Model\"\uff0c\u53c2\u89c1\u5de5\u7a0bLinux-OS\u7684 IO-model \u7ae0\u8282\u3002 \u5728**Reactor**\u4e2d\uff0c**\u4e8b\u4ef6\u5206\u79bb\u5668**\u8d1f\u8d23\u7b49\u5f85\u6587\u4ef6\u63cf\u8ff0\u7b26\u6216socket\u4e3a\u8bfb\u5199\u64cd\u4f5c\u51c6\u5907\u5c31\u7eea\uff0c\u7136\u540e\u5c06\u5c31\u7eea\u4e8b\u4ef6\u4f20\u9012\u7ed9\u5bf9\u5e94\u7684\u5904\u7406\u5668\uff0c\u6700\u540e\u7531**\u5904\u7406\u5668**\u8d1f\u8d23\u5b8c\u6210\u5b9e\u9645\u7684\u8bfb\u5199\u5de5\u4f5c\u3002 \u5728**Proactor**\u6a21\u5f0f\u4e2d\uff0c\u5904\u7406\u5668--\u6216\u8005\u517c\u4efb\u5904\u7406\u5668\u7684**\u4e8b\u4ef6\u5206\u79bb\u5668**\uff0c\u53ea\u8d1f\u8d23\u53d1\u8d77\u5f02\u6b65\u8bfb\u5199\u64cd\u4f5c\u3002IO\u64cd\u4f5c\u672c\u8eab\u7531**\u64cd\u4f5c\u7cfb\u7edf**\u6765\u5b8c\u6210\u3002\u4f20\u9012\u7ed9\u64cd\u4f5c\u7cfb\u7edf\u7684\u53c2\u6570\u9700\u8981\u5305\u62ec\u7528\u6237\u5b9a\u4e49\u7684**\u6570\u636e\u7f13\u51b2\u533a\u5730\u5740**\u548c**\u6570\u636e\u5927\u5c0f**\uff0c\u64cd\u4f5c\u7cfb\u7edf\u624d\u80fd\u4ece\u4e2d\u5f97\u5230\u5199\u51fa\u64cd\u4f5c\u6240\u9700\u6570\u636e\uff0c\u6216\u5199\u5165\u4ecesocket\u8bfb\u5230\u7684\u6570\u636e\u3002 \u4e8b\u4ef6\u5206\u79bb\u5668**\u6355\u83b7**IO\u64cd\u4f5c\u5b8c\u6210\u4e8b\u4ef6 \uff0c\u7136\u540e\u5c06\u4e8b\u4ef6\u4f20\u9012\u7ed9\u5bf9\u5e94\u5904\u7406\u5668\u3002\u6bd4\u5982\uff0c\u5728windows\u4e0a\uff0c\u5904\u7406\u5668\u53d1\u8d77\u4e00\u4e2a\u5f02\u6b65IO\u64cd\u4f5c\uff0c\u518d\u7531\u4e8b\u4ef6\u5206\u79bb\u5668\u7b49\u5f85IOCompletion\u4e8b\u4ef6\u3002\u5178\u578b\u7684\u5f02\u6b65\u6a21\u5f0f\u5b9e\u73b0\uff0c\u90fd\u5efa\u7acb\u5728\u64cd\u4f5c\u7cfb\u7edf\u652f\u6301\u5f02\u6b65API\u7684\u57fa\u7840\u4e4b\u4e0a\uff0c\u6211\u4eec\u5c06\u8fd9\u79cd\u5b9e\u73b0\u79f0\u4e3a\u201c\u7cfb\u7edf\u7ea7\u201d\u5f02\u6b65\u6216\u201c\u771f\u201d\u5f02\u6b65\uff0c\u56e0\u4e3a\u5e94\u7528\u7a0b\u5e8f\u5b8c\u5168\u4f9d\u8d56\u64cd\u4f5c\u7cfb\u7edf\u6267\u884c\u771f\u6b63\u7684IO\u5de5\u4f5c\u3002","title":"\u540c\u6b65IO\u6216\u5f02\u6b65IO"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/zhihu-%E5%A6%82%E4%BD%95%E6%B7%B1%E5%88%BB%E7%90%86%E8%A7%A3reactor%E5%92%8Cproactor/#example","text":"\u4e3e\u4e2a\u4f8b\u5b50\uff0c\u5c06\u6709\u52a9\u4e8e\u7406\u89e3Reactor\u4e0eProactor\u4e8c\u8005\u7684\u5dee\u5f02\uff0c\u4ee5\u8bfb\u64cd\u4f5c\u4e3a\u4f8b\uff08\u7c7b\u64cd\u4f5c\u7c7b\u4f3c\uff09\u3002 \u5728Reactor\u4e2d\u5b9e\u73b0\u8bfb\uff1a - \u6ce8\u518c\u8bfb\u5c31\u7eea\u4e8b\u4ef6\u548c\u76f8\u5e94\u7684\u4e8b\u4ef6\u5904\u7406\u5668 - \u4e8b\u4ef6\u5206\u79bb\u5668\u7b49\u5f85\u4e8b\u4ef6 - \u4e8b\u4ef6\u5230\u6765\uff0c\u6fc0\u6d3b\u5206\u79bb\u5668\uff0c\u5206\u79bb\u5668\u8c03\u7528\u4e8b\u4ef6\u5bf9\u5e94\u7684\u5904\u7406\u5668\u3002 - \u4e8b\u4ef6\u5904\u7406\u5668\u5b8c\u6210\u5b9e\u9645\u7684\u8bfb\u64cd\u4f5c\uff0c\u5904\u7406\u8bfb\u5230\u7684\u6570\u636e\uff0c\u6ce8\u518c\u65b0\u7684\u4e8b\u4ef6\uff0c\u7136\u540e\u8fd4\u8fd8\u63a7\u5236\u6743\u3002 \u5728Proactor\u4e2d\u5b9e\u73b0\u8bfb\uff1a - **\u5904\u7406\u5668**\u53d1\u8d77\u5f02\u6b65\u8bfb\u64cd\u4f5c\uff08\u6ce8\u610f\uff1a\u64cd\u4f5c\u7cfb\u7edf\u5fc5\u987b\u652f\u6301\u5f02\u6b65IO\uff09\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5904\u7406\u5668\u65e0\u89c6IO\u5c31\u7eea\u4e8b\u4ef6\uff0c\u5b83\u5173\u6ce8\u7684\u662f\u5b8c\u6210\u4e8b\u4ef6\u3002 - \u4e8b\u4ef6\u5206\u79bb\u5668\u7b49\u5f85\u64cd\u4f5c\u5b8c\u6210\u4e8b\u4ef6 - \u5728\u5206\u79bb\u5668\u7b49\u5f85\u8fc7\u7a0b\u4e2d\uff0c\u64cd\u4f5c\u7cfb\u7edf\u5229\u7528\u5e76\u884c\u7684\u5185\u6838\u7ebf\u7a0b\u6267\u884c\u5b9e\u9645\u7684\u8bfb\u64cd\u4f5c\uff0c\u5e76\u5c06\u7ed3\u679c\u6570\u636e\u5b58\u5165\u7528\u6237\u81ea\u5b9a\u4e49\u7f13\u51b2\u533a\uff0c\u6700\u540e\u901a\u77e5\u4e8b\u4ef6\u5206\u79bb\u5668\u8bfb\u64cd\u4f5c\u5b8c\u6210\u3002 - \u4e8b\u4ef6\u5206\u79bb\u5668\u547c\u5524**\u5904\u7406\u5668**\u3002 - \u4e8b\u4ef6\u5904\u7406\u5668\u5904\u7406\u7528\u6237\u81ea\u5b9a\u4e49\u7f13\u51b2\u533a\u4e2d\u7684\u6570\u636e\uff0c\u7136\u540e\u542f\u52a8\u4e00\u4e2a\u65b0\u7684\u5f02\u6b65\u64cd\u4f5c\uff0c\u5e76\u5c06\u63a7\u5236\u6743\u8fd4\u56de\u4e8b\u4ef6\u5206\u79bb\u5668\u3002 \u4e24\u4e2a\u6a21\u5f0f\u7684\u76f8\u540c\u70b9\uff0c\u90fd\u662f\u5bf9\u67d0\u4e2aIO\u4e8b\u4ef6\u7684\u4e8b\u4ef6\u901a\u77e5(\u5373\u544a\u8bc9\u67d0\u4e2a\u6a21\u5757\uff0c\u8fd9\u4e2aIO\u64cd\u4f5c\u53ef\u4ee5\u8fdb\u884c\u6216\u5df2\u7ecf\u5b8c\u6210)\u3002\u5728\u7ed3\u6784\u4e0a\uff0c\u4e24\u8005\u4e5f\u6709\u76f8\u540c\u70b9\uff1ademultiplexor\u8d1f\u8d23\u63d0\u4ea4IO\u64cd\u4f5c(\u5f02\u6b65)\u3001\u67e5\u8be2\u8bbe\u5907\u662f\u5426\u53ef\u64cd\u4f5c(\u540c\u6b65)\uff0c\u7136\u540e\u5f53\u6761\u4ef6\u6ee1\u8db3\u65f6\uff0c\u5c31\u56de\u8c03handler\uff1b \u4e0d\u540c\u70b9\u5728\u4e8e\uff0c\u5f02\u6b65\u60c5\u51b5\u4e0b(Proactor)\uff0c\u5f53\u56de\u8c03handler\u65f6\uff0c\u8868\u793aIO\u64cd\u4f5c\u5df2\u7ecf\u5b8c\u6210\uff1b\u540c\u6b65\u60c5\u51b5\u4e0b(Reactor)\uff0c\u56de\u8c03handler\u65f6\uff0c\u8868\u793aIO\u8bbe\u5907\u53ef\u4ee5\u8fdb\u884c\u67d0\u4e2a\u64cd\u4f5c(can read or can write)\u3002","title":"Example"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/zhihu-%E5%A6%82%E4%BD%95%E6%B7%B1%E5%88%BB%E7%90%86%E8%A7%A3reactor%E5%92%8Cproactor/#2","text":"\u4f7f\u7528Proactor\u6846\u67b6\u548cReactor\u6846\u67b6\u90fd\u53ef\u4ee5\u6781\u5927\u7684\u7b80\u5316\u7f51\u7edc\u5e94\u7528\u7684\u5f00\u53d1\uff0c\u4f46\u5b83\u4eec\u7684\u91cd\u70b9\u5374\u4e0d\u540c\u3002 Reactor\u6846\u67b6\u4e2d\u7528\u6237\u5b9a\u4e49\u7684\u64cd\u4f5c\u662f\u5728\u5b9e\u9645\u64cd\u4f5c\u4e4b\u524d\u8c03\u7528\u7684\u3002\u6bd4\u5982\u4f60\u5b9a\u4e49\u4e86\u64cd\u4f5c\u662f\u8981\u5411\u4e00\u4e2aSOCKET\u5199\u6570\u636e\uff0c\u90a3\u4e48\u5f53\u8be5SOCKET\u53ef\u4ee5\u63a5\u6536\u6570\u636e\u7684\u65f6\u5019\uff0c\u4f60\u7684\u64cd\u4f5c\u5c31\u4f1a\u88ab\u8c03\u7528\uff1b \u800cProactor\u6846\u67b6\u4e2d\u7528\u6237\u5b9a\u4e49\u7684\u64cd\u4f5c\u662f\u5728\u5b9e\u9645\u64cd\u4f5c\u4e4b\u540e\u8c03\u7528\u7684\u3002\u6bd4\u5982\u4f60\u5b9a\u4e49\u4e86\u4e00\u4e2a\u64cd\u4f5c\u8981\u663e\u793a\u4eceSOCKET\u4e2d\u8bfb\u5165\u7684\u6570\u636e\uff0c\u90a3\u4e48\u5f53\u8bfb\u64cd\u4f5c\u5b8c\u6210\u4ee5\u540e\uff0c\u4f60\u7684\u64cd\u4f5c\u624d\u4f1a\u88ab\u8c03\u7528\u3002 Proactor\u548cReactor\u90fd\u662f\u5e76\u53d1\u7f16\u7a0b\u4e2d\u7684\u8bbe\u8ba1\u6a21\u5f0f\u3002**\u5728\u6211\u770b\u6765\uff0c\u4ed6\u4eec\u90fd\u662f\u7528\u4e8e\u6d3e\u53d1/\u5206\u79bbIO\u64cd\u4f5c\u4e8b\u4ef6\u7684\u3002\u8fd9\u91cc\u6240\u8c13\u7684IO\u4e8b\u4ef6\u4e5f\u5c31\u662f\u8bf8\u5982read/write\u7684IO\u64cd\u4f5c\u3002\"\u6d3e\u53d1/\u5206\u79bb\"\u5c31\u662f\u5c06\u5355\u72ec\u7684IO\u4e8b\u4ef6\u901a\u77e5\u5230\u4e0a\u5c42\u6a21\u5757\u3002\u4e24\u4e2a\u6a21\u5f0f\u4e0d\u540c\u7684\u5730\u65b9\u5728\u4e8e\uff0c**Proactor\u7528\u4e8e\u5f02\u6b65IO\uff0c\u800cReactor\u7528\u4e8e\u540c\u6b65IO\u3002 \u90e8\u5206\u53c2\u8003\u81ea http://www.cnblogs.com/dawen/archive/2011/05/18/2050358.html","title":"2\u3001\u901a\u4fd7\u7406\u89e3"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/zhihu-%E5%A6%82%E4%BD%95%E6%B7%B1%E5%88%BB%E7%90%86%E8%A7%A3reactor%E5%92%8Cproactor/#a_1","text":"reactor\uff1a\u80fd\u6536\u4e86\u4f60\u8ddf\u4ffa\u8bf4\u4e00\u58f0\u3002 proactor: \u4f60\u7ed9\u6211\u6536\u5341\u4e2a\u5b57\u8282\uff0c\u6536\u597d\u4e86\u8ddf\u4ffa\u8bf4\u4e00\u58f0\u3002","title":"A"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/zhihu-%E5%A6%82%E4%BD%95%E6%B7%B1%E5%88%BB%E7%90%86%E8%A7%A3reactor%E5%92%8Cproactor/#a_2","text":"Reactor: libevent/libev/libuv/ZeroMQ/Event Library in Redis Proactor IOCP/Boost.Asio linux\u4e0b\u8fd8\u662fReactor\u628a, \u6ca1\u6709os\u652f\u6301, Proactor\u73a9\u4e0d\u8f6c.","title":"A"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/zhihu-%E5%A6%82%E4%BD%95%E6%B7%B1%E5%88%BB%E7%90%86%E8%A7%A3reactor%E5%92%8Cproactor/#coding","text":"\u522b\u5c0f\u770b\u8fd9\u4e24\u4e2a\u4e1c\u897f\uff0c\u7279\u522b\u662f Reactor \u6a21\u5f0f\uff0c\u5e02\u9762\u4e0a\u5e38\u89c1\u7684\u5f00\u6e90\u8f6f\u4ef6\u5f88\u591a\u90fd\u91c7\u7528\u4e86\u8fd9\u4e2a\u65b9\u6848\uff0c\u6bd4\u5982 Redis\u3001Nginx\u3001Netty \u7b49\u7b49\uff0c\u6240\u4ee5\u5b66\u597d\u8fd9\u4e2a\u6a21\u5f0f\u8bbe\u8ba1\u7684\u601d\u60f3\uff0c\u4e0d\u4ec5\u6709\u52a9\u4e8e\u6211\u4eec\u7406\u89e3\u5f88\u591a\u5f00\u6e90\u8f6f\u4ef6\uff0c\u800c\u4e14\u4e5f\u80fd\u5728\u9762\u8bd5\u65f6\u5439\u903c\u3002 \u53d1\u8f66\uff01 ![img](https://pic4.zhimg.com/80/v2-e7c8ec8a75fc13c602f394bb3c8e45b5_1440w.jpg?source=1940ef5c)","title":"\u5c0f\u6797coding\u7684\u56de\u7b54"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/zhihu-%E5%A6%82%E4%BD%95%E6%B7%B1%E5%88%BB%E7%90%86%E8%A7%A3reactor%E5%92%8Cproactor/#_1","text":"NOTE: C10K\u95ee\u9898","title":"\u6f14\u8fdb"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/zhihu-%E5%A6%82%E4%BD%95%E6%B7%B1%E5%88%BB%E7%90%86%E8%A7%A3reactor%E5%92%8Cproactor/#concurrent#server#clientthreadprocess","text":"\u5982\u679c\u8981\u8ba9\u670d\u52a1\u5668\u670d\u52a1\u591a\u4e2a\u5ba2\u6237\u7aef\uff0c\u90a3\u4e48\u6700\u76f4\u63a5\u7684\u65b9\u5f0f\u5c31\u662f\u4e3a\u6bcf\u4e00\u6761\u8fde\u63a5\u521b\u5efa\u7ebf\u7a0b\u3002 \u5176\u5b9e\u521b\u5efa\u8fdb\u7a0b\u4e5f\u662f\u53ef\u4ee5\u7684\uff0c\u539f\u7406\u662f\u4e00\u6837\u7684\uff0c\u8fdb\u7a0b\u548c\u7ebf\u7a0b\u7684\u533a\u522b\u5728\u4e8e\u7ebf\u7a0b\u6bd4\u8f83\u8f7b\u91cf\u7ea7\u4e9b\uff0c\u7ebf\u7a0b\u7684\u521b\u5efa\u548c\u7ebf\u7a0b\u95f4\u5207\u6362\u7684\u6210\u672c\u8981\u5c0f\u4e9b\uff0c\u4e3a\u4e86\u63cf\u8ff0\u7b80\u8ff0\uff0c\u540e\u9762\u90fd\u4ee5\u7ebf\u7a0b\u4e3a\u4f8b\u3002 \u5904\u7406\u5b8c\u4e1a\u52a1\u903b\u8f91\u540e\uff0c\u968f\u7740\u8fde\u63a5\u5173\u95ed\u540e\u7ebf\u7a0b\u4e5f\u540c\u6837\u8981\u9500\u6bc1\u4e86\uff0c\u4f46\u662f\u8fd9\u6837\u4e0d\u505c\u5730\u521b\u5efa\u548c\u9500\u6bc1\u7ebf\u7a0b\uff0c\u4e0d\u4ec5\u4f1a\u5e26\u6765\u6027\u80fd\u5f00\u9500\uff0c\u4e5f\u4f1a\u9020\u6210\u6d6a\u8d39\u8d44\u6e90\uff0c\u800c\u4e14\u5982\u679c\u8981\u8fde\u63a5\u51e0\u4e07\u6761\u8fde\u63a5\uff0c\u521b\u5efa\u51e0\u4e07\u4e2a\u7ebf\u7a0b\u53bb\u5e94\u5bf9\u4e5f\u662f\u4e0d\u73b0\u5b9e\u7684\u3002","title":"Concurrent server: \u6bcf\u4e2aclient\u4e00\u4e2athread\u3001process"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/zhihu-%E5%A6%82%E4%BD%95%E6%B7%B1%E5%88%BB%E7%90%86%E8%A7%A3reactor%E5%92%8Cproactor/#concurrent#server#thread#pool","text":"\u8981\u8fd9\u4e48\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u5462\uff1f\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u300c\u8d44\u6e90\u590d\u7528\u300d\u7684\u65b9\u5f0f\u3002 \u4e5f\u5c31\u662f\u4e0d\u7528\u518d\u4e3a\u6bcf\u4e2a\u8fde\u63a5\u521b\u5efa\u7ebf\u7a0b\uff0c\u800c\u662f\u521b\u5efa\u4e00\u4e2a\u300c\u7ebf\u7a0b\u6c60\u300d\uff0c\u5c06\u8fde\u63a5\u5206\u914d\u7ed9\u7ebf\u7a0b\uff0c\u7136\u540e\u4e00\u4e2a\u7ebf\u7a0b\u53ef\u4ee5\u5904\u7406\u591a\u4e2a\u8fde\u63a5\u7684\u4e1a\u52a1\u3002 \u4e0d\u8fc7\uff0c\u8fd9\u6837\u53c8\u5f15\u6765\u4e00\u4e2a\u65b0\u7684\u95ee\u9898\uff0c\u7ebf\u7a0b\u600e\u6837\u624d\u80fd\u9ad8\u6548\u5730\u5904\u7406\u591a\u4e2a\u8fde\u63a5\u7684\u4e1a\u52a1\uff1f \u5f53\u4e00\u4e2a\u8fde\u63a5\u5bf9\u5e94\u4e00\u4e2a\u7ebf\u7a0b\u65f6\uff0c\u7ebf\u7a0b\u4e00\u822c\u91c7\u7528\u300cread -> \u4e1a\u52a1\u5904\u7406 -> send\u300d\u7684\u5904\u7406\u6d41\u7a0b\uff0c\u5982\u679c\u5f53\u524d\u8fde\u63a5\u6ca1\u6709\u6570\u636e\u53ef\u8bfb\uff0c\u90a3\u4e48\u7ebf\u7a0b\u4f1a\u963b\u585e\u5728 read \u64cd\u4f5c\u4e0a\uff08 socket \u9ed8\u8ba4\u60c5\u51b5\u662f\u963b\u585e I/O\uff09\uff0c\u4e0d\u8fc7\u8fd9\u79cd\u963b\u585e\u65b9\u5f0f\u5e76\u4e0d\u5f71\u54cd\u5176\u4ed6\u7ebf\u7a0b\u3002 \u4f46\u662f\u5f15\u5165\u4e86\u7ebf\u7a0b\u6c60\uff0c\u90a3\u4e48\u4e00\u4e2a\u7ebf\u7a0b\u8981\u5904\u7406\u591a\u4e2a\u8fde\u63a5\u7684\u4e1a\u52a1\uff0c\u7ebf\u7a0b\u5728\u5904\u7406\u67d0\u4e2a\u8fde\u63a5\u7684 read \u64cd\u4f5c\u65f6\uff0c\u5982\u679c\u9047\u5230\u6ca1\u6709\u6570\u636e\u53ef\u8bfb\uff0c\u5c31\u4f1a\u53d1\u751f\u963b\u585e\uff0c\u90a3\u4e48\u7ebf\u7a0b\u5c31\u6ca1\u529e\u6cd5\u7ee7\u7eed\u5904\u7406\u5176\u4ed6\u8fde\u63a5\u7684\u4e1a\u52a1\u3002 \u8981\u89e3\u51b3\u8fd9\u4e00\u4e2a\u95ee\u9898\uff0c\u6700\u7b80\u5355\u7684\u65b9\u5f0f\u5c31\u662f\u5c06 socket \u6539\u6210\u975e\u963b\u585e\uff0c\u7136\u540e\u7ebf\u7a0b\u4e0d\u65ad\u5730\u8f6e\u8be2\u8c03\u7528 read \u64cd\u4f5c\u6765\u5224\u65ad\u662f\u5426\u6709\u6570\u636e\uff0c\u8fd9\u79cd\u65b9\u5f0f\u867d\u7136\u8be5\u80fd\u591f\u89e3\u51b3\u963b\u585e\u7684\u95ee\u9898\uff0c\u4f46\u662f\u89e3\u51b3\u7684\u65b9\u5f0f\u6bd4\u8f83\u7c97\u66b4\uff0c\u56e0\u4e3a\u8f6e\u8be2\u662f\u8981\u6d88\u8017 CPU \u7684\uff0c\u800c\u4e14\u968f\u7740\u4e00\u4e2a \u7ebf\u7a0b\u5904\u7406\u7684\u8fde\u63a5\u8d8a\u591a\uff0c\u8f6e\u8be2\u7684\u6548\u7387\u5c31\u4f1a\u8d8a\u4f4e\u3002 \u4e0a\u9762\u7684\u95ee\u9898\u5728\u4e8e\uff0c\u7ebf\u7a0b\u5e76\u4e0d\u77e5\u9053\u5f53\u524d\u8fde\u63a5\u662f\u5426\u6709\u6570\u636e\u53ef\u8bfb\uff0c\u4ece\u800c\u9700\u8981\u6bcf\u6b21\u901a\u8fc7 read \u53bb\u8bd5\u63a2\u3002","title":"Concurrent server: thread pool"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/zhihu-%E5%A6%82%E4%BD%95%E6%B7%B1%E5%88%BB%E7%90%86%E8%A7%A3reactor%E5%92%8Cproactor/#concurrent#server#event#driven#io#multiplexing","text":"\u90a3\u6709\u6ca1\u6709\u529e\u6cd5\u5728\u53ea\u6709\u5f53\u8fde\u63a5\u4e0a\u6709\u6570\u636e\u7684\u65f6\u5019\uff0c\u7ebf\u7a0b\u624d\u53bb\u53d1\u8d77\u8bfb\u8bf7\u6c42\u5462\uff1f\u7b54\u6848\u662f\u6709\u7684\uff0c\u5b9e\u73b0\u8fd9\u4e00\u6280\u672f\u7684\u5c31\u662f I/O \u591a\u8def\u590d\u7528\u3002 I/O \u591a\u8def\u590d\u7528\u6280\u672f\u4f1a\u7528\u4e00\u4e2a\u7cfb\u7edf\u8c03\u7528\u51fd\u6570\u6765\u76d1\u542c\u6211\u4eec\u6240\u6709\u5173\u5fc3\u7684\u8fde\u63a5\uff0c\u4e5f\u5c31\u8bf4\u53ef\u4ee5\u5728\u4e00\u4e2a\u76d1\u63a7\u7ebf\u7a0b\u91cc\u9762\u76d1\u63a7\u5f88\u591a\u7684\u8fde\u63a5\u3002 ![img](https://pic1.zhimg.com/50/v2-0a86ab90d8167860dec5c695064648f3_hd.jpg) \u6211\u4eec\u719f\u6089\u7684 select/poll/epoll \u5c31\u662f\u5185\u6838\u63d0\u4f9b\u7ed9\u7528\u6237\u6001\u7684\u591a\u8def\u590d\u7528\u7cfb\u7edf\u8c03\u7528\uff0c\u7ebf\u7a0b\u53ef\u4ee5\u901a\u8fc7\u4e00\u4e2a\u7cfb\u7edf\u8c03\u7528\u51fd\u6570\u4ece\u5185\u6838\u4e2d\u83b7\u53d6\u591a\u4e2a\u4e8b\u4ef6\u3002 PS\uff1a\u5982\u679c\u60f3\u77e5\u9053 select/poll/epoll \u7684\u533a\u522b\uff0c\u53ef\u4ee5\u770b\u770b\u5c0f\u6797\u4e4b\u524d\u5199\u7684\u8fd9\u7bc7\u6587\u7ae0\uff1a \u8fd9\u6b21\u7b54\u5e94\u6211\uff0c\u4e00\u4e3e\u62ff\u4e0b I/O \u591a\u8def\u590d\u7528\uff01 select/poll/epoll \u662f\u5982\u4f55\u83b7\u53d6\u7f51\u7edc\u4e8b\u4ef6\u7684\u5462\uff1f \u5728\u83b7\u53d6\u4e8b\u4ef6\u65f6\uff0c\u5148\u628a\u6211\u4eec\u8981\u5173\u5fc3\u7684\u8fde\u63a5\u4f20\u7ed9\u5185\u6838\uff0c\u518d\u7531\u5185\u6838\u68c0\u6d4b\uff1a 1\u3001\u5982\u679c\u6ca1\u6709\u4e8b\u4ef6\u53d1\u751f\uff0c\u7ebf\u7a0b\u53ea\u9700\u963b\u585e\u5728\u8fd9\u4e2a\u7cfb\u7edf\u8c03\u7528\uff0c\u800c\u65e0\u9700\u50cf\u524d\u9762\u7684\u7ebf\u7a0b\u6c60\u65b9\u6848\u90a3\u6837\u8f6e\u8bad\u8c03\u7528 read \u64cd\u4f5c\u6765\u5224\u65ad\u662f\u5426\u6709\u6570\u636e\u3002 2\u3001\u5982\u679c\u6709\u4e8b\u4ef6\u53d1\u751f\uff0c\u5185\u6838\u4f1a\u8fd4\u56de\u4ea7\u751f\u4e86\u4e8b\u4ef6\u7684\u8fde\u63a5\uff0c\u7ebf\u7a0b\u5c31\u4f1a\u4ece\u963b\u585e\u72b6\u6001\u8fd4\u56de\uff0c\u7136\u540e\u5728\u7528\u6237\u6001\u4e2d\u518d\u5904\u7406\u8fd9\u4e9b\u8fde\u63a5\u5bf9\u5e94\u7684\u4e1a\u52a1\u5373\u53ef\u3002","title":"Concurrent server: event driven IO multiplexing"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/zhihu-%E5%A6%82%E4%BD%95%E6%B7%B1%E5%88%BB%E7%90%86%E8%A7%A3reactor%E5%92%8Cproactor/#reactor","text":"\u5f53\u4e0b\u5f00\u6e90\u8f6f\u4ef6\u80fd\u505a\u5230\u7f51\u7edc\u9ad8\u6027\u80fd\u7684\u539f\u56e0\u5c31\u662f I/O \u591a\u8def\u590d\u7528\u5417\uff1f \u662f\u7684\uff0c\u57fa\u672c\u662f\u57fa\u4e8e I/O \u591a\u8def\u590d\u7528\uff0c\u7528\u8fc7 I/O \u591a\u8def\u590d\u7528\u63a5\u53e3\u5199\u7f51\u7edc\u7a0b\u5e8f\u7684\u540c\u5b66\uff0c\u80af\u5b9a\u77e5\u9053\u662f\u9762\u5411\u8fc7\u7a0b\u7684\u65b9\u5f0f\u5199\u4ee3\u7801\u7684\uff0c\u8fd9\u6837\u7684\u5f00\u53d1\u7684\u6548\u7387\u4e0d\u9ad8\u3002 \u4e8e\u662f\uff0c\u5927\u4f6c\u4eec\u57fa\u4e8e\u9762\u5411\u5bf9\u8c61\u7684\u601d\u60f3\uff0c\u5bf9 I/O \u591a\u8def\u590d\u7528\u4f5c\u4e86\u4e00\u5c42\u5c01\u88c5\uff0c\u8ba9\u4f7f\u7528\u8005\u4e0d\u7528\u8003\u8651\u5e95\u5c42\u7f51\u7edc API \u7684\u7ec6\u8282\uff0c\u53ea\u9700\u8981\u5173\u6ce8\u5e94\u7528\u4ee3\u7801\u7684\u7f16\u5199\u3002 \u5927\u4f6c\u4eec\u8fd8\u4e3a\u8fd9\u79cd\u6a21\u5f0f\u53d6\u4e86\u4e2a\u8ba9\u4eba\u7b2c\u4e00\u65f6\u95f4\u96be\u4ee5\u7406\u89e3\u7684\u540d\u5b57\uff1a Reactor \u6a21\u5f0f \u3002 Reactor \u7ffb\u8bd1\u8fc7\u6765\u7684\u610f\u601d\u662f\u300c\u53cd\u5e94\u5806\u300d\uff0c\u53ef\u80fd\u5927\u5bb6\u4f1a\u8054\u60f3\u5230\u7269\u7406\u5b66\u91cc\u7684\u6838\u53cd\u5e94\u5806\uff0c\u5b9e\u9645\u4e0a\u5e76\u4e0d\u662f\u7684\u8fd9\u4e2a\u610f\u601d\u3002 \u8fd9\u91cc\u7684\u53cd\u5e94\u6307\u7684\u662f\u300c \u5bf9\u4e8b\u4ef6\u53cd\u5e94 \u300d\uff0c\u4e5f\u5c31\u662f**\u6765\u4e86\u4e00\u4e2a\u4e8b\u4ef6\uff0cReactor \u5c31\u6709\u76f8\u5bf9\u5e94\u7684\u53cd\u5e94/\u54cd\u5e94**\u3002 \u4e8b\u5b9e\u4e0a\uff0cReactor \u6a21\u5f0f\u4e5f\u53eb Dispatcher \u6a21\u5f0f\uff0c\u6211\u89c9\u5f97\u8fd9\u4e2a\u540d\u5b57\u66f4\u8d34\u5408\u8be5\u6a21\u5f0f\u7684\u542b\u4e49\uff0c\u5373 I/O \u591a\u8def\u590d\u7528\u76d1\u542c\u4e8b\u4ef6\uff0c\u6536\u5230\u4e8b\u4ef6\u540e\uff0c\u6839\u636e\u4e8b\u4ef6\u7c7b\u578b\u5206\u914d\uff08Dispatch\uff09\u7ed9\u67d0\u4e2a\u8fdb\u7a0b / \u7ebf\u7a0b \u3002 Reactor \u6a21\u5f0f**\u4e3b\u8981\u7531 **Reactor \u548c**\u5904\u7406\u8d44\u6e90\u6c60**\u8fd9\u4e24\u4e2a\u6838\u5fc3\u90e8\u5206\u7ec4\u6210\uff0c\u5b83\u4fe9\u8d1f\u8d23\u7684\u4e8b\u60c5\u5982\u4e0b\uff1a 1\u3001Reactor \u8d1f\u8d23\u76d1\u542c\u548c\u5206\u53d1\u4e8b\u4ef6\uff0c\u4e8b\u4ef6\u7c7b\u578b\u5305\u542b\u8fde\u63a5\u4e8b\u4ef6\u3001\u8bfb\u5199\u4e8b\u4ef6\uff1b 2\u3001\u5904\u7406\u8d44\u6e90\u6c60\u8d1f\u8d23\u5904\u7406\u4e8b\u4ef6\uff0c\u5982 read -> \u4e1a\u52a1\u903b\u8f91 -> send\uff1b Reactor \u6a21\u5f0f\u662f\u7075\u6d3b\u591a\u53d8\u7684\uff0c\u53ef\u4ee5\u5e94\u5bf9\u4e0d\u540c\u7684\u4e1a\u52a1\u573a\u666f\uff0c\u7075\u6d3b\u5728\u4e8e\uff1a 1\u3001Reactor \u7684\u6570\u91cf\u53ef\u4ee5\u53ea\u6709\u4e00\u4e2a\uff0c\u4e5f\u53ef\u4ee5\u6709\u591a\u4e2a\uff1b 2\u3001\u5904\u7406\u8d44\u6e90\u6c60\u53ef\u4ee5\u662f\u5355\u4e2a\u8fdb\u7a0b / \u7ebf\u7a0b\uff0c\u4e5f\u53ef\u4ee5\u662f\u591a\u4e2a\u8fdb\u7a0b /\u7ebf\u7a0b\uff1b \u5c06\u4e0a\u9762\u7684\u4e24\u4e2a\u56e0\u7d20\u6392\u5217\u7ec4\u8bbe\u4e00\u4e0b\uff0c\u7406\u8bba\u4e0a\u5c31\u53ef\u4ee5\u6709 4 \u79cd\u65b9\u6848\u9009\u62e9\uff1a 1\u3001\u5355 Reactor \u5355\u8fdb\u7a0b / \u7ebf\u7a0b\uff1b 2\u3001\u5355 Reactor \u591a\u8fdb\u7a0b / \u7ebf\u7a0b\uff1b NOTE: Redis\u662f\u91c7\u7528\u7684\u8fd9\u79cd\u65b9\u6848 3\u3001\u591a Reactor \u5355\u8fdb\u7a0b / \u7ebf\u7a0b\uff1b NOTE: \u65e0\u5b9e\u9645\u7528\u9014 4\u3001\u591a Reactor \u591a\u8fdb\u7a0b / \u7ebf\u7a0b\uff1b \u5176\u4e2d\uff0c\u300c\u591a Reactor \u5355\u8fdb\u7a0b / \u7ebf\u7a0b\u300d\u5b9e\u73b0\u65b9\u6848\u76f8\u6bd4\u300c\u5355 Reactor \u5355\u8fdb\u7a0b / \u7ebf\u7a0b\u300d\u65b9\u6848\uff0c\u4e0d\u4ec5\u590d\u6742\u800c\u4e14\u4e5f\u6ca1\u6709\u6027\u80fd\u4f18\u52bf\uff0c\u56e0\u6b64\u5b9e\u9645\u4e2d\u5e76\u6ca1\u6709\u5e94\u7528\u3002 \u5269\u4e0b\u7684 3 \u4e2a\u65b9\u6848\u90fd\u662f\u6bd4\u8f83\u7ecf\u5178\u7684\uff0c\u4e14\u90fd\u6709\u5e94\u7528\u5728\u5b9e\u9645\u7684\u9879\u76ee\u4e2d\uff1a 1\u3001\u5355 Reactor \u5355\u8fdb\u7a0b / \u7ebf\u7a0b\uff1b 2\u3001\u5355 Reactor \u591a\u7ebf\u7a0b / \u8fdb\u7a0b\uff1b 4\u3001\u591a Reactor \u591a\u8fdb\u7a0b / \u7ebf\u7a0b\uff1b \u65b9\u6848\u5177\u4f53\u4f7f\u7528\u8fdb\u7a0b\u8fd8\u662f\u7ebf\u7a0b\uff0c\u8981\u770b\u4f7f\u7528\u7684\u7f16\u7a0b\u8bed\u8a00\u4ee5\u53ca\u5e73\u53f0\u6709\u5173\uff1a 2\u3001Java \u8bed\u8a00\u4e00\u822c\u4f7f\u7528\u7ebf\u7a0b\uff0c\u6bd4\u5982 Netty; 3\u3001C \u8bed\u8a00\u4f7f\u7528\u8fdb\u7a0b\u548c\u7ebf\u7a0b\u90fd\u53ef\u4ee5\uff0c\u4f8b\u5982 Nginx \u4f7f\u7528\u7684\u662f\u8fdb\u7a0b\uff0cMemcache \u4f7f\u7528\u7684\u662f\u7ebf\u7a0b\u3002 \u63a5\u4e0b\u6765\uff0c\u5206\u522b\u4ecb\u7ecd\u8fd9\u4e09\u4e2a\u7ecf\u5178\u7684 Reactor \u65b9\u6848\u3002","title":"Reactor \u6a21\u5f0f"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/zhihu-%E5%A6%82%E4%BD%95%E6%B7%B1%E5%88%BB%E7%90%86%E8%A7%A3reactor%E5%92%8Cproactor/#reactor_1","text":"","title":"Reactor"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/zhihu-%E5%A6%82%E4%BD%95%E6%B7%B1%E5%88%BB%E7%90%86%E8%A7%A3reactor%E5%92%8Cproactor/#reactor_2","text":"\u4e00\u822c\u6765\u8bf4\uff0cC \u8bed\u8a00\u5b9e\u73b0\u7684\u662f\u300c \u5355 Reactor \u5355\u8fdb\u7a0b \u300d\u7684\u65b9\u6848\uff0c\u56e0\u4e3a C \u8bed\u7f16\u5199\u5b8c\u7684\u7a0b\u5e8f\uff0c\u8fd0\u884c\u540e\u5c31\u662f\u4e00\u4e2a\u72ec\u7acb\u7684\u8fdb\u7a0b\uff0c\u4e0d\u9700\u8981\u5728\u8fdb\u7a0b\u4e2d\u518d\u521b\u5efa\u7ebf\u7a0b\u3002 \u800c Java \u8bed\u8a00\u5b9e\u73b0\u7684\u662f\u300c \u5355 Reactor \u5355\u7ebf\u7a0b \u300d\u7684\u65b9\u6848\uff0c\u56e0\u4e3a Java \u7a0b\u5e8f\u662f\u8dd1\u5728 Java \u865a\u62df\u673a\u8fd9\u4e2a\u8fdb\u7a0b\u4e0a\u9762\u7684\uff0c\u865a\u62df\u673a\u4e2d\u6709\u5f88\u591a\u7ebf\u7a0b\uff0c\u6211\u4eec\u5199\u7684 Java \u7a0b\u5e8f\u53ea\u662f\u5176\u4e2d\u7684\u4e00\u4e2a\u7ebf\u7a0b\u800c\u5df2\u3002 \u6211\u4eec\u6765\u770b\u770b\u300c \u5355 Reactor \u5355\u8fdb\u7a0b \u300d\u7684\u65b9\u6848\u793a\u610f\u56fe\uff1a ![img](https://pic4.zhimg.com/50/v2-614eb69d0186c32de123115b10c3c682_hd.jpg?source=1940ef5c) \u53ef\u4ee5\u770b\u5230\u8fdb\u7a0b\u91cc\u6709 Reactor\u3001Acceptor\u3001Handler \u8fd9\u4e09\u4e2a\u5bf9\u8c61\uff1a Reactor \u5bf9\u8c61\u7684\u4f5c\u7528\u662f\u76d1\u542c\u548c\u5206\u53d1\u4e8b\u4ef6\uff1b Acceptor \u5bf9\u8c61\u7684\u4f5c\u7528\u662f\u83b7\u53d6\u8fde\u63a5\uff1b Handler \u5bf9\u8c61\u7684\u4f5c\u7528\u662f\u5904\u7406\u4e1a\u52a1\uff1b \u5bf9\u8c61\u91cc\u7684 select\u3001accept\u3001read\u3001send \u662f\u7cfb\u7edf\u8c03\u7528\u51fd\u6570\uff0cdispatch \u548c \u300c\u4e1a\u52a1\u5904\u7406\u300d\u662f\u9700\u8981\u5b8c\u6210\u7684\u64cd\u4f5c\uff0c\u5176\u4e2d dispatch \u662f\u5206\u53d1\u4e8b\u4ef6\u64cd\u4f5c\u3002 \u63a5\u4e0b\u6765\uff0c\u4ecb\u7ecd\u4e0b\u300c\u5355 Reactor \u5355\u8fdb\u7a0b\u300d\u8fd9\u4e2a\u65b9\u6848\uff1a 1\u3001Reactor \u5bf9\u8c61\u901a\u8fc7 select \uff08IO \u591a\u8def\u590d\u7528\u63a5\u53e3\uff09 \u76d1\u542c\u4e8b\u4ef6\uff0c\u6536\u5230\u4e8b\u4ef6\u540e\u901a\u8fc7 dispatch \u8fdb\u884c\u5206\u53d1\uff0c\u5177\u4f53\u5206\u53d1\u7ed9 Acceptor \u5bf9\u8c61\u8fd8\u662f Handler \u5bf9\u8c61\uff0c\u8fd8\u8981\u770b\u6536\u5230\u7684\u4e8b\u4ef6\u7c7b\u578b\uff1b 2\u3001\u5982\u679c\u662f\u8fde\u63a5\u5efa\u7acb\u7684\u4e8b\u4ef6\uff0c\u5219\u4ea4\u7531 Acceptor \u5bf9\u8c61\u8fdb\u884c\u5904\u7406\uff0cAcceptor \u5bf9\u8c61\u4f1a\u901a\u8fc7 accept \u65b9\u6cd5 \u83b7\u53d6\u8fde\u63a5\uff0c\u5e76\u521b\u5efa\u4e00\u4e2a Handler \u5bf9\u8c61\u6765\u5904\u7406\u540e\u7eed\u7684\u54cd\u5e94\u4e8b\u4ef6\uff1b 3\u3001\u5982\u679c\u4e0d\u662f\u8fde\u63a5\u5efa\u7acb\u4e8b\u4ef6\uff0c \u5219\u4ea4\u7531\u5f53\u524d\u8fde\u63a5\u5bf9\u5e94\u7684 Handler \u5bf9\u8c61\u6765\u8fdb\u884c\u54cd\u5e94\uff1b 4\u3001Handler \u5bf9\u8c61\u901a\u8fc7 read -> \u4e1a\u52a1\u5904\u7406 -> send \u7684\u6d41\u7a0b\u6765\u5b8c\u6210\u5b8c\u6574\u7684\u4e1a\u52a1\u6d41\u7a0b\u3002 \u5355 Reactor \u5355\u8fdb\u7a0b\u7684\u65b9\u6848\u56e0\u4e3a\u5168\u90e8\u5de5\u4f5c\u90fd\u5728\u540c\u4e00\u4e2a\u8fdb\u7a0b\u5185\u5b8c\u6210\uff0c\u6240\u4ee5\u5b9e\u73b0\u8d77\u6765\u6bd4\u8f83\u7b80\u5355\uff0c\u4e0d\u9700\u8981\u8003\u8651\u8fdb\u7a0b\u95f4\u901a\u4fe1\uff0c\u4e5f\u4e0d\u7528\u62c5\u5fc3\u591a\u8fdb\u7a0b\u7ade\u4e89\u3002 \u4f46\u662f\uff0c\u8fd9\u79cd\u65b9\u6848\u5b58\u5728 2 \u4e2a\u7f3a\u70b9\uff1a 1\u3001\u7b2c\u4e00\u4e2a\u7f3a\u70b9\uff0c\u56e0\u4e3a\u53ea\u6709\u4e00\u4e2a\u8fdb\u7a0b\uff0c \u65e0\u6cd5\u5145\u5206\u5229\u7528 \u591a\u6838 CPU \u7684\u6027\u80fd \uff1b 2\u3001\u7b2c\u4e8c\u4e2a\u7f3a\u70b9\uff0cHandler \u5bf9\u8c61\u5728\u4e1a\u52a1\u5904\u7406\u65f6\uff0c\u6574\u4e2a\u8fdb\u7a0b\u662f\u65e0\u6cd5\u5904\u7406\u5176\u4ed6\u8fde\u63a5\u7684\u4e8b\u4ef6\u7684\uff0c \u5982\u679c\u4e1a\u52a1\u5904\u7406\u8017\u65f6\u6bd4\u8f83\u957f\uff0c\u90a3\u4e48\u5c31\u9020\u6210\u54cd\u5e94\u7684\u5ef6\u8fdf \uff1b \u6240\u4ee5\uff0c\u5355 Reactor \u5355\u8fdb\u7a0b\u7684\u65b9\u6848**\u4e0d\u9002\u7528\u8ba1\u7b97\u673a\u5bc6\u96c6\u578b\u7684\u573a\u666f\uff0c\u53ea\u9002\u7528\u4e8e\u4e1a\u52a1\u5904\u7406\u975e\u5e38\u5feb\u901f\u7684\u573a\u666f**\u3002 Redis \u662f\u7531 C \u8bed\u8a00\u5b9e\u73b0\u7684\uff0c\u5b83\u91c7\u7528\u7684\u6b63\u662f\u300c\u5355 Reactor \u5355\u8fdb\u7a0b\u300d\u7684\u65b9\u6848\uff0c\u56e0\u4e3a Redis \u4e1a\u52a1\u5904\u7406\u4e3b\u8981\u662f\u5728\u5185\u5b58\u4e2d\u5b8c\u6210\uff0c\u64cd\u4f5c\u7684\u901f\u5ea6\u662f\u5f88\u5feb\u7684\uff0c\u6027\u80fd\u74f6\u9888\u4e0d\u5728 CPU \u4e0a\uff0c\u6240\u4ee5 Redis \u5bf9\u4e8e\u547d\u4ee4\u7684\u5904\u7406\u662f\u5355\u8fdb\u7a0b\u7684\u65b9\u6848\u3002 NOTE: Redis\u7684\u65b0\u7248\u672c\u5df2\u7ecf\u66ff\u6362\u4e3amultiple thread\u4e86","title":"\u5355 Reactor \u5355\u8fdb\u7a0b / \u7ebf\u7a0b"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/zhihu-%E5%A6%82%E4%BD%95%E6%B7%B1%E5%88%BB%E7%90%86%E8%A7%A3reactor%E5%92%8Cproactor/#reactor_3","text":"\u5982\u679c\u8981\u514b\u670d\u300c\u5355 Reactor \u5355\u7ebf\u7a0b / \u8fdb\u7a0b\u300d\u65b9\u6848\u7684\u7f3a\u70b9\uff0c\u90a3\u4e48\u5c31\u9700\u8981\u5f15\u5165\u591a\u7ebf\u7a0b / \u591a\u8fdb\u7a0b\uff0c\u8fd9\u6837\u5c31\u4ea7\u751f\u4e86**\u5355 Reactor \u591a\u7ebf\u7a0b / \u591a\u8fdb\u7a0b**\u7684\u65b9\u6848\u3002 \u95fb\u5176\u540d\u4e0d\u5982\u770b\u5176\u56fe\uff0c\u5148\u6765\u770b\u770b\u300c\u5355 Reactor \u591a\u7ebf\u7a0b\u300d\u65b9\u6848\u7684\u793a\u610f\u56fe\u5982\u4e0b\uff1a \u8be6\u7ec6\u8bf4\u4e00\u4e0b\u8fd9\u4e2a\u65b9\u6848\uff1a Reactor \u5bf9\u8c61\u901a\u8fc7 select \uff08IO \u591a\u8def\u590d\u7528\u63a5\u53e3\uff09 \u76d1\u542c\u4e8b\u4ef6\uff0c\u6536\u5230\u4e8b\u4ef6\u540e\u901a\u8fc7 dispatch \u8fdb\u884c\u5206\u53d1\uff0c\u5177\u4f53\u5206\u53d1\u7ed9 Acceptor \u5bf9\u8c61\u8fd8\u662f Handler \u5bf9\u8c61\uff0c\u8fd8\u8981\u770b\u6536\u5230\u7684\u4e8b\u4ef6\u7c7b\u578b\uff1b \u5982\u679c\u662f\u8fde\u63a5\u5efa\u7acb\u7684\u4e8b\u4ef6\uff0c\u5219\u4ea4\u7531 Acceptor \u5bf9\u8c61\u8fdb\u884c\u5904\u7406\uff0cAcceptor \u5bf9\u8c61\u4f1a\u901a\u8fc7 accept \u65b9\u6cd5 \u83b7\u53d6\u8fde\u63a5\uff0c\u5e76\u521b\u5efa\u4e00\u4e2a Handler \u5bf9\u8c61\u6765\u5904\u7406\u540e\u7eed\u7684\u54cd\u5e94\u4e8b\u4ef6\uff1b \u5982\u679c\u4e0d\u662f\u8fde\u63a5\u5efa\u7acb\u4e8b\u4ef6\uff0c \u5219\u4ea4\u7531\u5f53\u524d\u8fde\u63a5\u5bf9\u5e94\u7684 Handler \u5bf9\u8c61\u6765\u8fdb\u884c\u54cd\u5e94\uff1b \u4e0a\u9762\u7684\u4e09\u4e2a\u6b65\u9aa4\u548c\u5355 Reactor \u5355\u7ebf\u7a0b\u65b9\u6848\u662f\u4e00\u6837\u7684\uff0c\u63a5\u4e0b\u6765\u7684\u6b65\u9aa4\u5c31\u5f00\u59cb\u4e0d\u4e00\u6837\u4e86\uff1a Handler \u5bf9\u8c61\u4e0d\u518d\u8d1f\u8d23\u4e1a\u52a1\u5904\u7406\uff0c\u53ea\u8d1f\u8d23\u6570\u636e\u7684\u63a5\u6536\u548c\u53d1\u9001\uff0cHandler \u5bf9\u8c61\u901a\u8fc7 read \u8bfb\u53d6\u5230\u6570\u636e\u540e\uff0c\u4f1a\u5c06\u6570\u636e\u53d1\u7ed9\u5b50\u7ebf\u7a0b\u91cc\u7684 Processor \u5bf9\u8c61\u8fdb\u884c\u4e1a\u52a1\u5904\u7406\uff1b \u5b50\u7ebf\u7a0b\u91cc\u7684 Processor \u5bf9\u8c61\u5c31\u8fdb\u884c\u4e1a\u52a1\u5904\u7406\uff0c\u5904\u7406\u5b8c\u540e\uff0c\u5c06\u7ed3\u679c\u53d1\u7ed9\u4e3b\u7ebf\u7a0b\u4e2d\u7684 Handler \u5bf9\u8c61\uff0c\u63a5\u7740\u7531 Handler \u901a\u8fc7 send \u65b9\u6cd5\u5c06\u54cd\u5e94\u7ed3\u679c\u53d1\u9001\u7ed9 client\uff1b \u5355 Reator \u591a\u7ebf\u7a0b\u7684\u65b9\u6848\u4f18\u52bf\u5728\u4e8e**\u80fd\u591f\u5145\u5206\u5229\u7528\u591a\u6838 CPU \u7684\u80fd**\uff0c\u90a3\u65e2\u7136\u5f15\u5165\u591a\u7ebf\u7a0b\uff0c\u90a3\u4e48\u81ea\u7136\u5c31\u5e26\u6765\u4e86\u591a\u7ebf\u7a0b\u7ade\u4e89\u8d44\u6e90\u7684\u95ee\u9898\u3002 \u4f8b\u5982\uff0c\u5b50\u7ebf\u7a0b\u5b8c\u6210\u4e1a\u52a1\u5904\u7406\u540e\uff0c\u8981\u628a\u7ed3\u679c\u4f20\u9012\u7ed9\u4e3b\u7ebf\u7a0b\u7684 Reactor \u8fdb\u884c\u53d1\u9001\uff0c\u8fd9\u91cc\u6d89\u53ca\u5171\u4eab\u6570\u636e\u7684\u7ade\u4e89\u3002 \u8981\u907f\u514d\u591a\u7ebf\u7a0b\u7531\u4e8e\u7ade\u4e89\u5171\u4eab\u8d44\u6e90\u800c\u5bfc\u81f4\u6570\u636e\u9519\u4e71\u7684\u95ee\u9898\uff0c\u5c31\u9700\u8981\u5728\u64cd\u4f5c\u5171\u4eab\u8d44\u6e90\u524d\u52a0\u4e0a\u4e92\u65a5\u9501\uff0c\u4ee5\u4fdd\u8bc1\u4efb\u610f\u65f6\u95f4\u91cc\u53ea\u6709\u4e00\u4e2a\u7ebf\u7a0b\u5728\u64cd\u4f5c\u5171\u4eab\u8d44\u6e90\uff0c\u5f85\u8be5\u7ebf\u7a0b\u64cd\u4f5c\u5b8c\u91ca\u653e\u4e92\u65a5\u9501\u540e\uff0c\u5176\u4ed6\u7ebf\u7a0b\u624d\u6709\u673a\u4f1a\u64cd\u4f5c\u5171\u4eab\u6570\u636e\u3002 \u804a\u5b8c\u5355 Reactor \u591a\u7ebf\u7a0b\u7684\u65b9\u6848\uff0c\u63a5\u7740\u6765\u770b\u770b\u5355 Reactor \u591a\u8fdb\u7a0b\u7684\u65b9\u6848\u3002 \u4e8b\u5b9e\u4e0a\uff0c\u5355 Reactor \u591a\u8fdb\u7a0b\u76f8\u6bd4\u5355 Reactor \u591a\u7ebf\u7a0b\u5b9e\u73b0\u8d77\u6765\u5f88\u9ebb\u70e6\uff0c\u4e3b\u8981\u56e0\u4e3a\u8981\u8003\u8651\u5b50\u8fdb\u7a0b <-> \u7236\u8fdb\u7a0b\u7684\u53cc\u5411\u901a\u4fe1\uff0c\u5e76\u4e14\u7236\u8fdb\u7a0b\u8fd8\u5f97\u77e5\u9053\u5b50\u8fdb\u7a0b\u8981\u5c06\u6570\u636e\u53d1\u9001\u7ed9\u54ea\u4e2a\u5ba2\u6237\u7aef\u3002 \u800c\u591a\u7ebf\u7a0b\u95f4\u53ef\u4ee5\u5171\u4eab\u6570\u636e\uff0c\u867d\u7136\u8981\u989d\u5916\u8003\u8651\u5e76\u53d1\u95ee\u9898\uff0c\u4f46\u662f\u8fd9\u8fdc\u6bd4\u8fdb\u7a0b\u95f4\u901a\u4fe1\u7684\u590d\u6742\u5ea6\u4f4e\u5f97\u591a\uff0c\u56e0\u6b64\u5b9e\u9645\u5e94\u7528\u4e2d\u4e5f\u770b\u4e0d\u5230\u5355 Reactor \u591a\u8fdb\u7a0b\u7684\u6a21\u5f0f\u3002 \u53e6\u5916\uff0c\u300c\u5355 Reactor\u300d\u7684\u6a21\u5f0f\u8fd8\u6709\u4e2a\u95ee\u9898\uff0c \u56e0\u4e3a\u4e00\u4e2a Reactor \u5bf9\u8c61\u627f\u62c5\u6240\u6709\u4e8b\u4ef6\u7684\u76d1\u542c\u548c\u54cd\u5e94\uff0c\u800c\u4e14\u53ea\u5728\u4e3b\u7ebf\u7a0b\u4e2d\u8fd0\u884c\uff0c\u5728\u9762\u5bf9\u77ac\u95f4\u9ad8\u5e76\u53d1\u7684\u573a\u666f\u65f6\uff0c\u5bb9\u6613\u6210\u4e3a\u6027\u80fd\u7684\u74f6\u9888\u7684\u5730\u65b9 \u3002","title":"\u5355 Reactor \u591a\u7ebf\u7a0b / \u591a\u8fdb\u7a0b"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/zhihu-%E5%A6%82%E4%BD%95%E6%B7%B1%E5%88%BB%E7%90%86%E8%A7%A3reactor%E5%92%8Cproactor/#reactor_4","text":"\u8981\u89e3\u51b3\u300c\u5355 Reactor\u300d\u7684\u95ee\u9898\uff0c\u5c31\u662f\u5c06\u300c\u5355 Reactor\u300d\u5b9e\u73b0\u6210\u300c\u591a Reactor\u300d\uff0c\u8fd9\u6837\u5c31\u4ea7\u751f\u4e86\u7b2c **\u591a Reactor \u591a\u8fdb\u7a0b / \u7ebf\u7a0b**\u7684\u65b9\u6848\u3002 \u8001\u89c4\u77e9\uff0c\u95fb\u5176\u540d\u4e0d\u5982\u770b\u5176\u56fe\u3002\u591a Reactor \u591a\u8fdb\u7a0b / \u7ebf\u7a0b\u65b9\u6848\u7684\u793a\u610f\u56fe\u5982\u4e0b\uff08\u4ee5\u7ebf\u7a0b\u4e3a\u4f8b\uff09\uff1a ![img](https://pic1.zhimg.com/50/v2-4da008d8b7f55a0c18bef0e87c5c5bb1_hd.jpg?source=1940ef5c) \u65b9\u6848\u8be6\u7ec6\u8bf4\u660e\u5982\u4e0b\uff1a \u4e3b\u7ebf\u7a0b\u4e2d\u7684 MainReactor \u5bf9\u8c61\u901a\u8fc7 select \u76d1\u63a7\u8fde\u63a5\u5efa\u7acb\u4e8b\u4ef6\uff0c\u6536\u5230\u4e8b\u4ef6\u540e\u901a\u8fc7 Acceptor \u5bf9\u8c61\u4e2d\u7684 accept \u83b7\u53d6\u8fde\u63a5\uff0c\u5c06\u65b0\u7684\u8fde\u63a5\u5206\u914d\u7ed9\u67d0\u4e2a\u5b50\u7ebf\u7a0b\uff1b \u5b50\u7ebf\u7a0b\u4e2d\u7684 SubReactor \u5bf9\u8c61\u5c06 MainReactor \u5bf9\u8c61\u5206\u914d\u7684\u8fde\u63a5\u52a0\u5165 select \u7ee7\u7eed\u8fdb\u884c\u76d1\u542c\uff0c\u5e76\u521b\u5efa\u4e00\u4e2a Handler \u7528\u4e8e\u5904\u7406\u8fde\u63a5\u7684\u54cd\u5e94\u4e8b\u4ef6\u3002 \u5982\u679c\u6709\u65b0\u7684\u4e8b\u4ef6\u53d1\u751f\u65f6\uff0cSubReactor \u5bf9\u8c61\u4f1a\u8c03\u7528\u5f53\u524d\u8fde\u63a5\u5bf9\u5e94\u7684 Handler \u5bf9\u8c61\u6765\u8fdb\u884c\u54cd\u5e94\u3002 Handler \u5bf9\u8c61\u901a\u8fc7 read -> \u4e1a\u52a1\u5904\u7406 -> send \u7684\u6d41\u7a0b\u6765\u5b8c\u6210\u5b8c\u6574\u7684\u4e1a\u52a1\u6d41\u7a0b\u3002 \u591a Reactor \u591a\u7ebf\u7a0b\u7684\u65b9\u6848\u867d\u7136\u770b\u8d77\u6765\u590d\u6742\u7684\uff0c\u4f46\u662f\u5b9e\u9645\u5b9e\u73b0\u65f6\u6bd4\u5355 Reactor \u591a\u7ebf\u7a0b\u7684\u65b9\u6848\u8981\u7b80\u5355\u7684\u591a\uff0c\u539f\u56e0\u5982\u4e0b\uff1a \u4e3b\u7ebf\u7a0b\u548c\u5b50\u7ebf\u7a0b\u5206\u5de5\u660e\u786e\uff0c\u4e3b\u7ebf\u7a0b\u53ea\u8d1f\u8d23\u63a5\u6536\u65b0\u8fde\u63a5\uff0c\u5b50\u7ebf\u7a0b\u8d1f\u8d23\u5b8c\u6210\u540e\u7eed\u7684\u4e1a\u52a1\u5904\u7406\u3002 \u4e3b\u7ebf\u7a0b\u548c\u5b50\u7ebf\u7a0b\u7684\u4ea4\u4e92\u5f88\u7b80\u5355\uff0c\u4e3b\u7ebf\u7a0b\u53ea\u9700\u8981\u628a\u65b0\u8fde\u63a5\u4f20\u7ed9\u5b50\u7ebf\u7a0b\uff0c\u5b50\u7ebf\u7a0b\u65e0\u987b\u8fd4\u56de\u6570\u636e\uff0c\u76f4\u63a5\u5c31\u53ef\u4ee5\u5728\u5b50\u7ebf\u7a0b\u5c06\u5904\u7406\u7ed3\u679c\u53d1\u9001\u7ed9\u5ba2\u6237\u7aef\u3002 \u5927\u540d\u9f0e\u9f0e\u7684\u4e24\u4e2a\u5f00\u6e90\u8f6f\u4ef6 Netty \u548c Memcache \u90fd\u91c7\u7528\u4e86\u300c\u591a Reactor \u591a\u7ebf\u7a0b\u300d\u7684\u65b9\u6848\u3002 NOTE: \u8fd9\u79cd\u65b9\u5f0f\uff0c\u4e0d\u9700\u8981\u7ebf\u7a0b\u5b9a\u4f4d \u91c7\u7528\u4e86\u300c\u591a Reactor \u591a\u8fdb\u7a0b\u300d\u65b9\u6848\u7684\u5f00\u6e90\u8f6f\u4ef6\u662f Nginx\uff0c\u4e0d\u8fc7\u65b9\u6848\u4e0e\u6807\u51c6\u7684\u591a Reactor \u591a\u8fdb\u7a0b\u6709\u4e9b\u5dee\u5f02\u3002 \u5177\u4f53\u5dee\u5f02\u8868\u73b0\u5728\u4e3b\u8fdb\u7a0b\u4e2d\u4ec5\u4ec5\u7528\u6765\u521d\u59cb\u5316 socket\uff0c\u5e76\u6ca1\u6709\u521b\u5efa mainReactor \u6765 accept \u8fde\u63a5\uff0c\u800c\u662f\u7531\u5b50\u8fdb\u7a0b\u7684 Reactor \u6765 accept \u8fde\u63a5\uff0c\u901a\u8fc7\u9501\u6765\u63a7\u5236\u4e00\u6b21\u53ea\u6709\u4e00\u4e2a\u5b50\u8fdb\u7a0b\u8fdb\u884c accept\uff08\u9632\u6b62\u51fa\u73b0\u60ca\u7fa4\u73b0\u8c61\uff09\uff0c\u5b50\u8fdb\u7a0b accept \u65b0\u8fde\u63a5\u540e\u5c31\u653e\u5230\u81ea\u5df1\u7684 Reactor \u8fdb\u884c\u5904\u7406\uff0c\u4e0d\u4f1a\u518d\u5206\u914d\u7ed9\u5176\u4ed6\u5b50\u8fdb\u7a0b\u3002","title":"\u591a Reactor \u591a\u8fdb\u7a0b / \u7ebf\u7a0b"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/zhihu-%E5%A6%82%E4%BD%95%E6%B7%B1%E5%88%BB%E7%90%86%E8%A7%A3reactor%E5%92%8Cproactor/#proactor","text":"\u524d\u9762\u63d0\u5230\u7684 Reactor \u662f\u975e\u963b\u585e\u540c\u6b65\u7f51\u7edc\u6a21\u5f0f \uff0c\u800c Proactor \u662f\u5f02\u6b65\u7f51\u7edc\u6a21\u5f0f \u3002 \u8fd9\u91cc\u5148\u7ed9\u5927\u5bb6\u590d\u4e60\u4e0b\u963b\u585e\u3001\u975e\u963b\u585e\u3001\u540c\u6b65\u3001\u5f02\u6b65 I/O \u7684\u6982\u5ff5\u3002 NOTE: \u4e0b\u9762\u7684\u5185\u5bb9\u662f\u57fa\u4e8e \"UNP 6.1 I/O Multiplexing: The select and poll Functions \u00b6 \"\u7684","title":"Proactor"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/zhihu-%E5%A6%82%E4%BD%95%E6%B7%B1%E5%88%BB%E7%90%86%E8%A7%A3reactor%E5%92%8Cproactor/#io","text":"\u5148\u6765\u770b\u770b**\u963b\u585e I/O**\uff0c\u5f53\u7528\u6237\u7a0b\u5e8f\u6267\u884c read \uff0c\u7ebf\u7a0b\u4f1a\u88ab\u963b\u585e\uff0c\u4e00\u76f4\u7b49\u5230\u5185\u6838\u6570\u636e\u51c6\u5907\u597d\uff0c\u5e76\u628a\u6570\u636e\u4ece\u5185\u6838\u7f13\u51b2\u533a\u62f7\u8d1d\u5230\u5e94\u7528\u7a0b\u5e8f\u7684\u7f13\u51b2\u533a\u4e2d\uff0c\u5f53\u62f7\u8d1d\u8fc7\u7a0b\u5b8c\u6210\uff0c read \u624d\u4f1a\u8fd4\u56de\u3002 \u6ce8\u610f\uff0c \u963b\u585e\u7b49\u5f85\u7684\u662f\u300c\u5185\u6838\u6570\u636e\u51c6\u5907\u597d\u300d\u548c\u300c\u6570\u636e\u4ece\u5185\u6838\u6001\u62f7\u8d1d\u5230\u7528\u6237\u6001\u300d\u8fd9\u4e24\u4e2a\u8fc7\u7a0b \u3002\u8fc7\u7a0b\u5982\u4e0b\u56fe\uff1a","title":"\u963b\u585e I/O"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/zhihu-%E5%A6%82%E4%BD%95%E6%B7%B1%E5%88%BB%E7%90%86%E8%A7%A3reactor%E5%92%8Cproactor/#io_1","text":"\u77e5\u9053\u4e86\u963b\u585e I/O \uff0c\u6765\u770b\u770b**\u975e\u963b\u585e I/O**\uff0c\u975e\u963b\u585e\u7684 read \u8bf7\u6c42\u5728\u6570\u636e\u672a\u51c6\u5907\u597d\u7684\u60c5\u51b5\u4e0b\u7acb\u5373\u8fd4\u56de\uff0c\u53ef\u4ee5\u7ee7\u7eed\u5f80\u4e0b\u6267\u884c\uff0c\u6b64\u65f6\u5e94\u7528\u7a0b\u5e8f\u4e0d\u65ad\u8f6e\u8be2\u5185\u6838\uff0c\u76f4\u5230\u6570\u636e\u51c6\u5907\u597d\uff0c\u5185\u6838\u5c06\u6570\u636e\u62f7\u8d1d\u5230\u5e94\u7528\u7a0b\u5e8f\u7f13\u51b2\u533a\uff0c read \u8c03\u7528\u624d\u53ef\u4ee5\u83b7\u53d6\u5230\u7ed3\u679c\u3002\u8fc7\u7a0b\u5982\u4e0b\u56fe\uff1a ![img](https://pic4.zhimg.com/50/v2-51e052e2beecef41da3aed3ebc2b80bd_hd.jpg?source=1940ef5c) \u6ce8\u610f\uff0c \u8fd9\u91cc\u6700\u540e\u4e00\u6b21 read \u8c03\u7528\uff0c\u83b7\u53d6\u6570\u636e\u7684\u8fc7\u7a0b\uff0c\u662f\u4e00\u4e2a\u540c\u6b65\u7684\u8fc7\u7a0b\uff0c\u662f\u9700\u8981\u7b49\u5f85\u7684\u8fc7\u7a0b\u3002\u8fd9\u91cc\u7684\u540c\u6b65\u6307\u7684\u662f\u5185\u6838\u6001\u7684\u6570\u636e\u62f7\u8d1d\u5230\u7528\u6237\u7a0b\u5e8f\u7684\u7f13\u5b58\u533a\u8fd9\u4e2a\u8fc7\u7a0b\u3002 \u4e3e\u4e2a\u4f8b\u5b50\uff0c\u5982\u679c socket \u8bbe\u7f6e\u4e86 O_NONBLOCK \u6807\u5fd7\uff0c\u90a3\u4e48\u5c31\u8868\u793a\u4f7f\u7528\u7684\u662f\u975e\u963b\u585e I/O \u7684\u65b9\u5f0f\u8bbf\u95ee\uff0c\u800c\u4e0d\u505a\u4efb\u4f55\u8bbe\u7f6e\u7684\u8bdd\uff0c\u9ed8\u8ba4\u662f\u963b\u585e I/O\u3002 \u56e0\u6b64\uff0c\u65e0\u8bba read \u548c send \u662f\u963b\u585e I/O\uff0c\u8fd8\u662f\u975e\u963b\u585e I/O \u90fd\u662f\u540c\u6b65\u8c03\u7528\u3002\u56e0\u4e3a\u5728 read \u8c03\u7528\u65f6\uff0c\u5185\u6838\u5c06\u6570\u636e\u4ece\u5185\u6838\u7a7a\u95f4\u62f7\u8d1d\u5230\u7528\u6237\u7a7a\u95f4\u7684\u8fc7\u7a0b\u90fd\u662f\u9700\u8981\u7b49\u5f85\u7684\uff0c\u4e5f\u5c31\u662f\u8bf4\u8fd9\u4e2a\u8fc7\u7a0b\u662f\u540c\u6b65\u7684\uff0c\u5982\u679c\u5185\u6838\u5b9e\u73b0\u7684\u62f7\u8d1d\u6548\u7387\u4e0d\u9ad8\uff0cread \u8c03\u7528\u5c31\u4f1a\u5728\u8fd9\u4e2a\u540c\u6b65\u8fc7\u7a0b\u4e2d\u7b49\u5f85\u6bd4\u8f83\u957f\u7684\u65f6\u95f4\u3002","title":"\u975e\u963b\u585e I/O"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/zhihu-%E5%A6%82%E4%BD%95%E6%B7%B1%E5%88%BB%E7%90%86%E8%A7%A3reactor%E5%92%8Cproactor/#io_2","text":"\u800c\u771f\u6b63\u7684**\u5f02\u6b65 I/O** \u662f\u300c\u5185\u6838\u6570\u636e\u51c6\u5907\u597d\u300d\u548c\u300c\u6570\u636e\u4ece\u5185\u6838\u6001\u62f7\u8d1d\u5230\u7528\u6237\u6001\u300d\u8fd9**\u4e24\u4e2a\u8fc7\u7a0b\u90fd\u4e0d\u7528\u7b49\u5f85**\u3002 \u5f53\u6211\u4eec\u53d1\u8d77 aio_read \uff08\u5f02\u6b65 I/O\uff09 \u4e4b\u540e\uff0c\u5c31\u7acb\u5373\u8fd4\u56de\uff0c\u5185\u6838\u81ea\u52a8\u5c06\u6570\u636e\u4ece\u5185\u6838\u7a7a\u95f4\u62f7\u8d1d\u5230\u7528\u6237\u7a7a\u95f4\uff0c\u8fd9\u4e2a\u62f7\u8d1d\u8fc7\u7a0b\u540c\u6837\u662f\u5f02\u6b65\u7684\uff0c\u5185\u6838\u81ea\u52a8\u5b8c\u6210\u7684\uff0c\u548c\u524d\u9762\u7684\u540c\u6b65\u64cd\u4f5c\u4e0d\u4e00\u6837\uff0c \u5e94\u7528\u7a0b\u5e8f\u5e76\u4e0d\u9700\u8981\u4e3b\u52a8\u53d1\u8d77\u62f7\u8d1d\u52a8\u4f5c \u3002\u8fc7\u7a0b\u5982\u4e0b\u56fe\uff1a","title":"\u5f02\u6b65 I/O"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/zhihu-%E5%A6%82%E4%BD%95%E6%B7%B1%E5%88%BB%E7%90%86%E8%A7%A3reactor%E5%92%8Cproactor/#_2","text":"\u4e3e\u4e2a\u4f60\u53bb\u996d\u5802\u5403\u996d\u7684\u4f8b\u5b50\uff0c\u4f60\u597d\u6bd4\u5e94\u7528\u7a0b\u5e8f\uff0c\u996d\u5802\u597d\u6bd4\u64cd\u4f5c\u7cfb\u7edf\u3002 \u963b\u585e I/O \u597d\u6bd4\uff0c\u4f60\u53bb\u996d\u5802\u5403\u996d\uff0c\u4f46\u662f\u996d\u5802\u7684\u83dc\u8fd8\u6ca1\u505a\u597d\uff0c\u7136\u540e\u4f60\u5c31\u4e00\u76f4\u5728\u90a3\u91cc\u7b49\u554a\u7b49\uff0c\u7b49\u4e86\u597d\u957f\u4e00\u6bb5\u65f6\u95f4\u7ec8\u4e8e\u7b49\u5230\u996d\u5802\u963f\u59e8\u628a\u83dc\u7aef\u4e86\u51fa\u6765\uff08\u6570\u636e\u51c6\u5907\u7684\u8fc7\u7a0b\uff09\uff0c\u4f46\u662f\u4f60\u8fd8\u5f97\u7ee7\u7eed\u7b49\u963f\u59e8\u628a\u83dc\uff08\u5185\u6838\u7a7a\u95f4\uff09\u6253\u5230\u4f60\u7684\u996d\u76d2\u91cc\uff08\u7528\u6237\u7a7a\u95f4\uff09\uff0c\u7ecf\u5386\u5b8c\u8fd9\u4e24\u4e2a\u8fc7\u7a0b\uff0c\u4f60\u624d\u53ef\u4ee5\u79bb\u5f00\u3002 \u975e\u963b\u585e I/O \u597d\u6bd4\uff0c\u4f60\u53bb\u4e86\u996d\u5802\uff0c\u95ee\u963f\u59e8\u83dc\u505a\u597d\u4e86\u6ca1\u6709\uff0c\u963f\u59e8\u544a\u8bc9\u4f60\u6ca1\uff0c\u4f60\u5c31\u79bb\u5f00\u4e86\uff0c\u8fc7\u51e0\u5341\u5206\u949f\uff0c\u4f60\u53c8\u6765\u996d\u5802\u95ee\u963f\u59e8\uff0c\u963f\u59e8\u8bf4\u505a\u597d\u4e86\uff0c\u4e8e\u662f\u963f\u59e8\u5e2e\u4f60\u628a\u83dc\u6253\u5230\u4f60\u7684\u996d\u76d2\u91cc\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u4f60\u662f\u5f97\u7b49\u5f85\u7684\u3002 \u5f02\u6b65 I/O \u597d\u6bd4\uff0c\u4f60\u8ba9\u996d\u5802\u963f\u59e8\u5c06\u83dc\u505a\u597d\u5e76\u628a\u83dc\u6253\u5230\u996d\u76d2\u91cc\u540e\uff0c\u628a\u996d\u76d2\u9001\u5230\u4f60\u9762\u524d\uff0c\u6574\u4e2a\u8fc7\u7a0b\u4f60\u90fd\u4e0d\u9700\u8981\u4efb\u4f55\u7b49\u5f85\u3002 \u5f88\u660e\u663e\uff0c\u5f02\u6b65 I/O \u6bd4\u540c\u6b65 I/O \u6027\u80fd\u66f4\u597d\uff0c\u56e0\u4e3a\u5f02\u6b65 I/O \u5728\u300c\u5185\u6838\u6570\u636e\u51c6\u5907\u597d\u300d\u548c\u300c\u6570\u636e\u4ece\u5185\u6838\u7a7a\u95f4\u62f7\u8d1d\u5230\u7528\u6237\u7a7a\u95f4\u300d\u8fd9\u4e24\u4e2a\u8fc7\u7a0b\u90fd\u4e0d\u7528\u7b49\u5f85\u3002 Proactor \u6b63\u662f\u91c7\u7528\u4e86\u5f02\u6b65 I/O \u6280\u672f\uff0c\u6240\u4ee5\u88ab\u79f0\u4e3a\u5f02\u6b65\u7f51\u7edc\u6a21\u578b\u3002 \u73b0\u5728\u6211\u4eec\u518d\u6765\u7406\u89e3 Reactor \u548c Proactor \u7684\u533a\u522b\uff0c\u5c31\u6bd4\u8f83\u6e05\u6670\u4e86\u3002 1\u3001 Reactor \u662f\u975e\u963b\u585e\u540c\u6b65\u7f51\u7edc\u6a21\u5f0f\uff0c\u611f\u77e5\u7684\u662f\u5c31\u7eea\u53ef\u8bfb\u5199\u4e8b\u4ef6 \u3002\u5728\u6bcf\u6b21\u611f\u77e5\u5230\u6709\u4e8b\u4ef6\u53d1\u751f\uff08\u6bd4\u5982\u53ef\u8bfb\u5c31\u7eea\u4e8b\u4ef6\uff09\u540e\uff0c\u5c31\u9700\u8981\u5e94\u7528\u8fdb\u7a0b\u4e3b\u52a8\u8c03\u7528 read \u65b9\u6cd5\u6765\u5b8c\u6210\u6570\u636e\u7684\u8bfb\u53d6\uff0c\u4e5f\u5c31\u662f\u8981\u5e94\u7528\u8fdb\u7a0b\u4e3b\u52a8\u5c06 socket \u63a5\u6536\u7f13\u5b58\u4e2d\u7684\u6570\u636e\u8bfb\u5230\u5e94\u7528\u8fdb\u7a0b\u5185\u5b58\u4e2d\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u662f\u540c\u6b65\u7684\uff0c\u8bfb\u53d6\u5b8c\u6570\u636e\u540e\u5e94\u7528\u8fdb\u7a0b\u624d\u80fd\u5904\u7406\u6570\u636e\u3002 2\u3001 Proactor \u662f\u5f02\u6b65\u7f51\u7edc\u6a21\u5f0f\uff0c \u611f\u77e5\u7684\u662f\u5df2\u5b8c\u6210\u7684\u8bfb\u5199\u4e8b\u4ef6 \u3002\u5728\u53d1\u8d77\u5f02\u6b65\u8bfb\u5199\u8bf7\u6c42\u65f6\uff0c\u9700\u8981\u4f20\u5165\u6570\u636e\u7f13\u51b2\u533a\u7684\u5730\u5740\uff08\u7528\u6765\u5b58\u653e\u7ed3\u679c\u6570\u636e\uff09\u7b49\u4fe1\u606f\uff0c\u8fd9\u6837\u7cfb\u7edf\u5185\u6838\u624d\u53ef\u4ee5\u81ea\u52a8\u5e2e\u6211\u4eec\u628a\u6570\u636e\u7684\u8bfb\u5199\u5de5\u4f5c\u5b8c\u6210\uff0c\u8fd9\u91cc\u7684\u8bfb\u5199\u5de5\u4f5c\u5168\u7a0b\u7531\u64cd\u4f5c\u7cfb\u7edf\u6765\u505a\uff0c\u5e76\u4e0d\u9700\u8981\u50cf Reactor \u90a3\u6837\u8fd8\u9700\u8981\u5e94\u7528\u8fdb\u7a0b\u4e3b\u52a8\u53d1\u8d77 read/write \u6765\u8bfb\u5199\u6570\u636e\uff0c\u64cd\u4f5c\u7cfb\u7edf\u5b8c\u6210\u8bfb\u5199\u5de5\u4f5c\u540e\uff0c\u5c31\u4f1a\u901a\u77e5\u5e94\u7528\u8fdb\u7a0b\u76f4\u63a5\u5904\u7406\u6570\u636e\u3002 \u56e0\u6b64\uff0c Reactor \u53ef\u4ee5\u7406\u89e3\u4e3a\u300c\u6765\u4e86\u4e8b\u4ef6\u64cd\u4f5c\u7cfb\u7edf\u901a\u77e5\u5e94\u7528\u8fdb\u7a0b\uff0c\u8ba9\u5e94\u7528\u8fdb\u7a0b\u6765\u5904\u7406\u300d \uff0c\u800c Proactor \u53ef\u4ee5\u7406\u89e3\u4e3a\u300c\u6765\u4e86\u4e8b\u4ef6\u64cd\u4f5c\u7cfb\u7edf\u6765\u5904\u7406\uff0c\u5904\u7406\u5b8c\u518d\u901a\u77e5\u5e94\u7528\u8fdb\u7a0b\u300d \u3002\u8fd9\u91cc\u7684\u300c\u4e8b\u4ef6\u300d\u5c31\u662f\u6709\u65b0\u8fde\u63a5\u3001\u6709\u6570\u636e\u53ef\u8bfb\u3001\u6709\u6570\u636e\u53ef\u5199\u7684\u8fd9\u4e9b I/O \u4e8b\u4ef6\u8fd9\u91cc\u7684\u300c\u5904\u7406\u300d\u5305\u542b\u4ece\u9a71\u52a8\u8bfb\u53d6\u5230\u5185\u6838\u4ee5\u53ca\u4ece\u5185\u6838\u8bfb\u53d6\u5230\u7528\u6237\u7a7a\u95f4\u3002 \u4e3e\u4e2a\u5b9e\u9645\u751f\u6d3b\u4e2d\u7684\u4f8b\u5b50\uff0cReactor \u6a21\u5f0f\u5c31\u662f\u5feb\u9012\u5458\u5728\u697c\u4e0b\uff0c\u7ed9\u4f60\u6253\u7535\u8bdd\u544a\u8bc9\u4f60\u5feb\u9012\u5230\u4f60\u5bb6\u5c0f\u533a\u4e86\uff0c\u4f60\u9700\u8981\u81ea\u5df1\u4e0b\u697c\u6765\u62ff\u5feb\u9012\u3002\u800c\u5728 Proactor \u6a21\u5f0f\u4e0b\uff0c\u5feb\u9012\u5458\u76f4\u63a5\u5c06\u5feb\u9012\u9001\u5230\u4f60\u5bb6\u95e8\u53e3\uff0c\u7136\u540e\u901a\u77e5\u4f60\u3002 \u65e0\u8bba\u662f Reactor\uff0c\u8fd8\u662f Proactor\uff0c\u90fd\u662f\u4e00\u79cd\u57fa\u4e8e\u300c\u4e8b\u4ef6\u5206\u53d1\u300d\u7684\u7f51\u7edc\u7f16\u7a0b\u6a21\u5f0f\uff0c\u533a\u522b\u5728\u4e8e Reactor \u6a21\u5f0f\u662f\u57fa\u4e8e\u300c\u5f85\u5b8c\u6210\u300d\u7684 I/O \u4e8b\u4ef6\uff0c\u800c Proactor \u6a21\u5f0f\u5219\u662f\u57fa\u4e8e\u300c\u5df2\u5b8c\u6210\u300d\u7684 I/O \u4e8b\u4ef6 \u3002 \u63a5\u4e0b\u6765\uff0c\u4e00\u8d77\u770b\u770b Proactor \u6a21\u5f0f\u7684\u793a\u610f\u56fe\uff1a \u4ecb\u7ecd\u4e00\u4e0b Proactor \u6a21\u5f0f\u7684\u5de5\u4f5c\u6d41\u7a0b\uff1a Proactor Initiator \u8d1f\u8d23\u521b\u5efa Proactor \u548c Handler \u5bf9\u8c61\uff0c\u5e76\u5c06 Proactor \u548c Handler \u90fd\u901a\u8fc7 Asynchronous Operation Processor \u6ce8\u518c\u5230\u5185\u6838\uff1b Asynchronous Operation Processor \u8d1f\u8d23\u5904\u7406\u6ce8\u518c\u8bf7\u6c42\uff0c\u5e76\u5904\u7406 I/O \u64cd\u4f5c\uff1b Asynchronous Operation Processor \u5b8c\u6210 I/O \u64cd\u4f5c\u540e\u901a\u77e5 Proactor\uff1b Proactor \u6839\u636e\u4e0d\u540c\u7684\u4e8b\u4ef6\u7c7b\u578b\u56de\u8c03\u4e0d\u540c\u7684 Handler \u8fdb\u884c\u4e1a\u52a1\u5904\u7406\uff1b Handler \u5b8c\u6210\u4e1a\u52a1\u5904\u7406\uff1b \u53ef\u60dc\u7684\u662f\uff0c\u5728 Linux \u4e0b\u7684\u5f02\u6b65 I/O \u662f\u4e0d\u5b8c\u5584\u7684\uff0c aio \u7cfb\u5217\u51fd\u6570\u662f\u7531 POSIX \u5b9a\u4e49\u7684\u5f02\u6b65\u64cd\u4f5c\u63a5\u53e3\uff0c\u4e0d\u662f\u771f\u6b63\u7684\u64cd\u4f5c\u7cfb\u7edf\u7ea7\u522b\u652f\u6301\u7684\uff0c\u800c\u662f\u5728\u7528\u6237\u7a7a\u95f4\u6a21\u62df\u51fa\u6765\u7684\u5f02\u6b65\uff0c\u5e76\u4e14\u4ec5\u4ec5\u652f\u6301\u57fa\u4e8e\u672c\u5730\u6587\u4ef6\u7684 aio \u5f02\u6b65\u64cd\u4f5c\uff0c\u7f51\u7edc\u7f16\u7a0b\u4e2d\u7684 socket \u662f\u4e0d\u652f\u6301\u7684\uff0c\u8fd9\u4e5f\u4f7f\u5f97\u57fa\u4e8e Linux \u7684\u9ad8\u6027\u80fd\u7f51\u7edc\u7a0b\u5e8f\u90fd\u662f\u4f7f\u7528 Reactor \u65b9\u6848\u3002 \u800c Windows \u91cc\u5b9e\u73b0\u4e86\u4e00\u5957\u5b8c\u6574\u7684\u652f\u6301 socket \u7684\u5f02\u6b65\u7f16\u7a0b\u63a5\u53e3\uff0c\u8fd9\u5957\u63a5\u53e3\u5c31\u662f IOCP \uff0c\u662f\u7531\u64cd\u4f5c\u7cfb\u7edf\u7ea7\u522b\u5b9e\u73b0\u7684\u5f02\u6b65 I/O\uff0c\u771f\u6b63\u610f\u4e49\u4e0a\u5f02\u6b65 I/O\uff0c\u56e0\u6b64\u5728 Windows \u91cc\u5b9e\u73b0\u9ad8\u6027\u80fd\u7f51\u7edc\u7a0b\u5e8f\u53ef\u4ee5\u4f7f\u7528\u6548\u7387\u66f4\u9ad8\u7684 Proactor \u65b9\u6848\u3002","title":"\u5f62\u8c61\u7684\u4f8b\u5b50"},{"location":"Event-driven-concurrent-server/Design-pattern/Proactor-reactor/zhihu-%E5%A6%82%E4%BD%95%E6%B7%B1%E5%88%BB%E7%90%86%E8%A7%A3reactor%E5%92%8Cproactor/#_3","text":"\u5e38\u89c1\u7684 Reactor \u5b9e\u73b0\u65b9\u6848\u6709\u4e09\u79cd\u3002 \u7b2c\u4e00\u79cd\u65b9\u6848\u5355 Reactor \u5355\u8fdb\u7a0b / \u7ebf\u7a0b\uff0c\u4e0d\u7528\u8003\u8651\u8fdb\u7a0b\u95f4\u901a\u4fe1\u4ee5\u53ca\u6570\u636e\u540c\u6b65\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u5b9e\u73b0\u8d77\u6765\u6bd4\u8f83\u7b80\u5355\uff0c\u8fd9\u79cd\u65b9\u6848\u7684\u7f3a\u9677\u5728\u4e8e\u65e0\u6cd5\u5145\u5206\u5229\u7528\u591a\u6838 CPU\uff0c\u800c\u4e14\u5904\u7406\u4e1a\u52a1\u903b\u8f91\u7684\u65f6\u95f4\u4e0d\u80fd\u592a\u957f\uff0c\u5426\u5219\u4f1a\u5ef6\u8fdf\u54cd\u5e94\uff0c\u6240\u4ee5\u4e0d\u9002\u7528\u4e8e\u8ba1\u7b97\u673a\u5bc6\u96c6\u578b\u7684\u573a\u666f\uff0c\u9002\u7528\u4e8e\u4e1a\u52a1\u5904\u7406\u5feb\u901f\u7684\u573a\u666f\uff0c\u6bd4\u5982 Redis \u91c7\u7528\u7684\u662f\u5355 Reactor \u5355\u8fdb\u7a0b\u7684\u65b9\u6848\u3002 \u7b2c\u4e8c\u79cd\u65b9\u6848\u5355 Reactor \u591a\u7ebf\u7a0b\uff0c\u901a\u8fc7\u591a\u7ebf\u7a0b\u7684\u65b9\u5f0f\u89e3\u51b3\u4e86\u65b9\u6848\u4e00\u7684\u7f3a\u9677\uff0c\u4f46\u5b83\u79bb\u9ad8\u5e76\u53d1\u8fd8\u5dee\u4e00\u70b9\u8ddd\u79bb\uff0c\u5dee\u5728\u53ea\u6709\u4e00\u4e2a Reactor \u5bf9\u8c61\u6765\u627f\u62c5\u6240\u6709\u4e8b\u4ef6\u7684\u76d1\u542c\u548c\u54cd\u5e94\uff0c\u800c\u4e14\u53ea\u5728\u4e3b\u7ebf\u7a0b\u4e2d\u8fd0\u884c\uff0c\u5728\u9762\u5bf9\u77ac\u95f4\u9ad8\u5e76\u53d1\u7684\u573a\u666f\u65f6\uff0c\u5bb9\u6613\u6210\u4e3a\u6027\u80fd\u7684\u74f6\u9888\u7684\u5730\u65b9\u3002 \u7b2c\u4e09\u79cd\u65b9\u6848\u591a Reactor \u591a\u8fdb\u7a0b / \u7ebf\u7a0b\uff0c\u901a\u8fc7\u591a\u4e2a Reactor \u6765\u89e3\u51b3\u4e86\u65b9\u6848\u4e8c\u7684\u7f3a\u9677\uff0c\u4e3b Reactor \u53ea\u8d1f\u8d23\u76d1\u542c\u4e8b\u4ef6\uff0c\u54cd\u5e94\u4e8b\u4ef6\u7684\u5de5\u4f5c\u4ea4\u7ed9\u4e86\u4ece Reactor\uff0cNetty \u548c Memcache \u90fd\u91c7\u7528\u4e86\u300c\u591a Reactor \u591a\u7ebf\u7a0b\u300d\u7684\u65b9\u6848\uff0cNginx \u5219\u91c7\u7528\u4e86\u7c7b\u4f3c\u4e8e \u300c\u591a Reactor \u591a\u8fdb\u7a0b\u300d\u7684\u65b9\u6848\u3002 Reactor \u53ef\u4ee5\u7406\u89e3\u4e3a\u300c\u6765\u4e86\u4e8b\u4ef6\u64cd\u4f5c\u7cfb\u7edf\u901a\u77e5\u5e94\u7528\u8fdb\u7a0b\uff0c\u8ba9\u5e94\u7528\u8fdb\u7a0b\u6765\u5904\u7406\u300d\uff0c\u800c Proactor \u53ef\u4ee5\u7406\u89e3\u4e3a\u300c\u6765\u4e86\u4e8b\u4ef6\u64cd\u4f5c\u7cfb\u7edf\u6765\u5904\u7406\uff0c\u5904\u7406\u5b8c\u518d\u901a\u77e5\u5e94\u7528\u8fdb\u7a0b\u300d\u3002 \u56e0\u6b64\uff0c\u771f\u6b63\u7684\u5927\u6740\u5668\u8fd8\u662f Proactor\uff0c\u5b83\u662f\u91c7\u7528\u5f02\u6b65 I/O \u5b9e\u73b0\u7684\u5f02\u6b65\u7f51\u7edc\u6a21\u578b\uff0c\u611f\u77e5\u7684\u662f\u5df2\u5b8c\u6210\u7684\u8bfb\u5199\u4e8b\u4ef6\uff0c\u800c\u4e0d\u9700\u8981\u50cf Reactor \u611f\u77e5\u5230\u4e8b\u4ef6\u540e\uff0c\u8fd8\u9700\u8981\u8c03\u7528 read \u6765\u4ece\u5185\u6838\u4e2d\u83b7\u53d6\u6570\u636e\u3002 \u4e0d\u8fc7\uff0c\u65e0\u8bba\u662f Reactor\uff0c\u8fd8\u662f Proactor\uff0c\u90fd\u662f\u4e00\u79cd\u57fa\u4e8e\u300c\u4e8b\u4ef6\u5206\u53d1\u300d\u7684\u7f51\u7edc\u7f16\u7a0b\u6a21\u5f0f\uff0c\u533a\u522b\u5728\u4e8e Reactor \u6a21\u5f0f\u662f\u57fa\u4e8e\u300c\u5f85\u5b8c\u6210\u300d\u7684 I/O \u4e8b\u4ef6\uff0c\u800c Proactor \u6a21\u5f0f\u5219\u662f\u57fa\u4e8e\u300c\u5df2\u5b8c\u6210\u300d\u7684 I/O \u4e8b\u4ef6\u3002","title":"\u603b\u7ed3"},{"location":"Event-driven-concurrent-server/Event-driven-model/","text":"Event-driven model \u672c\u7ae0\u63cf\u8ff0\"Event-driven model\"\uff0c\u5b83\u662f\u4e00\u4e2a\u975e\u5e38\u5f3a\u5927\u7684\u6a21\u578b\uff0c\u80fd\u591f\u63cf\u8ff0\u975e\u5e38\u975e\u5e38\u5e7f\u6cdb\u7684\u95ee\u9898\u3002\u5728\u672c\u6587\uff0c\u6211\u4eec\u5c06\u8bf4\u660e\u4ec0\u4e48\u662fevent-driven model\u3001\u4ec0\u4e48\u662fevent\u3001\u5982\u4f55\u5b9e\u73b0event-driven model\u3002","title":"Introduction"},{"location":"Event-driven-concurrent-server/Event-driven-model/#event-driven#model","text":"\u672c\u7ae0\u63cf\u8ff0\"Event-driven model\"\uff0c\u5b83\u662f\u4e00\u4e2a\u975e\u5e38\u5f3a\u5927\u7684\u6a21\u578b\uff0c\u80fd\u591f\u63cf\u8ff0\u975e\u5e38\u975e\u5e38\u5e7f\u6cdb\u7684\u95ee\u9898\u3002\u5728\u672c\u6587\uff0c\u6211\u4eec\u5c06\u8bf4\u660e\u4ec0\u4e48\u662fevent-driven model\u3001\u4ec0\u4e48\u662fevent\u3001\u5982\u4f55\u5b9e\u73b0event-driven model\u3002","title":"Event-driven model"},{"location":"Event-driven-concurrent-server/Event-driven-model/Event-VS-exception-VS-signal/","text":"Event VS signal VS exception \u6b63\u5982\u5728 Event-driven-model \u4e2d\u6240\u603b\u7ed3\u7684\uff1a Event\u662f\u4e00\u4e2a\u975e\u5e38\u6982\u62ec\u3001\u5bbd\u6cdb\u7684\u6982\u5ff5 Exception\u53ef\u4ee5\u53ef\u770b\u505a\u662f\u4e00\u79cdevent\uff0c\u5373\u5b83\u662f\u7531external environment\u6216\u8005\u7a0b\u5e8f\u5185\u90e8\u6240\u4ea7\u751f\u7684\uff0c\u73b0\u4ee3programming language\u5bf9\u8fd9\u79cdevent\u8fdb\u884c\u4e86\u62bd\u8c61\uff0c\u4f7f\u7528\u66f4\u52a0\u5177\u4f53\u7684\u3001\u66f4\u52a0\u660e\u786e\u7684exception\u7684\u6982\u5ff5\u6765\u63cf\u8ff0\u5b83\u3002 Signal\u4e5f\u53ef\u4ee5\u770b\u505a\u662f\u4e00\u79cdevent\u3002","title":"Event-VS-exception-VS-signal"},{"location":"Event-driven-concurrent-server/Event-driven-model/Event-VS-exception-VS-signal/#event#vs#signal#vs#exception","text":"\u6b63\u5982\u5728 Event-driven-model \u4e2d\u6240\u603b\u7ed3\u7684\uff1a Event\u662f\u4e00\u4e2a\u975e\u5e38\u6982\u62ec\u3001\u5bbd\u6cdb\u7684\u6982\u5ff5 Exception\u53ef\u4ee5\u53ef\u770b\u505a\u662f\u4e00\u79cdevent\uff0c\u5373\u5b83\u662f\u7531external environment\u6216\u8005\u7a0b\u5e8f\u5185\u90e8\u6240\u4ea7\u751f\u7684\uff0c\u73b0\u4ee3programming language\u5bf9\u8fd9\u79cdevent\u8fdb\u884c\u4e86\u62bd\u8c61\uff0c\u4f7f\u7528\u66f4\u52a0\u5177\u4f53\u7684\u3001\u66f4\u52a0\u660e\u786e\u7684exception\u7684\u6982\u5ff5\u6765\u63cf\u8ff0\u5b83\u3002 Signal\u4e5f\u53ef\u4ee5\u770b\u505a\u662f\u4e00\u79cdevent\u3002","title":"Event VS signal VS exception"},{"location":"Event-driven-concurrent-server/Event-driven-model/Event-loop/","text":"Event loop 1\u3001\u7ed3\u5408Redis ae\u6765\u7406\u89e3\u662f\u975e\u5e38\u5bb9\u6613\u7684\u3002 2\u3001libuv \u3001 libevent \u3001 celery \u3001Redis ae wikipedia Event loop In computer science , the event loop , message dispatcher , message loop , message pump , or run loop is a programming construct that waits for and dispatches events or messages in a program . It works by making a request to some internal or external \"event provider\" (that generally blocks the request until an event has arrived), and then it calls the relevant event handler (\"dispatches the event\"). The event-loop may be used in conjunction with a reactor , if the event provider follows the file interface , which can be selected or 'polled' (the Unix system call, not actual polling ). The event loop almost always operates asynchronously with the message originator. NOTE:\u5173\u4e8e\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\"that generally blocks the request until an event has arrived\"\u7684\u89e3\u91ca\uff1a\u5b83\u662f\u6307\u901a\u5e38\u60c5\u51b5\u4e0b\uff0c\u4e3a\u4e86\u6548\u7387\u8d77\u89c1\uff0cevent provider\u5728\u6ca1\u6709event\u7684\u65f6\u5019\u4f1ablock \u6389event loop\u67e5\u8be2event\u7684\u8bf7\u6c42\uff0credis\u7684 BRPOP \u548c BLPOP \u5c31\u662f\u6309\u7167\u8fd9\u79cd\u601d\u8def\u6765\u5b9e\u73b0\u7684\uff0c\u5728 \u8fd9\u7bc7\u6587\u7ae0 \u4e2d\u5bf9\u8fd9\u4e9b\u8fdb\u884c\u4e86\u8be6\u7ec6\u4ecb\u7ecd\u3002 \u5176\u5b9e\uff0c\u4ece\u8fd9\u91cc\u4e5f\u53ef\u4ee5\u770b\u51fablock\u548casynchronous\u4e4b\u95f4\u7684\u5dee\u5f02\u6240\u5728\u3002 When the event loop forms the central control flow construct of a program, as it often does, it may be termed(\u79f0\u4e4b\u4e3a) the main loop or main event loop . This title is appropriate, because such an event loop is at the highest level of control within the program. NOTE: \u5927\u591a\u6570\u5b9e\u73b0\u90fd\u662f\u8fd9\u79cd\u6a21\u5f0f Message passing Message pumps are said to 'pump' messages from the program's message queue (assigned and usually owned by the underlying operating system) into the program for processing. In the strictest sense, an event loop is one of the methods for implementing inter-process communication . In fact, message processing exists in many systems, including a kernel-level component of the Mach operating system . The event loop is a specific implementation technique of systems that use message passing . Alternative designs This approach is in contrast to a number of other alternatives: 1\u3001Traditionally, a program simply ran once, then terminated. This type of program was very common in the early days of computing, and lacked any form of user interactivity. This is still used frequently, particularly in the form of command-line-driven programs. Any parameters are set up in advance and passed in one go when the program starts. 2\u3001Menu-driven designs. These still may feature a main loop, but are not usually thought of as event driven in the usual sense[ citation needed ]. Instead, the user is presented with an ever-narrowing set of options until the task they wish to carry out is the only option available. Limited interactivity through the menus is available. Usage Due to the predominance(\u4f18\u52bf) of graphical user interfaces , most modern applications feature a main loop . The get_next_message() routine is typically provided by the operating system , and blocks until a message is available. Thus, the loop is only entered when there is something to process. function main initialize() while message != quit message := get_next_message() process_message(message) end while end function File interface Under Unix , the \" everything is a file \" paradigm naturally leads to a file-based event loop . Reading from and writing to files, inter-process communication, network communication, and device control are all achieved using file I/O , with the target identified by a file descriptor . The select and poll system calls allow a set of file descriptors to be monitored for a change of state, e.g. when data becomes available to be read. For example, consider a program that reads from a continuously updated file and displays its contents in the X Window System , which communicates with clients over a socket (either Unix domain or Berkeley ): main(): file_fd = open (\"logfile\") x_fd = open_display () construct_interface () while changed_fds = select ({file_fd, x_fd}): if file_fd in changed_fds: data = read_from (file_fd) append_to_display (data) send_repaint_message () if x_fd in changed_fds: process_x_messages () Handling signals One of the few things in Unix that does not conform to the file interface are asynchronous events ( signals ). Signals are received in signal handlers , small, limited pieces of code that run while the rest of the task is suspended; if a signal is received and handled while the task is blocking in select() , select will return early with EINTR ; if a signal is received while the task is CPU bound , the task will be suspended between instructions until the signal handler returns. Thus an obvious way to handle signals is for signal handlers to set a global flag and have the event loop check for the flag immediately before and after the select() call; if it is set, handle the signal in the same manner as with events on file descriptors . Unfortunately, this gives rise to a race condition : if a signal arrives immediately between checking the flag and calling select() , it will not be handled until select() returns for some other reason (for example, being interrupted by a frustrated user). NOTE: \u4e0a\u8ff0\u8fd9\u79cd\u601d\u8def\u53ef\u4ee5\u53c2\u89c1\uff1a Interrupting blocked read \u4e2d\u7ed9\u51fa\u7684demo\uff1b NOTE : \u4e0d\u5e78\u7684\u662f\uff0c\u8fd9\u4f1a\u5f15\u8d77\u7ade\u4e89\u6761\u4ef6\uff1a\u5982\u679c\u4fe1\u53f7\u5728\u68c0\u67e5\u6807\u5fd7\u548c\u8c03\u7528select\uff08\uff09\u4e4b\u95f4\u7acb\u5373\u5230\u8fbe\uff0c\u5219\u76f4\u5230select\uff08\uff09\u56e0\u67d0\u4e9b\u5176\u4ed6\u539f\u56e0\uff08\u4f8b\u5982\u88ab\u6cae\u4e27\u7684\u7528\u6237\u4e2d\u65ad\uff09\u8fd4\u56de\u65f6\u624d\u4f1a\u5904\u7406\u5b83\uff1b\u8fd9\u6bb5\u8bdd\u4e2d\u6240\u63cf\u8ff0\u7684race condition\u5e76\u6ca1\u6709\u641e\u6e05\u695a\uff1b The solution arrived at by POSIX is the pselect() call, which is similar to select() but takes an additional sigmask parameter, which describes a signal mask . This allows an application to mask signals in the main task , then remove the mask for the duration of the select() call such that signal handlers are only called while the application is I/O bound . However, implementations of pselect() have only recently[ when? ] become reliable; versions of Linux prior to 2.6.16 do not have a pselect() system call, forcing glibc to emulate it via a method prone to the very same race condition pselect() is intended to avoid. An alternative, more portable solution, is to convert asynchronous events to file-based events using the self-pipe trick ,[ 1] where \"a signal handler writes a byte to a pipe whose other end is monitored by select() in the main program\".[ 2] In Linux kernel version 2.6.22, a new system call signalfd() was added, which allows receiving signals via a special file descriptor . NOTE :\u53c2\u89c1\u300athe linux program interface \u300b\u7684 63.5.2 The Self-Pipe Trick softwareengineering Is an event loop just a for/while loop with optimized polling? NOTE:\u770b\u4e86\u8fd9\u4e48\u591a\uff0c\u53d1\u73b0\u63d0\u95ee\u8005\u7684\u6700\u7ec8\u610f\u56fe\u662f\u60f3\u8981\u77e5\u9053event loop\u662f\u5982\u4f55\u5b9e\u73b0\u7684\uff0c\u63d0\u95ee\u8005\u8ba4\u4e3aevent loop\u7684\u5b9e\u73b0\u65b9\u5f0f\u662f\u7c7b\u4f3cpolling\u7684\uff1b\u6839\u636e\u4e0b\u9762\u7684\u56de\u7b54\uff0c\u663e\u7136\u7b54\u6848\u662fNo\u3002\u7b54\u6848 A4 \u662f\u975e\u5e38\u76f4\u63a5\u7684\u7ed9\u51fa\u4e86\u7b54\u6848\uff1b\u5b83\u5f15\u7528\u4e86Wikipedia\u7684\u4e0a\u7684 Polling \u6587\u7ae0\uff0c\u5728\u8fd9\u7bc7\u6587\u7ae0\u4e2d\uff0c\u6709\u4e0b\u9762\u7684\u4e00\u6bb5\u8bdd\uff1a Although not as wasteful of CPU cycles as busy waiting, this is generally not as efficient as the alternative to polling, interrupt -driven I/O. \u8fd9\u6bb5\u8bdd\u4e2d\u7684 interrupt -driven I/O\u6b63\u662fevent loop\u6240\u91c7\u7528\u7684\u3002 I'm trying to understand what an event loop is. Often the explanation is that in an event loop, you do something until you're notified that an event has occurred. You then handle the event and continue doing what you were doing before. NOTE\uff1anotify To map the above definition with an example. I have a server which 'listens' in a event loop, and when a socket connection is detected, the data from it gets read and displayed, after which the server resumes/starts listening as it did before. However, this event happening and us getting notified 'just like that' are to much for me to handle. You can say: \"It's not 'just like that' you have to register an event listener\". But what's an event listener but a function which for some reason isn't returning. Is it in it's own loop, waiting to be notified when an event happens? Should the event listener also register an event listener? Where does it end? Events are a nice abstraction to work with, however just an abstraction. I believe that in the end, polling is unavoidable . Perhaps we are not doing it in our code, but the lower levels (the programming language implementation or the OS) are doing it for us. NOTE: \u4ece\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u53ef\u4ee5\u770b\u51fa\uff0c\u63d0\u95ee\u8005\u7684\u6700\u7ec8\u610f\u56fe\u662f\u60f3\u8981\u77e5\u9053event loop\u5230\u5e95\u662f\u5982\u4f55\u5b9e\u73b0\u7684\u3002 It basically comes down to the following pseudo code which is running somewhere low enough so it doesn't result in busy waiting: while(True): do stuff check if event has happened (poll) do other stuff This is my understanding of the whole idea, and I would like to hear if this is correct. I'm open in accepting that the whole idea is fundamentally wrong, in which case I would like the correct explanation. COMMENTS Event systems are implementations of the Observer pattern . Understanding the pattern should cement your understanding of events. No polling is required. \u2013 Steven Evers Oct 18 '13 at 20:21 Ultimately yes, even if language constructs abstract it away. \u2013 GrandmasterB Oct 18 '13 at 20:30 @SteveEvers In your wiki link. What is the EventSource doing if not polling the keyboard input? \u2013 TheMeaningfulEngineer Oct 18 '13 at 20:36 @Alan : It could be doing anything, but for keyboard input specifically there do exist APIs to register your application as listening to keyboard events, which you can then use as your eventsource with no polling involved. Of course, if you go down far enough, USB is always polling, but let's assume we're using a PS/2 keyboard, which is interrupt driven, and then you have a keyboard input stack based on events with zero polling.\u2013 Phoshi Oct 25 '13 at 8:25 I've had this questions since years, but I can't tell why I never bothered to ask it. Thanks, I am now satisfied with the understanding @Karl Bielefeldt has enlightened me with. \u2013 0xc0de Dec 2 '18 at 5:03 A1 Most event loops will suspend if there are no events ready(\u6ca1\u6709event\u5c31\u7eea\uff0c\u5219event loop\u5c31\u4f1a\u88ab\u6302\u8d77), which means the operating system will not give the task any execution time until an event happens. NOTE:\u8fd9\u6bb5\u8bdd\u5c31\u76f4\u63a5\u70b9\u660e\u4e86event loop\u548cpolling\u4e4b\u95f4\u7684\u5dee\u5f02\u6240\u5728\u3002 Say the event is a key being pressed. You might ask if there's a loop somewhere in the operating system checking for keypresses. The answer is no. Keys being pressed generate an interrupt , which is handled asynchronously by the hardware. Likewise for timers, mouse movements, a packet arriving, etc. NOTE: interrupt \u662f\u7ed5\u5f00loop\u7684\u4e00\u79cd\u975e\u5e38\u597d\u7684\u65b9\u5f0f\uff0c\u8fd9\u5c31\u662f\u5b9e\u73b0event loop\u7684\u4e00\u79cd\u975e\u5e38\u597d\u7684\u65b9\u5f0f\u3002 In fact, for most operating systems, polling for events is the abstraction\uff08\u4e5f\u5c31\u662f\u8bf4\u5b9e\u73b0\u53ef\u80fd\u5e76\u975e\u5982\u6b64\uff09. The hardware and OS handle events asynchronously and put them in a queue that can be polled by applications. You only really see true polling at the hardware level in embedded systems, and even there not always. \u8ba8\u8bba\uff1a Isn't an interrupt a change of voltage on a wire? Can that trigger an event by itself or must we poll the pin for the voltage value? \u2013 TheMeaningfulEngineer That can trigger an event by itself. The processor is designed that way. In fact a processor can be woken up from sleep by an interrupt. \u2013 Karl Bielefeldt I am confused with the statement Most event loops will block . How does this fit into the \"the event loop paradigm, opposed to using threads, uses nonblocking asynchronous calls\"? \u2013 TheMeaningfulEngineer If you look at event loops for a library like GTK+, they check for new events then call event handlers in a loop, but if there aren't any events, they block on a semaphore or a timer or something. Individual developers make their own event loops that don't block on an empty event queue, but the widely-used libraries all block. Event loops are too inefficient otherwise. \u2013 Karl Bielefeldt NOTE:\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u6307\u51fa\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u5982empty queue\uff0cevent loop\u5c06\u88abblock\uff0c\u663e\u7136\u8fd9\u5c31\u89e3\u91ca\u4e86 Most event loops will block \uff1b\u81f3\u4e8e How does this fit into the \"the event loop paradigm, opposed to using threads, uses nonblocking asynchronous calls\"\uff0c\u6211\u60f3\u8fd9\u4e2a\u95ee\u9898\u4e2d\u6240\u63cf\u8ff0\u7684\u662f\u6307\uff0c\u5f53event loop\u88ab\u901a\u77e5\u67d0\u4e2aevent\u7684\u65f6\u5019\uff0c\u5b83\u6267\u884cevent handler\u662f\u91c7\u7528\u7684nonblocking asynchronous calls\u3002 A2 Event systems are implementations of the Observer pattern . Understanding the pattern should cement your understanding of events. No polling is required. \u2013 Steven Evers A3 I think of an event listener not as a function running its own loop, but as a relay race\uff08\u63a5\u529b\u6bd4\u8d5b\uff09 with the first runner waiting for the starting gun. A significant reason for using events instead of polling is that they are more efficient with CPU cycles. Why? Look at it from the hardware up (rather than the source code down). Consider a Web server. When your server calls listen() and blocks, your code is taking its place as a relay runner. When the first packet of a new connection arrives, the network card starts the race by interrupting \uff08\u4e2d\u65ad\uff09 the operating system. The OS runs an interrupt service routine (ISR) that grabs the packet. The ISR passes the baton\uff08\u63a5\u529b\u68d2\uff09 to a higher-level routine that establishes the connection. Once the connection is alive, that routine passes the baton to listen() , which passes the baton up to your code. At that point, you can do what you want with the connection. For all we know, between races each relay runner could be going to the pub. A strength of the event abstraction is that your code doesn't have to know or care. NOTE:\u4e0a\u9762\u5bf9\u89e6\u53d1\u5f0f\u7684\u6bd4\u55bb\u6bd4\u8f83\u5f62\u8c61\uff1b\u611f\u89c9\u8fd9\u975e\u5e38\u50cf\u7ecf\u5178\u7684Unix\u7f51\u7edc\u7f16\u7a0b\u6a21\u578b\u3002 Some operating systems include event-handling code that runs its portion of the race, hands off the baton, then loops back to its starting point to wait for the next race to start. In that sense, event handling is optimized polling in lots of concurrent loops. However, there is always an outside trigger that kicks off the process. The event listener is not a function that isn't returning, but a function that is waiting for that external trigger before it runs. Rather than: while(True): do stuff check if event has happened (poll) do other stuff I think of this as: on(some event): //I got the baton do stuff signal the next level up //Pass the baton and between the signal and the next time the handler runs, there is conceptually no code running or looping. NOTE:\u5bf9\u6bd4\u4e0a\u9762\u4e24\u6bb5\u4f2a\u4ee3\u7801\uff0con\u53ef\u4ee5\u7ffb\u8bd1\u4e3a\u201c\u5f53\u201d\uff0c\u8fd9\u662f\u975e\u5e38\u5951\u5408interrupt\u7684\uff1b A4 No. It is not \"optimized polling.\" An event-loop uses interrupt-driven I/O instead of polling. While, Until, For, etc. loops are polling loops. \"Polling\" is the process of repeatedly checking something. Since the loop code executes continuously, and because it is a small, \"tight\" loop, there is little time for the processor to switch tasks and do anything else. Almost all \"hangs,\" \"freezes,\" \"lockups\" or whatever you want to call it when the computer becomes unresponsive, are the manifestation\uff08\u5c55\u793a\uff09 of code being stuck in an unintended polling loop. Instrumentation will show 100% CPU usage. Interrupt-driven event loops are far more efficient than polling loops. Polling is an extremely wasteful use of CPU cycles so every effort is made to eliminate or minimize it. However, to optimize code quality, most languages try to use the polling loop paradigm as closely as possible for event handing commands since they serve functionally similar purposes within a program\uff08\u56e0\u4e3a\u5b83\u4eec\u5728\u7a0b\u5e8f\u4e2d\u63d0\u4f9b\u529f\u80fd\u4e0a\u7c7b\u4f3c\u7684\u7528\u9014\uff09. Thus, with polling being the more familiar way to wait for a keypress or something, it is easy for the inexperienced\uff08\u4e0d\u719f\u7ec3\u7684\uff09 to use it and wind up with a program that may run fine by itself\uff08\u6700\u7ec8\u4f7f\u7528\u53ef\u80fd\u81ea\u884c\u8fd0\u884c\u7684\u7a0b\u5e8f\uff09, but nothing else works while it's running. It has \"taken over\" the machine. As explained in other answers, in interrupt-driven event handing , essentially a \"flag\" is set within the CPU and the process is \"suspended\" (not allowed to run) until that flag is changed by some other process (such as the keyboard driver changing it when the user has pressed a key). If the flag is an actual hardware condition such as a line being \"pulled high,\" it's called an \"interrupt\" or \"hardware interrupt.\" Most however, are implemented as just a memory address on the CPU or in main memory (RAM) and are called \"semaphores.\" Semaphores can be changed under software control and so can provide a very fast, simple signalling mechanism between software processes. Interrupts, however, can only be changed by hardware. The most ubiquitous\uff08\u666e\u904d\u5b58\u5728\u7684\uff09 use of interrupts is the one triggered at regular intervals by the internal clock chip. One of the countless kinds of software actions activated by clock interrupts, is the changing of semaphores. I've left out a lot but had to stop somewhere. Please ask if you need more details.","title":"Event-loop"},{"location":"Event-driven-concurrent-server/Event-driven-model/Event-loop/#event#loop","text":"1\u3001\u7ed3\u5408Redis ae\u6765\u7406\u89e3\u662f\u975e\u5e38\u5bb9\u6613\u7684\u3002 2\u3001libuv \u3001 libevent \u3001 celery \u3001Redis ae","title":"Event loop"},{"location":"Event-driven-concurrent-server/Event-driven-model/Event-loop/#wikipedia#event#loop","text":"In computer science , the event loop , message dispatcher , message loop , message pump , or run loop is a programming construct that waits for and dispatches events or messages in a program . It works by making a request to some internal or external \"event provider\" (that generally blocks the request until an event has arrived), and then it calls the relevant event handler (\"dispatches the event\"). The event-loop may be used in conjunction with a reactor , if the event provider follows the file interface , which can be selected or 'polled' (the Unix system call, not actual polling ). The event loop almost always operates asynchronously with the message originator. NOTE:\u5173\u4e8e\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\"that generally blocks the request until an event has arrived\"\u7684\u89e3\u91ca\uff1a\u5b83\u662f\u6307\u901a\u5e38\u60c5\u51b5\u4e0b\uff0c\u4e3a\u4e86\u6548\u7387\u8d77\u89c1\uff0cevent provider\u5728\u6ca1\u6709event\u7684\u65f6\u5019\u4f1ablock \u6389event loop\u67e5\u8be2event\u7684\u8bf7\u6c42\uff0credis\u7684 BRPOP \u548c BLPOP \u5c31\u662f\u6309\u7167\u8fd9\u79cd\u601d\u8def\u6765\u5b9e\u73b0\u7684\uff0c\u5728 \u8fd9\u7bc7\u6587\u7ae0 \u4e2d\u5bf9\u8fd9\u4e9b\u8fdb\u884c\u4e86\u8be6\u7ec6\u4ecb\u7ecd\u3002 \u5176\u5b9e\uff0c\u4ece\u8fd9\u91cc\u4e5f\u53ef\u4ee5\u770b\u51fablock\u548casynchronous\u4e4b\u95f4\u7684\u5dee\u5f02\u6240\u5728\u3002 When the event loop forms the central control flow construct of a program, as it often does, it may be termed(\u79f0\u4e4b\u4e3a) the main loop or main event loop . This title is appropriate, because such an event loop is at the highest level of control within the program. NOTE: \u5927\u591a\u6570\u5b9e\u73b0\u90fd\u662f\u8fd9\u79cd\u6a21\u5f0f","title":"wikipedia Event loop"},{"location":"Event-driven-concurrent-server/Event-driven-model/Event-loop/#message#passing","text":"Message pumps are said to 'pump' messages from the program's message queue (assigned and usually owned by the underlying operating system) into the program for processing. In the strictest sense, an event loop is one of the methods for implementing inter-process communication . In fact, message processing exists in many systems, including a kernel-level component of the Mach operating system . The event loop is a specific implementation technique of systems that use message passing .","title":"Message passing"},{"location":"Event-driven-concurrent-server/Event-driven-model/Event-loop/#alternative#designs","text":"This approach is in contrast to a number of other alternatives: 1\u3001Traditionally, a program simply ran once, then terminated. This type of program was very common in the early days of computing, and lacked any form of user interactivity. This is still used frequently, particularly in the form of command-line-driven programs. Any parameters are set up in advance and passed in one go when the program starts. 2\u3001Menu-driven designs. These still may feature a main loop, but are not usually thought of as event driven in the usual sense[ citation needed ]. Instead, the user is presented with an ever-narrowing set of options until the task they wish to carry out is the only option available. Limited interactivity through the menus is available.","title":"Alternative designs"},{"location":"Event-driven-concurrent-server/Event-driven-model/Event-loop/#usage","text":"Due to the predominance(\u4f18\u52bf) of graphical user interfaces , most modern applications feature a main loop . The get_next_message() routine is typically provided by the operating system , and blocks until a message is available. Thus, the loop is only entered when there is something to process. function main initialize() while message != quit message := get_next_message() process_message(message) end while end function","title":"Usage"},{"location":"Event-driven-concurrent-server/Event-driven-model/Event-loop/#file#interface","text":"Under Unix , the \" everything is a file \" paradigm naturally leads to a file-based event loop . Reading from and writing to files, inter-process communication, network communication, and device control are all achieved using file I/O , with the target identified by a file descriptor . The select and poll system calls allow a set of file descriptors to be monitored for a change of state, e.g. when data becomes available to be read. For example, consider a program that reads from a continuously updated file and displays its contents in the X Window System , which communicates with clients over a socket (either Unix domain or Berkeley ): main(): file_fd = open (\"logfile\") x_fd = open_display () construct_interface () while changed_fds = select ({file_fd, x_fd}): if file_fd in changed_fds: data = read_from (file_fd) append_to_display (data) send_repaint_message () if x_fd in changed_fds: process_x_messages ()","title":"File interface"},{"location":"Event-driven-concurrent-server/Event-driven-model/Event-loop/#handling#signals","text":"One of the few things in Unix that does not conform to the file interface are asynchronous events ( signals ). Signals are received in signal handlers , small, limited pieces of code that run while the rest of the task is suspended; if a signal is received and handled while the task is blocking in select() , select will return early with EINTR ; if a signal is received while the task is CPU bound , the task will be suspended between instructions until the signal handler returns. Thus an obvious way to handle signals is for signal handlers to set a global flag and have the event loop check for the flag immediately before and after the select() call; if it is set, handle the signal in the same manner as with events on file descriptors . Unfortunately, this gives rise to a race condition : if a signal arrives immediately between checking the flag and calling select() , it will not be handled until select() returns for some other reason (for example, being interrupted by a frustrated user). NOTE: \u4e0a\u8ff0\u8fd9\u79cd\u601d\u8def\u53ef\u4ee5\u53c2\u89c1\uff1a Interrupting blocked read \u4e2d\u7ed9\u51fa\u7684demo\uff1b NOTE : \u4e0d\u5e78\u7684\u662f\uff0c\u8fd9\u4f1a\u5f15\u8d77\u7ade\u4e89\u6761\u4ef6\uff1a\u5982\u679c\u4fe1\u53f7\u5728\u68c0\u67e5\u6807\u5fd7\u548c\u8c03\u7528select\uff08\uff09\u4e4b\u95f4\u7acb\u5373\u5230\u8fbe\uff0c\u5219\u76f4\u5230select\uff08\uff09\u56e0\u67d0\u4e9b\u5176\u4ed6\u539f\u56e0\uff08\u4f8b\u5982\u88ab\u6cae\u4e27\u7684\u7528\u6237\u4e2d\u65ad\uff09\u8fd4\u56de\u65f6\u624d\u4f1a\u5904\u7406\u5b83\uff1b\u8fd9\u6bb5\u8bdd\u4e2d\u6240\u63cf\u8ff0\u7684race condition\u5e76\u6ca1\u6709\u641e\u6e05\u695a\uff1b The solution arrived at by POSIX is the pselect() call, which is similar to select() but takes an additional sigmask parameter, which describes a signal mask . This allows an application to mask signals in the main task , then remove the mask for the duration of the select() call such that signal handlers are only called while the application is I/O bound . However, implementations of pselect() have only recently[ when? ] become reliable; versions of Linux prior to 2.6.16 do not have a pselect() system call, forcing glibc to emulate it via a method prone to the very same race condition pselect() is intended to avoid. An alternative, more portable solution, is to convert asynchronous events to file-based events using the self-pipe trick ,[ 1] where \"a signal handler writes a byte to a pipe whose other end is monitored by select() in the main program\".[ 2] In Linux kernel version 2.6.22, a new system call signalfd() was added, which allows receiving signals via a special file descriptor . NOTE :\u53c2\u89c1\u300athe linux program interface \u300b\u7684 63.5.2 The Self-Pipe Trick","title":"Handling signals"},{"location":"Event-driven-concurrent-server/Event-driven-model/Event-loop/#softwareengineering#is#an#event#loop#just#a#forwhile#loop#with#optimized#polling","text":"NOTE:\u770b\u4e86\u8fd9\u4e48\u591a\uff0c\u53d1\u73b0\u63d0\u95ee\u8005\u7684\u6700\u7ec8\u610f\u56fe\u662f\u60f3\u8981\u77e5\u9053event loop\u662f\u5982\u4f55\u5b9e\u73b0\u7684\uff0c\u63d0\u95ee\u8005\u8ba4\u4e3aevent loop\u7684\u5b9e\u73b0\u65b9\u5f0f\u662f\u7c7b\u4f3cpolling\u7684\uff1b\u6839\u636e\u4e0b\u9762\u7684\u56de\u7b54\uff0c\u663e\u7136\u7b54\u6848\u662fNo\u3002\u7b54\u6848 A4 \u662f\u975e\u5e38\u76f4\u63a5\u7684\u7ed9\u51fa\u4e86\u7b54\u6848\uff1b\u5b83\u5f15\u7528\u4e86Wikipedia\u7684\u4e0a\u7684 Polling \u6587\u7ae0\uff0c\u5728\u8fd9\u7bc7\u6587\u7ae0\u4e2d\uff0c\u6709\u4e0b\u9762\u7684\u4e00\u6bb5\u8bdd\uff1a Although not as wasteful of CPU cycles as busy waiting, this is generally not as efficient as the alternative to polling, interrupt -driven I/O. \u8fd9\u6bb5\u8bdd\u4e2d\u7684 interrupt -driven I/O\u6b63\u662fevent loop\u6240\u91c7\u7528\u7684\u3002 I'm trying to understand what an event loop is. Often the explanation is that in an event loop, you do something until you're notified that an event has occurred. You then handle the event and continue doing what you were doing before. NOTE\uff1anotify To map the above definition with an example. I have a server which 'listens' in a event loop, and when a socket connection is detected, the data from it gets read and displayed, after which the server resumes/starts listening as it did before. However, this event happening and us getting notified 'just like that' are to much for me to handle. You can say: \"It's not 'just like that' you have to register an event listener\". But what's an event listener but a function which for some reason isn't returning. Is it in it's own loop, waiting to be notified when an event happens? Should the event listener also register an event listener? Where does it end? Events are a nice abstraction to work with, however just an abstraction. I believe that in the end, polling is unavoidable . Perhaps we are not doing it in our code, but the lower levels (the programming language implementation or the OS) are doing it for us. NOTE: \u4ece\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u53ef\u4ee5\u770b\u51fa\uff0c\u63d0\u95ee\u8005\u7684\u6700\u7ec8\u610f\u56fe\u662f\u60f3\u8981\u77e5\u9053event loop\u5230\u5e95\u662f\u5982\u4f55\u5b9e\u73b0\u7684\u3002 It basically comes down to the following pseudo code which is running somewhere low enough so it doesn't result in busy waiting: while(True): do stuff check if event has happened (poll) do other stuff This is my understanding of the whole idea, and I would like to hear if this is correct. I'm open in accepting that the whole idea is fundamentally wrong, in which case I would like the correct explanation. COMMENTS Event systems are implementations of the Observer pattern . Understanding the pattern should cement your understanding of events. No polling is required. \u2013 Steven Evers Oct 18 '13 at 20:21 Ultimately yes, even if language constructs abstract it away. \u2013 GrandmasterB Oct 18 '13 at 20:30 @SteveEvers In your wiki link. What is the EventSource doing if not polling the keyboard input? \u2013 TheMeaningfulEngineer Oct 18 '13 at 20:36 @Alan : It could be doing anything, but for keyboard input specifically there do exist APIs to register your application as listening to keyboard events, which you can then use as your eventsource with no polling involved. Of course, if you go down far enough, USB is always polling, but let's assume we're using a PS/2 keyboard, which is interrupt driven, and then you have a keyboard input stack based on events with zero polling.\u2013 Phoshi Oct 25 '13 at 8:25 I've had this questions since years, but I can't tell why I never bothered to ask it. Thanks, I am now satisfied with the understanding @Karl Bielefeldt has enlightened me with. \u2013 0xc0de Dec 2 '18 at 5:03","title":"softwareengineering Is an event loop just a for/while loop with optimized polling?"},{"location":"Event-driven-concurrent-server/Event-driven-model/Event-loop/#a1","text":"Most event loops will suspend if there are no events ready(\u6ca1\u6709event\u5c31\u7eea\uff0c\u5219event loop\u5c31\u4f1a\u88ab\u6302\u8d77), which means the operating system will not give the task any execution time until an event happens. NOTE:\u8fd9\u6bb5\u8bdd\u5c31\u76f4\u63a5\u70b9\u660e\u4e86event loop\u548cpolling\u4e4b\u95f4\u7684\u5dee\u5f02\u6240\u5728\u3002 Say the event is a key being pressed. You might ask if there's a loop somewhere in the operating system checking for keypresses. The answer is no. Keys being pressed generate an interrupt , which is handled asynchronously by the hardware. Likewise for timers, mouse movements, a packet arriving, etc. NOTE: interrupt \u662f\u7ed5\u5f00loop\u7684\u4e00\u79cd\u975e\u5e38\u597d\u7684\u65b9\u5f0f\uff0c\u8fd9\u5c31\u662f\u5b9e\u73b0event loop\u7684\u4e00\u79cd\u975e\u5e38\u597d\u7684\u65b9\u5f0f\u3002 In fact, for most operating systems, polling for events is the abstraction\uff08\u4e5f\u5c31\u662f\u8bf4\u5b9e\u73b0\u53ef\u80fd\u5e76\u975e\u5982\u6b64\uff09. The hardware and OS handle events asynchronously and put them in a queue that can be polled by applications. You only really see true polling at the hardware level in embedded systems, and even there not always. \u8ba8\u8bba\uff1a Isn't an interrupt a change of voltage on a wire? Can that trigger an event by itself or must we poll the pin for the voltage value? \u2013 TheMeaningfulEngineer That can trigger an event by itself. The processor is designed that way. In fact a processor can be woken up from sleep by an interrupt. \u2013 Karl Bielefeldt I am confused with the statement Most event loops will block . How does this fit into the \"the event loop paradigm, opposed to using threads, uses nonblocking asynchronous calls\"? \u2013 TheMeaningfulEngineer If you look at event loops for a library like GTK+, they check for new events then call event handlers in a loop, but if there aren't any events, they block on a semaphore or a timer or something. Individual developers make their own event loops that don't block on an empty event queue, but the widely-used libraries all block. Event loops are too inefficient otherwise. \u2013 Karl Bielefeldt NOTE:\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u6307\u51fa\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u5982empty queue\uff0cevent loop\u5c06\u88abblock\uff0c\u663e\u7136\u8fd9\u5c31\u89e3\u91ca\u4e86 Most event loops will block \uff1b\u81f3\u4e8e How does this fit into the \"the event loop paradigm, opposed to using threads, uses nonblocking asynchronous calls\"\uff0c\u6211\u60f3\u8fd9\u4e2a\u95ee\u9898\u4e2d\u6240\u63cf\u8ff0\u7684\u662f\u6307\uff0c\u5f53event loop\u88ab\u901a\u77e5\u67d0\u4e2aevent\u7684\u65f6\u5019\uff0c\u5b83\u6267\u884cevent handler\u662f\u91c7\u7528\u7684nonblocking asynchronous calls\u3002","title":"A1"},{"location":"Event-driven-concurrent-server/Event-driven-model/Event-loop/#a2","text":"Event systems are implementations of the Observer pattern . Understanding the pattern should cement your understanding of events. No polling is required. \u2013 Steven Evers","title":"A2"},{"location":"Event-driven-concurrent-server/Event-driven-model/Event-loop/#a3","text":"I think of an event listener not as a function running its own loop, but as a relay race\uff08\u63a5\u529b\u6bd4\u8d5b\uff09 with the first runner waiting for the starting gun. A significant reason for using events instead of polling is that they are more efficient with CPU cycles. Why? Look at it from the hardware up (rather than the source code down). Consider a Web server. When your server calls listen() and blocks, your code is taking its place as a relay runner. When the first packet of a new connection arrives, the network card starts the race by interrupting \uff08\u4e2d\u65ad\uff09 the operating system. The OS runs an interrupt service routine (ISR) that grabs the packet. The ISR passes the baton\uff08\u63a5\u529b\u68d2\uff09 to a higher-level routine that establishes the connection. Once the connection is alive, that routine passes the baton to listen() , which passes the baton up to your code. At that point, you can do what you want with the connection. For all we know, between races each relay runner could be going to the pub. A strength of the event abstraction is that your code doesn't have to know or care. NOTE:\u4e0a\u9762\u5bf9\u89e6\u53d1\u5f0f\u7684\u6bd4\u55bb\u6bd4\u8f83\u5f62\u8c61\uff1b\u611f\u89c9\u8fd9\u975e\u5e38\u50cf\u7ecf\u5178\u7684Unix\u7f51\u7edc\u7f16\u7a0b\u6a21\u578b\u3002 Some operating systems include event-handling code that runs its portion of the race, hands off the baton, then loops back to its starting point to wait for the next race to start. In that sense, event handling is optimized polling in lots of concurrent loops. However, there is always an outside trigger that kicks off the process. The event listener is not a function that isn't returning, but a function that is waiting for that external trigger before it runs. Rather than: while(True): do stuff check if event has happened (poll) do other stuff I think of this as: on(some event): //I got the baton do stuff signal the next level up //Pass the baton and between the signal and the next time the handler runs, there is conceptually no code running or looping. NOTE:\u5bf9\u6bd4\u4e0a\u9762\u4e24\u6bb5\u4f2a\u4ee3\u7801\uff0con\u53ef\u4ee5\u7ffb\u8bd1\u4e3a\u201c\u5f53\u201d\uff0c\u8fd9\u662f\u975e\u5e38\u5951\u5408interrupt\u7684\uff1b","title":"A3"},{"location":"Event-driven-concurrent-server/Event-driven-model/Event-loop/#a4","text":"No. It is not \"optimized polling.\" An event-loop uses interrupt-driven I/O instead of polling. While, Until, For, etc. loops are polling loops. \"Polling\" is the process of repeatedly checking something. Since the loop code executes continuously, and because it is a small, \"tight\" loop, there is little time for the processor to switch tasks and do anything else. Almost all \"hangs,\" \"freezes,\" \"lockups\" or whatever you want to call it when the computer becomes unresponsive, are the manifestation\uff08\u5c55\u793a\uff09 of code being stuck in an unintended polling loop. Instrumentation will show 100% CPU usage. Interrupt-driven event loops are far more efficient than polling loops. Polling is an extremely wasteful use of CPU cycles so every effort is made to eliminate or minimize it. However, to optimize code quality, most languages try to use the polling loop paradigm as closely as possible for event handing commands since they serve functionally similar purposes within a program\uff08\u56e0\u4e3a\u5b83\u4eec\u5728\u7a0b\u5e8f\u4e2d\u63d0\u4f9b\u529f\u80fd\u4e0a\u7c7b\u4f3c\u7684\u7528\u9014\uff09. Thus, with polling being the more familiar way to wait for a keypress or something, it is easy for the inexperienced\uff08\u4e0d\u719f\u7ec3\u7684\uff09 to use it and wind up with a program that may run fine by itself\uff08\u6700\u7ec8\u4f7f\u7528\u53ef\u80fd\u81ea\u884c\u8fd0\u884c\u7684\u7a0b\u5e8f\uff09, but nothing else works while it's running. It has \"taken over\" the machine. As explained in other answers, in interrupt-driven event handing , essentially a \"flag\" is set within the CPU and the process is \"suspended\" (not allowed to run) until that flag is changed by some other process (such as the keyboard driver changing it when the user has pressed a key). If the flag is an actual hardware condition such as a line being \"pulled high,\" it's called an \"interrupt\" or \"hardware interrupt.\" Most however, are implemented as just a memory address on the CPU or in main memory (RAM) and are called \"semaphores.\" Semaphores can be changed under software control and so can provide a very fast, simple signalling mechanism between software processes. Interrupts, however, can only be changed by hardware. The most ubiquitous\uff08\u666e\u904d\u5b58\u5728\u7684\uff09 use of interrupts is the one triggered at regular intervals by the internal clock chip. One of the countless kinds of software actions activated by clock interrupts, is the changing of semaphores. I've left out a lot but had to stop somewhere. Please ask if you need more details.","title":"A4"},{"location":"Event-driven-concurrent-server/Event-driven-model/Event-loop/Implementation/","text":"Implementation troglobit Threads vs Event Loop, Again I still get asked this, from time to time. Maybe it\u2019s because I only use event loops , maybe it\u2019s because I\u2019ve written libuEv , or maybe people still don\u2019t understand: Why an event loop, why not use threads? So here\u2019s my response, once more. With the advent of light-weight processes (threads) programmers these days have a golden hammer they often swing without consideration. Event loops and non-blocking I/O is often a far easier approach, as well as less error prone. The purpose of many applications is, with a little logic sprinkled on top, to act on network packets entering an interface, timeouts expiring, mouse clicks, or other types of events. Such applications are often very well suited to use an event loop . NOTE: \u9002\u5408event loop\u7684\u60c5\u5f62 Applications that need to churn massively parallel algorithms are more suitable for running multiple (independent) threads on several CPU cores. However, threaded applications must deal with the side effects of concurrency, like race conditions, deadlocks, live locks, etc. Writing error free threaded applications is hard, debugging them can be even harder. NOTE : \u9002\u5408multiple threads\u7684\u60c5\u5f62 Sometimes the combination of multiple threads and an event loop per thread can be the best approach, but each application of course needs to be broken down individually to find the most optimal approach. Do keep in mind, however, that not all systems your application will run on have multiple CPU cores \u2013 some small embedded systems still use a single CPU core, even though they run Linux, with multiple threads a program may actually run slower! Always profile your program, and if possible, test it on different architectures. NOTE: combine event loop and multiple threads 20190710 \u73b0\u5728\u60f3\u60f3\uff0c\u5176\u5b9e\u8fd9\u4e2a\u95ee\u9898\u66f4\u5e94\u8be5\u662freactor\u548cmultithread\u4e4b\u95f4\u7684\u5f02\u4e0e\u540c","title":"Implementation"},{"location":"Event-driven-concurrent-server/Event-driven-model/Event-loop/Implementation/#implementation","text":"","title":"Implementation"},{"location":"Event-driven-concurrent-server/Event-driven-model/Event-loop/Implementation/#troglobit#threads#vs#event#loop#again","text":"I still get asked this, from time to time. Maybe it\u2019s because I only use event loops , maybe it\u2019s because I\u2019ve written libuEv , or maybe people still don\u2019t understand: Why an event loop, why not use threads? So here\u2019s my response, once more. With the advent of light-weight processes (threads) programmers these days have a golden hammer they often swing without consideration. Event loops and non-blocking I/O is often a far easier approach, as well as less error prone. The purpose of many applications is, with a little logic sprinkled on top, to act on network packets entering an interface, timeouts expiring, mouse clicks, or other types of events. Such applications are often very well suited to use an event loop . NOTE: \u9002\u5408event loop\u7684\u60c5\u5f62 Applications that need to churn massively parallel algorithms are more suitable for running multiple (independent) threads on several CPU cores. However, threaded applications must deal with the side effects of concurrency, like race conditions, deadlocks, live locks, etc. Writing error free threaded applications is hard, debugging them can be even harder. NOTE : \u9002\u5408multiple threads\u7684\u60c5\u5f62 Sometimes the combination of multiple threads and an event loop per thread can be the best approach, but each application of course needs to be broken down individually to find the most optimal approach. Do keep in mind, however, that not all systems your application will run on have multiple CPU cores \u2013 some small embedded systems still use a single CPU core, even though they run Linux, with multiple threads a program may actually run slower! Always profile your program, and if possible, test it on different architectures. NOTE: combine event loop and multiple threads","title":"troglobit Threads vs Event Loop, Again"},{"location":"Event-driven-concurrent-server/Event-driven-model/Event-loop/Implementation/#20190710","text":"\u73b0\u5728\u60f3\u60f3\uff0c\u5176\u5b9e\u8fd9\u4e2a\u95ee\u9898\u66f4\u5e94\u8be5\u662freactor\u548cmultithread\u4e4b\u95f4\u7684\u5f02\u4e0e\u540c","title":"20190710"},{"location":"Event-driven-concurrent-server/Event-driven-model/Event-passing/","text":"Event/message source and passing \u5728 What-is-event-driven model \u7ae0\u8282\u4e2d\u5df2\u7ecf\u63cf\u8ff0\u4e86event source\uff1b\u4e0d\u540c\u7684source\uff0c\u5b83\u4eec\u7684passing\u65b9\u5f0f\u662f\u4e0d\u540c\u7684\uff0c\u6700\u6700\u5178\u578b\u7684\u533a\u5206\u662f\u662f\u5426\u9700\u8981\u7ecf\u8fc7network\u3001\u662finter-process\u8fd8\u662fintra-process\u3002 Wikipedia Message passing Inter-process message passing 1\u3001\u663e\u7136\uff0c\u8fd9\u6d89\u53cainter-process communication\uff0c\u5728\u5de5\u7a0bLinux-OS\u4e2d\u6709\u7740\u975e\u5e38\u597d\u7684\u603b\u7ed3 2\u3001Inter-process message passing\u7684\u65b9\u5f0f\u662f\u591a\u79cd\u591a\u6837\u7684\uff0c\u672c\u8282\u65e0\u6cd5\u679a\u4e3e 3\u3001\u6700\u6700\u5178\u578b\u7684\u662f\u7ecf\u8fc7network 4\u3001\u5173\u4e8e\u8fd9\u79cd\u573a\u666f\u7684design pattern\uff0c\u53c2\u89c1 Message-processing-system\\Event-driven-model\\Design-pattern \u7ae0\u8282 Intra-process message passing 1\u3001\u663e\u7136\uff0c\u53ef\u80fd\u7684message passing\u65b9\u5f0f\u662f\u975e\u5e38\u591a\u7684\uff0c\u6bd4\u5982inter-thread communication","title":"Introduction"},{"location":"Event-driven-concurrent-server/Event-driven-model/Event-passing/#eventmessage#source#and#passing","text":"\u5728 What-is-event-driven model \u7ae0\u8282\u4e2d\u5df2\u7ecf\u63cf\u8ff0\u4e86event source\uff1b\u4e0d\u540c\u7684source\uff0c\u5b83\u4eec\u7684passing\u65b9\u5f0f\u662f\u4e0d\u540c\u7684\uff0c\u6700\u6700\u5178\u578b\u7684\u533a\u5206\u662f\u662f\u5426\u9700\u8981\u7ecf\u8fc7network\u3001\u662finter-process\u8fd8\u662fintra-process\u3002","title":"Event/message source and passing"},{"location":"Event-driven-concurrent-server/Event-driven-model/Event-passing/#wikipedia#message#passing","text":"","title":"Wikipedia Message passing"},{"location":"Event-driven-concurrent-server/Event-driven-model/Event-passing/#inter-process#message#passing","text":"1\u3001\u663e\u7136\uff0c\u8fd9\u6d89\u53cainter-process communication\uff0c\u5728\u5de5\u7a0bLinux-OS\u4e2d\u6709\u7740\u975e\u5e38\u597d\u7684\u603b\u7ed3 2\u3001Inter-process message passing\u7684\u65b9\u5f0f\u662f\u591a\u79cd\u591a\u6837\u7684\uff0c\u672c\u8282\u65e0\u6cd5\u679a\u4e3e 3\u3001\u6700\u6700\u5178\u578b\u7684\u662f\u7ecf\u8fc7network 4\u3001\u5173\u4e8e\u8fd9\u79cd\u573a\u666f\u7684design pattern\uff0c\u53c2\u89c1 Message-processing-system\\Event-driven-model\\Design-pattern \u7ae0\u8282","title":"Inter-process message passing"},{"location":"Event-driven-concurrent-server/Event-driven-model/Event-passing/#intra-process#message#passing","text":"1\u3001\u663e\u7136\uff0c\u53ef\u80fd\u7684message passing\u65b9\u5f0f\u662f\u975e\u5e38\u591a\u7684\uff0c\u6bd4\u5982inter-thread communication","title":"Intra-process message passing"},{"location":"Event-driven-concurrent-server/Event-driven-model/What-is-event-driven-model/","text":"What is event-driven model Event-driven model\u6982\u8ff0 Event-driven model\u53ef\u4ee5\u770b\u505a\u662f\u4e00\u4e2a**abstract machine**: \u5b83\u9700\u8981\u6301\u7eed\u4e0d\u65ad\u5730\u8fdb\u884c\u8fd0\u8f6c\u4ee5**\u76d1\u63a7**\u4e8b\u4ef6\u6e90\u3001\u6536\u96c6event\uff0c\u4e00\u65e6\u6536\u96c6\u5230\u4e86event\uff0c\u5c31\u8fdb\u884cdispatch\uff0c\u5373\u6839\u636eevent\u548cevent handler\u4e4b\u95f4\u7684\u6620\u5c04\u5173\u7cfb\uff0c\u8c03\u7528\u8fd9\u4e2aevent\u5bf9\u5e94\u7684event handler\u3002 on\u3001when \u53ef\u4ee5\u4f7f\u7528Event-driven model\u6765\u8fdb\u884c\u63cf\u8ff0\u7684\u95ee\u9898\u7684\u4e00\u4e2a\u5178\u578b\u7684\u6a21\u5f0f\u5c31\u662f: \" \u5f53**\u67d0\u4e2a\u4e8b\u4ef6(**event )\u53d1\u751f\u7684\u65f6\u5019\uff0c\u5c31\u6267\u884c\u67d0\u4e2a\u51fd\u6570( event handler \u3001callback)\"\u3002 \u73b0\u4ee3programming language\u5982\u4f55\u6765\u63cf\u8ff0\u8fd9\u79cd\u6a21\u5f0f\u5462\uff1f\u5bf9\u6bd4\u5730\u770b\u4e86\u4e00\u4e0b\uff0c\u73b0\u4ee3\u4e3b\u6d41programming language\u90fd\u63d0\u4f9b\u4e86**\u7c7b\u4f3c**\u7684**programming model**\u6765\u63cf\u8ff0\u8fd9\u79cd\u6a21\u5f0f\uff0c\u6bd4\u5982: \u4e00\u3001JavaScript\uff0c\u53c2\u89c1\u5de5\u7a0bprogramming\u7684 JavaScript\\Event-driven-model \u7ae0\u8282 \u4f7f\u7528\u8fd9\u4e9bprogramming model\u80fd\u591f\u8ba9programmer\u65b9\u4fbf\u5730\u6765\u63cf\u8ff0\u8fd9\u79cd\u6a21\u5f0f\u3002\u603b\u7684\u6765\u8bf4\u5b83\u4f9d\u8d56\u4e8e\u5982\u4e0bprogramming language feature: 1) First class function \u53c2\u89c1: 1) Wikipedia First-class function 2) \u5de5\u7a0bprogramming-language\u7684 Theory\\Programming-paradigm\\Functional-programming \u7ae0\u8282 2) Fluent interface \u53c2\u89c1: 1) Wikipedia Fluent interface 2) \u5de5\u7a0bprogramming-language\u7684 Theory\\Programming-paradigm\\Object-oriented-programming\\Design-pattern\\Fluent-API \u7ae0\u8282 Callback callback-based notification\u3002 Example \u7406\u89e3event-driven model\u7684\u6700\u6700\u7b80\u5355\u7684\u4f8b\u5b50\u5c31\u662fLinux OS kernel\u548chardware\u7684\u4ea4\u4e92\uff0c\u5728\u5de5\u7a0b Linux-OS \u7684\u6587\u7ae0 Linux-OS-kernel-is-event-driven \u4e2d\u6211\u4eec\u5df2\u7ecf\u603b\u7ed3\u4e86\u53ef\u4ee5\u4f7f\u7528 Event-driven model \u6765\u63cf\u8ff0linux OS kernel\u548chardware\u7684\u4ea4\u4e92\u3002\u5728hardware\u5c42\uff0c\u4e00\u65e6\u901a\u7535\uff0c\u5219hardware\u5c31\u6301\u7eed\u4e0d\u65ad\u5730\u8fd0\u8f6c\u8d77\u6765\u4e86\uff0c\u4e00\u65e6\u89e6\u53d1\u4e86interrupt\uff0c\u5219\u5b83\u7684**interrupt handler**\u5c31\u4f1a\u88ab\u6267\u884c\uff0cLinux OS kernel\u7ef4\u62a4\u4e86interrupt\u548cinterrupt handler\u4e4b\u95f4\u7684\u6620\u5c04\u5173\u7cfb\u3002 Event-driven model/machine\u7684component(\u7ec4\u6210) \u4e00\u4e2aevent-driven model\u7684\u7ec4\u6210\u6210\u5206\uff1a \u7ec4\u6210\u90e8\u5206 \u662f\u5426\u5fc5\u987b \u8bf4\u660e monitor/listener yes \u76d1\u63a7\u4e8b\u4ef6\u6e90\u3001\u6536\u96c6event event\u548cevent handler\u4e4b\u95f4\u7684\u6620\u5c04\u5173\u7cfb\u8868 yes message passing yes \u4f20\u9012\u4fe1\u606f\uff0cmessage queue/broker dispatcher yes \u6d3e\u53d1event\uff0c\u5373\u6309\u7167event\u548cevent handler\u4e4b\u95f4\u7684\u6620\u5c04\u5173\u7cfb\uff0c\u901a\u77e5executor\u6267\u884cevent handler executor yes \u6267\u884cevent handler See also martinfowler What do you mean by \u201cEvent-Driven\u201d? What is event? Event\u662f\u4e00\u4e2a\u975e\u5e38\u6982\u62ec\u3001\u5bbd\u6cdb\u7684\u6982\u5ff5\uff0c\u5728computer science\u4e2d\uff0c\u975e\u5e38\u591a\u7684\u884c\u4e3a\u90fd\u4f1a\u4ea7\u751fevent\uff0c\u6211\u4eec\u628a\u5b83\u79f0\u4e3asource of event\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06source of event\u5212\u5206\u4e3a\u4e24\u7c7b\uff1a 1) external environment \u6211\u4eec\u4ee5\u81ea\u5e95\u5411\u4e0a\u7684\u601d\u8def\u6765\u5206\u6790\u6e90\u81eaexternal environment\u7684event\uff0c\u4e00\u4e2a computing system \u7684\u6700\u5e95\u5c42\u662fhardware\uff0chardware\u4ea7\u751f\u7684 interrupt \uff0c\u7136\u540e\u7531OS kernel\u5c06\u8fd9\u4e9binterrupt\u201c\u8f6c\u6362\u201d\u4e3a**signal**\uff08\u73b0\u4ee3programming language\u4f1a\u4f7f\u7528**exception**\u6765\u62bd\u8c61**signal**\uff09\u3001IO event\uff08\u56e0\u4e3aIO\u7684\u5b9e\u73b0\u662f\u4f9d\u8d56\u4e8einterrupt\u7684\uff0cIO\u5305\u62ec\u4e86\u975e\u5e38\u591a\u7684\u5185\u5bb9\uff0c\u7528\u6237\u64cd\u4f5c\u3001\u7f51\u7edc\u901a\u4fe1\u7b49\u90fd\u53ef\u4ee5\u770b\u505a\u662fIO\uff0cevents can represent availability of new data for reading a file or network stream.\uff09\u7b49\uff0c\u5e76\u901a\u77e5\u5230application process\u3002 \u5173\u4e8e\u8fd9\u4e00\u70b9\uff0c\u53c2\u89c1\u7ef4\u57fa\u767e\u79d1 Event (computing) \u7684 Event handler \u6bb5\u3002 2) \u7a0b\u5e8f\u5185\u90e8 Event\u53ef\u80fd\u6e90\u81ea\u4e8eexternal environment\uff0c\u4e5f\u53ef\u80fd\u6e90\u81ea\u4e8e\u7a0b\u5e8f\u4e4b\u5185\uff0c\u5373\u7a0b\u5e8f\u5185\u90e8\u5c06\u4e00\u4e9b\u6761\u4ef6\u7b49\u770b\u505aevent\uff0c\u6bd4\u5982condition variable\u3001timer\u3002 TODO: \u9700\u8981\u8865\u5145\u4e00\u4e9b\u5177\u4f53\u4f8b\u5b50\u3002 \u4e0b\u9762\u8865\u5145\u4e86\u7ef4\u57fa\u767e\u79d1 Event (computing) \u6765\u8fdb\u884c\u8be6\u7ec6\u8bf4\u660e\u3002 See also: \u7ef4\u57fa\u767e\u79d1 Event (computing) Implementation of event-driven model \u5982\u4f55\u5b9e\u73b0event-driven model\uff1f\u524d\u9762\u5df2\u7ecf\u63cf\u8ff0\u4e86even driven model\u7684\u7ec4\u6210\uff0c\u663e\u7136\u6211\u4eec\u9700\u8981\u5206\u522b\u5b9e\u73b0\u8fd9\u4e9bcomponent\u3002\u7531\u4e8eevent-driven model\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u5b83\u5df2\u7ecf\u6210\u4e3asoftware engineering\u4e2d\u7684\u57fa\u7840\u4e86\uff0c\u5e76\u4e14\u5bf9\u4e8e\u5b83\u7814\u7a76\u5df2\u7ecf\u975e\u5e38\u6210\u719f\u4e86\uff0c\u56e0\u6b64\u76ee\u524d\u6709\u975e\u5e38\u591a\u7684\u3001\u975e\u5e38\u6210\u719f\u7684library\uff0c\u8fd9\u5728 Library \u7ae0\u8282\u8fdb\u884c\u4e86\u603b\u7ed3\u3002 IoC\u539f\u5219 \u975e\u5e38\u9002\u5408\u4f7f\u7528IoC\uff0c\u53c2\u89c1 Software-design\\Control-theory\\Inversion-of-control \u7ae0\u8282\u3002 Monitor\uff1a\u5982\u4f55\u8fdb\u884c\u6301\u7eed\u76d1\u63a7 \u5728\u786c\u4ef6\u5c42\uff0c\u53ea\u8981\u901a\u7535\u540e\uff0c\u5219hardware\u5c31\u6301\u7eed\u4e0d\u65ad\u5730\u8fd0\u8f6c\u8d77\u6765\u4e86\uff0c\u4e00\u65e6\u89e6\u53d1\u4e86interrupt\uff0c\u5219\u5b83\u7684interrupt handler\u5c31\u4f1a\u88ab\u6267\u884c\u3002\u5728\u8f6f\u4ef6\u5c42\uff0c\u6211\u4eec\u9700\u8981\u663e\u5f0f\u5730\u4f7f\u7528\u4e00\u4e2a event loop \u6765\u6307\u793a\u6574\u4e2amodel/machine\u9700\u8981\u4e0d\u65ad\u5730\u8fd0\u8f6c\u4e0b\u53bb\u3002 \u5173\u4e8e\u8fd9\u4e00\u70b9\uff0c\u53c2\u89c1\u7ef4\u57fa\u767e\u79d1 Event-driven programming \u7684\u7b2c\u4e8c\u6bb5\u3002 \u5efa\u7acbevent\u548cevent handler\u6620\u5c04\u5173\u7cfb Event-driven model\u80af\u5b9a\u9700\u8981\u8bb0\u5f55\u4e0bevent\u548cevent handler\u4e4b\u95f4\u7684\u6620\u5c04\u5173\u7cfb\uff0c\u5b9e\u73b0\u8fd9\u79cd\u6620\u5c04\u5173\u7cfb\u7684\u65b9\u5f0f\u662f\u975e\u5e38\u591a\u7684\uff0c\u53ef\u4ee5\u663e\u793a\u5730\u4f7f\u7528\u8bf8\u5982map\u7684\u6570\u636e\u7ed3\u6784\u3002 \u4e0b\u9762\u7f57\u5217\u4e86\u4e00\u4e9b\u5b9e\u73b0\u6848\u4f8b\uff1a \u6848\u4f8b \u8bf4\u660e Interrupt Descriptor Table \u53c2\u89c1\uff1a - \u5de5\u7a0b Linux-OS \u7684 4.2-Interrupts-and-Exceptions \u7684 4.2.3. Interrupt Descriptor Table - \u7ef4\u57fa\u767e\u79d1 Interrupt descriptor table Dispatcher/demultiplexing \u6240\u8c13\u7684event dispatcher\u662f\u6307\u5f53event\u53d1\u751f\u65f6\uff0cevent-driven model\u5c06event\u4f20\u9012\u5230executor\uff0c\u901a\u77e5executor\u6267\u884c\u5bf9\u5e94\u7684handler\u3002Dispatcher\u7684\u5b9e\u73b0\u65b9\u5f0f\u662f\u6709executor\u51b3\u5b9a\u7684\uff0c \u5728\u4e0b\u4e00\u8282\u5bf9\u6b64\u8fdb\u884c\u5177\u4f53\u60c5\u51b5\u5177\u4f53\u8bf4\u660e\u3002 Executor: execution of event handler \u5982\u4f55\u6765\u6267\u884cevent handler\uff1f\u5728\u51b3\u5b9a\u5982\u4f55\u6765\u6267\u884cevent handler\u7684\u65f6\u5019\uff0c\u5f00\u53d1\u8005\u9700\u8981\u8003\u8651\u5982\u4e0b\u95ee\u9898\uff1a 1) event handler\u6267\u884c\u7684\u6210\u672c\uff0c\u6b64\u5904\u7684\u6210\u672c\u53ef\u4ee5\u6709\u591a\u79cd\u89e3\u91ca\uff0c\u6bd4\u5982\uff0c\u5b83\u53ef\u4ee5\u8868\u793aevent handler\u6267\u884c\u7684\u65f6\u957f\u3001\u53ef\u4ee5\u8868\u793aevent handler\u6267\u884c\u7684\u8d44\u6e90\u8017\u8d39 2) \u5e76\u53d1\u6027\uff0c\u540c\u65f6\u53d1\u751f\u7684\u4e8b\u4ef6\u53d1\u751f\u53ef\u80fd\u591a\uff0c\u5982\u4f55\u5feb\u901f\u5730\u5904\u7406\u8fd9\u4e9b\u4e8b\u4ef6\u5462\uff1f\u663e\u7136\u8fd9\u5c31\u6d89\u53ca\u4e86concurrency\u7684\u95ee\u9898\uff0c\u5373\u5e76\u53d1\u5730\u6267\u884chandler\uff08event and concurrency\uff09 \u6240\u4ee5\u5f00\u53d1\u8005\u9700\u8981\u6839\u636e\u9700\u6c42\u9009\u62e9\u5408\u9002\u7684\u5b9e\u73b0\u65b9\u5f0f\u3002\u4e0b\u9762\u7f57\u5217\u4e00\u4e9b\u6267\u884c\u65b9\u5f0f\uff1a Single process Multiple process Single thread monitor\u548cexecutor\u4f4d\u4e8e\u540c\u4e00\u4e2a\u7ebf\u7a0b\uff0c\u8fd9\u79cd\u6bd4\u8f83\u9002\u5408event handler\u7684\u6267\u884c\u6210\u672c\u6bd4\u8f83\u5c0f\u7684\u60c5\u51b5 monitor\u548cexecutor\u5206\u522b\u5904\u4e8e\u4e24\u4e2a\u4e0d\u540c\u7684\u8fdb\u7a0b\uff0c\u8fd9\u79cd\u60c5\u51b5dispatcher\u7684\u5b9e\u73b0\u663e\u7136\u6d89\u53cainter-process communication\u3002 \u5bf9\u4e8e\u8fd9\u79cd\u60c5\u51b5\uff0c\u53ef\u4ee5\u5c06\u6574\u4f53\u770b\u505a\u662f\u4e00\u4e2aevent-driven system\uff0c\u4e5f\u53ef\u4ee5\u770b\u505a\u662f\u591a\u4e2aevent-driven system\u8fdb\u884cpipeline Multi thread monitor\u548cexecutor\u5206\u522b\u5904\u4e8e\u4e24\u4e2a\u4e0d\u540c\u7684\u7ebf\u7a0b\uff0c\u8fd9\u79cd\u60c5\u51b5dispatcher\u7684\u5b9e\u73b0\u663e\u7136\u6d89\u53ca\u5230inter-thread communication wikipedia Event-driven programming In computer programming , event-driven programming is a programming paradigm in which the flow of the program is determined by events such as user actions ( mouse clicks, key presses), sensor outputs, or messages from other programs or threads . Event-driven programming is the dominant paradigm used in graphical user interfaces and other applications (e.g., JavaScript web applications ) that are centered on performing certain actions in response to user input . This is also true of programming for device drivers (e.g., P in USB device driver stacks). In an event-driven application, there is generally a main loop that listens for events, and then triggers a callback function when one of those events is detected. In embedded systems , the same may be achieved using hardware interrupts instead of a constantly running main loop. Event-driven programs can be written in any programming language , although the task is easier in languages that provide high-level abstractions , such as await and closures . NOTE: \u7ef4\u57fa\u767e\u79d1\u7684\u8fd9\u7bc7\u6587\u7ae0\u5bf9event-driven programming\u603b\u7ed3\u5730\u975e\u5e38\u597d\u3002 Event handlers Main article: Event handler A trivial event handler Because the code for checking for events and the main loop are common amongst applications, many programming frameworks take care of their implementation and expect the user to provide only the code for the event handlers. NOTE: programming framework always do things common In this simple example there may be a call to an event handler called OnKeyEnter() that includes an argument with a string of characters, corresponding to what the user typed before hitting the ENTER key. To add two numbers, storage outside the event handler must be used. The implementation might look like below. Common uses In addition, systems such as Node.js are also event-driven. Pattern \u53c2\u89c1 ./Design-pattern \u3002 Framework/library \u53c2\u89c1 ./Library \u3002 Case study Case: Linux OS kernel \u53c2\u89c1: Linux OS kernel is event-driven TODO \u4e00\u4e9b\u5173\u4e4eevent-driven model\u7684\u5185\u5bb9\uff1a Event (computing) Event-driven programming Event-driven architecture Event handler Interrupt Interrupt-driven Interrupt handler","title":"Introduction"},{"location":"Event-driven-concurrent-server/Event-driven-model/What-is-event-driven-model/#what#is#event-driven#model","text":"","title":"What is event-driven model"},{"location":"Event-driven-concurrent-server/Event-driven-model/What-is-event-driven-model/#event-driven#model","text":"Event-driven model\u53ef\u4ee5\u770b\u505a\u662f\u4e00\u4e2a**abstract machine**: \u5b83\u9700\u8981\u6301\u7eed\u4e0d\u65ad\u5730\u8fdb\u884c\u8fd0\u8f6c\u4ee5**\u76d1\u63a7**\u4e8b\u4ef6\u6e90\u3001\u6536\u96c6event\uff0c\u4e00\u65e6\u6536\u96c6\u5230\u4e86event\uff0c\u5c31\u8fdb\u884cdispatch\uff0c\u5373\u6839\u636eevent\u548cevent handler\u4e4b\u95f4\u7684\u6620\u5c04\u5173\u7cfb\uff0c\u8c03\u7528\u8fd9\u4e2aevent\u5bf9\u5e94\u7684event handler\u3002","title":"Event-driven model\u6982\u8ff0"},{"location":"Event-driven-concurrent-server/Event-driven-model/What-is-event-driven-model/#onwhen","text":"\u53ef\u4ee5\u4f7f\u7528Event-driven model\u6765\u8fdb\u884c\u63cf\u8ff0\u7684\u95ee\u9898\u7684\u4e00\u4e2a\u5178\u578b\u7684\u6a21\u5f0f\u5c31\u662f: \" \u5f53**\u67d0\u4e2a\u4e8b\u4ef6(**event )\u53d1\u751f\u7684\u65f6\u5019\uff0c\u5c31\u6267\u884c\u67d0\u4e2a\u51fd\u6570( event handler \u3001callback)\"\u3002 \u73b0\u4ee3programming language\u5982\u4f55\u6765\u63cf\u8ff0\u8fd9\u79cd\u6a21\u5f0f\u5462\uff1f\u5bf9\u6bd4\u5730\u770b\u4e86\u4e00\u4e0b\uff0c\u73b0\u4ee3\u4e3b\u6d41programming language\u90fd\u63d0\u4f9b\u4e86**\u7c7b\u4f3c**\u7684**programming model**\u6765\u63cf\u8ff0\u8fd9\u79cd\u6a21\u5f0f\uff0c\u6bd4\u5982: \u4e00\u3001JavaScript\uff0c\u53c2\u89c1\u5de5\u7a0bprogramming\u7684 JavaScript\\Event-driven-model \u7ae0\u8282 \u4f7f\u7528\u8fd9\u4e9bprogramming model\u80fd\u591f\u8ba9programmer\u65b9\u4fbf\u5730\u6765\u63cf\u8ff0\u8fd9\u79cd\u6a21\u5f0f\u3002\u603b\u7684\u6765\u8bf4\u5b83\u4f9d\u8d56\u4e8e\u5982\u4e0bprogramming language feature: 1) First class function \u53c2\u89c1: 1) Wikipedia First-class function 2) \u5de5\u7a0bprogramming-language\u7684 Theory\\Programming-paradigm\\Functional-programming \u7ae0\u8282 2) Fluent interface \u53c2\u89c1: 1) Wikipedia Fluent interface 2) \u5de5\u7a0bprogramming-language\u7684 Theory\\Programming-paradigm\\Object-oriented-programming\\Design-pattern\\Fluent-API \u7ae0\u8282","title":"on\u3001when"},{"location":"Event-driven-concurrent-server/Event-driven-model/What-is-event-driven-model/#callback","text":"callback-based notification\u3002","title":"Callback"},{"location":"Event-driven-concurrent-server/Event-driven-model/What-is-event-driven-model/#example","text":"\u7406\u89e3event-driven model\u7684\u6700\u6700\u7b80\u5355\u7684\u4f8b\u5b50\u5c31\u662fLinux OS kernel\u548chardware\u7684\u4ea4\u4e92\uff0c\u5728\u5de5\u7a0b Linux-OS \u7684\u6587\u7ae0 Linux-OS-kernel-is-event-driven \u4e2d\u6211\u4eec\u5df2\u7ecf\u603b\u7ed3\u4e86\u53ef\u4ee5\u4f7f\u7528 Event-driven model \u6765\u63cf\u8ff0linux OS kernel\u548chardware\u7684\u4ea4\u4e92\u3002\u5728hardware\u5c42\uff0c\u4e00\u65e6\u901a\u7535\uff0c\u5219hardware\u5c31\u6301\u7eed\u4e0d\u65ad\u5730\u8fd0\u8f6c\u8d77\u6765\u4e86\uff0c\u4e00\u65e6\u89e6\u53d1\u4e86interrupt\uff0c\u5219\u5b83\u7684**interrupt handler**\u5c31\u4f1a\u88ab\u6267\u884c\uff0cLinux OS kernel\u7ef4\u62a4\u4e86interrupt\u548cinterrupt handler\u4e4b\u95f4\u7684\u6620\u5c04\u5173\u7cfb\u3002","title":"Example"},{"location":"Event-driven-concurrent-server/Event-driven-model/What-is-event-driven-model/#event-driven#modelmachinecomponent","text":"\u4e00\u4e2aevent-driven model\u7684\u7ec4\u6210\u6210\u5206\uff1a \u7ec4\u6210\u90e8\u5206 \u662f\u5426\u5fc5\u987b \u8bf4\u660e monitor/listener yes \u76d1\u63a7\u4e8b\u4ef6\u6e90\u3001\u6536\u96c6event event\u548cevent handler\u4e4b\u95f4\u7684\u6620\u5c04\u5173\u7cfb\u8868 yes message passing yes \u4f20\u9012\u4fe1\u606f\uff0cmessage queue/broker dispatcher yes \u6d3e\u53d1event\uff0c\u5373\u6309\u7167event\u548cevent handler\u4e4b\u95f4\u7684\u6620\u5c04\u5173\u7cfb\uff0c\u901a\u77e5executor\u6267\u884cevent handler executor yes \u6267\u884cevent handler","title":"Event-driven model/machine\u7684component(\u7ec4\u6210)"},{"location":"Event-driven-concurrent-server/Event-driven-model/What-is-event-driven-model/#see#also","text":"martinfowler What do you mean by \u201cEvent-Driven\u201d?","title":"See also"},{"location":"Event-driven-concurrent-server/Event-driven-model/What-is-event-driven-model/#what#is#event","text":"Event\u662f\u4e00\u4e2a\u975e\u5e38\u6982\u62ec\u3001\u5bbd\u6cdb\u7684\u6982\u5ff5\uff0c\u5728computer science\u4e2d\uff0c\u975e\u5e38\u591a\u7684\u884c\u4e3a\u90fd\u4f1a\u4ea7\u751fevent\uff0c\u6211\u4eec\u628a\u5b83\u79f0\u4e3asource of event\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06source of event\u5212\u5206\u4e3a\u4e24\u7c7b\uff1a","title":"What is event?"},{"location":"Event-driven-concurrent-server/Event-driven-model/What-is-event-driven-model/#1#external#environment","text":"\u6211\u4eec\u4ee5\u81ea\u5e95\u5411\u4e0a\u7684\u601d\u8def\u6765\u5206\u6790\u6e90\u81eaexternal environment\u7684event\uff0c\u4e00\u4e2a computing system \u7684\u6700\u5e95\u5c42\u662fhardware\uff0chardware\u4ea7\u751f\u7684 interrupt \uff0c\u7136\u540e\u7531OS kernel\u5c06\u8fd9\u4e9binterrupt\u201c\u8f6c\u6362\u201d\u4e3a**signal**\uff08\u73b0\u4ee3programming language\u4f1a\u4f7f\u7528**exception**\u6765\u62bd\u8c61**signal**\uff09\u3001IO event\uff08\u56e0\u4e3aIO\u7684\u5b9e\u73b0\u662f\u4f9d\u8d56\u4e8einterrupt\u7684\uff0cIO\u5305\u62ec\u4e86\u975e\u5e38\u591a\u7684\u5185\u5bb9\uff0c\u7528\u6237\u64cd\u4f5c\u3001\u7f51\u7edc\u901a\u4fe1\u7b49\u90fd\u53ef\u4ee5\u770b\u505a\u662fIO\uff0cevents can represent availability of new data for reading a file or network stream.\uff09\u7b49\uff0c\u5e76\u901a\u77e5\u5230application process\u3002 \u5173\u4e8e\u8fd9\u4e00\u70b9\uff0c\u53c2\u89c1\u7ef4\u57fa\u767e\u79d1 Event (computing) \u7684 Event handler \u6bb5\u3002","title":"1) external environment"},{"location":"Event-driven-concurrent-server/Event-driven-model/What-is-event-driven-model/#2","text":"Event\u53ef\u80fd\u6e90\u81ea\u4e8eexternal environment\uff0c\u4e5f\u53ef\u80fd\u6e90\u81ea\u4e8e\u7a0b\u5e8f\u4e4b\u5185\uff0c\u5373\u7a0b\u5e8f\u5185\u90e8\u5c06\u4e00\u4e9b\u6761\u4ef6\u7b49\u770b\u505aevent\uff0c\u6bd4\u5982condition variable\u3001timer\u3002 TODO: \u9700\u8981\u8865\u5145\u4e00\u4e9b\u5177\u4f53\u4f8b\u5b50\u3002 \u4e0b\u9762\u8865\u5145\u4e86\u7ef4\u57fa\u767e\u79d1 Event (computing) \u6765\u8fdb\u884c\u8be6\u7ec6\u8bf4\u660e\u3002","title":"2) \u7a0b\u5e8f\u5185\u90e8"},{"location":"Event-driven-concurrent-server/Event-driven-model/What-is-event-driven-model/#see#also_1","text":"\u7ef4\u57fa\u767e\u79d1 Event (computing)","title":"See also:"},{"location":"Event-driven-concurrent-server/Event-driven-model/What-is-event-driven-model/#implementation#of#event-driven#model","text":"\u5982\u4f55\u5b9e\u73b0event-driven model\uff1f\u524d\u9762\u5df2\u7ecf\u63cf\u8ff0\u4e86even driven model\u7684\u7ec4\u6210\uff0c\u663e\u7136\u6211\u4eec\u9700\u8981\u5206\u522b\u5b9e\u73b0\u8fd9\u4e9bcomponent\u3002\u7531\u4e8eevent-driven model\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u5b83\u5df2\u7ecf\u6210\u4e3asoftware engineering\u4e2d\u7684\u57fa\u7840\u4e86\uff0c\u5e76\u4e14\u5bf9\u4e8e\u5b83\u7814\u7a76\u5df2\u7ecf\u975e\u5e38\u6210\u719f\u4e86\uff0c\u56e0\u6b64\u76ee\u524d\u6709\u975e\u5e38\u591a\u7684\u3001\u975e\u5e38\u6210\u719f\u7684library\uff0c\u8fd9\u5728 Library \u7ae0\u8282\u8fdb\u884c\u4e86\u603b\u7ed3\u3002","title":"Implementation of event-driven model"},{"location":"Event-driven-concurrent-server/Event-driven-model/What-is-event-driven-model/#ioc","text":"\u975e\u5e38\u9002\u5408\u4f7f\u7528IoC\uff0c\u53c2\u89c1 Software-design\\Control-theory\\Inversion-of-control \u7ae0\u8282\u3002","title":"IoC\u539f\u5219"},{"location":"Event-driven-concurrent-server/Event-driven-model/What-is-event-driven-model/#monitor","text":"\u5728\u786c\u4ef6\u5c42\uff0c\u53ea\u8981\u901a\u7535\u540e\uff0c\u5219hardware\u5c31\u6301\u7eed\u4e0d\u65ad\u5730\u8fd0\u8f6c\u8d77\u6765\u4e86\uff0c\u4e00\u65e6\u89e6\u53d1\u4e86interrupt\uff0c\u5219\u5b83\u7684interrupt handler\u5c31\u4f1a\u88ab\u6267\u884c\u3002\u5728\u8f6f\u4ef6\u5c42\uff0c\u6211\u4eec\u9700\u8981\u663e\u5f0f\u5730\u4f7f\u7528\u4e00\u4e2a event loop \u6765\u6307\u793a\u6574\u4e2amodel/machine\u9700\u8981\u4e0d\u65ad\u5730\u8fd0\u8f6c\u4e0b\u53bb\u3002 \u5173\u4e8e\u8fd9\u4e00\u70b9\uff0c\u53c2\u89c1\u7ef4\u57fa\u767e\u79d1 Event-driven programming \u7684\u7b2c\u4e8c\u6bb5\u3002","title":"Monitor\uff1a\u5982\u4f55\u8fdb\u884c\u6301\u7eed\u76d1\u63a7"},{"location":"Event-driven-concurrent-server/Event-driven-model/What-is-event-driven-model/#eventevent#handler","text":"Event-driven model\u80af\u5b9a\u9700\u8981\u8bb0\u5f55\u4e0bevent\u548cevent handler\u4e4b\u95f4\u7684\u6620\u5c04\u5173\u7cfb\uff0c\u5b9e\u73b0\u8fd9\u79cd\u6620\u5c04\u5173\u7cfb\u7684\u65b9\u5f0f\u662f\u975e\u5e38\u591a\u7684\uff0c\u53ef\u4ee5\u663e\u793a\u5730\u4f7f\u7528\u8bf8\u5982map\u7684\u6570\u636e\u7ed3\u6784\u3002 \u4e0b\u9762\u7f57\u5217\u4e86\u4e00\u4e9b\u5b9e\u73b0\u6848\u4f8b\uff1a \u6848\u4f8b \u8bf4\u660e Interrupt Descriptor Table \u53c2\u89c1\uff1a - \u5de5\u7a0b Linux-OS \u7684 4.2-Interrupts-and-Exceptions \u7684 4.2.3. Interrupt Descriptor Table - \u7ef4\u57fa\u767e\u79d1 Interrupt descriptor table","title":"\u5efa\u7acbevent\u548cevent handler\u6620\u5c04\u5173\u7cfb"},{"location":"Event-driven-concurrent-server/Event-driven-model/What-is-event-driven-model/#dispatcherdemultiplexing","text":"\u6240\u8c13\u7684event dispatcher\u662f\u6307\u5f53event\u53d1\u751f\u65f6\uff0cevent-driven model\u5c06event\u4f20\u9012\u5230executor\uff0c\u901a\u77e5executor\u6267\u884c\u5bf9\u5e94\u7684handler\u3002Dispatcher\u7684\u5b9e\u73b0\u65b9\u5f0f\u662f\u6709executor\u51b3\u5b9a\u7684\uff0c \u5728\u4e0b\u4e00\u8282\u5bf9\u6b64\u8fdb\u884c\u5177\u4f53\u60c5\u51b5\u5177\u4f53\u8bf4\u660e\u3002","title":"Dispatcher/demultiplexing"},{"location":"Event-driven-concurrent-server/Event-driven-model/What-is-event-driven-model/#executor#execution#of#event#handler","text":"\u5982\u4f55\u6765\u6267\u884cevent handler\uff1f\u5728\u51b3\u5b9a\u5982\u4f55\u6765\u6267\u884cevent handler\u7684\u65f6\u5019\uff0c\u5f00\u53d1\u8005\u9700\u8981\u8003\u8651\u5982\u4e0b\u95ee\u9898\uff1a 1) event handler\u6267\u884c\u7684\u6210\u672c\uff0c\u6b64\u5904\u7684\u6210\u672c\u53ef\u4ee5\u6709\u591a\u79cd\u89e3\u91ca\uff0c\u6bd4\u5982\uff0c\u5b83\u53ef\u4ee5\u8868\u793aevent handler\u6267\u884c\u7684\u65f6\u957f\u3001\u53ef\u4ee5\u8868\u793aevent handler\u6267\u884c\u7684\u8d44\u6e90\u8017\u8d39 2) \u5e76\u53d1\u6027\uff0c\u540c\u65f6\u53d1\u751f\u7684\u4e8b\u4ef6\u53d1\u751f\u53ef\u80fd\u591a\uff0c\u5982\u4f55\u5feb\u901f\u5730\u5904\u7406\u8fd9\u4e9b\u4e8b\u4ef6\u5462\uff1f\u663e\u7136\u8fd9\u5c31\u6d89\u53ca\u4e86concurrency\u7684\u95ee\u9898\uff0c\u5373\u5e76\u53d1\u5730\u6267\u884chandler\uff08event and concurrency\uff09 \u6240\u4ee5\u5f00\u53d1\u8005\u9700\u8981\u6839\u636e\u9700\u6c42\u9009\u62e9\u5408\u9002\u7684\u5b9e\u73b0\u65b9\u5f0f\u3002\u4e0b\u9762\u7f57\u5217\u4e00\u4e9b\u6267\u884c\u65b9\u5f0f\uff1a Single process Multiple process Single thread monitor\u548cexecutor\u4f4d\u4e8e\u540c\u4e00\u4e2a\u7ebf\u7a0b\uff0c\u8fd9\u79cd\u6bd4\u8f83\u9002\u5408event handler\u7684\u6267\u884c\u6210\u672c\u6bd4\u8f83\u5c0f\u7684\u60c5\u51b5 monitor\u548cexecutor\u5206\u522b\u5904\u4e8e\u4e24\u4e2a\u4e0d\u540c\u7684\u8fdb\u7a0b\uff0c\u8fd9\u79cd\u60c5\u51b5dispatcher\u7684\u5b9e\u73b0\u663e\u7136\u6d89\u53cainter-process communication\u3002 \u5bf9\u4e8e\u8fd9\u79cd\u60c5\u51b5\uff0c\u53ef\u4ee5\u5c06\u6574\u4f53\u770b\u505a\u662f\u4e00\u4e2aevent-driven system\uff0c\u4e5f\u53ef\u4ee5\u770b\u505a\u662f\u591a\u4e2aevent-driven system\u8fdb\u884cpipeline Multi thread monitor\u548cexecutor\u5206\u522b\u5904\u4e8e\u4e24\u4e2a\u4e0d\u540c\u7684\u7ebf\u7a0b\uff0c\u8fd9\u79cd\u60c5\u51b5dispatcher\u7684\u5b9e\u73b0\u663e\u7136\u6d89\u53ca\u5230inter-thread communication","title":"Executor: execution of event handler"},{"location":"Event-driven-concurrent-server/Event-driven-model/What-is-event-driven-model/#wikipedia#event-driven#programming","text":"In computer programming , event-driven programming is a programming paradigm in which the flow of the program is determined by events such as user actions ( mouse clicks, key presses), sensor outputs, or messages from other programs or threads . Event-driven programming is the dominant paradigm used in graphical user interfaces and other applications (e.g., JavaScript web applications ) that are centered on performing certain actions in response to user input . This is also true of programming for device drivers (e.g., P in USB device driver stacks). In an event-driven application, there is generally a main loop that listens for events, and then triggers a callback function when one of those events is detected. In embedded systems , the same may be achieved using hardware interrupts instead of a constantly running main loop. Event-driven programs can be written in any programming language , although the task is easier in languages that provide high-level abstractions , such as await and closures . NOTE: \u7ef4\u57fa\u767e\u79d1\u7684\u8fd9\u7bc7\u6587\u7ae0\u5bf9event-driven programming\u603b\u7ed3\u5730\u975e\u5e38\u597d\u3002","title":"wikipedia Event-driven programming"},{"location":"Event-driven-concurrent-server/Event-driven-model/What-is-event-driven-model/#event#handlers","text":"Main article: Event handler","title":"Event handlers"},{"location":"Event-driven-concurrent-server/Event-driven-model/What-is-event-driven-model/#a#trivial#event#handler","text":"Because the code for checking for events and the main loop are common amongst applications, many programming frameworks take care of their implementation and expect the user to provide only the code for the event handlers. NOTE: programming framework always do things common In this simple example there may be a call to an event handler called OnKeyEnter() that includes an argument with a string of characters, corresponding to what the user typed before hitting the ENTER key. To add two numbers, storage outside the event handler must be used. The implementation might look like below.","title":"A trivial event handler"},{"location":"Event-driven-concurrent-server/Event-driven-model/What-is-event-driven-model/#common#uses","text":"In addition, systems such as Node.js are also event-driven.","title":"Common uses"},{"location":"Event-driven-concurrent-server/Event-driven-model/What-is-event-driven-model/#pattern","text":"\u53c2\u89c1 ./Design-pattern \u3002","title":"Pattern"},{"location":"Event-driven-concurrent-server/Event-driven-model/What-is-event-driven-model/#frameworklibrary","text":"\u53c2\u89c1 ./Library \u3002","title":"Framework/library"},{"location":"Event-driven-concurrent-server/Event-driven-model/What-is-event-driven-model/#case#study","text":"","title":"Case study"},{"location":"Event-driven-concurrent-server/Event-driven-model/What-is-event-driven-model/#case#linux#os#kernel","text":"\u53c2\u89c1: Linux OS kernel is event-driven","title":"Case: Linux OS kernel"},{"location":"Event-driven-concurrent-server/Event-driven-model/What-is-event-driven-model/#todo","text":"\u4e00\u4e9b\u5173\u4e4eevent-driven model\u7684\u5185\u5bb9\uff1a Event (computing) Event-driven programming Event-driven architecture Event handler Interrupt Interrupt-driven Interrupt handler","title":"TODO"},{"location":"Event-driven-concurrent-server/Event-driven-model/What-is-event-driven-model/Monitor-notify-dispatch/","text":"Monitor\u3001notify\u3001dispatch \u5e95\u5c42OS\u63d0\u4f9b\u4e86notification mechanism(\u6bd4\u5982IO multiplexing) Monitor\u4e0d\u65ad\u5730\u76d1\u63a7\uff0c\u4e00\u65e6\u53d7\u5230\u4e86notification\uff0c\u5c31\u6309\u7167\"event\u548cevent handler\u4e4b\u95f4\u7684\u6620\u5c04\u5173\u7cfb\u8868\"\u8fdb\u884cdispatch\u3002 Notification Linux \u63d0\u4f9b\u4e86\u5f88\u591a\u7684notification: 1\u3001IO multiplexing 2\u3001condition variable\u4e5f\u662f\u4e00\u79cdnotification\uff0c\u5b83\u7528\u4e8e\u8ba9programmer\u81ea\u5b9a\u4e49notification\u3002 3\u3001Semaphore 4\u3001Observer pattern\u4e5f\u662f\u4e00\u79cdnotification\u3002 \u5178\u578b\u7684notification\u65b9\u5f0f\u662fcallback\u3002","title":"Introduction"},{"location":"Event-driven-concurrent-server/Event-driven-model/What-is-event-driven-model/Monitor-notify-dispatch/#monitornotifydispatch","text":"\u5e95\u5c42OS\u63d0\u4f9b\u4e86notification mechanism(\u6bd4\u5982IO multiplexing) Monitor\u4e0d\u65ad\u5730\u76d1\u63a7\uff0c\u4e00\u65e6\u53d7\u5230\u4e86notification\uff0c\u5c31\u6309\u7167\"event\u548cevent handler\u4e4b\u95f4\u7684\u6620\u5c04\u5173\u7cfb\u8868\"\u8fdb\u884cdispatch\u3002","title":"Monitor\u3001notify\u3001dispatch"},{"location":"Event-driven-concurrent-server/Event-driven-model/What-is-event-driven-model/Monitor-notify-dispatch/#notification","text":"Linux \u63d0\u4f9b\u4e86\u5f88\u591a\u7684notification: 1\u3001IO multiplexing 2\u3001condition variable\u4e5f\u662f\u4e00\u79cdnotification\uff0c\u5b83\u7528\u4e8e\u8ba9programmer\u81ea\u5b9a\u4e49notification\u3002 3\u3001Semaphore 4\u3001Observer pattern\u4e5f\u662f\u4e00\u79cdnotification\u3002 \u5178\u578b\u7684notification\u65b9\u5f0f\u662fcallback\u3002","title":"Notification"},{"location":"Event-driven-concurrent-server/Event-library/","text":"Event library \u975e\u5e38\u591a\u7684\u6210\u719f\u7684\u6846\u67b6\uff0c\u8fdb\u884c\u4e86\u975e\u5e38\u62bd\u8c61\u3001\u5c01\u88c5\uff0c\u6700\u7ec8user\u53ea\u9700\u8981\u6307\u5b9aevent\u548cevent handler\u4e4b\u95f4\u7684\u6620\u5c04\u5173\u7cfb\u5373\u53ef\u3002\u6bd4\u5982flask\u3001libevent\u3001libuv\u3002 web framework\u90fd\u53ef\u4ee5\u770b\u505a\u662fevent-driven model\u7684\u5b9e\u73b0\u3002 Python Eventlet gevent C\u3001C++ 1\u3001\u90fd\u662f\u57fa\u4e8eIO-multiplexing 2\u3001\u90fd\u662fasynchronous\u3001callback based notifications of IO \u6709\u975e\u5e38\u591a\u7684\u57fa\u4e8eevent loop\u6a21\u578b\u7684software 1\u3001 redis 2\u3001 libevent 3\u3001 libuv libuv is a multi-platform support library with a focus on asynchronous I/O. JavaScript 1\u3001 node.js 2\u3001 JavaScript Java Netty Netty is an asynchronous event-driven network application framework for rapid development of maintainable high performance protocol servers & clients.","title":"Introduction"},{"location":"Event-driven-concurrent-server/Event-library/#event#library","text":"\u975e\u5e38\u591a\u7684\u6210\u719f\u7684\u6846\u67b6\uff0c\u8fdb\u884c\u4e86\u975e\u5e38\u62bd\u8c61\u3001\u5c01\u88c5\uff0c\u6700\u7ec8user\u53ea\u9700\u8981\u6307\u5b9aevent\u548cevent handler\u4e4b\u95f4\u7684\u6620\u5c04\u5173\u7cfb\u5373\u53ef\u3002\u6bd4\u5982flask\u3001libevent\u3001libuv\u3002 web framework\u90fd\u53ef\u4ee5\u770b\u505a\u662fevent-driven model\u7684\u5b9e\u73b0\u3002","title":"Event library"},{"location":"Event-driven-concurrent-server/Event-library/#python","text":"Eventlet gevent","title":"Python"},{"location":"Event-driven-concurrent-server/Event-library/#cc","text":"1\u3001\u90fd\u662f\u57fa\u4e8eIO-multiplexing 2\u3001\u90fd\u662fasynchronous\u3001callback based notifications of IO \u6709\u975e\u5e38\u591a\u7684\u57fa\u4e8eevent loop\u6a21\u578b\u7684software 1\u3001 redis 2\u3001 libevent 3\u3001 libuv libuv is a multi-platform support library with a focus on asynchronous I/O.","title":"C\u3001C++"},{"location":"Event-driven-concurrent-server/Event-library/#javascript","text":"1\u3001 node.js 2\u3001 JavaScript","title":"JavaScript"},{"location":"Event-driven-concurrent-server/Event-library/#java","text":"","title":"Java"},{"location":"Event-driven-concurrent-server/Event-library/#netty","text":"Netty is an asynchronous event-driven network application framework for rapid development of maintainable high performance protocol servers & clients.","title":"Netty"},{"location":"Event-driven-concurrent-server/Event-library/library-Netty/","text":"Netty Netty project Netty is an asynchronous event-driven network application framework for rapid development of maintainable high performance protocol servers & clients. NOTE: 1\u3001\u611f\u89c9\u5b83\u548clibuv\u6709\u70b9\u50cf github netty / netty Netty project - an event-driven asynchronous network application framework TODO zhihu \u6ca1\u5403\u900fNetty-ChanneL\u67b6\u6784\u4f53\u7cfb\uff0c\u8fd8\u80fd\u7b97\u5f97\u4e0aC++\u8001\u53f8\u673a\uff1f zhihu \u901a\u4fd7\u5730\u8bb2\uff0cNetty \u80fd\u505a\u4ec0\u4e48\uff1f","title":"Introduction"},{"location":"Event-driven-concurrent-server/Event-library/library-Netty/#netty","text":"","title":"Netty"},{"location":"Event-driven-concurrent-server/Event-library/library-Netty/#netty#project","text":"Netty is an asynchronous event-driven network application framework for rapid development of maintainable high performance protocol servers & clients. NOTE: 1\u3001\u611f\u89c9\u5b83\u548clibuv\u6709\u70b9\u50cf","title":"Netty project"},{"location":"Event-driven-concurrent-server/Event-library/library-Netty/#github#nettynetty","text":"Netty project - an event-driven asynchronous network application framework","title":"github netty/netty"},{"location":"Event-driven-concurrent-server/Event-library/library-Netty/#todo","text":"zhihu \u6ca1\u5403\u900fNetty-ChanneL\u67b6\u6784\u4f53\u7cfb\uff0c\u8fd8\u80fd\u7b97\u5f97\u4e0aC++\u8001\u53f8\u673a\uff1f zhihu \u901a\u4fd7\u5730\u8bb2\uff0cNetty \u80fd\u505a\u4ec0\u4e48\uff1f","title":"TODO"},{"location":"Event-driven-concurrent-server/Event-library/library-libevent/","text":"libevent \u2013 an event notification library The libevent API provides a mechanism to execute a callback function when a specific event occurs on a file descriptor or after a timeout has been reached. Furthermore, libevent also support callbacks due to signals or regular timeouts . libevent is meant to replace the event loop found in event driven network servers . An application just needs to call event_dispatch() and then add or remove events dynamically without having to change the event loop . NOTE: For event driven network servers , please refer to this article . Currently, libevent supports /dev/poll , kqueue(2) , event ports , POSIX select(2) , Windows select() , poll(2) , and epoll(4) . The internal event mechanism is completely independent of the exposed event API, and a simple update of libevent can provide new functionality without having to redesign the applications. As a result, libevent allows for portable application development and provides the most scalable event notification mechanism available on an operating system. NOTE: obviously,the implementation of libevent is very good,which conforms to the idea of interface-oriented programming. Libevent can also be used for multi-threaded applications, either by isolating each event_base so that only a single thread accesses it, or by locked access to a single shared event_base . libevent should compile on Linux, *BSD , Mac OS X, Solaris, Windows, and more. Libevent additionally provides a sophisticated framework for buffered network IO, with support for sockets, filters, rate-limiting, SSL , zero-copy file transmission, and IOCP . Libevent includes support for several useful protocols, including DNS , HTTP, and a minimal RPC framework. More information about event notification mechanisms for network servers can be found on Dan Kegel's \" The C10K problem \" web page.","title":"Introduction"},{"location":"Event-driven-concurrent-server/Event-library/library-libevent/#libevent#an#event#notification#library","text":"The libevent API provides a mechanism to execute a callback function when a specific event occurs on a file descriptor or after a timeout has been reached. Furthermore, libevent also support callbacks due to signals or regular timeouts . libevent is meant to replace the event loop found in event driven network servers . An application just needs to call event_dispatch() and then add or remove events dynamically without having to change the event loop . NOTE: For event driven network servers , please refer to this article . Currently, libevent supports /dev/poll , kqueue(2) , event ports , POSIX select(2) , Windows select() , poll(2) , and epoll(4) . The internal event mechanism is completely independent of the exposed event API, and a simple update of libevent can provide new functionality without having to redesign the applications. As a result, libevent allows for portable application development and provides the most scalable event notification mechanism available on an operating system. NOTE: obviously,the implementation of libevent is very good,which conforms to the idea of interface-oriented programming. Libevent can also be used for multi-threaded applications, either by isolating each event_base so that only a single thread accesses it, or by locked access to a single shared event_base . libevent should compile on Linux, *BSD , Mac OS X, Solaris, Windows, and more. Libevent additionally provides a sophisticated framework for buffered network IO, with support for sockets, filters, rate-limiting, SSL , zero-copy file transmission, and IOCP . Libevent includes support for several useful protocols, including DNS , HTTP, and a minimal RPC framework. More information about event notification mechanisms for network servers can be found on Dan Kegel's \" The C10K problem \" web page.","title":"libevent \u2013 an event notification library"},{"location":"Event-driven-concurrent-server/Event-library/library-libuv/","text":"Welcome to the libuv documentation \u00b6 Overview \u00b6 libuv is a multi-platform support library with a focus on asynchronous I/O. It was primarily developed for use by Node.js , but it\u2019s also used by Luvit , Julia , pyuv , and others . NOTE: 1\u3001\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u4e0a\u8ff0\"asynchronous IO\"\uff0c\u5e76\u4e0d\u662f\u5728\u5de5\u7a0b Linux-OS \u7684 Programming\\IO\\IO-model \u7ae0\u8282\u4e2d\u7684asynchronous IO\uff0c\u4ece\u4e0b\u9762\u7684\u63cf\u8ff0\u6765\u770b\uff0clibuv \u4e5f\u662f\u4f7f\u7528\u7684 IO-multiplexing\uff0c\u4ece\u51c6\u786e\u7684IO-multiplexing\u7684\u5b9e\u73b0\u6765\u770b\uff0c\u5b83\u662f\u91c7\u7528\u7684non-blocking IO\uff0c\u5173\u4e8e\u6b64\uff0c\u5728 Design overview \u00b6 \u4e2d\u4e5f\u8fdb\u884c\u4e86\u8bf4\u660e: The event loop follows the rather usual single threaded asynchronous I/O approach : all (network) I/O is performed on non-blocking sockets which are polled using the best mechanism available on the given platform Features \u00b6 1\u3001Full-featured event loop backed by epoll, kqueue, IOCP, event ports. NOTE: 1\u3001I have seen the event loop in many places, including libevent , celery 2\u3001Asynchronous TCP and UDP sockets NOTE: 1\u3001\u66f4\u52a0\u51c6\u786e\u5730\u6765\u8bf4\uff0c\u662fnon-blocking IO 2\u3001\u8fd9\u662fnetwork IO 3\u3001Asynchronous DNS resolution 4\u3001Asynchronous file and file system operations NOTE: 1\u3001\u5982\u4f55\u5b9e\u73b0\u7684\uff1f 2\u3001\u8fd9\u662ffile-IO 5\u3001File system events 6\u3001ANSI escape code controlled TTY 7\u3001IPC with socket sharing, using Unix domain sockets or named pipes (Windows) 8\u3001Child processes NOTE: 1\u3001process\u529f\u80fd 9\u3001Thread pool 10\u3001Signal handling 11\u3001High resolution clock NOTE: \u5173\u4e8e\u5b83\u7684\u5b9e\u73b0\uff0c\u53c2\u89c1 : 1\u3001Design overview \u00b6 # The I/O loop 12\u3001Threading and synchronization primitives NOTE: 1\u3001\u4ece\u4e0a\u9762\u7684\u63cf\u8ff0\u53ef\u4ee5\u770b\u51fa\uff0clibuv\u5bf9\u5f88\u591a\u529f\u80fd\u8fdb\u884c\u4e86\u975e\u5e38\u597d\u7684 \u5c01\u88c5 stackoverflow Single Threaded Event Loop vs Multi Threaded Non Blocking Worker in Node.JS Node.JS biggest advantage is it's non blocking nature. It's single threaded, so it doesn't need to spawn a new thread for each new incoming connection. Behind the event-loop (which is in fact single threaded ), there is a \"Non blocking Worker\". This thing is not single threaded anymore, so (as far as I understood) it can spawn a new thread for each task. Maybe I misunderstood something, but where exactly is the advantage. If there are to many tasks to handle, wouldn't be the Non Blocking Working turn into a Blocking Worker? Thanks Christian A You need to read about libuv , the \"magic\" behind node's non-blocking I/O. The important thing to take away from the libuv book is that libuv uses the host OS's asynchronous I/O facilities ; it does not simply create a new thread for every connection. libuv tells the OS that it would like to know about any changes (connection state, data received, etc) that happen on a particular set of sockets . It is then up to the OS to deal with managing the connections. The OS itself may create one or more threads to accomplish that, but that's not our concern. (And it certainly won't create a thread for every connection.) For other types of operations like calls to C libraries that may be computationally expensive (ie crypto), libuv provides a thread pool on which those operations may run. Since it is a thread pool, again you don't have to worry about thread count growing without bound. When the pool is fully busy, operations are queued. So yes, JavaScript runs on a single thread . Yes, node (via libuv) spawns many threads in the background to do work so that it need not block the JavaScript thread. However, the thread count is always controlled, and I/O generally doesn't even get its own node-allocated thread because that's handled by the OS. A Alright, let's break this down a bit. Single threaded applications have advantages: you can never get deadlocks or race conditions. These issues stem from simultaneous memory access in multi-threaded systems. If two threads access the same piece of information weird things can happen. So why does JavaScript have Workers? If you need do some heavy processing you're going to block the event loop, you could try to split up the workload by generating timer events but that's tedious. A Worker allows you to spawn a thread under one condition: no shared memory access . This solves the issue of heavy processing in a single threaded environment while avoiding the pitfalls of multi-threaded environments (deadlocks, race-conditions). And as @dandavis said, if you have a multi-core CPU (which everyone does these days) the Worker threads can be offloaded to the other cores. You have to appreciate that, although JavaScript is single threaded, the environment around it is still very much multi-threaded. Reading a file is non-blocking in Node.JS but there is likely a thread to support it in the OS. As a minor addendum I would say that Node.JS's biggest advantage is that it allows you to write JavaScript on the server, which allows you to share code between the client and the server. The fact that it's non-blocking is nice but threads already solve that. The non-blocking IO stems from the single threaded-ness. It's very inconvenient to have a single thread with blocking IO.","title":"Introduction"},{"location":"Event-driven-concurrent-server/Event-library/library-libuv/#welcome#to#the#libuv#documentation","text":"","title":"Welcome to the libuv documentation\u00b6"},{"location":"Event-driven-concurrent-server/Event-library/library-libuv/#overview","text":"libuv is a multi-platform support library with a focus on asynchronous I/O. It was primarily developed for use by Node.js , but it\u2019s also used by Luvit , Julia , pyuv , and others . NOTE: 1\u3001\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u4e0a\u8ff0\"asynchronous IO\"\uff0c\u5e76\u4e0d\u662f\u5728\u5de5\u7a0b Linux-OS \u7684 Programming\\IO\\IO-model \u7ae0\u8282\u4e2d\u7684asynchronous IO\uff0c\u4ece\u4e0b\u9762\u7684\u63cf\u8ff0\u6765\u770b\uff0clibuv \u4e5f\u662f\u4f7f\u7528\u7684 IO-multiplexing\uff0c\u4ece\u51c6\u786e\u7684IO-multiplexing\u7684\u5b9e\u73b0\u6765\u770b\uff0c\u5b83\u662f\u91c7\u7528\u7684non-blocking IO\uff0c\u5173\u4e8e\u6b64\uff0c\u5728 Design overview \u00b6 \u4e2d\u4e5f\u8fdb\u884c\u4e86\u8bf4\u660e: The event loop follows the rather usual single threaded asynchronous I/O approach : all (network) I/O is performed on non-blocking sockets which are polled using the best mechanism available on the given platform","title":"Overview\u00b6"},{"location":"Event-driven-concurrent-server/Event-library/library-libuv/#features","text":"1\u3001Full-featured event loop backed by epoll, kqueue, IOCP, event ports. NOTE: 1\u3001I have seen the event loop in many places, including libevent , celery 2\u3001Asynchronous TCP and UDP sockets NOTE: 1\u3001\u66f4\u52a0\u51c6\u786e\u5730\u6765\u8bf4\uff0c\u662fnon-blocking IO 2\u3001\u8fd9\u662fnetwork IO 3\u3001Asynchronous DNS resolution 4\u3001Asynchronous file and file system operations NOTE: 1\u3001\u5982\u4f55\u5b9e\u73b0\u7684\uff1f 2\u3001\u8fd9\u662ffile-IO 5\u3001File system events 6\u3001ANSI escape code controlled TTY 7\u3001IPC with socket sharing, using Unix domain sockets or named pipes (Windows) 8\u3001Child processes NOTE: 1\u3001process\u529f\u80fd 9\u3001Thread pool 10\u3001Signal handling 11\u3001High resolution clock NOTE: \u5173\u4e8e\u5b83\u7684\u5b9e\u73b0\uff0c\u53c2\u89c1 : 1\u3001Design overview \u00b6 # The I/O loop 12\u3001Threading and synchronization primitives NOTE: 1\u3001\u4ece\u4e0a\u9762\u7684\u63cf\u8ff0\u53ef\u4ee5\u770b\u51fa\uff0clibuv\u5bf9\u5f88\u591a\u529f\u80fd\u8fdb\u884c\u4e86\u975e\u5e38\u597d\u7684 \u5c01\u88c5","title":"Features\u00b6"},{"location":"Event-driven-concurrent-server/Event-library/library-libuv/#stackoverflow#single#threaded#event#loop#vs#multi#threaded#non#blocking#worker#in#nodejs","text":"Node.JS biggest advantage is it's non blocking nature. It's single threaded, so it doesn't need to spawn a new thread for each new incoming connection. Behind the event-loop (which is in fact single threaded ), there is a \"Non blocking Worker\". This thing is not single threaded anymore, so (as far as I understood) it can spawn a new thread for each task. Maybe I misunderstood something, but where exactly is the advantage. If there are to many tasks to handle, wouldn't be the Non Blocking Working turn into a Blocking Worker? Thanks Christian","title":"stackoverflow Single Threaded Event Loop vs Multi Threaded Non Blocking Worker in Node.JS"},{"location":"Event-driven-concurrent-server/Event-library/library-libuv/#a","text":"You need to read about libuv , the \"magic\" behind node's non-blocking I/O. The important thing to take away from the libuv book is that libuv uses the host OS's asynchronous I/O facilities ; it does not simply create a new thread for every connection. libuv tells the OS that it would like to know about any changes (connection state, data received, etc) that happen on a particular set of sockets . It is then up to the OS to deal with managing the connections. The OS itself may create one or more threads to accomplish that, but that's not our concern. (And it certainly won't create a thread for every connection.) For other types of operations like calls to C libraries that may be computationally expensive (ie crypto), libuv provides a thread pool on which those operations may run. Since it is a thread pool, again you don't have to worry about thread count growing without bound. When the pool is fully busy, operations are queued. So yes, JavaScript runs on a single thread . Yes, node (via libuv) spawns many threads in the background to do work so that it need not block the JavaScript thread. However, the thread count is always controlled, and I/O generally doesn't even get its own node-allocated thread because that's handled by the OS.","title":"A"},{"location":"Event-driven-concurrent-server/Event-library/library-libuv/#a_1","text":"Alright, let's break this down a bit. Single threaded applications have advantages: you can never get deadlocks or race conditions. These issues stem from simultaneous memory access in multi-threaded systems. If two threads access the same piece of information weird things can happen. So why does JavaScript have Workers? If you need do some heavy processing you're going to block the event loop, you could try to split up the workload by generating timer events but that's tedious. A Worker allows you to spawn a thread under one condition: no shared memory access . This solves the issue of heavy processing in a single threaded environment while avoiding the pitfalls of multi-threaded environments (deadlocks, race-conditions). And as @dandavis said, if you have a multi-core CPU (which everyone does these days) the Worker threads can be offloaded to the other cores. You have to appreciate that, although JavaScript is single threaded, the environment around it is still very much multi-threaded. Reading a file is non-blocking in Node.JS but there is likely a thread to support it in the OS. As a minor addendum I would say that Node.JS's biggest advantage is that it allows you to write JavaScript on the server, which allows you to share code between the client and the server. The fact that it's non-blocking is nice but threads already solve that. The non-blocking IO stems from the single threaded-ness. It's very inconvenient to have a single thread with blocking IO.","title":"A"},{"location":"Event-driven-concurrent-server/Event-library/library-libuv/Design-overview/","text":"Design overview \u00b6 libuv is cross-platform support library which was originally written for NodeJS . It\u2019s designed around the event-driven asynchronous I/O model . The library provides much more than a simple abstraction over different I/O polling mechanisms : \u2018handles\u2019 and \u2018streams\u2019 provide a high level abstraction for sockets and other entities; cross-platform file I/O and threading functionality is also provided, amongst other things. Here is a diagram illustrating the different parts that compose libuv and what subsystem they relate to: The I/O loop The I/O (or event) loop is the central part of libuv . It establishes the content for all I/O operations, and it\u2019s meant to be tied to a single thread . One can run multiple event loops as long as each runs in a different thread . The libuv event loop (or any other API involving the loop or handles, for that matter) is not thread-safe except where stated otherwise(\u9664\u975e\u53e6\u6709\u8bf4\u660e\uff0c\u5426\u5219libuv\u4e8b\u4ef6\u5faa\u73af\uff08\u6216\u6d89\u53ca\u5faa\u73af\u6216\u53e5\u67c4\u7684\u4efb\u4f55\u5176\u4ed6API\uff09\u90fd\u4e0d\u662f\u7ebf\u7a0b\u5b89\u5168\u7684). The event loop follows the rather usual single threaded asynchronous I/O approach : all (network) I/O is performed on non-blocking sockets which are polled using the best mechanism available on the given platform: epoll on Linux, kqueue on OSX and other BSDs, event ports on SunOS and IOCP on Windows. As part of a loop iteration the loop will block waiting for I/O activity on sockets which have been added to the poller and callbacks will be fired indicating socket conditions (readable, writable hangup) so handles can read, write or perform the desired I/O operation. NOTE: 1\u3001\u6700\u540e\u4e00\u6bb5\u8bdd\u8bf4\u660e: callback\u662f\u5728IO loop thread\u4e2d\u6267\u884c\u7684\uff0c\u5bf9\u5e94\u7684\u662f\u4e0b\u9762\u7684\u56fe\u4e2d\u7684\"call pending callback\" In order to better understand how the event loop operates, the following diagram illustrates all stages of a loop iteration: 1\u3001The loop concept of \u2018now\u2019 is updated. The event loop caches the current time at the start of the event loop tick in order to reduce the number of time-related system calls. NOTE: 1\u3001\u8fd9\u6837\u505a\u7684\u76ee\u7684\u662f\u4ec0\u4e48\uff1f\u662f\u4e3a\u4e86\u5b9e\u73b0 \"High resolution clock\"\uff1f 2\u3001If the loop is alive an iteration is started, otherwise the loop will exit immediately. So, when is a loop considered to be alive ? If a loop has active and ref\u2019d handles, active requests or closing handles it\u2019s considered to be alive . NOTE: ref\u2019d \u53c2\u8003\u7684\uff08referenced\uff09 3\u3001 Due timers are run. All active timers scheduled for a time before the loop\u2019s concept of now get their callbacks called. NOTE:due timer \u53ef\u4ee5\u7406\u89e3\u4e3a\u201c\u5230\u671f\u8ba1\u6570\u5668\u201d\uff1b\u7b2c\u4e8c\u53e5\u8bdd\u7684\u4e3b\u5e72\u662f\uff1aAll active timers get their callbacks called. 3\u3001 Pending callbacks are called. All I/O callbacks are called right after polling for I/O, for the most part. There are cases, however, in which calling such a callback is deferred for the next loop iteration. If the previous iteration deferred any I/O callback it will be run at this point. 4\u3001 Idle handle callbacks are called. Despite the unfortunate name, idle handles are run on every loop iteration, if they are active. 5\u3001 Prepare handle callbacks are called. Prepare handles get their callbacks called right before the loop will block for I/O. 6\u3001Poll timeout is calculated. Before blocking for I/O the loop calculates for how long it should block. These are the rules when calculating the timeout: NOTE: 1\u3001\u5178\u578b\u7684\" block with timeout-system call with timeout\" If the loop was run with the UV_RUN_NOWAIT flag, the timeout is 0. If the loop is going to be stopped ( uv_stop() was called), the timeout is 0. If there are no active handles or requests, the timeout is 0. If there are any idle handles active, the timeout is 0. If there are any handles pending to be closed, the timeout is 0. If none of the above cases matches, the timeout of the closest timer is taken, or if there are no active timers, infinity. Mock asynchronous file I/O libuv uses a thread pool to make asynchronous file I/O operations possible, but network I/O is always performed in a single thread, each loop\u2019s thread. NOTE: 1\u3001\u4e0b\u9762\u4f1a\u8fdb\u884c\u8bf4\u660e Note While the polling mechanism is different, libuv makes the execution model consistent across Unix systems and Windows. File I/O NOTE: 1\u3001\u8fd9\u662f\u5178\u578b\u7684asynchronous method invocation(sync to async)\uff0c\u5b83\u7684\u5b9e\u73b0\u65b9\u5f0f\u662f: thread pool + queue Unlike network I/O, there are no platform-specific file I/O primitives libuv could rely on, so the current approach is to run blocking file I/O operations in a thread pool. For a thorough explanation of the cross-platform file I/O landscape, checkout this post . libuv currently uses a global thread pool on which all loops can queue work. 3 types of operations are currently run on this pool: File system operations DNS functions (getaddrinfo and getnameinfo) User specified code via uv_queue_work() Warning See the Thread pool work scheduling section for more details, but keep in mind the thread pool size is quite limited.","title":"Introduction"},{"location":"Event-driven-concurrent-server/Event-library/library-libuv/Design-overview/#design#overview","text":"libuv is cross-platform support library which was originally written for NodeJS . It\u2019s designed around the event-driven asynchronous I/O model . The library provides much more than a simple abstraction over different I/O polling mechanisms : \u2018handles\u2019 and \u2018streams\u2019 provide a high level abstraction for sockets and other entities; cross-platform file I/O and threading functionality is also provided, amongst other things. Here is a diagram illustrating the different parts that compose libuv and what subsystem they relate to:","title":"Design overview\u00b6"},{"location":"Event-driven-concurrent-server/Event-library/library-libuv/Design-overview/#the#io#loop","text":"The I/O (or event) loop is the central part of libuv . It establishes the content for all I/O operations, and it\u2019s meant to be tied to a single thread . One can run multiple event loops as long as each runs in a different thread . The libuv event loop (or any other API involving the loop or handles, for that matter) is not thread-safe except where stated otherwise(\u9664\u975e\u53e6\u6709\u8bf4\u660e\uff0c\u5426\u5219libuv\u4e8b\u4ef6\u5faa\u73af\uff08\u6216\u6d89\u53ca\u5faa\u73af\u6216\u53e5\u67c4\u7684\u4efb\u4f55\u5176\u4ed6API\uff09\u90fd\u4e0d\u662f\u7ebf\u7a0b\u5b89\u5168\u7684). The event loop follows the rather usual single threaded asynchronous I/O approach : all (network) I/O is performed on non-blocking sockets which are polled using the best mechanism available on the given platform: epoll on Linux, kqueue on OSX and other BSDs, event ports on SunOS and IOCP on Windows. As part of a loop iteration the loop will block waiting for I/O activity on sockets which have been added to the poller and callbacks will be fired indicating socket conditions (readable, writable hangup) so handles can read, write or perform the desired I/O operation. NOTE: 1\u3001\u6700\u540e\u4e00\u6bb5\u8bdd\u8bf4\u660e: callback\u662f\u5728IO loop thread\u4e2d\u6267\u884c\u7684\uff0c\u5bf9\u5e94\u7684\u662f\u4e0b\u9762\u7684\u56fe\u4e2d\u7684\"call pending callback\" In order to better understand how the event loop operates, the following diagram illustrates all stages of a loop iteration: 1\u3001The loop concept of \u2018now\u2019 is updated. The event loop caches the current time at the start of the event loop tick in order to reduce the number of time-related system calls. NOTE: 1\u3001\u8fd9\u6837\u505a\u7684\u76ee\u7684\u662f\u4ec0\u4e48\uff1f\u662f\u4e3a\u4e86\u5b9e\u73b0 \"High resolution clock\"\uff1f 2\u3001If the loop is alive an iteration is started, otherwise the loop will exit immediately. So, when is a loop considered to be alive ? If a loop has active and ref\u2019d handles, active requests or closing handles it\u2019s considered to be alive . NOTE: ref\u2019d \u53c2\u8003\u7684\uff08referenced\uff09 3\u3001 Due timers are run. All active timers scheduled for a time before the loop\u2019s concept of now get their callbacks called. NOTE:due timer \u53ef\u4ee5\u7406\u89e3\u4e3a\u201c\u5230\u671f\u8ba1\u6570\u5668\u201d\uff1b\u7b2c\u4e8c\u53e5\u8bdd\u7684\u4e3b\u5e72\u662f\uff1aAll active timers get their callbacks called. 3\u3001 Pending callbacks are called. All I/O callbacks are called right after polling for I/O, for the most part. There are cases, however, in which calling such a callback is deferred for the next loop iteration. If the previous iteration deferred any I/O callback it will be run at this point. 4\u3001 Idle handle callbacks are called. Despite the unfortunate name, idle handles are run on every loop iteration, if they are active. 5\u3001 Prepare handle callbacks are called. Prepare handles get their callbacks called right before the loop will block for I/O. 6\u3001Poll timeout is calculated. Before blocking for I/O the loop calculates for how long it should block. These are the rules when calculating the timeout: NOTE: 1\u3001\u5178\u578b\u7684\" block with timeout-system call with timeout\" If the loop was run with the UV_RUN_NOWAIT flag, the timeout is 0. If the loop is going to be stopped ( uv_stop() was called), the timeout is 0. If there are no active handles or requests, the timeout is 0. If there are any idle handles active, the timeout is 0. If there are any handles pending to be closed, the timeout is 0. If none of the above cases matches, the timeout of the closest timer is taken, or if there are no active timers, infinity.","title":"The I/O loop"},{"location":"Event-driven-concurrent-server/Event-library/library-libuv/Design-overview/#mock#asynchronous#file#io","text":"libuv uses a thread pool to make asynchronous file I/O operations possible, but network I/O is always performed in a single thread, each loop\u2019s thread. NOTE: 1\u3001\u4e0b\u9762\u4f1a\u8fdb\u884c\u8bf4\u660e","title":"Mock asynchronous file I/O"},{"location":"Event-driven-concurrent-server/Event-library/library-libuv/Design-overview/#note","text":"While the polling mechanism is different, libuv makes the execution model consistent across Unix systems and Windows.","title":"Note"},{"location":"Event-driven-concurrent-server/Event-library/library-libuv/Design-overview/#file#io","text":"NOTE: 1\u3001\u8fd9\u662f\u5178\u578b\u7684asynchronous method invocation(sync to async)\uff0c\u5b83\u7684\u5b9e\u73b0\u65b9\u5f0f\u662f: thread pool + queue Unlike network I/O, there are no platform-specific file I/O primitives libuv could rely on, so the current approach is to run blocking file I/O operations in a thread pool. For a thorough explanation of the cross-platform file I/O landscape, checkout this post . libuv currently uses a global thread pool on which all loops can queue work. 3 types of operations are currently run on this pool: File system operations DNS functions (getaddrinfo and getnameinfo) User specified code via uv_queue_work() Warning See the Thread pool work scheduling section for more details, but keep in mind the thread pool size is quite limited.","title":"File I/O"},{"location":"Event-driven-concurrent-server/Event-library/library-libuv/User-guide/Basics-of-libuv/","text":"Basics of libuv \u00b6 libuv enforces an asynchronous , event-driven style of programming. Its core job is to provide an event loop and callback based notifications of I/O and other activities. libuv offers core utilities like timers, non-blocking networking support, asynchronous file system access, child processes and more. NOTE: 1\u3001 \" callback based notifications of I/O \"\u662f\u4e00\u4e2a\u6bd4\u8f83\u597d\u7684\u603b\u7ed3 Event loops \u00b6 In event-driven programming, an application expresses interest in certain events and respond to them when they occur. The responsibility of gathering events from the operating system or monitoring other sources of events is handled by libuv , and the user can register callbacks to be invoked when an event occurs. The event-loop usually keeps running forever . In pseudocode: while there are still events to process: e = get the next event if there is a callback associated with e: call the callback Some examples of events are: File is ready for writing A socket has data ready to be read A timer has timed out This event loop is encapsulated by uv_run() \u2013 the end-all(\u7ec8\u7ed3) function when using libuv . NOTE: \u8fd9\u53e5\u8bdd\u7684\u610f\u601d\u5c31\u662f\u8bf4\u51fd\u6570 uv_run() \u4ee3\u8868\u7684\u5c31\u662f**event loop**\u3002\u6211\u9700\u8981\u7406\u89e3libuv\u7684programming paradigm\uff0c\u5b83\u4e3a\u4e86\u4fbf\u4e8e\u7528\u6237\u4f7f\u7528\uff0c\u63d0\u4f9b\u4e86\u975e\u5e38\u9ad8\u7ea7\u7684API\uff0c\u8fd9\u4e9bAPI\u671f\u521d\u770b\u8d77\u6765\u662f\u975e\u5e38\u62bd\u8c61\u7684\uff0c\u96be\u4ee5\u7406\u89e3\u7684\uff0c\u4f46\u662f\u4e00\u65e6\u638c\u63e1\u4e86libuv\u7684programming paradigm\u5c31\u975e\u5e38\u597d\u7406\u89e3\u4e86\u3002 The most common activity of systems programs is to deal with input and output, rather than a lot of number-crunching. The problem with using conventional input/output functions ( read , fprintf , etc.) is that they are blocking . The actual write to a hard disk or reading from a network, takes a disproportionately long time compared to the speed of the processor. The functions don\u2019t return until the task is done, so that your program is doing nothing. For programs which require high performance this is a major roadblock as other activities and other I/O operations are kept waiting. NOTE: Asynchronous I/O also compare I/O and the processing of data. One of the standard solutions is to use threads. Each blocking I/O operation is started in a separate thread (or in a thread pool). When the blocking function gets invoked in the thread, the processor can schedule another thread to run, which actually needs the CPU. NOTE: 1\u3001I vaguely remember where I saw a similar description, but now I can't remember it. 20190107\uff1a\u662f\u5728APUE\u7684blocking IO\u7ae0\u8282\u4e2d\u3002 The approach followed by libuv uses another style, which is the asynchronous, non-blocking style. Most modern operating systems provide event notification subsystems . For example, a normal read call on a socket would block until the sender actually sent something. Instead, the application can request the operating system to watch the socket and put an event notification in the queue. The application can inspect the events at its convenience (perhaps doing some number crunching before to use the processor to the maximum) and grab the data. It is asynchronous because the application expressed interest at one point, then used the data at another point (in time and space). It is non-blocking because the application process was free to do other tasks. This fits in well with libuv \u2019s event-loop approach, since the operating system events can be treated as just another libuv event. The non-blocking ensures that other events can continue to be handled as fast as they come in [ 1] . Note How the I/O is run in the background is not of our concern, but due to the way our computer hardware works, with the thread as the basic unit of the processor\uff08processor\u4ee5thread\u4f5c\u4e3abasic unit\uff09, libuv and OSes will usually run background/worker threads and/or polling to perform tasks in a non-blocking manner. Bert Belder, one of the libuv core developers has a small video explaining the architecture of libuv and its background. If you have no prior experience with either libuv or libev , it is a quick, useful watch. libuv \u2019s event loop is explained in more detail in the documentation . Hello World \u00b6 With the basics out of the way, lets write our first libuv program. It does nothing, except start a loop which will exit immediately. helloworld/main.c #include <stdio.h> #include <stdlib.h> #include <uv.h> int main () { uv_loop_t * loop = malloc ( sizeof ( uv_loop_t )); uv_loop_init ( loop ); printf ( \"Now quitting. \\n \" ); uv_run ( loop , UV_RUN_DEFAULT ); uv_loop_close ( loop ); free ( loop ); return 0 ; } This program quits immediately because it has no events to process. A libuv event loop has to be told to watch out for events using the various API functions. NOTE\uff1a\u5728\u8fd0\u884clibuv\u7684event loop\u7684\u65f6\u5019\uff0c\u9700\u8981\u544a\u77e5event loop\u53bbwatch out\u4ec0\u4e48event\uff1b Starting with libuv v1.0, users should allocate the memory for the loops before initializing it with uv_loop_init(uv_loop_t *) . This allows you to plug in custom memory management . Remember to de-initialize the loop using uv_loop_close(uv_loop_t *) and then delete the storage. The examples never close loops since the program quits after the loop ends and the system will reclaim memory. Production grade projects\uff08\u751f\u4ea7\u7ea7\u522b\u7684\u9879\u76ee\uff09, especially long running systems programs, should take care to release correctly. Default loop \u00b6 A default loop is provided by libuv and can be accessed using uv_default_loop() . You should use this loop if you only want a single loop. Note:node.js uses the default loop as its main loop . If you are writing bindings you should be aware of this. Error handling \u00b6 Initialization functions or synchronous functions which may fail return a negative number on error. Async functions that may fail will pass a status parameter to their callbacks. The error messages are defined as UV_E* constants . You can use the uv_strerror(int) and uv_err_name(int) functions to get a const char * describing the error or the error name respectively. I/O read callbacks (such as for files and sockets) are passed a parameter nread . If nread is less than 0, there was an error (UV_EOF is the end of file error, which you may want to handle differently). Handles and Requests \u00b6 libuv works by the user expressing interest in particular events . This is usually done by creating a handle to an I/O device, timer or process. Handles are opaque structs named as uv_TYPE_t where type signifies\uff08\u8868\u793a\uff09 what the handle is used for. NOTE: 1\u3001libuv handle\u7684\u6982\u5ff5\uff0c\u5176\u5b9e\u5c31\u662f\u6211\u4eec\u5e73\u65f6\u6240\u8bf4\u7684\"resource handle\"\uff0c\u53ef\u4ee5\u4f7f\u7528Linux \u4e2d\u7684file descriptor\u6765\u8fdb\u884c\u7406\u89e3\uff0c\u663e\u7136\u4e00\u4e2a\u597dhandle\u5bf9\u5e94\u4e86\u4e00\u4e2asystem resource\uff0c\u4e0a\u9762\u7ed9\u51fa\u4e86\u4f8b\u5b50\uff0c \u6bd4\u5982IO device\u3001timer\u3001process\uff1b\u4ecelibuv handle\uff0c\u4f53\u73b0\u4e86libuv\u7684\u62bd\u8c61\u3002 libuv watchers UV_REQ_TYPE_PRIVATE UV_REQ_TYPE_MAX } uv_req_type ; /* Handle types. */ typedef struct uv_loop_s uv_loop_t ; typedef struct uv_handle_s uv_handle_t ; typedef struct uv_stream_s uv_stream_t ; typedef struct uv_tcp_s uv_tcp_t ; typedef struct uv_udp_s uv_udp_t ; typedef struct uv_pipe_s uv_pipe_t ; typedef struct uv_tty_s uv_tty_t ; typedef struct uv_poll_s uv_poll_t ; typedef struct uv_timer_s uv_timer_t ; typedef struct uv_prepare_s uv_prepare_t ; typedef struct uv_check_s uv_check_t ; typedef struct uv_idle_s uv_idle_t ; typedef struct uv_async_s uv_async_t ; typedef struct uv_process_s uv_process_t ; typedef struct uv_fs_event_s uv_fs_event_t ; typedef struct uv_fs_poll_s uv_fs_poll_t ; typedef struct uv_signal_s uv_signal_t ; /* Request types. */ typedef struct uv_req_s uv_req_t ; typedef struct uv_getaddrinfo_s uv_getaddrinfo_t ; typedef struct uv_getnameinfo_s uv_getnameinfo_t ; typedef struct uv_shutdown_s uv_shutdown_t ; typedef struct uv_write_s uv_write_t ; typedef struct uv_connect_s uv_connect_t ; typedef struct uv_udp_send_s uv_udp_send_t ; typedef struct uv_fs_s uv_fs_t ; typedef struct uv_work_s uv_work_t ; Handles represent long-lived objects. Async operations on such handles are identified using requests . A request is short-lived (usually used across only one callback\u3002\u8bd1\u6587\uff1a request**\u7684\u4f5c\u7528\u57df\u53ea\u5728\u4e00\u4e2a**callback**\u4e4b\u95f4) and usually indicates one I/O operation on a **handle \uff08 request**\u901a\u5e38\u8868\u793a\u5728\u4e00\u4e2a**handle**\u4e0a\u7684\u4e00\u4e2aI/0\u64cd\u4f5c\uff09. **Requests are used to preserve\uff08\u7ef4\u62a4\uff09 context between the initiation\uff08\u542f\u52a8\uff09 and the callback of individual actions. For example, an UDP socket is represented by a uv_udp_t , while individual writes to the socket use a uv_udp_send_t structure that is passed to the callback after the write is done. NOTE: request\u8868\u793a\u5f53handle\u6240watch\u7684event\u53d1\u751f\u7684\u65f6\u5019\uff0c\u8981\u6267\u884c\u7684asynchronous operation\uff0c\u8fd9\u4e9b\u64cd\u4f5c\u901a\u5e38\u662fI/O\u64cd\u4f5c\uff1b Handles are setup by a corresponding: uv_TYPE_init ( uv_loop_t * , uv_TYPE_t * ) function. Callbacks are functions which are called by libuv whenever an event the watcher is interested in has taken place. Application specific logic will usually be implemented in the callback . For example, an IO watcher\u2019s callback will receive the data read from a file, a timer callback will be triggered on timeout and so on.","title":"Introduction"},{"location":"Event-driven-concurrent-server/Event-library/library-libuv/User-guide/Basics-of-libuv/#basics#of#libuv","text":"libuv enforces an asynchronous , event-driven style of programming. Its core job is to provide an event loop and callback based notifications of I/O and other activities. libuv offers core utilities like timers, non-blocking networking support, asynchronous file system access, child processes and more. NOTE: 1\u3001 \" callback based notifications of I/O \"\u662f\u4e00\u4e2a\u6bd4\u8f83\u597d\u7684\u603b\u7ed3","title":"Basics of libuv\u00b6"},{"location":"Event-driven-concurrent-server/Event-library/library-libuv/User-guide/Basics-of-libuv/#event#loops","text":"In event-driven programming, an application expresses interest in certain events and respond to them when they occur. The responsibility of gathering events from the operating system or monitoring other sources of events is handled by libuv , and the user can register callbacks to be invoked when an event occurs. The event-loop usually keeps running forever . In pseudocode: while there are still events to process: e = get the next event if there is a callback associated with e: call the callback Some examples of events are: File is ready for writing A socket has data ready to be read A timer has timed out This event loop is encapsulated by uv_run() \u2013 the end-all(\u7ec8\u7ed3) function when using libuv . NOTE: \u8fd9\u53e5\u8bdd\u7684\u610f\u601d\u5c31\u662f\u8bf4\u51fd\u6570 uv_run() \u4ee3\u8868\u7684\u5c31\u662f**event loop**\u3002\u6211\u9700\u8981\u7406\u89e3libuv\u7684programming paradigm\uff0c\u5b83\u4e3a\u4e86\u4fbf\u4e8e\u7528\u6237\u4f7f\u7528\uff0c\u63d0\u4f9b\u4e86\u975e\u5e38\u9ad8\u7ea7\u7684API\uff0c\u8fd9\u4e9bAPI\u671f\u521d\u770b\u8d77\u6765\u662f\u975e\u5e38\u62bd\u8c61\u7684\uff0c\u96be\u4ee5\u7406\u89e3\u7684\uff0c\u4f46\u662f\u4e00\u65e6\u638c\u63e1\u4e86libuv\u7684programming paradigm\u5c31\u975e\u5e38\u597d\u7406\u89e3\u4e86\u3002 The most common activity of systems programs is to deal with input and output, rather than a lot of number-crunching. The problem with using conventional input/output functions ( read , fprintf , etc.) is that they are blocking . The actual write to a hard disk or reading from a network, takes a disproportionately long time compared to the speed of the processor. The functions don\u2019t return until the task is done, so that your program is doing nothing. For programs which require high performance this is a major roadblock as other activities and other I/O operations are kept waiting. NOTE: Asynchronous I/O also compare I/O and the processing of data. One of the standard solutions is to use threads. Each blocking I/O operation is started in a separate thread (or in a thread pool). When the blocking function gets invoked in the thread, the processor can schedule another thread to run, which actually needs the CPU. NOTE: 1\u3001I vaguely remember where I saw a similar description, but now I can't remember it. 20190107\uff1a\u662f\u5728APUE\u7684blocking IO\u7ae0\u8282\u4e2d\u3002 The approach followed by libuv uses another style, which is the asynchronous, non-blocking style. Most modern operating systems provide event notification subsystems . For example, a normal read call on a socket would block until the sender actually sent something. Instead, the application can request the operating system to watch the socket and put an event notification in the queue. The application can inspect the events at its convenience (perhaps doing some number crunching before to use the processor to the maximum) and grab the data. It is asynchronous because the application expressed interest at one point, then used the data at another point (in time and space). It is non-blocking because the application process was free to do other tasks. This fits in well with libuv \u2019s event-loop approach, since the operating system events can be treated as just another libuv event. The non-blocking ensures that other events can continue to be handled as fast as they come in [ 1] . Note How the I/O is run in the background is not of our concern, but due to the way our computer hardware works, with the thread as the basic unit of the processor\uff08processor\u4ee5thread\u4f5c\u4e3abasic unit\uff09, libuv and OSes will usually run background/worker threads and/or polling to perform tasks in a non-blocking manner. Bert Belder, one of the libuv core developers has a small video explaining the architecture of libuv and its background. If you have no prior experience with either libuv or libev , it is a quick, useful watch. libuv \u2019s event loop is explained in more detail in the documentation .","title":"Event loops\u00b6"},{"location":"Event-driven-concurrent-server/Event-library/library-libuv/User-guide/Basics-of-libuv/#hello#world","text":"With the basics out of the way, lets write our first libuv program. It does nothing, except start a loop which will exit immediately. helloworld/main.c #include <stdio.h> #include <stdlib.h> #include <uv.h> int main () { uv_loop_t * loop = malloc ( sizeof ( uv_loop_t )); uv_loop_init ( loop ); printf ( \"Now quitting. \\n \" ); uv_run ( loop , UV_RUN_DEFAULT ); uv_loop_close ( loop ); free ( loop ); return 0 ; } This program quits immediately because it has no events to process. A libuv event loop has to be told to watch out for events using the various API functions. NOTE\uff1a\u5728\u8fd0\u884clibuv\u7684event loop\u7684\u65f6\u5019\uff0c\u9700\u8981\u544a\u77e5event loop\u53bbwatch out\u4ec0\u4e48event\uff1b Starting with libuv v1.0, users should allocate the memory for the loops before initializing it with uv_loop_init(uv_loop_t *) . This allows you to plug in custom memory management . Remember to de-initialize the loop using uv_loop_close(uv_loop_t *) and then delete the storage. The examples never close loops since the program quits after the loop ends and the system will reclaim memory. Production grade projects\uff08\u751f\u4ea7\u7ea7\u522b\u7684\u9879\u76ee\uff09, especially long running systems programs, should take care to release correctly.","title":"Hello World\u00b6"},{"location":"Event-driven-concurrent-server/Event-library/library-libuv/User-guide/Basics-of-libuv/#default#loop","text":"A default loop is provided by libuv and can be accessed using uv_default_loop() . You should use this loop if you only want a single loop. Note:node.js uses the default loop as its main loop . If you are writing bindings you should be aware of this.","title":"Default loop\u00b6"},{"location":"Event-driven-concurrent-server/Event-library/library-libuv/User-guide/Basics-of-libuv/#error#handling","text":"Initialization functions or synchronous functions which may fail return a negative number on error. Async functions that may fail will pass a status parameter to their callbacks. The error messages are defined as UV_E* constants . You can use the uv_strerror(int) and uv_err_name(int) functions to get a const char * describing the error or the error name respectively. I/O read callbacks (such as for files and sockets) are passed a parameter nread . If nread is less than 0, there was an error (UV_EOF is the end of file error, which you may want to handle differently).","title":"Error handling\u00b6"},{"location":"Event-driven-concurrent-server/Event-library/library-libuv/User-guide/Basics-of-libuv/#handles#and#requests","text":"libuv works by the user expressing interest in particular events . This is usually done by creating a handle to an I/O device, timer or process. Handles are opaque structs named as uv_TYPE_t where type signifies\uff08\u8868\u793a\uff09 what the handle is used for. NOTE: 1\u3001libuv handle\u7684\u6982\u5ff5\uff0c\u5176\u5b9e\u5c31\u662f\u6211\u4eec\u5e73\u65f6\u6240\u8bf4\u7684\"resource handle\"\uff0c\u53ef\u4ee5\u4f7f\u7528Linux \u4e2d\u7684file descriptor\u6765\u8fdb\u884c\u7406\u89e3\uff0c\u663e\u7136\u4e00\u4e2a\u597dhandle\u5bf9\u5e94\u4e86\u4e00\u4e2asystem resource\uff0c\u4e0a\u9762\u7ed9\u51fa\u4e86\u4f8b\u5b50\uff0c \u6bd4\u5982IO device\u3001timer\u3001process\uff1b\u4ecelibuv handle\uff0c\u4f53\u73b0\u4e86libuv\u7684\u62bd\u8c61\u3002 libuv watchers UV_REQ_TYPE_PRIVATE UV_REQ_TYPE_MAX } uv_req_type ; /* Handle types. */ typedef struct uv_loop_s uv_loop_t ; typedef struct uv_handle_s uv_handle_t ; typedef struct uv_stream_s uv_stream_t ; typedef struct uv_tcp_s uv_tcp_t ; typedef struct uv_udp_s uv_udp_t ; typedef struct uv_pipe_s uv_pipe_t ; typedef struct uv_tty_s uv_tty_t ; typedef struct uv_poll_s uv_poll_t ; typedef struct uv_timer_s uv_timer_t ; typedef struct uv_prepare_s uv_prepare_t ; typedef struct uv_check_s uv_check_t ; typedef struct uv_idle_s uv_idle_t ; typedef struct uv_async_s uv_async_t ; typedef struct uv_process_s uv_process_t ; typedef struct uv_fs_event_s uv_fs_event_t ; typedef struct uv_fs_poll_s uv_fs_poll_t ; typedef struct uv_signal_s uv_signal_t ; /* Request types. */ typedef struct uv_req_s uv_req_t ; typedef struct uv_getaddrinfo_s uv_getaddrinfo_t ; typedef struct uv_getnameinfo_s uv_getnameinfo_t ; typedef struct uv_shutdown_s uv_shutdown_t ; typedef struct uv_write_s uv_write_t ; typedef struct uv_connect_s uv_connect_t ; typedef struct uv_udp_send_s uv_udp_send_t ; typedef struct uv_fs_s uv_fs_t ; typedef struct uv_work_s uv_work_t ; Handles represent long-lived objects. Async operations on such handles are identified using requests . A request is short-lived (usually used across only one callback\u3002\u8bd1\u6587\uff1a request**\u7684\u4f5c\u7528\u57df\u53ea\u5728\u4e00\u4e2a**callback**\u4e4b\u95f4) and usually indicates one I/O operation on a **handle \uff08 request**\u901a\u5e38\u8868\u793a\u5728\u4e00\u4e2a**handle**\u4e0a\u7684\u4e00\u4e2aI/0\u64cd\u4f5c\uff09. **Requests are used to preserve\uff08\u7ef4\u62a4\uff09 context between the initiation\uff08\u542f\u52a8\uff09 and the callback of individual actions. For example, an UDP socket is represented by a uv_udp_t , while individual writes to the socket use a uv_udp_send_t structure that is passed to the callback after the write is done. NOTE: request\u8868\u793a\u5f53handle\u6240watch\u7684event\u53d1\u751f\u7684\u65f6\u5019\uff0c\u8981\u6267\u884c\u7684asynchronous operation\uff0c\u8fd9\u4e9b\u64cd\u4f5c\u901a\u5e38\u662fI/O\u64cd\u4f5c\uff1b Handles are setup by a corresponding: uv_TYPE_init ( uv_loop_t * , uv_TYPE_t * ) function. Callbacks are functions which are called by libuv whenever an event the watcher is interested in has taken place. Application specific logic will usually be implemented in the callback . For example, an IO watcher\u2019s callback will receive the data read from a file, a timer callback will be triggered on timeout and so on.","title":"Handles and Requests\u00b6"},{"location":"Event-driven-concurrent-server/Event-library/library-muduo/","text":"muduo \u5728\u9605\u8bfbzhihu SO_REUSEPORT\u3001EPOLLEXCLUSIVE\u90fd\u7528\u6765\u89e3\u51b3epoll\u7684\u60ca\u7fa4\uff0c\u4fa7\u91cd\u70b9\u8ddf\u533a\u522b\u662f\u4ec0\u4e48? \u65f6\uff0c\u770b\u5230\u4e86 \u9648\u7855\u7684\u56de\u7b54 \uff0c\u4ed6\u5728\u77e5\u4e4e\u4e0a\u83b7\u5f97\u4e86\"C++\u8bdd\u9898\u4e0b\u7684\u4f18\u79c0\u7b54\u4e3b\"\uff0c\u9042\u770b\u4e86\u4ed6\u7684\u4e3b\u9875: \u300aLinux \u591a\u7ebf\u7a0b\u670d\u52a1\u7aef\u7f16\u7a0b\uff1a\u4f7f\u7528 muduo C++ \u7f51\u7edc\u5e93\u300b\u4f5c\u8005\u3002 chenshuo / muduo NOTE: \u9879\u76ee\u5e76\u4e0d\u6d3b\u8dc3 zhihu muduo\u5e93\u5728\u5b9e\u9645\u9879\u76ee\u4e2d\u4f7f\u7528\u7684\u4eba\u591a\u5417\uff1f","title":"Introduction"},{"location":"Event-driven-concurrent-server/Event-library/library-muduo/#muduo","text":"\u5728\u9605\u8bfbzhihu SO_REUSEPORT\u3001EPOLLEXCLUSIVE\u90fd\u7528\u6765\u89e3\u51b3epoll\u7684\u60ca\u7fa4\uff0c\u4fa7\u91cd\u70b9\u8ddf\u533a\u522b\u662f\u4ec0\u4e48? \u65f6\uff0c\u770b\u5230\u4e86 \u9648\u7855\u7684\u56de\u7b54 \uff0c\u4ed6\u5728\u77e5\u4e4e\u4e0a\u83b7\u5f97\u4e86\"C++\u8bdd\u9898\u4e0b\u7684\u4f18\u79c0\u7b54\u4e3b\"\uff0c\u9042\u770b\u4e86\u4ed6\u7684\u4e3b\u9875: \u300aLinux \u591a\u7ebf\u7a0b\u670d\u52a1\u7aef\u7f16\u7a0b\uff1a\u4f7f\u7528 muduo C++ \u7f51\u7edc\u5e93\u300b\u4f5c\u8005\u3002","title":"muduo"},{"location":"Event-driven-concurrent-server/Event-library/library-muduo/#chenshuomuduo","text":"NOTE: \u9879\u76ee\u5e76\u4e0d\u6d3b\u8dc3","title":"chenshuo/muduo"},{"location":"Event-driven-concurrent-server/Event-library/library-muduo/#zhihu#muduo","text":"","title":"zhihu muduo\u5e93\u5728\u5b9e\u9645\u9879\u76ee\u4e2d\u4f7f\u7528\u7684\u4eba\u591a\u5417\uff1f"},{"location":"Event-driven-concurrent-server/Expert-Jeff-Darcy/","text":"Jeff Darcy \u5728 nick-black Fast UNIX Servers \u4e2d\uff0c\u63d0\u53ca\u4e86Jeff Darcy Jeff Darcy's \" High-Performance Server Architecture \" adding to our understanding \u4e0b\u9762\u662f\u4ed6\u5199\u7684\u975e\u5e38\u597d\u7684\u6587\u7ae0: High-Performance Server Architecture","title":"Introduction"},{"location":"Event-driven-concurrent-server/Expert-Jeff-Darcy/#jeff#darcy","text":"\u5728 nick-black Fast UNIX Servers \u4e2d\uff0c\u63d0\u53ca\u4e86Jeff Darcy Jeff Darcy's \" High-Performance Server Architecture \" adding to our understanding \u4e0b\u9762\u662f\u4ed6\u5199\u7684\u975e\u5e38\u597d\u7684\u6587\u7ae0: High-Performance Server Architecture","title":"Jeff Darcy"},{"location":"Event-driven-concurrent-server/Expert-Jeff-Darcy/High-Performance-Server-Architecture/","text":"High-Performance Server Architecture The rest of this article is going to be centered around what I'll call the Four Horsemen of Poor Performance: 1\u3001Data copies 2\u3001Context switches 3\u3001Memory allocation 4\u3001Lock contention There will also be a catch-all section at the end, but these are the biggest performance-killers. If you can handle most requests without copying data, without a context switch, without going through the memory allocator and without contending for locks, you'll have a server that performs well even if it gets some of the minor parts wrong.","title":"Introduction"},{"location":"Event-driven-concurrent-server/Expert-Jeff-Darcy/High-Performance-Server-Architecture/#high-performance#server#architecture","text":"The rest of this article is going to be centered around what I'll call the Four Horsemen of Poor Performance: 1\u3001Data copies 2\u3001Context switches 3\u3001Memory allocation 4\u3001Lock contention There will also be a catch-all section at the end, but these are the biggest performance-killers. If you can handle most requests without copying data, without a context switch, without going through the memory allocator and without contending for locks, you'll have a server that performs well even if it gets some of the minor parts wrong.","title":"High-Performance Server Architecture"},{"location":"Event-driven-concurrent-server/Expert-Nick-Black/","text":"Nick Black","title":"Introduction"},{"location":"Event-driven-concurrent-server/Expert-Nick-Black/#nick#black","text":"","title":"Nick Black"},{"location":"Event-driven-concurrent-server/Expert-Nick-Black/Fast-UNIX-Servers/","text":"nick-black Fast UNIX Servers NOTE: \u4e00\u3001\u5728 folly / folly / io / async / README.md \u4e2d\uff0c\u63d0\u53ca\u4e86\u672c\u6587: Facebook has a lot of experience running services. For background reading, see The C10k problem and Fast UNIX servers \u4e8c\u3001\u8fd9\u7bc7\u6587\u7ae0\u975e\u5e38\u597d\uff0c\u975e\u5e38\u8be6\u5c3d Dan Kegel's classic site \" The C10K Problem \" (still updated from time to time) put a Promethean order to the arcana(\u5965\u79d8) of years, with Jeff Darcy's \" High-Performance Server Architecture \" adding to our understanding. I'm collecting here some followup material to these excellent works (and of course the books of W. Richard Stevens, whose torch we merely carry). Some of these techniques have found, or will find, their way into libtorque , my multithreaded event unification library (and master's thesis). NOTE: \u7ffb\u8bd1\u5982\u4e0b: \"Dan Kegel\u7684\u7ecf\u5178\u7f51\u7ad9\u201cThe C10K Problem\u201d(\u4ecd\u7136\u4e0d\u65f6\u66f4\u65b0)\u7528\u666e\u7f57\u7c73\u4fee\u65af\u5f0f\u7684\u987a\u5e8f\u89e3\u91ca\u4e86\u591a\u5e74\u6765\u7684\u5965\u79d8\uff0cJeff Darcy\u7684\u201c\u9ad8\u6027\u80fd\u670d\u52a1\u5668\u67b6\u6784\u201d\u589e\u52a0\u4e86\u6211\u4eec\u7684\u7406\u89e3\u3002 \u6211\u5728\u8fd9\u91cc\u6536\u96c6\u4e86\u4e00\u4e9b\u5173\u4e8e\u8fd9\u4e9b\u4f18\u79c0\u4f5c\u54c1\u7684\u540e\u7eed\u6750\u6599(\u5f53\u7136\u8fd8\u6709w\u00b7\u7406\u67e5\u5fb7\u00b7\u53f2\u8482\u6587\u65af\u7684\u4e66\uff0c\u6211\u4eec\u53ea\u662f\u62ff\u7740\u4ed6\u7684\u706b\u70ac)\u3002 \u8fd9\u4e9b\u6280\u672f\u4e2d\u7684\u4e00\u4e9b\u5df2\u7ecf\u6216\u5c06\u8981\u5728\u6211\u7684\u591a\u7ebf\u7a0b\u4e8b\u4ef6\u7edf\u4e00\u5e93libtorque\u4e2d\u627e\u5230\u5b83\u4eec\u7684\u65b9\u6cd5(\u4ee5\u53ca\u6211\u7684\u7855\u58eb\u8bba\u6587)\u3002\" Central Design Principles Varghese's Network Algorithmics: An Interdisciplinary Approach to Designing Fast Networked Devices is in a league of its own in this regard. NOTE: \u7ffb\u8bd1\u5982\u4e0b: \"Varghese\u7684\u300a\u7f51\u7edc\u7b97\u6cd5:\u8bbe\u8ba1\u5feb\u901f\u7f51\u7edc\u8bbe\u5907\u7684\u8de8\u5b66\u79d1\u65b9\u6cd5\u300b\u5728\u8fd9\u65b9\u9762\u6709\u81ea\u5df1\u7684\u89c1\u89e3\u3002\" 1\u3001Principle 1: Exploit all cycles/bandwidth. Avoid blocking I/O and unnecessary evictions of cache, but prefetch into cache where appropriate (this applies to page caches just as much as processor caches or any other layer of the memory hierarchy ). Be prepared to exploit multiple processing elements. Properly align data and avoid cache-aliasing effects. Use jumbo frames in appropriate scenarios and proactively warn on network degradation (e.g., half-duplex Ethernet due to failed link negotiation). 2\u3001Principle 2: Don't duplicate work. Avoid unnecessary copies, context switches, system calls and signals. Use double-buffering or ringbuffers, and calls like Linux's splice(2) . NOTE: \"Avoid unnecessary copies\" \u5bf9\u5e94\u7684\u662f zero copy\uff0c\u6bd4\u5982 \" Linux'shttps://nick-black.com/dankwiki/index.php?title=Linux_APIs splice(2) . \" 3\u3001Principle 3: Measure, measure, and measure again, preferably automatically. Hardware, software and networks will all surprise you. Become friends with your hardware's performance counters and tools like eBPF , dtrace, ktrace, etc. Build explicit support for performance analysis into the application, especially domain-specific statistics. Queueing Theory NOTE: \u6682\u672a\u4e86\u89e3 Event Cores","title":"Introduction"},{"location":"Event-driven-concurrent-server/Expert-Nick-Black/Fast-UNIX-Servers/#nick-black#fast#unix#servers","text":"NOTE: \u4e00\u3001\u5728 folly / folly / io / async / README.md \u4e2d\uff0c\u63d0\u53ca\u4e86\u672c\u6587: Facebook has a lot of experience running services. For background reading, see The C10k problem and Fast UNIX servers \u4e8c\u3001\u8fd9\u7bc7\u6587\u7ae0\u975e\u5e38\u597d\uff0c\u975e\u5e38\u8be6\u5c3d Dan Kegel's classic site \" The C10K Problem \" (still updated from time to time) put a Promethean order to the arcana(\u5965\u79d8) of years, with Jeff Darcy's \" High-Performance Server Architecture \" adding to our understanding. I'm collecting here some followup material to these excellent works (and of course the books of W. Richard Stevens, whose torch we merely carry). Some of these techniques have found, or will find, their way into libtorque , my multithreaded event unification library (and master's thesis). NOTE: \u7ffb\u8bd1\u5982\u4e0b: \"Dan Kegel\u7684\u7ecf\u5178\u7f51\u7ad9\u201cThe C10K Problem\u201d(\u4ecd\u7136\u4e0d\u65f6\u66f4\u65b0)\u7528\u666e\u7f57\u7c73\u4fee\u65af\u5f0f\u7684\u987a\u5e8f\u89e3\u91ca\u4e86\u591a\u5e74\u6765\u7684\u5965\u79d8\uff0cJeff Darcy\u7684\u201c\u9ad8\u6027\u80fd\u670d\u52a1\u5668\u67b6\u6784\u201d\u589e\u52a0\u4e86\u6211\u4eec\u7684\u7406\u89e3\u3002 \u6211\u5728\u8fd9\u91cc\u6536\u96c6\u4e86\u4e00\u4e9b\u5173\u4e8e\u8fd9\u4e9b\u4f18\u79c0\u4f5c\u54c1\u7684\u540e\u7eed\u6750\u6599(\u5f53\u7136\u8fd8\u6709w\u00b7\u7406\u67e5\u5fb7\u00b7\u53f2\u8482\u6587\u65af\u7684\u4e66\uff0c\u6211\u4eec\u53ea\u662f\u62ff\u7740\u4ed6\u7684\u706b\u70ac)\u3002 \u8fd9\u4e9b\u6280\u672f\u4e2d\u7684\u4e00\u4e9b\u5df2\u7ecf\u6216\u5c06\u8981\u5728\u6211\u7684\u591a\u7ebf\u7a0b\u4e8b\u4ef6\u7edf\u4e00\u5e93libtorque\u4e2d\u627e\u5230\u5b83\u4eec\u7684\u65b9\u6cd5(\u4ee5\u53ca\u6211\u7684\u7855\u58eb\u8bba\u6587)\u3002\"","title":"nick-black Fast UNIX Servers"},{"location":"Event-driven-concurrent-server/Expert-Nick-Black/Fast-UNIX-Servers/#central#design#principles","text":"Varghese's Network Algorithmics: An Interdisciplinary Approach to Designing Fast Networked Devices is in a league of its own in this regard. NOTE: \u7ffb\u8bd1\u5982\u4e0b: \"Varghese\u7684\u300a\u7f51\u7edc\u7b97\u6cd5:\u8bbe\u8ba1\u5feb\u901f\u7f51\u7edc\u8bbe\u5907\u7684\u8de8\u5b66\u79d1\u65b9\u6cd5\u300b\u5728\u8fd9\u65b9\u9762\u6709\u81ea\u5df1\u7684\u89c1\u89e3\u3002\" 1\u3001Principle 1: Exploit all cycles/bandwidth. Avoid blocking I/O and unnecessary evictions of cache, but prefetch into cache where appropriate (this applies to page caches just as much as processor caches or any other layer of the memory hierarchy ). Be prepared to exploit multiple processing elements. Properly align data and avoid cache-aliasing effects. Use jumbo frames in appropriate scenarios and proactively warn on network degradation (e.g., half-duplex Ethernet due to failed link negotiation). 2\u3001Principle 2: Don't duplicate work. Avoid unnecessary copies, context switches, system calls and signals. Use double-buffering or ringbuffers, and calls like Linux's splice(2) . NOTE: \"Avoid unnecessary copies\" \u5bf9\u5e94\u7684\u662f zero copy\uff0c\u6bd4\u5982 \" Linux'shttps://nick-black.com/dankwiki/index.php?title=Linux_APIs splice(2) . \" 3\u3001Principle 3: Measure, measure, and measure again, preferably automatically. Hardware, software and networks will all surprise you. Become friends with your hardware's performance counters and tools like eBPF , dtrace, ktrace, etc. Build explicit support for performance analysis into the application, especially domain-specific statistics.","title":"Central Design Principles"},{"location":"Event-driven-concurrent-server/Expert-Nick-Black/Fast-UNIX-Servers/#queueing#theory","text":"NOTE: \u6682\u672a\u4e86\u89e3","title":"Queueing Theory"},{"location":"Event-driven-concurrent-server/Expert-Nick-Black/Fast-UNIX-Servers/#event#cores","text":"","title":"Event Cores"},{"location":"Event-driven-concurrent-server/Expert-Nick-Black/library-libtorque/","text":"Libtorque","title":"Introduction"},{"location":"Event-driven-concurrent-server/Expert-Nick-Black/library-libtorque/#libtorque","text":"","title":"Libtorque"},{"location":"Event-driven-concurrent-server/Timer/","text":"","title":"Introduction"},{"location":"Event-driven-concurrent-server/Timer/cpp-Implementing-Timed-Callback-function/","text":"c++ Implementing Timed Callback function I want to implement some system in c++ so that I can call a function and ask for another function to be called in X milliseconds. Something like this: callfunctiontimed ( 25 , funcName ); 25 being the amount of milliseconds before the function should be called. I would like to know if multithreading is required for this and then use some delay function? Other than using function pointer how would a feature like this work? A For a portable solution, you can use boost::asio . Below is a demo I wrote a while ago. You can change t . expires_from_now ( boost :: posix_time :: seconds ( 1 )); to suit you need say make function call after 200 milliseonds. t . expires_from_now ( boost :: posix_time :: milliseconds ( 200 )); Below is a complete working example. It's calling repeatedly but I think it should be easy to call only once by just change a bit. #include <iostream> #include <boost/bind.hpp> #include <boost/thread.hpp> #include <boost/asio.hpp> #include <boost/date_time/posix_time/posix_time.hpp> using namespace boost :: asio ; using namespace std ; class Deadline { public : Deadline ( deadline_timer & timer ) : t ( timer ) { wait (); } void timeout ( const boost :: system :: error_code & e ) { if ( e ) return ; cout << \"tick\" << endl ; wait (); } void cancel () { t . cancel (); } private : void wait () { t . expires_from_now ( boost :: posix_time :: seconds ( 1 )); //repeat rate here t . async_wait ( boost :: bind ( & Deadline :: timeout , this , boost :: asio :: placeholders :: error )); } deadline_timer & t ; }; class CancelDeadline { public : CancelDeadline ( Deadline & d ) : dl ( d ) { } void operator ()() { string cancel ; cin >> cancel ; dl . cancel (); return ; } private : Deadline & dl ; }; int main () { io_service io ; deadline_timer t ( io ); Deadline d ( t ); CancelDeadline cd ( d ); boost :: thread thr1 ( cd ); io . run (); return 0 ; } //result: //it keeps printing tick every second until you enter cancel and enter in the console tick tick tick A Many folks have contributed good answers here on the matter, but I will address the question directly, because I had a similar problem a couple of years ago. I could not use Boost for several reasons--I know Boost has excellent use in a lot of open source software. Moreover, I really wanted to understand timers and callbacks specifically as it pertains to Linux based environments. So, I wrote my own. Fundamentally, I have a Timer class and a TimerCallback class. A typical callback, implemented as a inherited class of the TimerCallback class, will place the operations to be executed upon callback in the triggered () method, implemented specifically for the needs. Per the usual semantics, a Timer object is associated with a callback object , which presumably contains all the required information needed for the callback to execute. The timer scheduling is managed by one environment-wide timer minheap which has to be maintained in a separate thread/process. This minheap task does only one thing: it minheapifies the minheap of callback events set in the future. A minheap selects the next event to fire in O(1) and can minheapify the remaining in O(log n) for n timer events. It can also insert a new timer event in O(log n) (Read a gentle introduction to heaps here ). When a timer fires, the minheap scheduler checks if it is a periodic timer, one shot timer or a timer that will execute a specific number of times. Accordingly, the timer object is either removed from the minheap or reinserted back into the minheap with the next execution time. If a timer object is to be removed, then it is removed from the minheap (but the timer object deletion may or may not be left to the task that created it) and the rest of the heap is minheap-ified; i.e., rearranged to satisfy the minheap property. A working and unit tested implementation is here , and may contain bugs (excerpted from my application), but I thought it may help someone. The implementation is multi-process ( fork() ed-process) based (and also uses pthread s in the main task (process)), and uses POSIX shared memory and POSIX message queues for communication between the processes. A Do you want it to by asynchronous so that the callback gets executed when 25 miliseconds are over without blocking the main executing thread? If so, you can execute the callback in a separate thread from the timer / timed callback function you implement. If you do not use multithreading, then your main or the calling function of callfunctiontimed(25, funcName); would block while you run a sleep / usleep . It is your choice now as to what behavior you want to implement. The real solution would not be as simple as to multithread or not. There are things like, how do you keep different timers/callbacks information considering the function can be called multiple times with different timeouts and functions. One way to do it, would be like this: Create a sorted list of timers/callbacks, sorted based on the time to expire. Have one main thread and one thread which looks at callbacks/timers, call it timer thread . When a new callback is added, add it in the sorted list. The timer thread could be initialized to wait for the least amount of time in the sorted list, or the head. Re-initialize it as new callbacks get added. There would be some math and conditions to take care of. As the timer thread is done sleeping, it removes and looks at head of the list and executes the function pointer in a new thread. Timer thread is re-initialized with sleep time on the new head of the list. main () { //spawn a timer thread with pthread create callfunctiontimed ( 25 , test ); callfunctiontimed ( 35 , show ); callfunctiontimed ( 4 , print ); } callfunctionTImed ( int time , ( func * ) function , void * data ) // { //add information to sorted list of timer and callbacks //re-initialize sleep_time for timer thread if needed. return . } timerThread () { while ( 1 ){ sleep ( sleep_time ); //look at head of timer list, remove it, call function in a new thread //adjust sleep time as per new head } } Hopefully this gives an idea of what I meant, although this is not perfect and has several problems. Timed Callbacks in C++ TODO","title":"[c++ Implementing Timed Callback function](https://stackoverflow.com/questions/12904098/c-implementing-timed-callback-function)"},{"location":"Event-driven-concurrent-server/Timer/cpp-Implementing-Timed-Callback-function/#c#implementing#timed#callback#function","text":"I want to implement some system in c++ so that I can call a function and ask for another function to be called in X milliseconds. Something like this: callfunctiontimed ( 25 , funcName ); 25 being the amount of milliseconds before the function should be called. I would like to know if multithreading is required for this and then use some delay function? Other than using function pointer how would a feature like this work?","title":"c++ Implementing Timed Callback function"},{"location":"Event-driven-concurrent-server/Timer/cpp-Implementing-Timed-Callback-function/#a","text":"For a portable solution, you can use boost::asio . Below is a demo I wrote a while ago. You can change t . expires_from_now ( boost :: posix_time :: seconds ( 1 )); to suit you need say make function call after 200 milliseonds. t . expires_from_now ( boost :: posix_time :: milliseconds ( 200 )); Below is a complete working example. It's calling repeatedly but I think it should be easy to call only once by just change a bit. #include <iostream> #include <boost/bind.hpp> #include <boost/thread.hpp> #include <boost/asio.hpp> #include <boost/date_time/posix_time/posix_time.hpp> using namespace boost :: asio ; using namespace std ; class Deadline { public : Deadline ( deadline_timer & timer ) : t ( timer ) { wait (); } void timeout ( const boost :: system :: error_code & e ) { if ( e ) return ; cout << \"tick\" << endl ; wait (); } void cancel () { t . cancel (); } private : void wait () { t . expires_from_now ( boost :: posix_time :: seconds ( 1 )); //repeat rate here t . async_wait ( boost :: bind ( & Deadline :: timeout , this , boost :: asio :: placeholders :: error )); } deadline_timer & t ; }; class CancelDeadline { public : CancelDeadline ( Deadline & d ) : dl ( d ) { } void operator ()() { string cancel ; cin >> cancel ; dl . cancel (); return ; } private : Deadline & dl ; }; int main () { io_service io ; deadline_timer t ( io ); Deadline d ( t ); CancelDeadline cd ( d ); boost :: thread thr1 ( cd ); io . run (); return 0 ; } //result: //it keeps printing tick every second until you enter cancel and enter in the console tick tick tick","title":"A"},{"location":"Event-driven-concurrent-server/Timer/cpp-Implementing-Timed-Callback-function/#a_1","text":"Many folks have contributed good answers here on the matter, but I will address the question directly, because I had a similar problem a couple of years ago. I could not use Boost for several reasons--I know Boost has excellent use in a lot of open source software. Moreover, I really wanted to understand timers and callbacks specifically as it pertains to Linux based environments. So, I wrote my own. Fundamentally, I have a Timer class and a TimerCallback class. A typical callback, implemented as a inherited class of the TimerCallback class, will place the operations to be executed upon callback in the triggered () method, implemented specifically for the needs. Per the usual semantics, a Timer object is associated with a callback object , which presumably contains all the required information needed for the callback to execute. The timer scheduling is managed by one environment-wide timer minheap which has to be maintained in a separate thread/process. This minheap task does only one thing: it minheapifies the minheap of callback events set in the future. A minheap selects the next event to fire in O(1) and can minheapify the remaining in O(log n) for n timer events. It can also insert a new timer event in O(log n) (Read a gentle introduction to heaps here ). When a timer fires, the minheap scheduler checks if it is a periodic timer, one shot timer or a timer that will execute a specific number of times. Accordingly, the timer object is either removed from the minheap or reinserted back into the minheap with the next execution time. If a timer object is to be removed, then it is removed from the minheap (but the timer object deletion may or may not be left to the task that created it) and the rest of the heap is minheap-ified; i.e., rearranged to satisfy the minheap property. A working and unit tested implementation is here , and may contain bugs (excerpted from my application), but I thought it may help someone. The implementation is multi-process ( fork() ed-process) based (and also uses pthread s in the main task (process)), and uses POSIX shared memory and POSIX message queues for communication between the processes.","title":"A"},{"location":"Event-driven-concurrent-server/Timer/cpp-Implementing-Timed-Callback-function/#a_2","text":"Do you want it to by asynchronous so that the callback gets executed when 25 miliseconds are over without blocking the main executing thread? If so, you can execute the callback in a separate thread from the timer / timed callback function you implement. If you do not use multithreading, then your main or the calling function of callfunctiontimed(25, funcName); would block while you run a sleep / usleep . It is your choice now as to what behavior you want to implement. The real solution would not be as simple as to multithread or not. There are things like, how do you keep different timers/callbacks information considering the function can be called multiple times with different timeouts and functions. One way to do it, would be like this: Create a sorted list of timers/callbacks, sorted based on the time to expire. Have one main thread and one thread which looks at callbacks/timers, call it timer thread . When a new callback is added, add it in the sorted list. The timer thread could be initialized to wait for the least amount of time in the sorted list, or the head. Re-initialize it as new callbacks get added. There would be some math and conditions to take care of. As the timer thread is done sleeping, it removes and looks at head of the list and executes the function pointer in a new thread. Timer thread is re-initialized with sleep time on the new head of the list. main () { //spawn a timer thread with pthread create callfunctiontimed ( 25 , test ); callfunctiontimed ( 35 , show ); callfunctiontimed ( 4 , print ); } callfunctionTImed ( int time , ( func * ) function , void * data ) // { //add information to sorted list of timer and callbacks //re-initialize sleep_time for timer thread if needed. return . } timerThread () { while ( 1 ){ sleep ( sleep_time ); //look at head of timer list, remove it, call function in a new thread //adjust sleep time as per new head } } Hopefully this gives an idea of what I meant, although this is not perfect and has several problems.","title":"A"},{"location":"Event-driven-concurrent-server/Timer/cpp-Implementing-Timed-Callback-function/#timed#callbacks#in#c","text":"TODO","title":"Timed Callbacks in C++"},{"location":"Event-driven-concurrent-server/Timer/cpp-Portable%20periodic-or-one-shot%20timer%20implementation/","text":"Portable periodic/one-shot timer implementation Note: I have posted a follow-up question for a significantly updated version of this code. I have implemented a class that provides portable one-shot or periodic timers. The API provides a way to schedule one or more timer callbacks to fire some number of milliseconds in the future, and optionally fire again every so many milliseconds. The API returns an ID which can be used later to see if the timer still exists, or destroy it. The implementation assigns increasing IDs to the timers, starting at one. The timer IDs use a 64-bit unsigned integer, to avoid dealing with wraparound . THINKING : \u4e0a\u8ff0wraparound\u8be5\u5982\u4f55\u6765\u8fdb\u884c\u7406\u89e3\uff1f The implementation stores the context of each timer Instance in an unordered_map called active , keyed by ID . Each instance has a next member, which is a time_point that indicates when it needs to fire next. The implementation uses a multiset called queue to sort the timers by the next member, using a functor. The queue multiset stores reference_wrapper objects that refer directly to the Instance objects. This allows the queue's comparator to refer directly to the instances. A worker thread is created to service the timer queue. A mutex and condition_variable are used for synchronization. The condition variable is used to notify the worker thread when a timer is created or destroyed, and to request worker thread shutdown. The worker thread uses wait to wait for notification when there are no timers, and uses wait_for to wait until earliest notification needs to fire, or until awakened. The lock is released during the callback when a timer fires. The destructor sets the done flag to true, notifies the condition variable, releases the lock, then joins with the worker to wait for it to exit. EDIT : I realized that there was a race condition that occurs if a timer is destroyed while its callback is in progress. I solved that by having a running flag on each instance. The worker thread checks it when a callback returns to see if that Instance needs to be destroyed. This avoids dereferencing an Instance that was destroyed while the lock was not held during the callback. The destroy method was also updated to see if the callback is running, and set running to false if so, to indicate to the worker that it needs to be destroyed. If the callback for that timer is not running, destroy destroys the Instance itself. timer.h #ifndef TIMER_H #define TIMER_H #include <thread> #include <mutex> #include <condition_variable> #include <algorithm> #include <functional> #include <chrono> #include <unordered_map> #include <set> #include <cstdint> class Timer { public : typedef uint64_t timer_id ; typedef std :: function < void () > handler_type ; private : std :: mutex sync ; typedef std :: unique_lock < std :: mutex > ScopedLock ; std :: condition_variable wakeUp ; private : typedef std :: chrono :: steady_clock Clock ; typedef std :: chrono :: time_point < Clock > Timestamp ; typedef std :: chrono :: milliseconds Duration ; struct Instance { Instance ( timer_id id = 0 ) : id ( id ) , running ( false ) { } template < typename Tfunction > Instance ( timer_id id , Timestamp next , Duration period , Tfunction && handler ) noexcept : id ( id ) , next ( next ) , period ( period ) , handler ( std :: forward < Tfunction > ( handler )) , running ( false ) { } Instance ( Instance const & r ) = delete ; Instance ( Instance && r ) noexcept : id ( r . id ) , next ( r . next ) , period ( r . period ) , handler ( std :: move ( r . handler )) , running ( r . running ) { } Instance & operator = ( Instance const & r ) = delete ; Instance & operator = ( Instance && r ) { if ( this != & r ) { id = r . id ; next = r . next ; period = r . period ; handler = std :: move ( r . handler ); running = r . running ; } return * this ; } timer_id id ; Timestamp next ; Duration period ; handler_type handler ; bool running ; }; typedef std :: unordered_map < timer_id , Instance > InstanceMap ; timer_id nextId ; InstanceMap active ; // Comparison functor to sort the timer \"queue\" by Instance::next struct NextActiveComparator { bool operator ()( const Instance & a , const Instance & b ) const { return a . next < b . next ; } }; NextActiveComparator comparator ; // Queue is a set of references to Instance objects, sorted by next typedef std :: reference_wrapper < Instance > QueueValue ; typedef std :: multiset < QueueValue , NextActiveComparator > Queue ; Queue queue ; // Thread and exit flag std :: thread worker ; bool done ; void threadStart (); public : Timer (); ~ Timer (); timer_id create ( uint64_t when , uint64_t period , const handler_type & handler ); timer_id create ( uint64_t when , uint64_t period , handler_type && handler ); private : timer_id createImpl ( Instance && item ); public : bool destroy ( timer_id id ); bool exists ( timer_id id ); }; #endif // TIMER_H timer.cpp #include \"timer.h\" void Timer :: threadStart () { ScopedLock lock ( sync ); while ( ! done ) { if ( queue . empty ()) { // Wait (forever) for work wakeUp . wait ( lock ); } else { auto firstInstance = queue . begin (); Instance & instance = * firstInstance ; auto now = Clock :: now (); if ( now >= instance . next ) { queue . erase ( firstInstance ); // Mark it as running to handle racing destroy instance . running = true ; // Call the handler lock . unlock (); instance . handler (); lock . lock (); if ( done ) { break ; } else if ( ! instance . running ) { // Running was set to false, destroy was called // for this Instance while the callback was in progress // (this thread was not holding the lock during the callback) active . erase ( instance . id ); } else { instance . running = false ; // If it is periodic, schedule a new one if ( instance . period . count () > 0 ) { instance . next = instance . next + instance . period ; queue . insert ( instance ); } else { active . erase ( instance . id ); } } } else { // Wait until the timer is ready or a timer creation notifies wakeUp . wait_until ( lock , instance . next ); } } } } Timer :: Timer () : nextId ( 1 ) , queue ( comparator ) , done ( false ) { ScopedLock lock ( sync ); worker = std :: thread ( std :: bind ( & Timer :: threadStart , this )); } Timer ::~ Timer () { ScopedLock lock ( sync ); done = true ; wakeUp . notify_all (); lock . unlock (); worker . join (); } Timer :: timer_id Timer :: create ( uint64_t msFromNow , uint64_t msPeriod , const std :: function < void () > & handler ) { return createImpl ( Instance ( 0 , Clock :: now () + Duration ( msFromNow ), Duration ( msPeriod ), handler )); } Timer :: timer_id Timer :: create ( uint64_t msFromNow , uint64_t msPeriod , std :: function < void () >&& handler ) { return createImpl ( Instance ( 0 , Clock :: now () + Duration ( msFromNow ), Duration ( msPeriod ), std :: move ( handler ))); } Timer :: timer_id Timer :: createImpl ( Instance && item ) { ScopedLock lock ( sync ); item . id = nextId ++ ; auto iter = active . emplace ( item . id , std :: move ( item )); queue . insert ( iter . first -> second ); wakeUp . notify_all (); return item . id ; } bool Timer :: destroy ( timer_id id ) { ScopedLock lock ( sync ); auto i = active . find ( id ); if ( i == active . end ()) return false ; else if ( i -> second . running ) { // A callback is in progress for this Instance, // so flag it for deletion in the worker i -> second . running = false ; } else { queue . erase ( std :: ref ( i -> second )); active . erase ( i ); } wakeUp . notify_all (); return true ; } bool Timer :: exists ( timer_id id ) { ScopedLock lock ( sync ); return active . find ( id ) != active . end (); } Example: #include \"timer.h\" int main () { Timer t ; // Timer fires once, one second from now t . create ( 1000 , 0 , []() { std :: cout << \"Non-periodic timer fired\" << std :: endl ; }); // Timer fires every second, starting five seconds from now t . create ( 5000 , 1000 , []() { std :: cout << \"Timer fired 0\" << std :: endl ; }); // Timer fires every second, starting now t . create ( 0 , 1000 , []() { std :: cout << \"Timer fired 1\" << std :: endl ; }); // Timer fires every 100ms, starting now t . create ( 0 , 100 , []() { std :: cout << \"Timer fired 2\" << std :: endl ; }); } The third parameter is a std::function, so it could call a method on some instance of an object, like this: class Foo { public : void bar () { std :: cout << \"Foo::bar called\" << std :: endl ; } }; int something () { Foo example ; // Assume \"t\" is a Timer auto tid = t . create ( 0 , 100 , std :: bind ( & Foo :: bar , & example )); // ... do stuff ... The exists method is intended for scenarios where you desperately need to wait for a timer to go away before something goes out of scope. Definitely not pretty but I don't expect this scenario to occur a lot in my intended use cases: // Not pretty, but better than nothing: t . destroy ( tid ); while ( t . exists ( tid )) std :: this_thread :: sleep_for ( std :: chrono :: milliseconds ( 100 )); }","title":"[Portable periodic/one-shot timer implementation](https://codereview.stackexchange.com/questions/40473/portable-periodic-one-shot-timer-implementation)"},{"location":"Event-driven-concurrent-server/Timer/cpp-Portable%20periodic-or-one-shot%20timer%20implementation/#portable#periodicone-shot#timer#implementation","text":"Note: I have posted a follow-up question for a significantly updated version of this code. I have implemented a class that provides portable one-shot or periodic timers. The API provides a way to schedule one or more timer callbacks to fire some number of milliseconds in the future, and optionally fire again every so many milliseconds. The API returns an ID which can be used later to see if the timer still exists, or destroy it. The implementation assigns increasing IDs to the timers, starting at one. The timer IDs use a 64-bit unsigned integer, to avoid dealing with wraparound . THINKING : \u4e0a\u8ff0wraparound\u8be5\u5982\u4f55\u6765\u8fdb\u884c\u7406\u89e3\uff1f The implementation stores the context of each timer Instance in an unordered_map called active , keyed by ID . Each instance has a next member, which is a time_point that indicates when it needs to fire next. The implementation uses a multiset called queue to sort the timers by the next member, using a functor. The queue multiset stores reference_wrapper objects that refer directly to the Instance objects. This allows the queue's comparator to refer directly to the instances. A worker thread is created to service the timer queue. A mutex and condition_variable are used for synchronization. The condition variable is used to notify the worker thread when a timer is created or destroyed, and to request worker thread shutdown. The worker thread uses wait to wait for notification when there are no timers, and uses wait_for to wait until earliest notification needs to fire, or until awakened. The lock is released during the callback when a timer fires. The destructor sets the done flag to true, notifies the condition variable, releases the lock, then joins with the worker to wait for it to exit. EDIT : I realized that there was a race condition that occurs if a timer is destroyed while its callback is in progress. I solved that by having a running flag on each instance. The worker thread checks it when a callback returns to see if that Instance needs to be destroyed. This avoids dereferencing an Instance that was destroyed while the lock was not held during the callback. The destroy method was also updated to see if the callback is running, and set running to false if so, to indicate to the worker that it needs to be destroyed. If the callback for that timer is not running, destroy destroys the Instance itself. timer.h #ifndef TIMER_H #define TIMER_H #include <thread> #include <mutex> #include <condition_variable> #include <algorithm> #include <functional> #include <chrono> #include <unordered_map> #include <set> #include <cstdint> class Timer { public : typedef uint64_t timer_id ; typedef std :: function < void () > handler_type ; private : std :: mutex sync ; typedef std :: unique_lock < std :: mutex > ScopedLock ; std :: condition_variable wakeUp ; private : typedef std :: chrono :: steady_clock Clock ; typedef std :: chrono :: time_point < Clock > Timestamp ; typedef std :: chrono :: milliseconds Duration ; struct Instance { Instance ( timer_id id = 0 ) : id ( id ) , running ( false ) { } template < typename Tfunction > Instance ( timer_id id , Timestamp next , Duration period , Tfunction && handler ) noexcept : id ( id ) , next ( next ) , period ( period ) , handler ( std :: forward < Tfunction > ( handler )) , running ( false ) { } Instance ( Instance const & r ) = delete ; Instance ( Instance && r ) noexcept : id ( r . id ) , next ( r . next ) , period ( r . period ) , handler ( std :: move ( r . handler )) , running ( r . running ) { } Instance & operator = ( Instance const & r ) = delete ; Instance & operator = ( Instance && r ) { if ( this != & r ) { id = r . id ; next = r . next ; period = r . period ; handler = std :: move ( r . handler ); running = r . running ; } return * this ; } timer_id id ; Timestamp next ; Duration period ; handler_type handler ; bool running ; }; typedef std :: unordered_map < timer_id , Instance > InstanceMap ; timer_id nextId ; InstanceMap active ; // Comparison functor to sort the timer \"queue\" by Instance::next struct NextActiveComparator { bool operator ()( const Instance & a , const Instance & b ) const { return a . next < b . next ; } }; NextActiveComparator comparator ; // Queue is a set of references to Instance objects, sorted by next typedef std :: reference_wrapper < Instance > QueueValue ; typedef std :: multiset < QueueValue , NextActiveComparator > Queue ; Queue queue ; // Thread and exit flag std :: thread worker ; bool done ; void threadStart (); public : Timer (); ~ Timer (); timer_id create ( uint64_t when , uint64_t period , const handler_type & handler ); timer_id create ( uint64_t when , uint64_t period , handler_type && handler ); private : timer_id createImpl ( Instance && item ); public : bool destroy ( timer_id id ); bool exists ( timer_id id ); }; #endif // TIMER_H timer.cpp #include \"timer.h\" void Timer :: threadStart () { ScopedLock lock ( sync ); while ( ! done ) { if ( queue . empty ()) { // Wait (forever) for work wakeUp . wait ( lock ); } else { auto firstInstance = queue . begin (); Instance & instance = * firstInstance ; auto now = Clock :: now (); if ( now >= instance . next ) { queue . erase ( firstInstance ); // Mark it as running to handle racing destroy instance . running = true ; // Call the handler lock . unlock (); instance . handler (); lock . lock (); if ( done ) { break ; } else if ( ! instance . running ) { // Running was set to false, destroy was called // for this Instance while the callback was in progress // (this thread was not holding the lock during the callback) active . erase ( instance . id ); } else { instance . running = false ; // If it is periodic, schedule a new one if ( instance . period . count () > 0 ) { instance . next = instance . next + instance . period ; queue . insert ( instance ); } else { active . erase ( instance . id ); } } } else { // Wait until the timer is ready or a timer creation notifies wakeUp . wait_until ( lock , instance . next ); } } } } Timer :: Timer () : nextId ( 1 ) , queue ( comparator ) , done ( false ) { ScopedLock lock ( sync ); worker = std :: thread ( std :: bind ( & Timer :: threadStart , this )); } Timer ::~ Timer () { ScopedLock lock ( sync ); done = true ; wakeUp . notify_all (); lock . unlock (); worker . join (); } Timer :: timer_id Timer :: create ( uint64_t msFromNow , uint64_t msPeriod , const std :: function < void () > & handler ) { return createImpl ( Instance ( 0 , Clock :: now () + Duration ( msFromNow ), Duration ( msPeriod ), handler )); } Timer :: timer_id Timer :: create ( uint64_t msFromNow , uint64_t msPeriod , std :: function < void () >&& handler ) { return createImpl ( Instance ( 0 , Clock :: now () + Duration ( msFromNow ), Duration ( msPeriod ), std :: move ( handler ))); } Timer :: timer_id Timer :: createImpl ( Instance && item ) { ScopedLock lock ( sync ); item . id = nextId ++ ; auto iter = active . emplace ( item . id , std :: move ( item )); queue . insert ( iter . first -> second ); wakeUp . notify_all (); return item . id ; } bool Timer :: destroy ( timer_id id ) { ScopedLock lock ( sync ); auto i = active . find ( id ); if ( i == active . end ()) return false ; else if ( i -> second . running ) { // A callback is in progress for this Instance, // so flag it for deletion in the worker i -> second . running = false ; } else { queue . erase ( std :: ref ( i -> second )); active . erase ( i ); } wakeUp . notify_all (); return true ; } bool Timer :: exists ( timer_id id ) { ScopedLock lock ( sync ); return active . find ( id ) != active . end (); } Example: #include \"timer.h\" int main () { Timer t ; // Timer fires once, one second from now t . create ( 1000 , 0 , []() { std :: cout << \"Non-periodic timer fired\" << std :: endl ; }); // Timer fires every second, starting five seconds from now t . create ( 5000 , 1000 , []() { std :: cout << \"Timer fired 0\" << std :: endl ; }); // Timer fires every second, starting now t . create ( 0 , 1000 , []() { std :: cout << \"Timer fired 1\" << std :: endl ; }); // Timer fires every 100ms, starting now t . create ( 0 , 100 , []() { std :: cout << \"Timer fired 2\" << std :: endl ; }); } The third parameter is a std::function, so it could call a method on some instance of an object, like this: class Foo { public : void bar () { std :: cout << \"Foo::bar called\" << std :: endl ; } }; int something () { Foo example ; // Assume \"t\" is a Timer auto tid = t . create ( 0 , 100 , std :: bind ( & Foo :: bar , & example )); // ... do stuff ... The exists method is intended for scenarios where you desperately need to wait for a timer to go away before something goes out of scope. Definitely not pretty but I don't expect this scenario to occur a lot in my intended use cases: // Not pretty, but better than nothing: t . destroy ( tid ); while ( t . exists ( tid )) std :: this_thread :: sleep_for ( std :: chrono :: milliseconds ( 100 )); }","title":"Portable periodic/one-shot timer implementation"},{"location":"Event-driven-concurrent-server/Timer/cpp-event-loop/","text":"C++ std::thread Event Loop with Message Queue and Timer Introduction An event loop, or sometimes called a message loop, is a thread that waits for and dispatches incoming events. The thread blocks waiting for requests to arrive and then dispatches the event to an event handler function . A message queue is typically used by the loop to hold incoming messages. Each message is sequentially dequeued, decoded, and then an action is performed. Event loops are one way to implement inter-process communication. All operating systems provide support for multi-threaded applications. Each OS has unique function calls for creating threads , message queues and timers . With the advent of the C++11 thread support library, it\u2019s now possible to create portable code and avoid the OS-specific function calls. This article provides a simple example of how to create a thread event loop, message queue and timer services while only relying upon the C++ Standard Library. Any C++11 compiler supporting the thread library should be able to compile the attached source. Background Typically, I need a thread to operate as an event loop. Incoming messages are dequeued by the thread and data is dispatched to an appropriate function handler based on a unique message identifier. Timer support capable of invoking a function is handy for low speed polling or to generate a timeout if something doesn\u2019t happen in the expected amount of time. Many times, the worker thread is created at startup and isn\u2019t destroyed until the application terminates. A key requirement for the implementation is that the incoming messages must execute on the same thread instance. Whereas say std::async may use a temporary thread from a pool, this class ensures that all incoming messages use the same thread. For instance, a subsystem could be implemented with code that is not thread-safe. A single WorkerThread instance is used to safely dispatch function calls into the subsystem. At first glance, the C++ thread support seems to be missing some key features. Yes, std::thread is available to spin off(\u7529\u6389) a thread but there is no thread-safe queue and no timers \u2013 services that most OS\u2019s provide. I\u2019ll show how to use the C++ Standard Library to create these \u201cmissing\u201d features and provide an event processing loop familiar to many programmers. WorkerThread The WorkerThread class encapsulates all the necessary event loop mechanisms. A simple class interface allows thread creation, posting messages to the event loop, and eventual thread termination. The interface is shown below: Hide Shrink Copy Code class WorkerThread { public : /// Constructor WorkerThread ( const char * threadName ); /// Destructor ~ WorkerThread (); /// Called once to create the worker thread /// @return TRUE if thread is created. FALSE otherwise. bool CreateThread (); /// Called once a program exit to exit the worker thread void ExitThread (); /// Get the ID of this thread instance /// @return The worker thread ID std :: thread :: id GetThreadId (); /// Get the ID of the currently executing thread /// @return The current thread ID static std :: thread :: id GetCurrentThreadId (); /// Add a message to thread queue. /// @param[in] data - thread specific information created on the heap using operator new. void PostMsg ( const UserData * data ); private : WorkerThread ( const WorkerThread & ); WorkerThread & operator = ( const WorkerThread & ); /// Entry point for the worker thread void Process (); /// Entry point for timer thread void TimerThread (); std :: thread * m_thread ; std :: queue < ThreadMsg *> m_queue ; std :: mutex m_mutex ; std :: condition_variable m_cv ; std :: atomic < bool > m_timerExit ; const char * THREAD_NAME ; }; The first thing to notice is that std::thread is used to create a main worker thread. The main worker thread function is Process() . Hide Copy Code bool WorkerThread :: CreateThread () { if ( ! m_thread ) m_thread = new thread ( & WorkerThread :: Process , this ); return true ; } Event Loop The Process() event loop is shown below. The thread relies upon a std::queue<ThreadMsg*> for the message queue. std::queue is not thread-safe so all access to the queue must be protected by mutex. A std::condition_variable is used to suspend the thread until notified that a new message has been added to the queue. Hide Shrink Copy Code void WorkerThread :: Process () { m_timerExit = false ; std :: thread timerThread ( & WorkerThread :: TimerThread , this ); while ( 1 ) { ThreadMsg * msg = 0 ; { // Wait for a message to be added to the queue std :: unique_lock < std :: mutex > lk ( m_mutex ); while ( m_queue . empty ()) m_cv . wait ( lk ); if ( m_queue . empty ()) continue ; msg = m_queue . front (); m_queue . pop (); } switch ( msg -> id ) { case MSG_POST_USER_DATA : { ASSERT_TRUE ( msg -> msg != NULL ); // Convert the ThreadMsg void* data back to a UserData* const UserData * userData = static_cast < const UserData *> ( msg -> msg ); cout << userData -> msg . c_str () << \" \" << userData -> year << \" on \" << THREAD_NAME << endl ; // Delete dynamic data passed through message queue delete userData ; delete msg ; break ; } case MSG_TIMER : cout << \"Timer expired on \" << THREAD_NAME << endl ; delete msg ; break ; case MSG_EXIT_THREAD : { m_timerExit = true ; timerThread . join (); delete msg ; std :: unique_lock < std :: mutex > lk ( m_mutex ); while ( ! m_queue . empty ()) { msg = m_queue . front (); m_queue . pop (); delete msg ; } cout << \"Exit thread on \" << THREAD_NAME << endl ; return ; } default : ASSERT (); } } } PostMsg() creates a new ThreadMsg on the heap, adds the message to the queue, and then notifies the worker thread using a condition variable. Hide Copy Code void WorkerThread::PostMsg(const UserData* data) { ASSERT_TRUE(m_thread); ThreadMsg* threadMsg = new ThreadMsg(MSG_POST_USER_DATA, data); // Add user data msg to queue and notify worker thread std::unique_lock<std::mutex> lk(m_mutex); m_queue.push(threadMsg); m_cv.notify_one(); } The loop will continue to process messages until the MSG_EXIT_THREAD is received and the thread exits. Hide Copy Code void WorkerThread::ExitThread() { if (!m_thread) return; // Create a new ThreadMsg ThreadMsg* threadMsg = new ThreadMsg(MSG_EXIT_THREAD, 0); // Put exit thread message into the queue { lock_guard<mutex> lock(m_mutex); m_queue.push(threadMsg); m_cv.notify_one(); } m_thread->join(); delete m_thread; m_thread = 0; } Event Loop (Win32) The code snippet below contrasts the std::thread event loop above with a similar Win32 version using the Windows API. Notice GetMessage() API is used in lieu of the std::queue . Messages are posted to the OS message queue using PostThreadMessage() . And finally, timerSetEvent() is used to place WM_USER_TIMER messages into the queue. All of these services are provided by the OS. The std::thread WorkerThread implementation presented here avoids the raw OS calls yet the implementation functionality is the same as the Win32 version while relying only upon only the C++ Standard Library. Hide Shrink Copy Code unsigned long WorkerThread::Process(void* parameter) { MSG msg; BOOL bRet; // Start periodic timer MMRESULT timerId = timeSetEvent(250, 10, &WorkerThread::TimerExpired, reinterpret_cast<DWORD>(this), TIME_PERIODIC); while ((bRet = GetMessage(&msg, NULL, WM_USER_BEGIN, WM_USER_END)) != 0) { switch (msg.message) { case WM_DISPATCH_DELEGATE: { ASSERT_TRUE(msg.wParam != NULL); // Convert the ThreadMsg void* data back to a UserData* const UserData* userData = static_cast<const UserData*>(msg.wParam); cout << userData->msg.c_str() << \" \" << userData->year << \" on \" << THREAD_NAME << endl; // Delete dynamic data passed through message queue delete userData; break; } case WM_USER_TIMER: cout << \"Timer expired on \" << THREAD_NAME << endl; break; case WM_EXIT_THREAD: timeKillEvent(timerId); return 0; default: ASSERT(); } } return 0; } Timer A low-resolution periodic timer message is inserted into the queue using a secondary private thread. The timer thread is created inside Process() . Hide Copy Code void WorkerThread::Process() { m_timerExit = false; std::thread timerThread(&WorkerThread::TimerThread, this); ... The timer thread\u2019s sole responsibility is to insert a MSG_TIMER message every 250ms. In this implementation, there\u2019s no protection against the timer thread injecting more than one timer message into the queue. This could happen if the worker thread falls behind and can\u2019t service the message queue fast enough. Depending on the worker thread, processing load, and how fast the timer messages are inserted, additional logic could be employed to prevent flooding the queue. Hide Copy Code void WorkerThread::TimerThread() { while (!m_timerExit) { // Sleep for 250ms then put a MSG_TIMER message into queue std::this_thread::sleep_for(250ms); ThreadMsg* threadMsg = new ThreadMsg(MSG_TIMER, 0); // Add timer msg to queue and notify worker thread std::unique_lock<std::mutex> lk(m_mutex); m_queue.push(threadMsg); m_cv.notify_one(); } } Usage The main() function below shows how to use the WorkerThread class. Two worker threads are created and a message is posted to each one. After a short delay, both threads exit. Hide Shrink Copy Code // Worker thread instances WorkerThread workerThread1 ( \"WorkerThread1\" ); WorkerThread workerThread2 ( \"WorkerThread2\" ); int main ( void ) { // Create worker threads workerThread1 . CreateThread (); workerThread2 . CreateThread (); // Create message to send to worker thread 1 UserData * userData1 = new UserData (); userData1 -> msg = \"Hello world\" ; userData1 -> year = 2017 ; // Post the message to worker thread 1 workerThread1 . PostMsg ( userData1 ); // Create message to send to worker thread 2 UserData * userData2 = new UserData (); userData2 -> msg = \"Goodbye world\" ; userData2 -> year = 2017 ; // Post the message to worker thread 2 workerThread2 . PostMsg ( userData2 ); // Give time for messages processing on worker threads this_thread :: sleep_for ( 1 s ); workerThread1 . ExitThread (); workerThread2 . ExitThread (); return 0 ; } Conclusion The C++ thread support library offers a platform independent way to write multi-threaded application code without reliance upon OS-specific API\u2019s. The WorkerThread class presented here is a bare-bones implementation of an event loop, yet all the basics are there ready to be expanded upon.","title":"[C++ std::thread Event Loop with Message Queue and Timer](https://www.codeproject.com/Articles/1169105/%2FArticles%2F1169105%2FCplusplus-std-thread-Event-Loop-with-Message-Queue)"},{"location":"Event-driven-concurrent-server/Timer/cpp-event-loop/#c#stdthread#event#loop#with#message#queue#and#timer","text":"","title":"C++ std::thread Event Loop with Message Queue and Timer"},{"location":"Event-driven-concurrent-server/Timer/cpp-event-loop/#introduction","text":"An event loop, or sometimes called a message loop, is a thread that waits for and dispatches incoming events. The thread blocks waiting for requests to arrive and then dispatches the event to an event handler function . A message queue is typically used by the loop to hold incoming messages. Each message is sequentially dequeued, decoded, and then an action is performed. Event loops are one way to implement inter-process communication. All operating systems provide support for multi-threaded applications. Each OS has unique function calls for creating threads , message queues and timers . With the advent of the C++11 thread support library, it\u2019s now possible to create portable code and avoid the OS-specific function calls. This article provides a simple example of how to create a thread event loop, message queue and timer services while only relying upon the C++ Standard Library. Any C++11 compiler supporting the thread library should be able to compile the attached source.","title":"Introduction"},{"location":"Event-driven-concurrent-server/Timer/cpp-event-loop/#background","text":"Typically, I need a thread to operate as an event loop. Incoming messages are dequeued by the thread and data is dispatched to an appropriate function handler based on a unique message identifier. Timer support capable of invoking a function is handy for low speed polling or to generate a timeout if something doesn\u2019t happen in the expected amount of time. Many times, the worker thread is created at startup and isn\u2019t destroyed until the application terminates. A key requirement for the implementation is that the incoming messages must execute on the same thread instance. Whereas say std::async may use a temporary thread from a pool, this class ensures that all incoming messages use the same thread. For instance, a subsystem could be implemented with code that is not thread-safe. A single WorkerThread instance is used to safely dispatch function calls into the subsystem. At first glance, the C++ thread support seems to be missing some key features. Yes, std::thread is available to spin off(\u7529\u6389) a thread but there is no thread-safe queue and no timers \u2013 services that most OS\u2019s provide. I\u2019ll show how to use the C++ Standard Library to create these \u201cmissing\u201d features and provide an event processing loop familiar to many programmers.","title":"Background"},{"location":"Event-driven-concurrent-server/Timer/cpp-event-loop/#workerthread","text":"The WorkerThread class encapsulates all the necessary event loop mechanisms. A simple class interface allows thread creation, posting messages to the event loop, and eventual thread termination. The interface is shown below: Hide Shrink Copy Code class WorkerThread { public : /// Constructor WorkerThread ( const char * threadName ); /// Destructor ~ WorkerThread (); /// Called once to create the worker thread /// @return TRUE if thread is created. FALSE otherwise. bool CreateThread (); /// Called once a program exit to exit the worker thread void ExitThread (); /// Get the ID of this thread instance /// @return The worker thread ID std :: thread :: id GetThreadId (); /// Get the ID of the currently executing thread /// @return The current thread ID static std :: thread :: id GetCurrentThreadId (); /// Add a message to thread queue. /// @param[in] data - thread specific information created on the heap using operator new. void PostMsg ( const UserData * data ); private : WorkerThread ( const WorkerThread & ); WorkerThread & operator = ( const WorkerThread & ); /// Entry point for the worker thread void Process (); /// Entry point for timer thread void TimerThread (); std :: thread * m_thread ; std :: queue < ThreadMsg *> m_queue ; std :: mutex m_mutex ; std :: condition_variable m_cv ; std :: atomic < bool > m_timerExit ; const char * THREAD_NAME ; }; The first thing to notice is that std::thread is used to create a main worker thread. The main worker thread function is Process() . Hide Copy Code bool WorkerThread :: CreateThread () { if ( ! m_thread ) m_thread = new thread ( & WorkerThread :: Process , this ); return true ; }","title":"WorkerThread"},{"location":"Event-driven-concurrent-server/Timer/cpp-event-loop/#event#loop","text":"The Process() event loop is shown below. The thread relies upon a std::queue<ThreadMsg*> for the message queue. std::queue is not thread-safe so all access to the queue must be protected by mutex. A std::condition_variable is used to suspend the thread until notified that a new message has been added to the queue. Hide Shrink Copy Code void WorkerThread :: Process () { m_timerExit = false ; std :: thread timerThread ( & WorkerThread :: TimerThread , this ); while ( 1 ) { ThreadMsg * msg = 0 ; { // Wait for a message to be added to the queue std :: unique_lock < std :: mutex > lk ( m_mutex ); while ( m_queue . empty ()) m_cv . wait ( lk ); if ( m_queue . empty ()) continue ; msg = m_queue . front (); m_queue . pop (); } switch ( msg -> id ) { case MSG_POST_USER_DATA : { ASSERT_TRUE ( msg -> msg != NULL ); // Convert the ThreadMsg void* data back to a UserData* const UserData * userData = static_cast < const UserData *> ( msg -> msg ); cout << userData -> msg . c_str () << \" \" << userData -> year << \" on \" << THREAD_NAME << endl ; // Delete dynamic data passed through message queue delete userData ; delete msg ; break ; } case MSG_TIMER : cout << \"Timer expired on \" << THREAD_NAME << endl ; delete msg ; break ; case MSG_EXIT_THREAD : { m_timerExit = true ; timerThread . join (); delete msg ; std :: unique_lock < std :: mutex > lk ( m_mutex ); while ( ! m_queue . empty ()) { msg = m_queue . front (); m_queue . pop (); delete msg ; } cout << \"Exit thread on \" << THREAD_NAME << endl ; return ; } default : ASSERT (); } } } PostMsg() creates a new ThreadMsg on the heap, adds the message to the queue, and then notifies the worker thread using a condition variable. Hide Copy Code void WorkerThread::PostMsg(const UserData* data) { ASSERT_TRUE(m_thread); ThreadMsg* threadMsg = new ThreadMsg(MSG_POST_USER_DATA, data); // Add user data msg to queue and notify worker thread std::unique_lock<std::mutex> lk(m_mutex); m_queue.push(threadMsg); m_cv.notify_one(); } The loop will continue to process messages until the MSG_EXIT_THREAD is received and the thread exits. Hide Copy Code void WorkerThread::ExitThread() { if (!m_thread) return; // Create a new ThreadMsg ThreadMsg* threadMsg = new ThreadMsg(MSG_EXIT_THREAD, 0); // Put exit thread message into the queue { lock_guard<mutex> lock(m_mutex); m_queue.push(threadMsg); m_cv.notify_one(); } m_thread->join(); delete m_thread; m_thread = 0; }","title":"Event Loop"},{"location":"Event-driven-concurrent-server/Timer/cpp-event-loop/#event#loop#win32","text":"The code snippet below contrasts the std::thread event loop above with a similar Win32 version using the Windows API. Notice GetMessage() API is used in lieu of the std::queue . Messages are posted to the OS message queue using PostThreadMessage() . And finally, timerSetEvent() is used to place WM_USER_TIMER messages into the queue. All of these services are provided by the OS. The std::thread WorkerThread implementation presented here avoids the raw OS calls yet the implementation functionality is the same as the Win32 version while relying only upon only the C++ Standard Library. Hide Shrink Copy Code unsigned long WorkerThread::Process(void* parameter) { MSG msg; BOOL bRet; // Start periodic timer MMRESULT timerId = timeSetEvent(250, 10, &WorkerThread::TimerExpired, reinterpret_cast<DWORD>(this), TIME_PERIODIC); while ((bRet = GetMessage(&msg, NULL, WM_USER_BEGIN, WM_USER_END)) != 0) { switch (msg.message) { case WM_DISPATCH_DELEGATE: { ASSERT_TRUE(msg.wParam != NULL); // Convert the ThreadMsg void* data back to a UserData* const UserData* userData = static_cast<const UserData*>(msg.wParam); cout << userData->msg.c_str() << \" \" << userData->year << \" on \" << THREAD_NAME << endl; // Delete dynamic data passed through message queue delete userData; break; } case WM_USER_TIMER: cout << \"Timer expired on \" << THREAD_NAME << endl; break; case WM_EXIT_THREAD: timeKillEvent(timerId); return 0; default: ASSERT(); } } return 0; }","title":"Event Loop (Win32)"},{"location":"Event-driven-concurrent-server/Timer/cpp-event-loop/#timer","text":"A low-resolution periodic timer message is inserted into the queue using a secondary private thread. The timer thread is created inside Process() . Hide Copy Code void WorkerThread::Process() { m_timerExit = false; std::thread timerThread(&WorkerThread::TimerThread, this); ... The timer thread\u2019s sole responsibility is to insert a MSG_TIMER message every 250ms. In this implementation, there\u2019s no protection against the timer thread injecting more than one timer message into the queue. This could happen if the worker thread falls behind and can\u2019t service the message queue fast enough. Depending on the worker thread, processing load, and how fast the timer messages are inserted, additional logic could be employed to prevent flooding the queue. Hide Copy Code void WorkerThread::TimerThread() { while (!m_timerExit) { // Sleep for 250ms then put a MSG_TIMER message into queue std::this_thread::sleep_for(250ms); ThreadMsg* threadMsg = new ThreadMsg(MSG_TIMER, 0); // Add timer msg to queue and notify worker thread std::unique_lock<std::mutex> lk(m_mutex); m_queue.push(threadMsg); m_cv.notify_one(); } }","title":"Timer"},{"location":"Event-driven-concurrent-server/Timer/cpp-event-loop/#usage","text":"The main() function below shows how to use the WorkerThread class. Two worker threads are created and a message is posted to each one. After a short delay, both threads exit. Hide Shrink Copy Code // Worker thread instances WorkerThread workerThread1 ( \"WorkerThread1\" ); WorkerThread workerThread2 ( \"WorkerThread2\" ); int main ( void ) { // Create worker threads workerThread1 . CreateThread (); workerThread2 . CreateThread (); // Create message to send to worker thread 1 UserData * userData1 = new UserData (); userData1 -> msg = \"Hello world\" ; userData1 -> year = 2017 ; // Post the message to worker thread 1 workerThread1 . PostMsg ( userData1 ); // Create message to send to worker thread 2 UserData * userData2 = new UserData (); userData2 -> msg = \"Goodbye world\" ; userData2 -> year = 2017 ; // Post the message to worker thread 2 workerThread2 . PostMsg ( userData2 ); // Give time for messages processing on worker threads this_thread :: sleep_for ( 1 s ); workerThread1 . ExitThread (); workerThread2 . ExitThread (); return 0 ; }","title":"Usage"},{"location":"Event-driven-concurrent-server/Timer/cpp-event-loop/#conclusion","text":"The C++ thread support library offers a platform independent way to write multi-threaded application code without reliance upon OS-specific API\u2019s. The WorkerThread class presented here is a bare-bones implementation of an event loop, yet all the basics are there ready to be expanded upon.","title":"Conclusion"},{"location":"Event-driven-concurrent-server/Timer/cpp-ticker/","text":"[How to call a function every x seconds closed] A A [c++] I want a timer to trigger a function every two seconds without interfering with the rest of the program Creating a ticker thread Thread-Pool-Timer C++ 11: Calling a C++ function periodically Linux, timerfd accuracy [How to call a function every x seconds closed] comments : for asking whether he needs polling approach or asynchronous timer. \u2013 Ahmed Saleh A There are two approaches: 1.Asynchronous 2. Synchronous Assuming that you are using Win32, C++ . You can use Win32 API SetTimer UINT_PTR timerid = SetTimer(NULL, 0, milliseconds, &callback); If you would like a Polling Approach You would better use something like that for (;;) { Say_Hello (); // Sleep for 50*1000ms Sleep ( 50000 ); } A The Boost library provides for this in Boost.Asio , and explicitly covers this in its tutorials: Synchronous timer , i.e. waiting until the timer expires. Asynchronous timer , i.e. continuing with your program and having the callback function invoked when the timer expires. If you didn't find the Boost library when searching the web for C++, your google-fu is weak. ;-) [ c++] I want a timer to trigger a function every two seconds without interfering with the rest of the program That code will waste an awful lot of processor cycles constantly checking the time as frequently as it possibly can. Adding a simple sleep call to the above loop would make it much more processor friendly, but I probably wouldn't do it that way at all. I've used boost::asio with success before. It provides a nice portable solution, though some people are opposed to using boost. The deadline_timer is useful for this kind of thing: boost :: asio :: deadline_timer timer ( io_service ); // asynchronously calls handler after 2 seoncds timer . expires_from_now ( boost :: posix_time :: seconds ( 2 )) timer . async_wait ( handler ); Creating a ticker thread A new question about a revised version of the code in this question, based on the advice in the accepted answer can be found here. I need to signal my program to do something every X seconds. So I decided to make a separate thread that sends such a signal using ZeroMQ . All other behaviour is also triggered by ZeroMQ messages, so this seems like a safe way to do it, with respect to multithreading. I have never used threads in C++ before though, so I'd like some feedback on my code. Note that in the code below I have replaced the actual task with a simple cout statement. EDIT: I just realized the thread could very well terminate between setting run = false and calling join , which causes a std::system_error to be thrown. Putting the join statement in an if -block with isjoinable still suffers from the same problem. How should I handle this? try-catch? Using a locking mechanism instead of an atomic bool? Ticker.hpp #include <thread> #include <chrono> #include <atomic> class Ticker { private : const unsigned int interval ; std :: atomic_bool run ; std :: thread tickthread ; public : Ticker ( unsigned int interval ); Ticker ( const Ticker & orig ) = delete ; virtual ~ Ticker (); void stop (); private : void tickfunction (); }; Ticker.cpp #include \"Ticker.hpp\" #include <iostream> using namespace std ; Ticker :: Ticker ( unsigned int interval ) : interval ( interval ), run ( true ), tickthread ( & Ticker :: tickfunction , this ) { } Ticker ::~ Ticker () { if ( tickthread . joinable ()) { stop (); } } void Ticker :: stop () { cout << \"stopping...\" << endl ; run = false ; // interrupt sleep? tickthread . join (); cout << \"joined\" << endl ; } void Ticker :: tickfunction () { while ( true ) { this_thread :: sleep_for ( chrono :: seconds ( interval )); if ( run ) { cout << \"tick\" << endl ; } else { break ; } } } There are also two specific things I'd like recommendations on: How can I change this class to accept any std::chrono::duration object as input for the interval, instead of just an integer? I tried this, but couldn't figure out how to do it with the templates. Is there a way to interrupt the sleep_for when I call stop() ? Especially when interval is large, it'd be nice to not have to wait for the sleeping thread when stopping it. Thread-Pool-Timer TODO C++ 11: Calling a C++ function periodically SUMMARY : \u8fd9\u7bc7\u6587\u7ae0\u7684\u4ee3\u7801\u867d\u7136\u4e0d\u7b26\u5408\u6211\u7684\u8981\u6c42\uff0c\u4f46\u662f\u5374\u5bf9\u4e8e\u5b66\u4e60 c++ thread \u6709\u53c2\u8003\u4ef7\u503c\u3002 TODO Linux, timerfd accuracy TODO","title":"Cpp ticker"},{"location":"Event-driven-concurrent-server/Timer/cpp-ticker/#how#to#call#a#function#every#x#seconds#closed","text":"comments : for asking whether he needs polling approach or asynchronous timer. \u2013 Ahmed Saleh","title":"[How to call a function every x seconds closed]"},{"location":"Event-driven-concurrent-server/Timer/cpp-ticker/#a","text":"There are two approaches: 1.Asynchronous 2. Synchronous Assuming that you are using Win32, C++ . You can use Win32 API SetTimer UINT_PTR timerid = SetTimer(NULL, 0, milliseconds, &callback); If you would like a Polling Approach You would better use something like that for (;;) { Say_Hello (); // Sleep for 50*1000ms Sleep ( 50000 ); }","title":"A"},{"location":"Event-driven-concurrent-server/Timer/cpp-ticker/#a_1","text":"The Boost library provides for this in Boost.Asio , and explicitly covers this in its tutorials: Synchronous timer , i.e. waiting until the timer expires. Asynchronous timer , i.e. continuing with your program and having the callback function invoked when the timer expires. If you didn't find the Boost library when searching the web for C++, your google-fu is weak. ;-)","title":"A"},{"location":"Event-driven-concurrent-server/Timer/cpp-ticker/#c#i#want#a#timer#to#trigger#a#function#every#two#seconds#without#interfering#with#the#rest#of#the#program","text":"That code will waste an awful lot of processor cycles constantly checking the time as frequently as it possibly can. Adding a simple sleep call to the above loop would make it much more processor friendly, but I probably wouldn't do it that way at all. I've used boost::asio with success before. It provides a nice portable solution, though some people are opposed to using boost. The deadline_timer is useful for this kind of thing: boost :: asio :: deadline_timer timer ( io_service ); // asynchronously calls handler after 2 seoncds timer . expires_from_now ( boost :: posix_time :: seconds ( 2 )) timer . async_wait ( handler );","title":"[c++] I want a timer to trigger a function every two seconds without interfering with the rest of the program"},{"location":"Event-driven-concurrent-server/Timer/cpp-ticker/#creating#a#ticker#thread","text":"A new question about a revised version of the code in this question, based on the advice in the accepted answer can be found here. I need to signal my program to do something every X seconds. So I decided to make a separate thread that sends such a signal using ZeroMQ . All other behaviour is also triggered by ZeroMQ messages, so this seems like a safe way to do it, with respect to multithreading. I have never used threads in C++ before though, so I'd like some feedback on my code. Note that in the code below I have replaced the actual task with a simple cout statement. EDIT: I just realized the thread could very well terminate between setting run = false and calling join , which causes a std::system_error to be thrown. Putting the join statement in an if -block with isjoinable still suffers from the same problem. How should I handle this? try-catch? Using a locking mechanism instead of an atomic bool? Ticker.hpp #include <thread> #include <chrono> #include <atomic> class Ticker { private : const unsigned int interval ; std :: atomic_bool run ; std :: thread tickthread ; public : Ticker ( unsigned int interval ); Ticker ( const Ticker & orig ) = delete ; virtual ~ Ticker (); void stop (); private : void tickfunction (); }; Ticker.cpp #include \"Ticker.hpp\" #include <iostream> using namespace std ; Ticker :: Ticker ( unsigned int interval ) : interval ( interval ), run ( true ), tickthread ( & Ticker :: tickfunction , this ) { } Ticker ::~ Ticker () { if ( tickthread . joinable ()) { stop (); } } void Ticker :: stop () { cout << \"stopping...\" << endl ; run = false ; // interrupt sleep? tickthread . join (); cout << \"joined\" << endl ; } void Ticker :: tickfunction () { while ( true ) { this_thread :: sleep_for ( chrono :: seconds ( interval )); if ( run ) { cout << \"tick\" << endl ; } else { break ; } } } There are also two specific things I'd like recommendations on: How can I change this class to accept any std::chrono::duration object as input for the interval, instead of just an integer? I tried this, but couldn't figure out how to do it with the templates. Is there a way to interrupt the sleep_for when I call stop() ? Especially when interval is large, it'd be nice to not have to wait for the sleeping thread when stopping it.","title":"Creating a ticker thread"},{"location":"Event-driven-concurrent-server/Timer/cpp-ticker/#thread-pool-timer","text":"TODO","title":"Thread-Pool-Timer"},{"location":"Event-driven-concurrent-server/Timer/cpp-ticker/#c#11#calling#a#c#function#periodically","text":"SUMMARY : \u8fd9\u7bc7\u6587\u7ae0\u7684\u4ee3\u7801\u867d\u7136\u4e0d\u7b26\u5408\u6211\u7684\u8981\u6c42\uff0c\u4f46\u662f\u5374\u5bf9\u4e8e\u5b66\u4e60 c++ thread \u6709\u53c2\u8003\u4ef7\u503c\u3002 TODO","title":"C++ 11: Calling a C++ function periodically"},{"location":"Event-driven-concurrent-server/Timer/cpp-ticker/#linux#timerfd#accuracy","text":"TODO","title":"Linux, timerfd accuracy"},{"location":"Event-driven-concurrent-server/Timer/thread-implement-event-loop-based-timer/","text":"how to implement event loop based timer C++ std::thread Event Loop with Message Queue and Timer unix thread-based timer Is it possible to direct linux timer notification signal to a specific thread? The Node.js Event Loop, Timers, and process.nextTick() [Deeper understanding of event loops and timers closed] how to \u4ea4\u6613\u65f6\u95f4\u6bb5 \u7eafC++\u65b9\u5f0f \u4f7f\u7528OS\u63d0\u4f9b\u7684\u65b9\u6cd5 Is it possible to direct linux timer notification signal to a specific thread? A timer_create timerfd_create \u591a\u79cd\u5b9e\u73b0\u65b9\u5f0f how to implement event loop based timer \u9996\u5148\u6e90\u4e8e\u6211\u4e4b\u524d\u5728JavaScript\u4e2d\u4f7f\u7528\u8fc7 setInterval \uff0c\u800c\u6211\u73b0\u5728\u7684project\u4e2d\uff0c\u6709\u975e\u5e38\u591a\u57fa\u4e8e\u529f\u80fd\u662f\u9700\u8981\u7c7b\u4f3c setInterval \u529f\u80fd\u7684\uff0c\u6bd4\u5982\u5b9a\u65f6\u8bf7\u6c42celery\uff1b\u4e0d\u5e78\u7684\u662f\uff0c\u6211\u7684project\u4f7f\u7528\u7684\u662f c++ \uff0c\u6240\u4ee5\u6211\u7b2c\u4e00\u60f3\u6cd5\u662f\uff1a Google setIntervel C++\uff0c\u4ece\u68c0\u7d22\u7ed3\u679c\u4e2d\uff0c\u6211\u8bb0\u5f55\u4e86\u5982\u4e0b\u6709\u4ef7\u503c\u7684\uff1a Using setInterval() in C++ timercpp A Simple Timer in C++ \u5355\u72ec\u521b\u5efa\u4e00\u4e2athread\u6765\u5b8c\u6210\u8fd9\u4e2a\u5de5\u4f5c\uff0c\u8be5\u51fd\u6570\u7684\u5de5\u4f5c\u975e\u5e38\u7b80\u5355\uff0c\u5c31\u662f setinterval real time example of multithreading in c++ \u521a\u5f00\u59cb\uff0c \u6211\u9605\u8bfb\u7684\u662f A Simple Timer in C++ \uff0c\u663e\u7136\u8fd9\u662f\u4e00\u4e2a\u975e\u5e38\u5c0f\u578b\u7684\uff0c\u8003\u8651\u7684\u60c5\u51b5\u975e\u5e38\u5c11\uff1b \u63a5\u7740\u9605\u8bfb Using setInterval() in C++ \uff0c\u8fd9\u5176\u4e2d\u5bf9JavaScript\u7684 setIntervel \u5b9e\u73b0\u8fdb\u884c\u4e86\u4e00\u4e9b\u5206\u6790\uff1b C++ std::thread Event Loop with Message Queue and Timer unix thread-based timer https://davejingtian.org/2013/09/28/timer-queue-a-way-to-handle-multiple-timers-using-one-thread/ https://github.com/daveti/tq Is it possible to direct linux timer notification signal to a specific thread? timer event loop implementation[Deeper understanding of event loops and timers closed] The Node.js Event Loop, Timers, and process.nextTick() [Deeper understanding of event loops and timers closed] how to \u6709\u4e24\u79cd\u5b9a\u65f6\uff1a \u6bcf\u96943S\u53d6\u4e00\u6b21\u884c\u60c5 \u6bcf\u96940.5S\uff0c\u8bf7\u6c42\u4e00\u6b21celery \u8fd9\u4e24\u8005\u90fd\u9700\u8981\u6709\u4e00\u4e2a\u524d\u63d0\uff1a\u5f53\u524d\u65f6\u95f4\u5728\u4ea4\u6613\u65f6\u95f4\u6bb5\u5185\uff0c\u90a3\u5982\u4f55\u5b9e\u73b0\uff1f \u6709\u591a\u79cd\u65b9\u5f0f\uff1a \u65b9\u5f0f1\uff1a\u4e0a\u8ff0\u4e24\u8005\u5206\u5f00\u6765\u8fdb\u884c\u5b9e\u73b0\uff0c\u5206\u522b\u641e\u4e00\u4e2a\u7ebf\u7a0b\uff0c\u4e24\u4e2a\u7ebf\u7a0b\uff0c\u90fd\u53ea\u9700\u8981\u6267\u884c setInterval \uff0c\u5728\u6bcf\u4e2ainterval\u4e2d\uff0c\u90fd\u9700\u8981\u6267\u884c\u5224\u65ad\u5f53\u524d\u662f\u5426\u5904\u4e8e\u4ea4\u6613\u65f6\u95f4\uff1b \u65b9\u5f0f2\uff1a\u5c06\u4e24\u8005\u653e\u5728\u540c\u4e00\u4e2athread\u4e2d\uff0c\u8fd9\u4e2athread\u6267\u884csetInterval\uff0c\u5728\u6bcf\u4e2ainterval\u4e2d\uff0c\u5148\u53d6\u884c\u60c5\uff0c\u7136\u540e\u518d\u6bcf\u96940.5S\uff0c\u8bf7\u6c42\u4e00\u6b21celery\uff1b \u4e0a\u8ff0\u4e24\u79cd\u65b9\u5f0f\uff0c\u90fd\u662fblocking\u7684\uff0c\u5373timer\u548c\u6267\u884c\u5904\u4e8e\u540c\u4e00\u4e2athread\uff1b\u8fd9\u603b\u65b9\u5f0f\u6bd4\u8f83\u4e0d\u597d\u7684\u5c31\u662ftimer\u7684\u5b9e\u73b0\u6bd4\u8f83\u590d\u6742\uff0c\u5b83\u9700\u8981\u8ba1\u7b97\u5728\u672cinterval\u4e2d\uff0c\u6240\u6709\u64cd\u4f5c\u7684\u6267\u884c\u65f6\u95f4\u7136\u540e\u518d\u6765\u786e\u5b9a\u5269\u4f59\u7684\u7761\u7720\u65f6\u95f4\uff1b\u66f4\u597d\u7684\u65b9\u5f0f\u662fnon-blocking\u7684\uff0c\u5373\u5c06\u8ba1\u65f6\u548c\u8d85\u65f6\u52a8\u4f5c\u5206\u9694\u5f00\u6765\uff1b \u6240\u4ee5\uff0c\u4e00\u4e2a\u95ee\u9898\u4e00\u4e2a\u95ee\u9898\u6765\u8fdb\u884c\u5904\u7406\uff1a \u4ea4\u6613\u65f6\u95f4\u6bb5 \u7cfb\u7edf\u7684\u65f6\u5e8f\u5e94\u8be5\u662f\u8fd9\u6837\u7684\uff1a\u5f00\u5e02\u5f00\u59cb\u8fd0\u884c\u7cfb\u7edf\uff1b\u7cfb\u7edf\u5f00\u59cb\u8fd0\u884c\uff0c\u6267\u884c\u6307\u5b9a\u7684\u64cd\u4f5c\uff1b\u95ed\u5e02\u505c\u6b62\u7cfb\u7edf\uff1b \u4f46\u662f\u76ee\u524d\u7684\u5b9e\u73b0\u65b9\u5f0f\u662f\uff1a\u6bcf\u6b21\u90fd\u53bb\u6821\u9a8c\u5f53\u524d\u65f6\u95f4\u662f\u5426\u5728\u4ea4\u6613\u65f6\u95f4\uff1b\u8fd9\u79cd\u505a\u6cd5\u4f1a\u5bfc\u81f4\u7f16\u7a0b\u7684\u590d\u6742\uff0c\u7cfb\u7edf\u7ef4\u62a4\u7684\u56f0\u96be\uff1b \u6bd4\u8f83\u597d\u7684\u505a\u6cd5\u662f\uff0c\u6709\u4e13\u95e8\u7684\u5b9a\u65f6\u7ebf\u7a0b\uff0c\u5f53\u5176\u76d1\u63a7\u5230\uff0c\u5df2\u7ecf\u5230\u8fbe\u4e86\u5f00\u5e02\u65f6\u95f4\u65f6\uff0c\u5219\u542f\u52a8\u7cfb\u7edf\uff1b\u5f53\u5176\u76d1\u63a7\u5230\u5230\u8fbe\u4e86\u95ed\u5e02\u65f6\u95f4\u540e\uff0c\u5219\u505c\u6b62\u7cfb\u7edf\uff1b \u5b9e\u73b0\u8fd9\u4e2a\u529f\u80fd\uff0c\u5c31\u53ef\u4ee5\u6709\u591a\u79cd\u65b9\u5f0f\uff1a\u53ef\u4ee5\u4f7f\u7528\u7eaf c++ \u7684\u65b9\u6cd5\uff0c\u4e5f\u53ef\u4ee5\u4f7f\u7528OS\u63d0\u4f9b\u7684\u65b9\u6cd5\uff1b \u7eaf C++ \u65b9\u5f0f How to create timer events using C++ 11? \u8fd9\u7bc7\u6587\u7ae0\u4e2d\u7684 \u8fd9\u4e2a\u7b54\u6848 \u662f\u5177\u6709\u501f\u9274\u610f\u4e49\u7684\uff0c\u4f46\u662f\u5b83\u5e76\u4e0d\u9002\u5408\u6211\u7684project\uff0c\u56e0\u4e3a\u5b83\u4e3a\u6bcf\u4e2atimer event\u90fd\u521b\u5efa\u4e00\u4e2athread\uff0c\u5e76\u4e0d\u9002\u5408\u6211\u7684project\uff1b Issue when scheduling tasks using clock() function \u8fd9\u7bc7\u6587\u7ae0\u4e2d\u7684 \u8fd9\u4e2a\u7b54\u6848 \u6bd4\u8f83\u9002\u5408\u6211\u7684project\uff0c\u4f46\u662f\u9700\u8981\u5bf9\u5176\u8fdb\u884c\u6539\u7f16\uff0c\u5b83\u4e3b\u8981\u7684\u4e0d\u8db3\u662f\uff1aCPU\u4e0d\u53cb\u597d\uff1b\u53ef\u4ee5\u53c2\u8003 Portable periodic/one-shot timer implementation \u4e2d\u7684\u5b9e\u73b0\u65b9\u5f0f\uff1b C++11 way to create a timer that can be \u201cstopped\u201d if needed \u4f7f\u7528OS\u63d0\u4f9b\u7684\u65b9\u6cd5 Is it possible to direct linux timer notification signal to a specific thread? I want to implement a timer handling module in my system using Linux POSIX timers API. A user can start a timer, and give a callback. Invocation of the callback will be done from a thread (and not from the signal handler). I was thinking of setting timer_create() with SIGEV_SIGNAL , and the thread waiting for signals using sigwaitinfo() , (the signal arg will give the timerId which will invoke the required callback) How does the signals are routed in the linux kernel? do I need to specify to which thread they are sent? COMMENT : Did you look into poll(2) and the Linux-specific signalfd(2) & timerfd_create(2) ? \u2013 Basile Starynkevitch Oct 30 '14 at 17:20 A By default a signal can be delivered to any thread, and you shouldn't make assumptions to which one. Posix threads, however, give you some control over this. You will normally use SIGALRM in conjunction with timer, but I have an example with SIGHUP, so I will show it here to demonstrate the workflow. When your application starts the main thread sets global disposition of the signal to be ignored: int n ; sigset_t set ; struct sigaction disp ; bzero ( & disp , sizeof ( disp )); disp . sa_handler = SIG_IGN ; if ( sigaction ( SIGHUP , & disp , NULL ) < 0 ) { syslog ( LOG_CRIT , \"sigaction_main: %m\" ); _exit ( 1 ); } Next, you make sure that the mask a newly spawned thread will inherit has also this signal blocked: sigemptyset ( & set ); sigaddset ( & set , SIGHUP ); if ( ( errno = pthread_sigmask ( SIG_BLOCK , & set , NULL )) != 0 ) { syslog ( LOG_CRIT , \"sigmask_main: %m\" ); _exit ( 1 ); } At this point, you spawn all your threads that should not be interrupted by the signal (HUP). After this is all done, the main thread goes into dedicated mode of the signal waiting and handling: for (; ;) { if ( ( errno = sigwait ( & set , & n )) != 0 ) { syslog ( LOG_CRIT , \"sigwait_main: %m\" ); _exit ( 1 ); } if ( n == SIGHUP ) { /* do the errands */ } } timer_create timerfd_create \u591a\u79cd\u5b9e\u73b0\u65b9\u5f0f 1\u53d6\u884c\u60c5-\u300b2\u7b97\u7279\u5f81-\u300b3\u4fdd\u5b58\u5230\u961f\u5217 \u5168\u90e8\u90fd\u5c01\u88c5\u6210\u4e00\u4e2a\u51fd\u6570\uff0c\u7531\u7ebf\u7a0b\u6c60\u6765\u8fdb\u884c\u6267\u884c \u7ebf\u7a0b\u6c60\u4e2d\u7684\u7ebf\u7a0b\u5747\u5206\u6240\u6709\u7684\u6807\u7684\u5238\uff0c\u5373\u6bcf\u4e2a\u7ebf\u7a0b\u90fd\u6709\u81ea\u5df1\u7684\u7279\u5b9a\u7684\u6807\u7684\u5238\uff1b \u6709\u591a\u79cd\u5b9e\u73b0\u65b9\u5f0f\uff1a \u65b9\u5f0f\u4e00\uff1a class QuotePredictWorker : QuotePredict ( int id , stocks ){ run_flag_ = false ; } /// \u8c03\u7528\u6b64\u51fd\u6570\u6765\u542f\u52a8worker_\u8fd0\u884c void run (){ run_flag_ = true ; cv_ . notify_all (); } void task (){ for stock in stocks : get_quote (); calculate_feature (); } std :: vector stocks_ ; //\u6807\u7684\u5238 bool run_flag_ ; // condition variable std :: condition_variable cv_ ; std :: thread worker_ ; \u4e00\u4e2a\u7ebf\u7a0b\u6c60\u6765\u5904\u7406\u53d6\u884c\u60c5\uff0c\u4e00\u4e2a\u7ebf\u7a0b\u6c60\u6765\u8ba1\u7b97\u7279\u5f81 \u73b0\u5728\u770b\u6765\uff0c\u6211\u66f4\u52a0\u503e\u5411\u4e8e\u7b2c\u4e00\u79cd\u65b9\u5f0f","title":"Thread implement event loop based timer"},{"location":"Event-driven-concurrent-server/Timer/thread-implement-event-loop-based-timer/#how#to#implement#event#loop#based#timer","text":"\u9996\u5148\u6e90\u4e8e\u6211\u4e4b\u524d\u5728JavaScript\u4e2d\u4f7f\u7528\u8fc7 setInterval \uff0c\u800c\u6211\u73b0\u5728\u7684project\u4e2d\uff0c\u6709\u975e\u5e38\u591a\u57fa\u4e8e\u529f\u80fd\u662f\u9700\u8981\u7c7b\u4f3c setInterval \u529f\u80fd\u7684\uff0c\u6bd4\u5982\u5b9a\u65f6\u8bf7\u6c42celery\uff1b\u4e0d\u5e78\u7684\u662f\uff0c\u6211\u7684project\u4f7f\u7528\u7684\u662f c++ \uff0c\u6240\u4ee5\u6211\u7b2c\u4e00\u60f3\u6cd5\u662f\uff1a Google setIntervel C++\uff0c\u4ece\u68c0\u7d22\u7ed3\u679c\u4e2d\uff0c\u6211\u8bb0\u5f55\u4e86\u5982\u4e0b\u6709\u4ef7\u503c\u7684\uff1a Using setInterval() in C++ timercpp A Simple Timer in C++ \u5355\u72ec\u521b\u5efa\u4e00\u4e2athread\u6765\u5b8c\u6210\u8fd9\u4e2a\u5de5\u4f5c\uff0c\u8be5\u51fd\u6570\u7684\u5de5\u4f5c\u975e\u5e38\u7b80\u5355\uff0c\u5c31\u662f setinterval real time example of multithreading in c++ \u521a\u5f00\u59cb\uff0c \u6211\u9605\u8bfb\u7684\u662f A Simple Timer in C++ \uff0c\u663e\u7136\u8fd9\u662f\u4e00\u4e2a\u975e\u5e38\u5c0f\u578b\u7684\uff0c\u8003\u8651\u7684\u60c5\u51b5\u975e\u5e38\u5c11\uff1b \u63a5\u7740\u9605\u8bfb Using setInterval() in C++ \uff0c\u8fd9\u5176\u4e2d\u5bf9JavaScript\u7684 setIntervel \u5b9e\u73b0\u8fdb\u884c\u4e86\u4e00\u4e9b\u5206\u6790\uff1b","title":"how to implement event loop based timer"},{"location":"Event-driven-concurrent-server/Timer/thread-implement-event-loop-based-timer/#c#stdthread#event#loop#with#message#queue#and#timer","text":"","title":"C++ std::thread Event Loop with Message Queue and Timer"},{"location":"Event-driven-concurrent-server/Timer/thread-implement-event-loop-based-timer/#unix#thread-based#timer","text":"https://davejingtian.org/2013/09/28/timer-queue-a-way-to-handle-multiple-timers-using-one-thread/ https://github.com/daveti/tq","title":"unix thread-based timer"},{"location":"Event-driven-concurrent-server/Timer/thread-implement-event-loop-based-timer/#is#it#possible#to#direct#linux#timer#notification#signal#to#a#specific#thread","text":"timer event loop implementation[Deeper understanding of event loops and timers closed]","title":"Is it possible to direct linux timer notification signal to a specific thread?"},{"location":"Event-driven-concurrent-server/Timer/thread-implement-event-loop-based-timer/#the#nodejs#event#loop#timers#and#processnexttick","text":"","title":"The Node.js Event Loop, Timers, and process.nextTick()"},{"location":"Event-driven-concurrent-server/Timer/thread-implement-event-loop-based-timer/#deeper#understanding#of#event#loops#and#timers#closed","text":"","title":"[Deeper understanding of event loops and timers closed]"},{"location":"Event-driven-concurrent-server/Timer/thread-implement-event-loop-based-timer/#how#to","text":"\u6709\u4e24\u79cd\u5b9a\u65f6\uff1a \u6bcf\u96943S\u53d6\u4e00\u6b21\u884c\u60c5 \u6bcf\u96940.5S\uff0c\u8bf7\u6c42\u4e00\u6b21celery \u8fd9\u4e24\u8005\u90fd\u9700\u8981\u6709\u4e00\u4e2a\u524d\u63d0\uff1a\u5f53\u524d\u65f6\u95f4\u5728\u4ea4\u6613\u65f6\u95f4\u6bb5\u5185\uff0c\u90a3\u5982\u4f55\u5b9e\u73b0\uff1f \u6709\u591a\u79cd\u65b9\u5f0f\uff1a \u65b9\u5f0f1\uff1a\u4e0a\u8ff0\u4e24\u8005\u5206\u5f00\u6765\u8fdb\u884c\u5b9e\u73b0\uff0c\u5206\u522b\u641e\u4e00\u4e2a\u7ebf\u7a0b\uff0c\u4e24\u4e2a\u7ebf\u7a0b\uff0c\u90fd\u53ea\u9700\u8981\u6267\u884c setInterval \uff0c\u5728\u6bcf\u4e2ainterval\u4e2d\uff0c\u90fd\u9700\u8981\u6267\u884c\u5224\u65ad\u5f53\u524d\u662f\u5426\u5904\u4e8e\u4ea4\u6613\u65f6\u95f4\uff1b \u65b9\u5f0f2\uff1a\u5c06\u4e24\u8005\u653e\u5728\u540c\u4e00\u4e2athread\u4e2d\uff0c\u8fd9\u4e2athread\u6267\u884csetInterval\uff0c\u5728\u6bcf\u4e2ainterval\u4e2d\uff0c\u5148\u53d6\u884c\u60c5\uff0c\u7136\u540e\u518d\u6bcf\u96940.5S\uff0c\u8bf7\u6c42\u4e00\u6b21celery\uff1b \u4e0a\u8ff0\u4e24\u79cd\u65b9\u5f0f\uff0c\u90fd\u662fblocking\u7684\uff0c\u5373timer\u548c\u6267\u884c\u5904\u4e8e\u540c\u4e00\u4e2athread\uff1b\u8fd9\u603b\u65b9\u5f0f\u6bd4\u8f83\u4e0d\u597d\u7684\u5c31\u662ftimer\u7684\u5b9e\u73b0\u6bd4\u8f83\u590d\u6742\uff0c\u5b83\u9700\u8981\u8ba1\u7b97\u5728\u672cinterval\u4e2d\uff0c\u6240\u6709\u64cd\u4f5c\u7684\u6267\u884c\u65f6\u95f4\u7136\u540e\u518d\u6765\u786e\u5b9a\u5269\u4f59\u7684\u7761\u7720\u65f6\u95f4\uff1b\u66f4\u597d\u7684\u65b9\u5f0f\u662fnon-blocking\u7684\uff0c\u5373\u5c06\u8ba1\u65f6\u548c\u8d85\u65f6\u52a8\u4f5c\u5206\u9694\u5f00\u6765\uff1b \u6240\u4ee5\uff0c\u4e00\u4e2a\u95ee\u9898\u4e00\u4e2a\u95ee\u9898\u6765\u8fdb\u884c\u5904\u7406\uff1a","title":"how to"},{"location":"Event-driven-concurrent-server/Timer/thread-implement-event-loop-based-timer/#_1","text":"\u7cfb\u7edf\u7684\u65f6\u5e8f\u5e94\u8be5\u662f\u8fd9\u6837\u7684\uff1a\u5f00\u5e02\u5f00\u59cb\u8fd0\u884c\u7cfb\u7edf\uff1b\u7cfb\u7edf\u5f00\u59cb\u8fd0\u884c\uff0c\u6267\u884c\u6307\u5b9a\u7684\u64cd\u4f5c\uff1b\u95ed\u5e02\u505c\u6b62\u7cfb\u7edf\uff1b \u4f46\u662f\u76ee\u524d\u7684\u5b9e\u73b0\u65b9\u5f0f\u662f\uff1a\u6bcf\u6b21\u90fd\u53bb\u6821\u9a8c\u5f53\u524d\u65f6\u95f4\u662f\u5426\u5728\u4ea4\u6613\u65f6\u95f4\uff1b\u8fd9\u79cd\u505a\u6cd5\u4f1a\u5bfc\u81f4\u7f16\u7a0b\u7684\u590d\u6742\uff0c\u7cfb\u7edf\u7ef4\u62a4\u7684\u56f0\u96be\uff1b \u6bd4\u8f83\u597d\u7684\u505a\u6cd5\u662f\uff0c\u6709\u4e13\u95e8\u7684\u5b9a\u65f6\u7ebf\u7a0b\uff0c\u5f53\u5176\u76d1\u63a7\u5230\uff0c\u5df2\u7ecf\u5230\u8fbe\u4e86\u5f00\u5e02\u65f6\u95f4\u65f6\uff0c\u5219\u542f\u52a8\u7cfb\u7edf\uff1b\u5f53\u5176\u76d1\u63a7\u5230\u5230\u8fbe\u4e86\u95ed\u5e02\u65f6\u95f4\u540e\uff0c\u5219\u505c\u6b62\u7cfb\u7edf\uff1b \u5b9e\u73b0\u8fd9\u4e2a\u529f\u80fd\uff0c\u5c31\u53ef\u4ee5\u6709\u591a\u79cd\u65b9\u5f0f\uff1a\u53ef\u4ee5\u4f7f\u7528\u7eaf c++ \u7684\u65b9\u6cd5\uff0c\u4e5f\u53ef\u4ee5\u4f7f\u7528OS\u63d0\u4f9b\u7684\u65b9\u6cd5\uff1b","title":"\u4ea4\u6613\u65f6\u95f4\u6bb5"},{"location":"Event-driven-concurrent-server/Timer/thread-implement-event-loop-based-timer/#c","text":"How to create timer events using C++ 11? \u8fd9\u7bc7\u6587\u7ae0\u4e2d\u7684 \u8fd9\u4e2a\u7b54\u6848 \u662f\u5177\u6709\u501f\u9274\u610f\u4e49\u7684\uff0c\u4f46\u662f\u5b83\u5e76\u4e0d\u9002\u5408\u6211\u7684project\uff0c\u56e0\u4e3a\u5b83\u4e3a\u6bcf\u4e2atimer event\u90fd\u521b\u5efa\u4e00\u4e2athread\uff0c\u5e76\u4e0d\u9002\u5408\u6211\u7684project\uff1b Issue when scheduling tasks using clock() function \u8fd9\u7bc7\u6587\u7ae0\u4e2d\u7684 \u8fd9\u4e2a\u7b54\u6848 \u6bd4\u8f83\u9002\u5408\u6211\u7684project\uff0c\u4f46\u662f\u9700\u8981\u5bf9\u5176\u8fdb\u884c\u6539\u7f16\uff0c\u5b83\u4e3b\u8981\u7684\u4e0d\u8db3\u662f\uff1aCPU\u4e0d\u53cb\u597d\uff1b\u53ef\u4ee5\u53c2\u8003 Portable periodic/one-shot timer implementation \u4e2d\u7684\u5b9e\u73b0\u65b9\u5f0f\uff1b C++11 way to create a timer that can be \u201cstopped\u201d if needed","title":"\u7eafC++\u65b9\u5f0f"},{"location":"Event-driven-concurrent-server/Timer/thread-implement-event-loop-based-timer/#os","text":"","title":"\u4f7f\u7528OS\u63d0\u4f9b\u7684\u65b9\u6cd5"},{"location":"Event-driven-concurrent-server/Timer/thread-implement-event-loop-based-timer/#is#it#possible#to#direct#linux#timer#notification#signal#to#a#specific#thread_1","text":"I want to implement a timer handling module in my system using Linux POSIX timers API. A user can start a timer, and give a callback. Invocation of the callback will be done from a thread (and not from the signal handler). I was thinking of setting timer_create() with SIGEV_SIGNAL , and the thread waiting for signals using sigwaitinfo() , (the signal arg will give the timerId which will invoke the required callback) How does the signals are routed in the linux kernel? do I need to specify to which thread they are sent? COMMENT : Did you look into poll(2) and the Linux-specific signalfd(2) & timerfd_create(2) ? \u2013 Basile Starynkevitch Oct 30 '14 at 17:20","title":"Is it possible to direct linux timer notification signal to a specific thread?"},{"location":"Event-driven-concurrent-server/Timer/thread-implement-event-loop-based-timer/#a","text":"By default a signal can be delivered to any thread, and you shouldn't make assumptions to which one. Posix threads, however, give you some control over this. You will normally use SIGALRM in conjunction with timer, but I have an example with SIGHUP, so I will show it here to demonstrate the workflow. When your application starts the main thread sets global disposition of the signal to be ignored: int n ; sigset_t set ; struct sigaction disp ; bzero ( & disp , sizeof ( disp )); disp . sa_handler = SIG_IGN ; if ( sigaction ( SIGHUP , & disp , NULL ) < 0 ) { syslog ( LOG_CRIT , \"sigaction_main: %m\" ); _exit ( 1 ); } Next, you make sure that the mask a newly spawned thread will inherit has also this signal blocked: sigemptyset ( & set ); sigaddset ( & set , SIGHUP ); if ( ( errno = pthread_sigmask ( SIG_BLOCK , & set , NULL )) != 0 ) { syslog ( LOG_CRIT , \"sigmask_main: %m\" ); _exit ( 1 ); } At this point, you spawn all your threads that should not be interrupted by the signal (HUP). After this is all done, the main thread goes into dedicated mode of the signal waiting and handling: for (; ;) { if ( ( errno = sigwait ( & set , & n )) != 0 ) { syslog ( LOG_CRIT , \"sigwait_main: %m\" ); _exit ( 1 ); } if ( n == SIGHUP ) { /* do the errands */ } }","title":"A"},{"location":"Event-driven-concurrent-server/Timer/thread-implement-event-loop-based-timer/#timer_create","text":"","title":"timer_create"},{"location":"Event-driven-concurrent-server/Timer/thread-implement-event-loop-based-timer/#timerfd_create","text":"","title":"timerfd_create"},{"location":"Event-driven-concurrent-server/Timer/thread-implement-event-loop-based-timer/#_2","text":"1\u53d6\u884c\u60c5-\u300b2\u7b97\u7279\u5f81-\u300b3\u4fdd\u5b58\u5230\u961f\u5217 \u5168\u90e8\u90fd\u5c01\u88c5\u6210\u4e00\u4e2a\u51fd\u6570\uff0c\u7531\u7ebf\u7a0b\u6c60\u6765\u8fdb\u884c\u6267\u884c \u7ebf\u7a0b\u6c60\u4e2d\u7684\u7ebf\u7a0b\u5747\u5206\u6240\u6709\u7684\u6807\u7684\u5238\uff0c\u5373\u6bcf\u4e2a\u7ebf\u7a0b\u90fd\u6709\u81ea\u5df1\u7684\u7279\u5b9a\u7684\u6807\u7684\u5238\uff1b \u6709\u591a\u79cd\u5b9e\u73b0\u65b9\u5f0f\uff1a \u65b9\u5f0f\u4e00\uff1a class QuotePredictWorker : QuotePredict ( int id , stocks ){ run_flag_ = false ; } /// \u8c03\u7528\u6b64\u51fd\u6570\u6765\u542f\u52a8worker_\u8fd0\u884c void run (){ run_flag_ = true ; cv_ . notify_all (); } void task (){ for stock in stocks : get_quote (); calculate_feature (); } std :: vector stocks_ ; //\u6807\u7684\u5238 bool run_flag_ ; // condition variable std :: condition_variable cv_ ; std :: thread worker_ ; \u4e00\u4e2a\u7ebf\u7a0b\u6c60\u6765\u5904\u7406\u53d6\u884c\u60c5\uff0c\u4e00\u4e2a\u7ebf\u7a0b\u6c60\u6765\u8ba1\u7b97\u7279\u5f81 \u73b0\u5728\u770b\u6765\uff0c\u6211\u66f4\u52a0\u503e\u5411\u4e8e\u7b2c\u4e00\u79cd\u65b9\u5f0f","title":"\u591a\u79cd\u5b9e\u73b0\u65b9\u5f0f"},{"location":"Event-driven-concurrent-server/Timer/Columbia-paper-Hashed-hierarchical-wheel-timer/","text":"Hashed and Hierarchical Timing Wheels: Data Structures for the Efficient Implementation of a Timer Facility NOTE: \u5728\u9605\u8bfb folly / folly / io / async / README.md \u65f6\uff0c\u5176\u4e2d\u63d0\u53ca\u4e86\u8fd9\u7bc7\u8bba\u6587","title":"Introduction"},{"location":"Event-driven-concurrent-server/Timer/Columbia-paper-Hashed-hierarchical-wheel-timer/#hashed#and#hierarchical#timing#wheels#data#structures#for#the#efficient#implementation#of#a#timer#facility","text":"NOTE: \u5728\u9605\u8bfb folly / folly / io / async / README.md \u65f6\uff0c\u5176\u4e2d\u63d0\u53ca\u4e86\u8fd9\u7bc7\u8bba\u6587","title":"Hashed and Hierarchical Timing Wheels: Data Structures for the Efficient Implementation of a Timer Facility"},{"location":"Event-driven-concurrent-server/Timer/Periodic-worker/","text":"Periodic worker Implementation spdlog / include / spdlog / details / periodic_worker.h // Copyright(c) 2015-present, Gabi Melman & spdlog contributors. // Distributed under the MIT License (http://opensource.org/licenses/MIT) #pragma once // periodic worker thread - periodically executes the given callback function. // // RAII over the owned thread: // creates the thread on construction. // stops and joins the thread on destruction (if the thread is executing a callback, wait for it to finish first). #include <chrono> #include <condition_variable> #include <functional> #include <mutex> #include <thread> namespace details { class periodic_worker { public : periodic_worker ( const std :: function < void () > & callback_fun , std :: chrono :: seconds interval ) { active_ = ( interval > std :: chrono :: seconds :: zero ()); if ( ! active_ ) { return ; } worker_thread_ = std :: thread ([ this , callback_fun , interval ]() { for (;;) { std :: unique_lock < std :: mutex > lock ( this -> mutex_ ); if ( this -> cv_ . wait_for ( lock , interval , [ this ] { return ! this -> active_ ;})) { return ; // active_ == false, so exit this thread } callback_fun (); } }); } periodic_worker ( const periodic_worker & ) = delete ; periodic_worker & operator = ( const periodic_worker & ) = delete ; // stop the worker thread and join it ~ periodic_worker () { if ( worker_thread_ . joinable ()) { { std :: lock_guard < std :: mutex > lock ( mutex_ ); active_ = false ; } cv_ . notify_one (); worker_thread_ . join (); } } private : bool active_ ; std :: thread worker_thread_ ; std :: mutex mutex_ ; std :: condition_variable cv_ ; }; } // namespace details","title":"Introduction"},{"location":"Event-driven-concurrent-server/Timer/Periodic-worker/#periodic#worker","text":"","title":"Periodic worker"},{"location":"Event-driven-concurrent-server/Timer/Periodic-worker/#implementation","text":"","title":"Implementation"},{"location":"Event-driven-concurrent-server/Timer/Periodic-worker/#spdlogincludespdlogdetailsperiodic_workerh","text":"// Copyright(c) 2015-present, Gabi Melman & spdlog contributors. // Distributed under the MIT License (http://opensource.org/licenses/MIT) #pragma once // periodic worker thread - periodically executes the given callback function. // // RAII over the owned thread: // creates the thread on construction. // stops and joins the thread on destruction (if the thread is executing a callback, wait for it to finish first). #include <chrono> #include <condition_variable> #include <functional> #include <mutex> #include <thread> namespace details { class periodic_worker { public : periodic_worker ( const std :: function < void () > & callback_fun , std :: chrono :: seconds interval ) { active_ = ( interval > std :: chrono :: seconds :: zero ()); if ( ! active_ ) { return ; } worker_thread_ = std :: thread ([ this , callback_fun , interval ]() { for (;;) { std :: unique_lock < std :: mutex > lock ( this -> mutex_ ); if ( this -> cv_ . wait_for ( lock , interval , [ this ] { return ! this -> active_ ;})) { return ; // active_ == false, so exit this thread } callback_fun (); } }); } periodic_worker ( const periodic_worker & ) = delete ; periodic_worker & operator = ( const periodic_worker & ) = delete ; // stop the worker thread and join it ~ periodic_worker () { if ( worker_thread_ . joinable ()) { { std :: lock_guard < std :: mutex > lock ( mutex_ ); active_ = false ; } cv_ . notify_one (); worker_thread_ . join (); } } private : bool active_ ; std :: thread worker_thread_ ; std :: mutex mutex_ ; std :: condition_variable cv_ ; }; } // namespace details","title":"spdlog/include/spdlog/details/periodic_worker.h"},{"location":"Event-driven-concurrent-server/library-POCO/","text":"POCO C++ Libraries FEATURES NOTE: \u57fa\u672c\u4e0a\u56ca\u62ec\u4e86\u5b83\u7684\u6240\u6709\u529f\u80fd github pocoproject / poco","title":"Introduction"},{"location":"Event-driven-concurrent-server/library-POCO/#poco#c#libraries","text":"","title":"POCO C++ Libraries"},{"location":"Event-driven-concurrent-server/library-POCO/#features","text":"NOTE: \u57fa\u672c\u4e0a\u56ca\u62ec\u4e86\u5b83\u7684\u6240\u6709\u529f\u80fd","title":"FEATURES"},{"location":"Event-driven-concurrent-server/library-POCO/#github#pocoprojectpoco","text":"","title":"github pocoproject/poco"},{"location":"Event-driven-concurrent-server/library-POCO/POCO-PRO-C%2B%2Bframework/","text":"POCO-PRO C++ FRAMEWORKS REMOTING FOR C++ MULTI-PROTOCOL STACK AND CODE GENERATOR NOTE\uff1a \u4e00\u3001\u5b83\u7684code generator\u662f\u8ba9\u6211\u611f\u5174\u8da3\u7684 \u4e8c\u3001\u4ece\u4e0b\u9762\u7684\u63cf\u8ff0\u6765\u770b\uff0c\u5b83\u5305\u542b\u4e24\u4e2acode generator: 1\u3001Remoting code generator \u540e\u9762\u7684\"REST API SAMPLE\"\u7ae0\u8282\u5c55\u793a\u4e86\u5b83\u7684\u4f8b\u5b50 2\u3001SOAP AND WSDL/XSD-TO-C++ CODE GENERATION Remoting for C++ is a web services and remote method call framework for C++. Remoting supports different communication protocols. This includes JSON-based REST APIs, JSON-RPC, SOAP and a highly efficient binary protocol. Remote services are implemented as C++ classes, annotated with special comments. The Remoting code generator takes care of the rest. There is no need to maintain a separate interface definition using an interface definition language (IDL). SOAP AND WSDL/XSD-TO-C++ CODE GENERATION Remoting also includes a tool for generating C++ code from WSDL 1.1 (document/literal wrapped) and XML Schema (XSD) documents. This makes it possible to build C++ clients for SOAP 1.1 and 1.2 web services built with Java, Microsoft .NET, or other middleware technologies. REST API SAMPLE NOTE: \u5b83\u7684\u8fd9\u79cd\u98ce\u683c\u662f\u7c7b\u4f3c\u4e8eJava\u3001Python\u7684 The following code snipped shows an example for how to define a RESTful API with Remoting. This definition is specific to the Remoting REST transport. Note how the method names correspond to HTTP methods. Also note the special comments. The Remoting code generator (RemoteGen) parses these definitions and generates C++ code that implements serialization, deserialization and remote invocation. //@ serialize struct User { Poco :: Optional < std :: string > name ; Poco :: Optional < std :: string > password ; Poco :: Optional < std :: set < std :: string >> permissions ; Poco :: Optional < std :: set < std :: string >> roles ; }; //@ remote //@ path=\"/api/1.0/users\" class UserCollectionEndpoint { public : User post ( const User & user ); /// Create a new user. //@ $maxResults={in=query, optional} //@ $start={in=query, optional} std :: vector < User > get ( int maxResults = 0 , int start = 0 ); /// Return a list of user objects, starting with /// the given start index, and return at most /// maxResults items. }; //@ remote //@ path=\"/api/1.0/users/{name}\" class UserEndpoint { public : //@ $name={in=path} User put ( const std :: string & name , const User & user ); /// Update a user (calls patch()). //@ $name={in=path} User patch ( const std :: string & name , const User & user ); /// Update a user. //@ $name={in=path} User get ( const std :: string & name ); /// Retrieve a user by name. //@ $name={in=path} void delete_ ( const std :: string & name ); /// Delete a user. }; OPEN SERVICE PLATFORM NOTE: \u548cHS CRES middleware \u975e\u5e38\u7c7b\u4f3c BUILD, DEPLOY AND MANAGE DYNAMIC, MODULAR C++ APPLICATIONS Open Service Platform (OSP) enables the creation, deployment and management of dynamically extensible, modular applications, based on a powerful plug-in and (nano-) services model. Applications built with OSP can be extended, upgraded and managed even when deployed in the field. At the core of OSP lies a powerful software component (plug-in) and services model based on the concept of bundles. A bundle is a deployable entity, consisting of both executable code (shared libraries) and the required configuration, data and resource files. PLUG-IN AND SERVICE-BASED ARCHITECTURE","title":"Introduction"},{"location":"Event-driven-concurrent-server/library-POCO/POCO-PRO-C%2B%2Bframework/#poco-pro#c#frameworks","text":"","title":"POCO-PRO C++ FRAMEWORKS"},{"location":"Event-driven-concurrent-server/library-POCO/POCO-PRO-C%2B%2Bframework/#remoting#for#c","text":"MULTI-PROTOCOL STACK AND CODE GENERATOR NOTE\uff1a \u4e00\u3001\u5b83\u7684code generator\u662f\u8ba9\u6211\u611f\u5174\u8da3\u7684 \u4e8c\u3001\u4ece\u4e0b\u9762\u7684\u63cf\u8ff0\u6765\u770b\uff0c\u5b83\u5305\u542b\u4e24\u4e2acode generator: 1\u3001Remoting code generator \u540e\u9762\u7684\"REST API SAMPLE\"\u7ae0\u8282\u5c55\u793a\u4e86\u5b83\u7684\u4f8b\u5b50 2\u3001SOAP AND WSDL/XSD-TO-C++ CODE GENERATION Remoting for C++ is a web services and remote method call framework for C++. Remoting supports different communication protocols. This includes JSON-based REST APIs, JSON-RPC, SOAP and a highly efficient binary protocol. Remote services are implemented as C++ classes, annotated with special comments. The Remoting code generator takes care of the rest. There is no need to maintain a separate interface definition using an interface definition language (IDL). SOAP AND WSDL/XSD-TO-C++ CODE GENERATION Remoting also includes a tool for generating C++ code from WSDL 1.1 (document/literal wrapped) and XML Schema (XSD) documents. This makes it possible to build C++ clients for SOAP 1.1 and 1.2 web services built with Java, Microsoft .NET, or other middleware technologies.","title":"REMOTING FOR C++"},{"location":"Event-driven-concurrent-server/library-POCO/POCO-PRO-C%2B%2Bframework/#rest#api#sample","text":"NOTE: \u5b83\u7684\u8fd9\u79cd\u98ce\u683c\u662f\u7c7b\u4f3c\u4e8eJava\u3001Python\u7684 The following code snipped shows an example for how to define a RESTful API with Remoting. This definition is specific to the Remoting REST transport. Note how the method names correspond to HTTP methods. Also note the special comments. The Remoting code generator (RemoteGen) parses these definitions and generates C++ code that implements serialization, deserialization and remote invocation. //@ serialize struct User { Poco :: Optional < std :: string > name ; Poco :: Optional < std :: string > password ; Poco :: Optional < std :: set < std :: string >> permissions ; Poco :: Optional < std :: set < std :: string >> roles ; }; //@ remote //@ path=\"/api/1.0/users\" class UserCollectionEndpoint { public : User post ( const User & user ); /// Create a new user. //@ $maxResults={in=query, optional} //@ $start={in=query, optional} std :: vector < User > get ( int maxResults = 0 , int start = 0 ); /// Return a list of user objects, starting with /// the given start index, and return at most /// maxResults items. }; //@ remote //@ path=\"/api/1.0/users/{name}\" class UserEndpoint { public : //@ $name={in=path} User put ( const std :: string & name , const User & user ); /// Update a user (calls patch()). //@ $name={in=path} User patch ( const std :: string & name , const User & user ); /// Update a user. //@ $name={in=path} User get ( const std :: string & name ); /// Retrieve a user by name. //@ $name={in=path} void delete_ ( const std :: string & name ); /// Delete a user. };","title":"REST API SAMPLE"},{"location":"Event-driven-concurrent-server/library-POCO/POCO-PRO-C%2B%2Bframework/#open#service#platform","text":"NOTE: \u548cHS CRES middleware \u975e\u5e38\u7c7b\u4f3c BUILD, DEPLOY AND MANAGE DYNAMIC, MODULAR C++ APPLICATIONS Open Service Platform (OSP) enables the creation, deployment and management of dynamically extensible, modular applications, based on a powerful plug-in and (nano-) services model. Applications built with OSP can be extended, upgraded and managed even when deployed in the field. At the core of OSP lies a powerful software component (plug-in) and services model based on the concept of bundles. A bundle is a deployable entity, consisting of both executable code (shared libraries) and the required configuration, data and resource files. PLUG-IN AND SERVICE-BASED ARCHITECTURE","title":"OPEN SERVICE PLATFORM"},{"location":"Event-driven-concurrent-server/library-linyacool-WebServer/","text":"linyacool / WebServer","title":"Introduction"},{"location":"Event-driven-concurrent-server/library-linyacool-WebServer/#linyacoolwebserver","text":"","title":"linyacool/WebServer"},{"location":"Event-driven-concurrent-server/library-yedf-handy/","text":"yedf / handy","title":"Introduction"},{"location":"Event-driven-concurrent-server/library-yedf-handy/#yedfhandy","text":"","title":"yedf/handy"},{"location":"Event-driven-concurrent-server/software-Nginx/","text":"Nginx nginx documentation Beginner\u2019s Guide nginx has one master process and several worker processes. The main purpose of the master process is to read and evaluate configuration, and maintain worker processes. Worker processes do actual processing of requests. nginx employs event-based model and OS-dependent mechanisms to efficiently distribute requests among worker processes. The number of worker processes is defined in the configuration file and may be fixed for a given configuration or automatically adjusted to the number of available CPU cores (see worker_processes ). NOTE: master-worker Starting, Stopping, and Reloading Configuration To start nginx, run the executable file. Once nginx is started, it can be controlled by invoking the executable with the -s parameter. Use the following syntax: nginx -s signal Where signal may be one of the following: 1\u3001 stop \u2014 fast shutdown 2\u3001 quit \u2014 graceful shutdown 3\u3001 reload \u2014 reloading the configuration file 4\u3001 reopen \u2014 reopening the log files NOTE: \u8fd9\u4e9b\u8bf7\u6c42\u90fd\u662f\u901a\u8fc7OS signal\u7684\u65b9\u5f0f\u4f20\u9001\u7ed9process\u7684","title":"Introduction"},{"location":"Event-driven-concurrent-server/software-Nginx/#nginx","text":"","title":"Nginx"},{"location":"Event-driven-concurrent-server/software-Nginx/#nginx#documentation","text":"","title":"nginx documentation"},{"location":"Event-driven-concurrent-server/software-Nginx/#beginners#guide","text":"nginx has one master process and several worker processes. The main purpose of the master process is to read and evaluate configuration, and maintain worker processes. Worker processes do actual processing of requests. nginx employs event-based model and OS-dependent mechanisms to efficiently distribute requests among worker processes. The number of worker processes is defined in the configuration file and may be fixed for a given configuration or automatically adjusted to the number of available CPU cores (see worker_processes ). NOTE: master-worker","title":"Beginner\u2019s Guide"},{"location":"Event-driven-concurrent-server/software-Nginx/#starting#stopping#and#reloading#configuration","text":"To start nginx, run the executable file. Once nginx is started, it can be controlled by invoking the executable with the -s parameter. Use the following syntax: nginx -s signal Where signal may be one of the following: 1\u3001 stop \u2014 fast shutdown 2\u3001 quit \u2014 graceful shutdown 3\u3001 reload \u2014 reloading the configuration file 4\u3001 reopen \u2014 reopening the log files NOTE: \u8fd9\u4e9b\u8bf7\u6c42\u90fd\u662f\u901a\u8fc7OS signal\u7684\u65b9\u5f0f\u4f20\u9001\u7ed9process\u7684","title":"Starting, Stopping, and Reloading Configuration"},{"location":"Event-driven-concurrent-server/software-Nginx/Concurrency-model/","text":"Nginx concurrency model csdn \u8be6\u89e3nginx\u7684master\u8fdb\u7a0b\u548cworker\u8fdb\u7a0b NOTE: \u8fd9\u7bc7\u6587\u7ae0\u6bd4\u8f83\u7cbe\u7b80\uff0c\u4e0d\u6d89\u53ca\u5b9e\u73b0\u7ec6\u8282 csdn Nginx\u8d44\u6599\u4e4bMaster\u4e0eWorker\u57fa\u7840\u6982\u5ff5 master\u8fdb\u7a0b\u4e3b\u8981\u7528\u6765\u7ba1\u7406worker\u8fdb\u7a0b\uff0c\u5305\u542b\uff1a\u63a5\u6536\u6765\u81ea\u5916\u754c\u7684\u4fe1\u53f7\uff0c\u5411\u5404worker\u8fdb\u7a0b\u53d1\u9001\u4fe1\u53f7\uff0c\u76d1\u63a7worker\u8fdb\u7a0b\u7684\u8fd0\u884c\u72b6\u6001\uff0c\u5f53worker\u8fdb\u7a0b\u9000\u51fa\u540e(\u5f02\u5e38\u60c5\u51b5\u4e0b)\uff0c\u4f1a\u81ea\u52a8\u91cd\u65b0\u542f\u52a8\u65b0\u7684worker\u8fdb\u7a0b\u3002 worker\u8fdb\u7a0b\u5219\u5904\u7406\u57fa\u672c\u7684\u7f51\u7edc\u4e8b\u4ef6\u3002\u591a\u4e2aworker\u8fdb\u7a0b\u4e4b\u95f4\u662f\u5bf9\u7b49\u7684\uff0c\u4ed6\u4eec\u540c\u7b49\u7ade\u4e89\u6765\u81ea\u5ba2\u6237\u7aef\u7684\u8bf7\u6c42\uff0c\u5404\u8fdb\u7a0b\u4e92\u76f8\u4e4b\u95f4\u662f\u72ec\u7acb\u7684\u3002\u4e00\u4e2a\u8bf7\u6c42\uff0c\u53ea\u53ef\u80fd\u5728\u4e00\u4e2aworker\u8fdb\u7a0b\u4e2d\u5904\u7406\uff0c\u4e00\u4e2aworker\u8fdb\u7a0b\uff0c\u4e0d\u53ef\u80fd\u5904\u7406\u5176\u5b83\u8fdb\u7a0b\u7684\u8bf7\u6c42\u3002worker\u8fdb\u7a0b\u7684\u4e2a\u6570\u662f\u53ef\u4ee5\u8bbe\u7f6e\u7684\uff0c\u4e00\u822c\u6211\u4eec\u4f1a\u8bbe\u7f6e\u4e0e\u673a\u5668cpu\u6838\u6570\u4e00\u81f4\u3002 nginx\u7684\u8fdb\u7a0b\u6a21\u578b\uff1a worker\u8fdb\u7a0b\u5904\u7406\u8bf7\u6c42\uff1a worker\u8fdb\u7a0b\u4e4b\u95f4\u662f\u5e73\u7b49\u7684\uff0c\u6bcf\u4e2a\u8fdb\u7a0b\uff0c\u5904\u7406\u8bf7\u6c42\u7684\u673a\u4f1a\u4e5f\u662f\u4e00\u6837\u7684\u3002\u5f53\u6211\u4eec\u63d0\u4f9b80\u7aef\u53e3\u7684http\u670d\u52a1\u65f6\uff0c\u4e00\u4e2a\u8fde\u63a5\u8bf7\u6c42\u8fc7\u6765\uff0c\u6bcf\u4e2a\u8fdb\u7a0b\u90fd\u6709\u53ef\u80fd\u5904\u7406\u8fd9\u4e2a\u8fde\u63a5\uff0c\u600e\u4e48\u505a\u5230\u7684\u5462\uff1f\u9996\u5148\uff0c\u6bcf\u4e2aworker\u8fdb\u7a0b\u90fd\u662f\u4ecemaster\u8fdb\u7a0bfork\u8fc7\u6765\uff0c\u5728master\u8fdb\u7a0b\u91cc\u9762\uff0c\u5148\u5efa\u7acb\u597d\u9700\u8981listen\u7684socket\uff08listenfd\uff09\u4e4b\u540e\uff0c\u7136\u540e\u518dfork\u51fa\u591a\u4e2aworker\u8fdb\u7a0b\u3002\u6240\u6709worker\u8fdb\u7a0b\u7684listenfd\u4f1a\u5728\u65b0\u8fde\u63a5\u5230\u6765\u65f6\u53d8\u5f97\u53ef\u8bfb\uff0c\u4e3a\u4fdd\u8bc1\u53ea\u6709\u4e00\u4e2a\u8fdb\u7a0b\u5904\u7406\u8be5\u8fde\u63a5\uff0c\u6240\u6709worker\u8fdb\u7a0b\u5728\u6ce8\u518clistenfd\u8bfb\u4e8b\u4ef6\u524d\u62a2 accept_mutex \uff0c\u62a2\u5230\u4e92\u65a5\u9501\u7684\u90a3\u4e2a\u8fdb\u7a0b\u6ce8\u518clistenfd\u8bfb\u4e8b\u4ef6\uff0c\u5728\u8bfb\u4e8b\u4ef6\u91cc\u8c03\u7528accept\u63a5\u53d7\u8be5\u8fde\u63a5\u3002\u5f53\u4e00\u4e2aworker\u8fdb\u7a0b\u5728accept\u8fd9\u4e2a\u8fde\u63a5\u4e4b\u540e\uff0c\u5c31\u5f00\u59cb\u8bfb\u53d6\u8bf7\u6c42\uff0c\u89e3\u6790\u8bf7\u6c42\uff0c\u5904\u7406\u8bf7\u6c42\uff0c\u4ea7\u751f\u6570\u636e\u540e\uff0c\u518d\u8fd4\u56de\u7ed9\u5ba2\u6237\u7aef\uff0c\u6700\u540e\u624d\u65ad\u5f00\u8fde\u63a5\uff0c\u8fd9\u6837\u4e00\u4e2a\u5b8c\u6574\u7684\u8bf7\u6c42\u5c31\u662f\u8fd9\u6837\u7684\u4e86\u3002\u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff0c\u4e00\u4e2a\u8bf7\u6c42\uff0c\u5b8c\u5168\u7531worker\u8fdb\u7a0b\u6765\u5904\u7406\uff0c\u800c\u4e14\u53ea\u5728\u4e00\u4e2aworker\u8fdb\u7a0b\u4e2d\u5904\u7406\u3002 NOTE: \u4e0a\u8ff0\u5e76\u6ca1\u6709\u8bf4\u5230\u4f7f\u7528 accept_mutex \u7684\u5173\u952e\uff0c\u5173\u952e\u662f: \"mitigate\u7f13\u548c\u7f13\u89e3thundering herd\u60ca\u7fa4\u6548\u5e94Nginx-serialize\u4e32\u884caccept-accept_mutex\" nginx\u8fdb\u7a0b\u6a21\u578b\u7684\u597d\u5904\uff1a NOTE: \"multithread\u591a\u7ebf\u7a0b-VS-multiprocess\u591a\u8fdb\u7a0b\" 1\u3001\u5bf9\u4e8e\u6bcf\u4e2aworker\u8fdb\u7a0b\u6765\u8bf4\uff0c\u72ec\u7acb\u7684\u8fdb\u7a0b\uff0c\u4e0d\u9700\u8981\u52a0\u9501\uff0c\u6240\u4ee5\u7701\u6389\u4e86\u9501\u5e26\u6765\u7684\u5f00\u9500\uff0c\u540c\u65f6\u5728\u7f16\u7a0b\u4ee5\u53ca\u95ee\u9898\u67e5\u627e\u65f6\uff0c\u4e5f\u4f1a\u65b9\u4fbf\u5f88\u591a\u3002 2\u3001\u91c7\u7528\u72ec\u7acb\u7684\u8fdb\u7a0b\uff0c\u53ef\u4ee5\u8ba9\u4e92\u76f8\u4e4b\u95f4\u4e0d\u4f1a\u5f71\u54cd\uff0c\u4e00\u4e2a\u8fdb\u7a0b\u9000\u51fa\u540e\uff0c\u5176\u5b83\u8fdb\u7a0b\u8fd8\u5728\u5de5\u4f5c\uff0c\u670d\u52a1\u4e0d\u4f1a\u4e2d\u65ad\uff0cmaster\u8fdb\u7a0b\u5219\u5f88\u5feb\u542f\u52a8\u65b0\u7684worker\u8fdb\u7a0b\u3002\u5f53\u7136\uff0cworker\u8fdb\u7a0b\u7684\u5f02\u5e38\u9000\u51fa\uff0c\u80af\u5b9a\u662f\u7a0b\u5e8f\u6709bug\u4e86\uff0c\u5f02\u5e38\u9000\u51fa\uff0c\u4f1a\u5bfc\u81f4\u5f53\u524dworker\u4e0a\u7684\u6240\u6709\u8bf7\u6c42\u5931\u8d25\uff0c\u4e0d\u8fc7\u4e0d\u4f1a\u5f71\u54cd\u5230\u6240\u6709\u8bf7\u6c42\uff0c\u6240\u4ee5\u964d\u4f4e\u4e86\u98ce\u9669\u3002 stackoverflow what is worker_processes and worker_connections in Nginx?","title":"Introduction"},{"location":"Event-driven-concurrent-server/software-Nginx/Concurrency-model/#nginx#concurrency#model","text":"","title":"Nginx concurrency model"},{"location":"Event-driven-concurrent-server/software-Nginx/Concurrency-model/#csdn#nginxmasterworker","text":"NOTE: \u8fd9\u7bc7\u6587\u7ae0\u6bd4\u8f83\u7cbe\u7b80\uff0c\u4e0d\u6d89\u53ca\u5b9e\u73b0\u7ec6\u8282","title":"csdn \u8be6\u89e3nginx\u7684master\u8fdb\u7a0b\u548cworker\u8fdb\u7a0b"},{"location":"Event-driven-concurrent-server/software-Nginx/Concurrency-model/#csdn#nginxmasterworker_1","text":"master\u8fdb\u7a0b\u4e3b\u8981\u7528\u6765\u7ba1\u7406worker\u8fdb\u7a0b\uff0c\u5305\u542b\uff1a\u63a5\u6536\u6765\u81ea\u5916\u754c\u7684\u4fe1\u53f7\uff0c\u5411\u5404worker\u8fdb\u7a0b\u53d1\u9001\u4fe1\u53f7\uff0c\u76d1\u63a7worker\u8fdb\u7a0b\u7684\u8fd0\u884c\u72b6\u6001\uff0c\u5f53worker\u8fdb\u7a0b\u9000\u51fa\u540e(\u5f02\u5e38\u60c5\u51b5\u4e0b)\uff0c\u4f1a\u81ea\u52a8\u91cd\u65b0\u542f\u52a8\u65b0\u7684worker\u8fdb\u7a0b\u3002 worker\u8fdb\u7a0b\u5219\u5904\u7406\u57fa\u672c\u7684\u7f51\u7edc\u4e8b\u4ef6\u3002\u591a\u4e2aworker\u8fdb\u7a0b\u4e4b\u95f4\u662f\u5bf9\u7b49\u7684\uff0c\u4ed6\u4eec\u540c\u7b49\u7ade\u4e89\u6765\u81ea\u5ba2\u6237\u7aef\u7684\u8bf7\u6c42\uff0c\u5404\u8fdb\u7a0b\u4e92\u76f8\u4e4b\u95f4\u662f\u72ec\u7acb\u7684\u3002\u4e00\u4e2a\u8bf7\u6c42\uff0c\u53ea\u53ef\u80fd\u5728\u4e00\u4e2aworker\u8fdb\u7a0b\u4e2d\u5904\u7406\uff0c\u4e00\u4e2aworker\u8fdb\u7a0b\uff0c\u4e0d\u53ef\u80fd\u5904\u7406\u5176\u5b83\u8fdb\u7a0b\u7684\u8bf7\u6c42\u3002worker\u8fdb\u7a0b\u7684\u4e2a\u6570\u662f\u53ef\u4ee5\u8bbe\u7f6e\u7684\uff0c\u4e00\u822c\u6211\u4eec\u4f1a\u8bbe\u7f6e\u4e0e\u673a\u5668cpu\u6838\u6570\u4e00\u81f4\u3002","title":"csdn Nginx\u8d44\u6599\u4e4bMaster\u4e0eWorker\u57fa\u7840\u6982\u5ff5"},{"location":"Event-driven-concurrent-server/software-Nginx/Concurrency-model/#nginx","text":"","title":"nginx\u7684\u8fdb\u7a0b\u6a21\u578b\uff1a"},{"location":"Event-driven-concurrent-server/software-Nginx/Concurrency-model/#worker","text":"worker\u8fdb\u7a0b\u4e4b\u95f4\u662f\u5e73\u7b49\u7684\uff0c\u6bcf\u4e2a\u8fdb\u7a0b\uff0c\u5904\u7406\u8bf7\u6c42\u7684\u673a\u4f1a\u4e5f\u662f\u4e00\u6837\u7684\u3002\u5f53\u6211\u4eec\u63d0\u4f9b80\u7aef\u53e3\u7684http\u670d\u52a1\u65f6\uff0c\u4e00\u4e2a\u8fde\u63a5\u8bf7\u6c42\u8fc7\u6765\uff0c\u6bcf\u4e2a\u8fdb\u7a0b\u90fd\u6709\u53ef\u80fd\u5904\u7406\u8fd9\u4e2a\u8fde\u63a5\uff0c\u600e\u4e48\u505a\u5230\u7684\u5462\uff1f\u9996\u5148\uff0c\u6bcf\u4e2aworker\u8fdb\u7a0b\u90fd\u662f\u4ecemaster\u8fdb\u7a0bfork\u8fc7\u6765\uff0c\u5728master\u8fdb\u7a0b\u91cc\u9762\uff0c\u5148\u5efa\u7acb\u597d\u9700\u8981listen\u7684socket\uff08listenfd\uff09\u4e4b\u540e\uff0c\u7136\u540e\u518dfork\u51fa\u591a\u4e2aworker\u8fdb\u7a0b\u3002\u6240\u6709worker\u8fdb\u7a0b\u7684listenfd\u4f1a\u5728\u65b0\u8fde\u63a5\u5230\u6765\u65f6\u53d8\u5f97\u53ef\u8bfb\uff0c\u4e3a\u4fdd\u8bc1\u53ea\u6709\u4e00\u4e2a\u8fdb\u7a0b\u5904\u7406\u8be5\u8fde\u63a5\uff0c\u6240\u6709worker\u8fdb\u7a0b\u5728\u6ce8\u518clistenfd\u8bfb\u4e8b\u4ef6\u524d\u62a2 accept_mutex \uff0c\u62a2\u5230\u4e92\u65a5\u9501\u7684\u90a3\u4e2a\u8fdb\u7a0b\u6ce8\u518clistenfd\u8bfb\u4e8b\u4ef6\uff0c\u5728\u8bfb\u4e8b\u4ef6\u91cc\u8c03\u7528accept\u63a5\u53d7\u8be5\u8fde\u63a5\u3002\u5f53\u4e00\u4e2aworker\u8fdb\u7a0b\u5728accept\u8fd9\u4e2a\u8fde\u63a5\u4e4b\u540e\uff0c\u5c31\u5f00\u59cb\u8bfb\u53d6\u8bf7\u6c42\uff0c\u89e3\u6790\u8bf7\u6c42\uff0c\u5904\u7406\u8bf7\u6c42\uff0c\u4ea7\u751f\u6570\u636e\u540e\uff0c\u518d\u8fd4\u56de\u7ed9\u5ba2\u6237\u7aef\uff0c\u6700\u540e\u624d\u65ad\u5f00\u8fde\u63a5\uff0c\u8fd9\u6837\u4e00\u4e2a\u5b8c\u6574\u7684\u8bf7\u6c42\u5c31\u662f\u8fd9\u6837\u7684\u4e86\u3002\u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff0c\u4e00\u4e2a\u8bf7\u6c42\uff0c\u5b8c\u5168\u7531worker\u8fdb\u7a0b\u6765\u5904\u7406\uff0c\u800c\u4e14\u53ea\u5728\u4e00\u4e2aworker\u8fdb\u7a0b\u4e2d\u5904\u7406\u3002 NOTE: \u4e0a\u8ff0\u5e76\u6ca1\u6709\u8bf4\u5230\u4f7f\u7528 accept_mutex \u7684\u5173\u952e\uff0c\u5173\u952e\u662f: \"mitigate\u7f13\u548c\u7f13\u89e3thundering herd\u60ca\u7fa4\u6548\u5e94Nginx-serialize\u4e32\u884caccept-accept_mutex\"","title":"worker\u8fdb\u7a0b\u5904\u7406\u8bf7\u6c42\uff1a"},{"location":"Event-driven-concurrent-server/software-Nginx/Concurrency-model/#nginx_1","text":"NOTE: \"multithread\u591a\u7ebf\u7a0b-VS-multiprocess\u591a\u8fdb\u7a0b\" 1\u3001\u5bf9\u4e8e\u6bcf\u4e2aworker\u8fdb\u7a0b\u6765\u8bf4\uff0c\u72ec\u7acb\u7684\u8fdb\u7a0b\uff0c\u4e0d\u9700\u8981\u52a0\u9501\uff0c\u6240\u4ee5\u7701\u6389\u4e86\u9501\u5e26\u6765\u7684\u5f00\u9500\uff0c\u540c\u65f6\u5728\u7f16\u7a0b\u4ee5\u53ca\u95ee\u9898\u67e5\u627e\u65f6\uff0c\u4e5f\u4f1a\u65b9\u4fbf\u5f88\u591a\u3002 2\u3001\u91c7\u7528\u72ec\u7acb\u7684\u8fdb\u7a0b\uff0c\u53ef\u4ee5\u8ba9\u4e92\u76f8\u4e4b\u95f4\u4e0d\u4f1a\u5f71\u54cd\uff0c\u4e00\u4e2a\u8fdb\u7a0b\u9000\u51fa\u540e\uff0c\u5176\u5b83\u8fdb\u7a0b\u8fd8\u5728\u5de5\u4f5c\uff0c\u670d\u52a1\u4e0d\u4f1a\u4e2d\u65ad\uff0cmaster\u8fdb\u7a0b\u5219\u5f88\u5feb\u542f\u52a8\u65b0\u7684worker\u8fdb\u7a0b\u3002\u5f53\u7136\uff0cworker\u8fdb\u7a0b\u7684\u5f02\u5e38\u9000\u51fa\uff0c\u80af\u5b9a\u662f\u7a0b\u5e8f\u6709bug\u4e86\uff0c\u5f02\u5e38\u9000\u51fa\uff0c\u4f1a\u5bfc\u81f4\u5f53\u524dworker\u4e0a\u7684\u6240\u6709\u8bf7\u6c42\u5931\u8d25\uff0c\u4e0d\u8fc7\u4e0d\u4f1a\u5f71\u54cd\u5230\u6240\u6709\u8bf7\u6c42\uff0c\u6240\u4ee5\u964d\u4f4e\u4e86\u98ce\u9669\u3002","title":"nginx\u8fdb\u7a0b\u6a21\u578b\u7684\u597d\u5904\uff1a"},{"location":"Event-driven-concurrent-server/software-Nginx/Concurrency-model/#stackoverflow#what#is#worker_processes#and#worker_connections#in#nginx","text":"","title":"stackoverflow what is worker_processes and worker_connections in Nginx?"},{"location":"Event-driven-concurrent-server/software-Nginx/Concurrency-model/Nginx-event-library/","text":"Nginx event library nginx docs Connection processing methods The following connection processing methods are supported: select \u2014 standard method. The supporting module is built automatically on platforms that lack more efficient methods. The --with-select_module and --without-select_module configuration parameters can be used to forcibly enable or disable the build of this module. poll \u2014 standard method. The supporting module is built automatically on platforms that lack more efficient methods. The --with-poll_module and --without-poll_module configuration parameters can be used to forcibly enable or disable the build of this module. kqueue \u2014 efficient method used on FreeBSD 4.1+, OpenBSD 2.9+, NetBSD 2.0, and macOS. epoll \u2014 efficient method used on Linux 2.6+. The EPOLLRDHUP (Linux 2.6.17, glibc 2.8) and EPOLLEXCLUSIVE (Linux 4.5, glibc 2.24) flags are supported since 1.11.3. Some older distributions like SuSE 8.2 provide patches that add epoll support to 2.4 kernels. /dev/poll \u2014 efficient method used on Solaris 7 11/99+, HP/UX 11.22+ (eventport), IRIX 6.5.15+, and Tru64 UNIX 5.1A+. eventport \u2014 event ports, method used on Solaris 10+ (due to known issues, it is recommended using the /dev/poll method instead). NOTE: \u4e00\u3001\u53ef\u4ee5\u770b\u5230\uff0cNginx\u4e5f\u662f\u57fa\u4e8eIO multiplexing\u7684","title":"Introduction"},{"location":"Event-driven-concurrent-server/software-Nginx/Concurrency-model/Nginx-event-library/#nginx#event#library","text":"","title":"Nginx event library"},{"location":"Event-driven-concurrent-server/software-Nginx/Concurrency-model/Nginx-event-library/#nginx#docs#connection#processing#methods","text":"The following connection processing methods are supported: select \u2014 standard method. The supporting module is built automatically on platforms that lack more efficient methods. The --with-select_module and --without-select_module configuration parameters can be used to forcibly enable or disable the build of this module. poll \u2014 standard method. The supporting module is built automatically on platforms that lack more efficient methods. The --with-poll_module and --without-poll_module configuration parameters can be used to forcibly enable or disable the build of this module. kqueue \u2014 efficient method used on FreeBSD 4.1+, OpenBSD 2.9+, NetBSD 2.0, and macOS. epoll \u2014 efficient method used on Linux 2.6+. The EPOLLRDHUP (Linux 2.6.17, glibc 2.8) and EPOLLEXCLUSIVE (Linux 4.5, glibc 2.24) flags are supported since 1.11.3. Some older distributions like SuSE 8.2 provide patches that add epoll support to 2.4 kernels. /dev/poll \u2014 efficient method used on Solaris 7 11/99+, HP/UX 11.22+ (eventport), IRIX 6.5.15+, and Tru64 UNIX 5.1A+. eventport \u2014 event ports, method used on Solaris 10+ (due to known issues, it is recommended using the /dev/poll method instead). NOTE: \u4e00\u3001\u53ef\u4ee5\u770b\u5230\uff0cNginx\u4e5f\u662f\u57fa\u4e8eIO multiplexing\u7684","title":"nginx docs Connection processing methods"},{"location":"Event-driven-concurrent-server/software-Nginx/Concurrency-model/Thundering-herd-in-Nginx/","text":"Thundering herd in Nginx \u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\uff0c\u5bf9Nginx\u89e3\u51b3\"Thundering herd\"\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u89e3\u91ca: nginx Inside NGINX: How We Designed for Performance & Scale The NGINX worker processes begin by waiting for events on the listen sockets ( accept_mutex and kernel socket sharding ). Core functionality # accept_mutex If accept_mutex is enabled, worker processes will accept new connections by turn. Otherwise, all worker processes will be notified about new connections, and if volume of new connections is low, some of the worker processes may just waste system resources. There is no need to enable accept_mutex on systems that support the EPOLLEXCLUSIVE flag (1.11.3) or when using reuseport . Prior to version 1.11.3, the default value was on . nginx Socket Sharding in NGINX Release 1.9.1 NGINX 1.9.1 introduces a new feature that enables use of the SO_REUSEPORT socket option, which is available in newer versions of many operating systems, including DragonFly BSD and Linux (kernel version 3.9 and later). This socket option allows multiple sockets to listen on the same IP address and port combination. The kernel then load balances incoming connections across the sockets. nginx Performance Tuning \u2013 Tips & Tricks accept_mutex \u2002 off \u2013 All worker processes are notified about new connections (the default in NGINX 1.11.3 and later, and NGINX Plus R10 and later). If enabled, worker processes accept new connections by turns. We recommend keeping the default value ( off ) unless you have extensive knowledge of your app\u2019s performance and the opportunity to test under a variety of conditions, but it can lead to inefficient use of system resources if the volume of new connections is low. Changing the value to on might be beneficial under some high loads. stackoverflow Why is 'accept_mutex' 'on' as default in Nginx? A As of nginx mainline version 1.11.3 (released 2016-07-26), accept_mutex now defaults to off . This is partly because the new EPOLLEXCLUSIVE flag provides the benefits of accept_mutex without the extra overhead. A Check the accept Serialization - Multiple Sockets accept Serialization - Single Socket sections of this Apache documentation. https://httpd.apache.org/docs/2.4/misc/perf-tuning.html . (This doc matters most, compared to the rest of 2 I posted) It's very inspiring and I also read the http://nginx.org/en/docs/ngx_core_module.html#accept_mutex explaination as well as https://www.nginx.com/blog/performance-tuning-tips-tricks/ (search page with accept_mutex ). Aftering reading all of these, I guess Nginx is very similar to Apache in this aspect.","title":"Introduction"},{"location":"Event-driven-concurrent-server/software-Nginx/Concurrency-model/Thundering-herd-in-Nginx/#thundering#herd#in#nginx","text":"\u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\uff0c\u5bf9Nginx\u89e3\u51b3\"Thundering herd\"\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u89e3\u91ca: nginx Inside NGINX: How We Designed for Performance & Scale The NGINX worker processes begin by waiting for events on the listen sockets ( accept_mutex and kernel socket sharding ). Core functionality # accept_mutex If accept_mutex is enabled, worker processes will accept new connections by turn. Otherwise, all worker processes will be notified about new connections, and if volume of new connections is low, some of the worker processes may just waste system resources. There is no need to enable accept_mutex on systems that support the EPOLLEXCLUSIVE flag (1.11.3) or when using reuseport . Prior to version 1.11.3, the default value was on . nginx Socket Sharding in NGINX Release 1.9.1 NGINX 1.9.1 introduces a new feature that enables use of the SO_REUSEPORT socket option, which is available in newer versions of many operating systems, including DragonFly BSD and Linux (kernel version 3.9 and later). This socket option allows multiple sockets to listen on the same IP address and port combination. The kernel then load balances incoming connections across the sockets. nginx Performance Tuning \u2013 Tips & Tricks accept_mutex \u2002 off \u2013 All worker processes are notified about new connections (the default in NGINX 1.11.3 and later, and NGINX Plus R10 and later). If enabled, worker processes accept new connections by turns. We recommend keeping the default value ( off ) unless you have extensive knowledge of your app\u2019s performance and the opportunity to test under a variety of conditions, but it can lead to inefficient use of system resources if the volume of new connections is low. Changing the value to on might be beneficial under some high loads.","title":"Thundering herd in Nginx"},{"location":"Event-driven-concurrent-server/software-Nginx/Concurrency-model/Thundering-herd-in-Nginx/#stackoverflow#why#is#accept_mutex#on#as#default#in#nginx","text":"A As of nginx mainline version 1.11.3 (released 2016-07-26), accept_mutex now defaults to off . This is partly because the new EPOLLEXCLUSIVE flag provides the benefits of accept_mutex without the extra overhead. A Check the accept Serialization - Multiple Sockets accept Serialization - Single Socket sections of this Apache documentation. https://httpd.apache.org/docs/2.4/misc/perf-tuning.html . (This doc matters most, compared to the rest of 2 I posted) It's very inspiring and I also read the http://nginx.org/en/docs/ngx_core_module.html#accept_mutex explaination as well as https://www.nginx.com/blog/performance-tuning-tips-tricks/ (search page with accept_mutex ). Aftering reading all of these, I guess Nginx is very similar to Apache in this aspect.","title":"stackoverflow Why is 'accept_mutex' 'on' as default in Nginx?"},{"location":"Event-driven-concurrent-server/software-Nginx/Concurrency-model/Thundering-herd-in-Nginx/Nginx-Socket-Sharding/","text":"nginx Socket Sharding in NGINX Release 1.9.1 NGINX 1.9.1 introduces a new feature that enables use of the SO_REUSEPORT socket option, which is available in newer versions of many operating systems, including DragonFly BSD and Linux (kernel version 3.9 and later). This socket option allows multiple sockets to listen on the same IP address and port combination. The kernel then load balances incoming connections across the sockets. As depicted in the figure, when the SO_REUSEPORT option is not enabled, a single listening socket notifies workers about incoming connections, and each worker tries to take a connection. With the SO_REUSEPORT option enabled, there are multiple socket listeners for each IP address and port combination, one for each worker process. The kernel determines which available socket listener (and by implication, which worker) gets the connection. This can reduce lock contention between workers accepting new connections, and improve performance on multicore systems. However, it can also mean that when a worker is stalled by a blocking operation, the block affects not only connections that the worker has already accepted, but also connection requests that the kernel has assigned to the worker since it became blocked.","title":"Introduction"},{"location":"Event-driven-concurrent-server/software-Nginx/Concurrency-model/Thundering-herd-in-Nginx/Nginx-Socket-Sharding/#nginx#socket#sharding#in#nginx#release#191","text":"NGINX 1.9.1 introduces a new feature that enables use of the SO_REUSEPORT socket option, which is available in newer versions of many operating systems, including DragonFly BSD and Linux (kernel version 3.9 and later). This socket option allows multiple sockets to listen on the same IP address and port combination. The kernel then load balances incoming connections across the sockets. As depicted in the figure, when the SO_REUSEPORT option is not enabled, a single listening socket notifies workers about incoming connections, and each worker tries to take a connection. With the SO_REUSEPORT option enabled, there are multiple socket listeners for each IP address and port combination, one for each worker process. The kernel determines which available socket listener (and by implication, which worker) gets the connection. This can reduce lock contention between workers accepting new connections, and improve performance on multicore systems. However, it can also mean that when a worker is stalled by a blocking operation, the block affects not only connections that the worker has already accepted, but also connection requests that the kernel has assigned to the worker since it became blocked.","title":"nginx Socket Sharding in NGINX Release 1.9.1"},{"location":"Event-driven-concurrent-server/software-Nginx/Concurrency-model/Thundering-herd-in-Nginx/Nginx-accept_mutex-implementation/","text":"accept_mutex \u7684\u5b9e\u73b0 \u4e00\u3001\u5982\u4f55\u907f\u514ddead lock\uff1f jianshu Nginx accept\u9501\u7684\u673a\u5236\u548c\u5b9e\u73b0 \u672c\u6587\u57fa\u4e8eNginx 0.8.55\u6e90\u4ee3\u7801\uff0c\u5e76\u57fa\u4e8eepoll\u673a\u5236\u5206\u6790 1.1 accpet\u9501\u662f\u4e2a\u4ec0\u4e48\u4e1c\u897f \u63d0\u5230accept\u9501\uff0c\u5c31\u4e0d\u5f97\u4e0d\u63d0\u8d77\u60ca\u7fa4\u95ee\u9898\u3002 \u800c\u5728Linux\u5185\u6838\u7684\u8f83\u65b0\u7248\u672c\u4e2d\uff0c accept \u8c03\u7528\u672c\u8eab\u6240\u5f15\u8d77\u7684\u60ca\u7fa4\u95ee\u9898\u5df2\u7ecf\u5f97\u5230\u4e86\u89e3\u51b3\uff0c\u4f46\u662f\u5728Nginx\u4e2d\uff0c accept \u662f\u4ea4\u7ed9 epoll \u673a\u5236\u6765\u5904\u7406\u7684\uff0c epoll \u7684 accept \u5e26\u6765\u7684\u60ca\u7fa4\u95ee\u9898\u5e76\u6ca1\u6709\u5f97\u5230\u89e3\u51b3\uff08\u5e94\u8be5\u662f epoll_wait \u672c\u8eab\u5e76\u6ca1\u6709\u533a\u522b\u8bfb\u4e8b\u4ef6\u662f\u5426\u6765\u81ea\u4e8e\u4e00\u4e2aListen\u5957\u63a5\u5b57\u7684\u80fd\u529b\uff0c\u6240\u4ee5\u6240\u6709\u76d1\u542c\u8fd9\u4e2a\u4e8b\u4ef6\u7684\u8fdb\u7a0b\u4f1a\u88ab\u8fd9\u4e2aepoll_wait\u5524\u9192\u3002\uff09\uff0c\u6240\u4ee5Nginx\u7684accept\u60ca\u7fa4\u95ee\u9898\u4ecd\u7136\u9700\u8981\u5b9a\u5236\u4e00\u4e2a\u81ea\u5df1\u7684\u89e3\u51b3\u65b9\u6848\u3002 NOTE: \u4e00\u3001 epoll_wait \u5bfc\u81f4\u7684 thundering herd \u4e8c\u3001\u53c2\u89c1 epoll(7) \uff0c\u65b0\u7248\u672c\u7684epoll\u4e2d\uff0c\u5df2\u7ecf\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u4e86 accept\u9501\u5c31\u662fnginx\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u672c\u8d28\u4e0a\u8fd9\u662f\u4e00\u4e2a\u8de8\u8fdb\u7a0b\u7684\u4e92\u65a5\u9501\uff0c\u4ee5\u8fd9\u4e2a\u4e92\u65a5\u9501\u6765\u4fdd\u8bc1\u53ea\u6709\u4e00\u4e2a\u8fdb\u7a0b\u5177\u5907\u76d1\u542caccept\u4e8b\u4ef6\u7684\u80fd\u529b\u3002 \u5b9e\u73b0\u4e0aaccept\u9501\u662f\u4e00\u4e2a\u8de8\u8fdb\u7a0b\u9501\uff0c\u5176\u5728Nginx\u4e2d\u662f\u4e00\u4e2a\u5168\u5c40\u53d8\u91cf\uff0c\u58f0\u660e\u5982\u4e0b\uff1a ngx_shmtx_t ngx_accept_mutex ; NOTE: \u4e00\u3001\u987e\u540d\u601d\u4e49\uff0c\u5b83\u4f1a\u662f\u4f7f\u7528shared memory IPC \u8fd9\u662f\u4e00\u4e2a\u5728event\u6a21\u5757\u521d\u59cb\u5316\u65f6\u5c31\u5206\u914d\u597d\u7684\u9501\uff0c\u653e\u5728\u4e00\u5757\u8fdb\u7a0b\u95f4\u5171\u4eab\u7684\u5185\u5b58\u4e2d\uff0c\u4ee5\u4fdd\u8bc1\u6240\u6709\u8fdb\u7a0b\u90fd\u80fd\u8bbf\u95ee\u8fd9\u4e00\u4e2a\u5b9e\u4f8b\uff0c\u5176\u52a0\u9501\u89e3\u9501\u662f\u501f\u7531linux\u7684\u539f\u5b50\u53d8\u91cf\u6765\u505aCAS\uff0c\u5982\u679c\u52a0\u9501\u5931\u8d25\u5219\u7acb\u5373\u8fd4\u56de\uff0c\u662f\u4e00\u79cd\u975e\u963b\u585e\u7684\u9501\u3002\u52a0\u89e3\u9501\u4ee3\u7801\u5982\u4e0b\uff1a static ngx_inline ngx_uint_t ngx_shmtx_trylock ( ngx_shmtx_t * mtx ) { return ( * mtx -> lock == 0 && ngx_atomic_cmp_set ( mtx -> lock , 0 , ngx_pid )); } #define ngx_shmtx_lock(mtx) ngx_spinlock((mtx)->lock, ngx_pid, 1024) #define ngx_shmtx_unlock(mtx) (void) ngx_atomic_cmp_set((mtx)->lock, ngx_pid, 0) \u53ef\u4ee5\u770b\u51fa\uff0c\u8c03\u7528 ngx_shmtx_trylock \u5931\u8d25\u540e\u4f1a\u7acb\u523b\u8fd4\u56de\u800c\u4e0d\u4f1a\u963b\u585e\u3002 1.2 accept\u9501\u5982\u4f55\u4fdd\u8bc1\u53ea\u6709\u4e00\u4e2a\u8fdb\u7a0b\u80fd\u591f\u5904\u7406\u65b0\u8fde\u63a5 \u8981\u89e3\u51b3epoll\u5e26\u6765\u7684accept\u9501\u7684\u95ee\u9898\u4e5f\u5f88\u7b80\u5355\uff0c\u53ea\u9700\u8981\u4fdd\u8bc1\u540c\u4e00\u65f6\u95f4\u53ea\u6709\u4e00\u4e2a\u8fdb\u7a0b\u6ce8\u518c\u4e86accept\u7684epoll\u4e8b\u4ef6\u5373\u53ef\u3002 Nginx\u91c7\u7528\u7684\u5904\u7406\u6a21\u5f0f\u4e5f\u6ca1\u4ec0\u4e48\u7279\u522b\u7684\uff0c\u5927\u6982\u5c31\u662f\u5982\u4e0b\u7684\u903b\u8f91\uff1a \u5c1d\u8bd5\u83b7\u53d6accept\u9501 if \u83b7\u53d6\u6210\u529f\uff1a \u5728epoll\u4e2d\u6ce8\u518caccept\u4e8b\u4ef6 else : \u5728epoll\u4e2d\u6ce8\u9500accept\u4e8b\u4ef6 \u5904\u7406\u6240\u6709\u4e8b\u4ef6 \u91ca\u653eaccept\u9501 \u5f53\u7136\u8fd9\u91cc\u5ffd\u7565\u4e86\u5ef6\u540e\u4e8b\u4ef6\u7684\u5904\u7406\uff0c\u8fd9\u90e8\u5206\u6211\u4eec\u653e\u5230\u540e\u9762\u8ba8\u8bba\u3002 \u5bf9\u4e8eaccept\u9501\u7684\u5904\u7406\u548cepoll\u4e2d\u6ce8\u518c\u6ce8\u9500accept\u4e8b\u4ef6\u7684\u7684\u5904\u7406\u90fd\u662f\u5728 ngx_trylock_accept_mutex \u4e2d\u8fdb\u884c\u7684\u3002\u800c\u8fd9\u4e00\u7cfb\u5217\u8fc7\u7a0b\u5219\u662f\u5728nginx\u4e3b\u4f53\u5faa\u73af\u4e2d\u53cd\u590d\u8c03\u7528\u7684 void ngx_process_events_and_timers(ngx_cycle_t *cycle) \u4e2d\u8fdb\u884c\u3002 \u4e5f\u5c31\u662f\u8bf4\uff0c\u6bcf\u8f6e\u4e8b\u4ef6\u7684\u5904\u7406\u90fd\u4f1a\u9996\u5148\u7ade\u4e89accept\u9501\uff0c\u7ade\u4e89\u6210\u529f\u5219\u5728epoll\u4e2d\u6ce8\u518caccept\u4e8b\u4ef6\uff0c\u5931\u8d25\u5219\u6ce8\u9500accept\u4e8b\u4ef6\uff0c\u7136\u540e\u5904\u7406\u5b8c\u4e8b\u4ef6\u4e4b\u540e\uff0c\u91ca\u653eaccept\u9501\u3002\u7531\u6b64\u53ea\u6709\u4e00\u4e2a\u8fdb\u7a0b\u76d1\u542c\u4e00\u4e2alisten\u5957\u63a5\u5b57\uff0c\u4ece\u800c\u907f\u514d\u4e86\u60ca\u7fa4\u95ee\u9898\u3002 1.3 \u4e8b\u4ef6\u5904\u7406\u673a\u5236\u4e3a\u4e0d\u957f\u65f6\u95f4\u5360\u7528accept\u9501\u4f5c\u4e86\u54ea\u4e9b\u52aa\u529b accept\u9501\u5904\u7406\u60ca\u7fa4\u95ee\u9898\u7684\u65b9\u6848\u770b\u8d77\u6765\u4f3c\u4e4e\u5f88\u7f8e\uff0c\u4f46\u5982\u679c\u5b8c\u5168\u4f7f\u7528\u4e0a\u8ff0\u903b\u8f91\uff0c\u5c31\u4f1a\u6709\u4e00\u4e2a\u95ee\u9898\uff1a\u5982\u679c\u670d\u52a1\u5668\u975e\u5e38\u5fd9\uff0c\u6709\u975e\u5e38\u591a\u4e8b\u4ef6\u8981\u5904\u7406\uff0c\u90a3\u4e48\u201c\u5904\u7406\u6240\u6709\u4e8b\u4ef6\u8fd9\u4e00\u6b65\u201d\u5c31\u4f1a\u6d88\u8017\u975e\u5e38\u957f\u7684\u65f6\u95f4\uff0c\u4e5f\u5c31\u662f\u8bf4\uff0c\u67d0\u4e00\u4e2a\u8fdb\u7a0b\u957f\u65f6\u95f4\u5360\u7528accept\u9501\uff0c\u800c\u53c8\u65e0\u6687\u5904\u7406\u65b0\u8fde\u63a5\uff1b\u5176\u4ed6\u8fdb\u7a0b\u53c8\u6ca1\u6709\u5360\u7528accept\u9501\uff0c\u540c\u6837\u65e0\u6cd5\u5904\u7406\u65b0\u8fde\u63a5\u2014\u2014\u81f3\u6b64\uff0c\u65b0\u8fde\u63a5\u5c31\u5904\u4e8e\u65e0\u4eba\u5904\u7406\u7684\u72b6\u6001\uff0c\u8fd9\u5bf9\u670d\u52a1\u7684\u5b9e\u65f6\u6027\u65e0\u7591\u662f\u5f88\u8981\u547d\u7684\u3002 \u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0cNginx\u91c7\u7528\u4e86\u5c06\u4e8b\u4ef6\u5904\u7406\u5ef6\u540e\u7684\u65b9\u5f0f\u3002\u5373\u5728 ngx_process_events \u7684\u5904\u7406\u4e2d\uff0c\u4ec5\u4ec5\u5c06\u4e8b\u4ef6\u653e\u5165\u4e24\u4e2a\u961f\u5217\u4e2d\uff1a ngx_thread_volatile ngx_event_t * ngx_posted_accept_events ; ngx_thread_volatile ngx_event_t * ngx_posted_events ; \u8fd4\u56de\u540e\u5148\u5904\u7406 ngx_posted_accept_events \u540e\u7acb\u523b\u91ca\u653eaccept\u9501\uff0c\u7136\u540e\u518d\u6162\u6162\u5904\u7406\u5176\u4ed6\u4e8b\u4ef6\u3002 \u5373 ngx_process_events \u4ec5\u5bf9 epoll_wait \u8fdb\u884c\u5904\u7406\uff0c\u4e8b\u4ef6\u7684\u6d88\u8d39\u5219\u653e\u5230accept\u9501\u91ca\u653e\u4e4b\u540e\uff0c\u6765\u6700\u5927\u9650\u5ea6\u5730\u7f29\u77ed\u5360\u6709accept\u7684\u65f6\u95f4\uff0c\u6765\u8ba9\u5176\u4ed6\u8fdb\u7a0b\u4e5f\u6709\u8db3\u591f\u7684\u65f6\u673a\u5904\u7406accept\u4e8b\u4ef6\u3002 \u90a3\u4e48\u5177\u4f53\u662f\u600e\u4e48\u5b9e\u73b0\u7684\u5462\uff1f\u5176\u5b9e\u5c31\u662f\u5728 static ngx_int_t ngx_epoll_process_events(ngx_cycle_t *cycle, ngx_msec_t timer, ngx_uint_t flags) \u7684flags\u53c2\u6570\u4e2d\u4f20\u5165\u4e00\u4e2a NGX_POST_EVENTS \u7684\u6807\u5fd7\u4f4d\uff0c\u5904\u7406\u4e8b\u4ef6\u65f6\u68c0\u67e5\u8fd9\u4e2a\u6807\u5fd7\u4f4d\u5373\u53ef\u3002 \u8fd9\u91cc\u53ea\u662f\u907f\u514d\u4e86\u4e8b\u4ef6\u7684\u6d88\u8d39\u5bf9\u4e8eaccept\u9501\u7684\u957f\u671f\u5360\u7528\uff0c\u90a3\u4e48\u4e07\u4e00epoll_wait\u672c\u8eab\u5360\u7528\u7684\u65f6\u95f4\u5f88\u957f\u5462\uff1f\u8fd9\u79cd\u4e8b\u60c5\u4e5f\u4e0d\u662f\u4e0d\u53ef\u80fd\u53d1\u751f\u3002\u8fd9\u65b9\u9762\u7684\u5904\u7406\u4e5f\u5f88\u7b80\u5355\uff0cepoll_wait\u672c\u8eab\u662f\u6709\u8d85\u65f6\u65f6\u95f4\u7684\uff0c\u9650\u5236\u4f4f\u5b83\u7684\u503c\u5c31\u53ef\u4ee5\u4e86\uff0c\u8fd9\u4e2a\u53c2\u6570\u4fdd\u5b58\u5728 ngx_accept_mutex_delay \u8fd9\u4e2a\u5168\u5c40\u53d8\u91cf\u4e2d\u3002 \u4e0b\u9762\u653e\u4e0a ngx_process_events_and_timers \u7684\u5b9e\u73b0\u4ee3\u7801\uff0c\u53ef\u4ee5\u5927\u6982\u4e00\u89c2\u76f8\u5173\u7684\u5904\u7406\uff1a void ngx_process_events_and_timers ( ngx_cycle_t * cycle ) { ngx_uint_t flags ; ngx_msec_t timer , delta ; /* \u7701\u7565\u4e00\u4e9b\u5904\u7406\u65f6\u95f4\u4e8b\u4ef6\u7684\u4ee3\u7801 */ // \u8fd9\u91cc\u662f\u5904\u7406\u8d1f\u8f7d\u5747\u8861\u9501\u548caccept\u9501\u7684\u65f6\u673a if ( ngx_use_accept_mutex ) { // \u5982\u679c\u8d1f\u8f7d\u5747\u8861token\u7684\u503c\u5927\u4e8e0, \u5219\u8bf4\u660e\u8d1f\u8f7d\u5df2\u6ee1\uff0c\u6b64\u65f6\u4e0d\u518d\u5904\u7406accept, \u540c\u65f6\u628a\u8fd9\u4e2a\u503c\u51cf\u4e00 if ( ngx_accept_disabled > 0 ) { ngx_accept_disabled -- ; } else { // \u5c1d\u8bd5\u62ff\u5230accept\u9501 if ( ngx_trylock_accept_mutex ( cycle ) == NGX_ERROR ) { return ; } // \u62ff\u5230\u9501\u4e4b\u540e\u628aflag\u52a0\u4e0apost\u6807\u5fd7\uff0c\u8ba9\u6240\u6709\u4e8b\u4ef6\u7684\u5904\u7406\u90fd\u5ef6\u540e // \u4ee5\u514d\u592a\u957f\u65f6\u95f4\u5360\u7528accept\u9501 if ( ngx_accept_mutex_held ) { flags |= NGX_POST_EVENTS ; } else { if ( timer == NGX_TIMER_INFINITE || timer > ngx_accept_mutex_delay ) { timer = ngx_accept_mutex_delay ; // \u6700\u591a\u7b49ngx_accept_mutex_delay\u4e2a\u6beb\u79d2\uff0c\u9632\u6b62\u5360\u7528\u592a\u4e45accept\u9501 } } } } delta = ngx_current_msec ; // \u8c03\u7528\u4e8b\u4ef6\u5904\u7406\u6a21\u5757\u7684process_events\uff0c\u5904\u7406\u4e00\u4e2aepoll_wait\u7684\u65b9\u6cd5 ( void ) ngx_process_events ( cycle , timer , flags ); delta = ngx_current_msec - delta ; //\u8ba1\u7b97\u5904\u7406events\u4e8b\u4ef6\u6240\u6d88\u8017\u7684\u65f6\u95f4 ngx_log_debug1 ( NGX_LOG_DEBUG_EVENT , cycle -> log , 0 , \"timer delta: %M\" , delta ); // \u5982\u679c\u6709\u5ef6\u540e\u5904\u7406\u7684accept\u4e8b\u4ef6\uff0c\u90a3\u4e48\u5ef6\u540e\u5904\u7406\u8fd9\u4e2a\u4e8b\u4ef6 if ( ngx_posted_accept_events ) { ngx_event_process_posted ( cycle , & ngx_posted_accept_events ); } // \u91ca\u653eaccept\u9501 if ( ngx_accept_mutex_held ) { ngx_shmtx_unlock ( & ngx_accept_mutex ); } // \u5904\u7406\u6240\u6709\u7684\u8d85\u65f6\u4e8b\u4ef6 if ( delta ) { ngx_event_expire_timers (); } ngx_log_debug1 ( NGX_LOG_DEBUG_EVENT , cycle -> log , 0 , \"posted events %p\" , ngx_posted_events ); if ( ngx_posted_events ) { if ( ngx_threaded ) { ngx_wakeup_worker_thread ( cycle ); } else { // \u5904\u7406\u6240\u6709\u7684\u5ef6\u540e\u4e8b\u4ef6 ngx_event_process_posted ( cycle , & ngx_posted_events ); } } } \u518d\u6765\u770b\u770b ngx_epoll_process_events \u7684\u76f8\u5173\u5904\u7406\uff1a // \u8bfb\u4e8b\u4ef6 if (( revents & EPOLLIN ) && rev -> active ) { if (( flags & NGX_POST_THREAD_EVENTS ) && ! rev -> accept ) { rev -> posted_ready = 1 ; } else { rev -> ready = 1 ; } if ( flags & NGX_POST_EVENTS ) { queue = ( ngx_event_t ** ) ( rev -> accept ? & ngx_posted_accept_events : & ngx_posted_events ); ngx_locked_post_event ( rev , queue ); } else { rev -> handler ( rev ); } } wev = c -> write ; // \u5199\u4e8b\u4ef6 if (( revents & EPOLLOUT ) && wev -> active ) { if ( flags & NGX_POST_THREAD_EVENTS ) { wev -> posted_ready = 1 ; } else { wev -> ready = 1 ; } if ( flags & NGX_POST_EVENTS ) { ngx_locked_post_event ( wev , & ngx_posted_events ); } else { wev -> handler ( wev ); } } \u5904\u7406\u4e5f\u76f8\u5bf9\u7b80\u5355\uff0c\u5982\u679c\u62ff\u5230\u4e86accept\u9501\uff0c\u5c31\u4f1a\u6709 NGX_POST_EVENTS \u6807\u5fd7\u90a3\u4e48\u5c31\u4f1a\u653e\u5230\u76f8\u5e94\u7684\u961f\u5217\u4e2d\u3002\u6ca1\u6709\u7684\u8bdd\u5c31\u4f1a\u76f4\u63a5\u5904\u7406\u4e8b\u4ef6\u3002","title":"Introduction"},{"location":"Event-driven-concurrent-server/software-Nginx/Concurrency-model/Thundering-herd-in-Nginx/Nginx-accept_mutex-implementation/#accept_mutex","text":"\u4e00\u3001\u5982\u4f55\u907f\u514ddead lock\uff1f","title":"accept_mutex \u7684\u5b9e\u73b0"},{"location":"Event-driven-concurrent-server/software-Nginx/Concurrency-model/Thundering-herd-in-Nginx/Nginx-accept_mutex-implementation/#jianshu#nginx#accept","text":"\u672c\u6587\u57fa\u4e8eNginx 0.8.55\u6e90\u4ee3\u7801\uff0c\u5e76\u57fa\u4e8eepoll\u673a\u5236\u5206\u6790","title":"jianshu Nginx accept\u9501\u7684\u673a\u5236\u548c\u5b9e\u73b0"},{"location":"Event-driven-concurrent-server/software-Nginx/Concurrency-model/Thundering-herd-in-Nginx/Nginx-accept_mutex-implementation/#11#accpet","text":"\u63d0\u5230accept\u9501\uff0c\u5c31\u4e0d\u5f97\u4e0d\u63d0\u8d77\u60ca\u7fa4\u95ee\u9898\u3002 \u800c\u5728Linux\u5185\u6838\u7684\u8f83\u65b0\u7248\u672c\u4e2d\uff0c accept \u8c03\u7528\u672c\u8eab\u6240\u5f15\u8d77\u7684\u60ca\u7fa4\u95ee\u9898\u5df2\u7ecf\u5f97\u5230\u4e86\u89e3\u51b3\uff0c\u4f46\u662f\u5728Nginx\u4e2d\uff0c accept \u662f\u4ea4\u7ed9 epoll \u673a\u5236\u6765\u5904\u7406\u7684\uff0c epoll \u7684 accept \u5e26\u6765\u7684\u60ca\u7fa4\u95ee\u9898\u5e76\u6ca1\u6709\u5f97\u5230\u89e3\u51b3\uff08\u5e94\u8be5\u662f epoll_wait \u672c\u8eab\u5e76\u6ca1\u6709\u533a\u522b\u8bfb\u4e8b\u4ef6\u662f\u5426\u6765\u81ea\u4e8e\u4e00\u4e2aListen\u5957\u63a5\u5b57\u7684\u80fd\u529b\uff0c\u6240\u4ee5\u6240\u6709\u76d1\u542c\u8fd9\u4e2a\u4e8b\u4ef6\u7684\u8fdb\u7a0b\u4f1a\u88ab\u8fd9\u4e2aepoll_wait\u5524\u9192\u3002\uff09\uff0c\u6240\u4ee5Nginx\u7684accept\u60ca\u7fa4\u95ee\u9898\u4ecd\u7136\u9700\u8981\u5b9a\u5236\u4e00\u4e2a\u81ea\u5df1\u7684\u89e3\u51b3\u65b9\u6848\u3002 NOTE: \u4e00\u3001 epoll_wait \u5bfc\u81f4\u7684 thundering herd \u4e8c\u3001\u53c2\u89c1 epoll(7) \uff0c\u65b0\u7248\u672c\u7684epoll\u4e2d\uff0c\u5df2\u7ecf\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u4e86 accept\u9501\u5c31\u662fnginx\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u672c\u8d28\u4e0a\u8fd9\u662f\u4e00\u4e2a\u8de8\u8fdb\u7a0b\u7684\u4e92\u65a5\u9501\uff0c\u4ee5\u8fd9\u4e2a\u4e92\u65a5\u9501\u6765\u4fdd\u8bc1\u53ea\u6709\u4e00\u4e2a\u8fdb\u7a0b\u5177\u5907\u76d1\u542caccept\u4e8b\u4ef6\u7684\u80fd\u529b\u3002 \u5b9e\u73b0\u4e0aaccept\u9501\u662f\u4e00\u4e2a\u8de8\u8fdb\u7a0b\u9501\uff0c\u5176\u5728Nginx\u4e2d\u662f\u4e00\u4e2a\u5168\u5c40\u53d8\u91cf\uff0c\u58f0\u660e\u5982\u4e0b\uff1a ngx_shmtx_t ngx_accept_mutex ; NOTE: \u4e00\u3001\u987e\u540d\u601d\u4e49\uff0c\u5b83\u4f1a\u662f\u4f7f\u7528shared memory IPC \u8fd9\u662f\u4e00\u4e2a\u5728event\u6a21\u5757\u521d\u59cb\u5316\u65f6\u5c31\u5206\u914d\u597d\u7684\u9501\uff0c\u653e\u5728\u4e00\u5757\u8fdb\u7a0b\u95f4\u5171\u4eab\u7684\u5185\u5b58\u4e2d\uff0c\u4ee5\u4fdd\u8bc1\u6240\u6709\u8fdb\u7a0b\u90fd\u80fd\u8bbf\u95ee\u8fd9\u4e00\u4e2a\u5b9e\u4f8b\uff0c\u5176\u52a0\u9501\u89e3\u9501\u662f\u501f\u7531linux\u7684\u539f\u5b50\u53d8\u91cf\u6765\u505aCAS\uff0c\u5982\u679c\u52a0\u9501\u5931\u8d25\u5219\u7acb\u5373\u8fd4\u56de\uff0c\u662f\u4e00\u79cd\u975e\u963b\u585e\u7684\u9501\u3002\u52a0\u89e3\u9501\u4ee3\u7801\u5982\u4e0b\uff1a static ngx_inline ngx_uint_t ngx_shmtx_trylock ( ngx_shmtx_t * mtx ) { return ( * mtx -> lock == 0 && ngx_atomic_cmp_set ( mtx -> lock , 0 , ngx_pid )); } #define ngx_shmtx_lock(mtx) ngx_spinlock((mtx)->lock, ngx_pid, 1024) #define ngx_shmtx_unlock(mtx) (void) ngx_atomic_cmp_set((mtx)->lock, ngx_pid, 0) \u53ef\u4ee5\u770b\u51fa\uff0c\u8c03\u7528 ngx_shmtx_trylock \u5931\u8d25\u540e\u4f1a\u7acb\u523b\u8fd4\u56de\u800c\u4e0d\u4f1a\u963b\u585e\u3002","title":"1.1 accpet\u9501\u662f\u4e2a\u4ec0\u4e48\u4e1c\u897f"},{"location":"Event-driven-concurrent-server/software-Nginx/Concurrency-model/Thundering-herd-in-Nginx/Nginx-accept_mutex-implementation/#12#accept","text":"\u8981\u89e3\u51b3epoll\u5e26\u6765\u7684accept\u9501\u7684\u95ee\u9898\u4e5f\u5f88\u7b80\u5355\uff0c\u53ea\u9700\u8981\u4fdd\u8bc1\u540c\u4e00\u65f6\u95f4\u53ea\u6709\u4e00\u4e2a\u8fdb\u7a0b\u6ce8\u518c\u4e86accept\u7684epoll\u4e8b\u4ef6\u5373\u53ef\u3002 Nginx\u91c7\u7528\u7684\u5904\u7406\u6a21\u5f0f\u4e5f\u6ca1\u4ec0\u4e48\u7279\u522b\u7684\uff0c\u5927\u6982\u5c31\u662f\u5982\u4e0b\u7684\u903b\u8f91\uff1a \u5c1d\u8bd5\u83b7\u53d6accept\u9501 if \u83b7\u53d6\u6210\u529f\uff1a \u5728epoll\u4e2d\u6ce8\u518caccept\u4e8b\u4ef6 else : \u5728epoll\u4e2d\u6ce8\u9500accept\u4e8b\u4ef6 \u5904\u7406\u6240\u6709\u4e8b\u4ef6 \u91ca\u653eaccept\u9501 \u5f53\u7136\u8fd9\u91cc\u5ffd\u7565\u4e86\u5ef6\u540e\u4e8b\u4ef6\u7684\u5904\u7406\uff0c\u8fd9\u90e8\u5206\u6211\u4eec\u653e\u5230\u540e\u9762\u8ba8\u8bba\u3002 \u5bf9\u4e8eaccept\u9501\u7684\u5904\u7406\u548cepoll\u4e2d\u6ce8\u518c\u6ce8\u9500accept\u4e8b\u4ef6\u7684\u7684\u5904\u7406\u90fd\u662f\u5728 ngx_trylock_accept_mutex \u4e2d\u8fdb\u884c\u7684\u3002\u800c\u8fd9\u4e00\u7cfb\u5217\u8fc7\u7a0b\u5219\u662f\u5728nginx\u4e3b\u4f53\u5faa\u73af\u4e2d\u53cd\u590d\u8c03\u7528\u7684 void ngx_process_events_and_timers(ngx_cycle_t *cycle) \u4e2d\u8fdb\u884c\u3002 \u4e5f\u5c31\u662f\u8bf4\uff0c\u6bcf\u8f6e\u4e8b\u4ef6\u7684\u5904\u7406\u90fd\u4f1a\u9996\u5148\u7ade\u4e89accept\u9501\uff0c\u7ade\u4e89\u6210\u529f\u5219\u5728epoll\u4e2d\u6ce8\u518caccept\u4e8b\u4ef6\uff0c\u5931\u8d25\u5219\u6ce8\u9500accept\u4e8b\u4ef6\uff0c\u7136\u540e\u5904\u7406\u5b8c\u4e8b\u4ef6\u4e4b\u540e\uff0c\u91ca\u653eaccept\u9501\u3002\u7531\u6b64\u53ea\u6709\u4e00\u4e2a\u8fdb\u7a0b\u76d1\u542c\u4e00\u4e2alisten\u5957\u63a5\u5b57\uff0c\u4ece\u800c\u907f\u514d\u4e86\u60ca\u7fa4\u95ee\u9898\u3002","title":"1.2 accept\u9501\u5982\u4f55\u4fdd\u8bc1\u53ea\u6709\u4e00\u4e2a\u8fdb\u7a0b\u80fd\u591f\u5904\u7406\u65b0\u8fde\u63a5"},{"location":"Event-driven-concurrent-server/software-Nginx/Concurrency-model/Thundering-herd-in-Nginx/Nginx-accept_mutex-implementation/#13#accept","text":"accept\u9501\u5904\u7406\u60ca\u7fa4\u95ee\u9898\u7684\u65b9\u6848\u770b\u8d77\u6765\u4f3c\u4e4e\u5f88\u7f8e\uff0c\u4f46\u5982\u679c\u5b8c\u5168\u4f7f\u7528\u4e0a\u8ff0\u903b\u8f91\uff0c\u5c31\u4f1a\u6709\u4e00\u4e2a\u95ee\u9898\uff1a\u5982\u679c\u670d\u52a1\u5668\u975e\u5e38\u5fd9\uff0c\u6709\u975e\u5e38\u591a\u4e8b\u4ef6\u8981\u5904\u7406\uff0c\u90a3\u4e48\u201c\u5904\u7406\u6240\u6709\u4e8b\u4ef6\u8fd9\u4e00\u6b65\u201d\u5c31\u4f1a\u6d88\u8017\u975e\u5e38\u957f\u7684\u65f6\u95f4\uff0c\u4e5f\u5c31\u662f\u8bf4\uff0c\u67d0\u4e00\u4e2a\u8fdb\u7a0b\u957f\u65f6\u95f4\u5360\u7528accept\u9501\uff0c\u800c\u53c8\u65e0\u6687\u5904\u7406\u65b0\u8fde\u63a5\uff1b\u5176\u4ed6\u8fdb\u7a0b\u53c8\u6ca1\u6709\u5360\u7528accept\u9501\uff0c\u540c\u6837\u65e0\u6cd5\u5904\u7406\u65b0\u8fde\u63a5\u2014\u2014\u81f3\u6b64\uff0c\u65b0\u8fde\u63a5\u5c31\u5904\u4e8e\u65e0\u4eba\u5904\u7406\u7684\u72b6\u6001\uff0c\u8fd9\u5bf9\u670d\u52a1\u7684\u5b9e\u65f6\u6027\u65e0\u7591\u662f\u5f88\u8981\u547d\u7684\u3002 \u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0cNginx\u91c7\u7528\u4e86\u5c06\u4e8b\u4ef6\u5904\u7406\u5ef6\u540e\u7684\u65b9\u5f0f\u3002\u5373\u5728 ngx_process_events \u7684\u5904\u7406\u4e2d\uff0c\u4ec5\u4ec5\u5c06\u4e8b\u4ef6\u653e\u5165\u4e24\u4e2a\u961f\u5217\u4e2d\uff1a ngx_thread_volatile ngx_event_t * ngx_posted_accept_events ; ngx_thread_volatile ngx_event_t * ngx_posted_events ; \u8fd4\u56de\u540e\u5148\u5904\u7406 ngx_posted_accept_events \u540e\u7acb\u523b\u91ca\u653eaccept\u9501\uff0c\u7136\u540e\u518d\u6162\u6162\u5904\u7406\u5176\u4ed6\u4e8b\u4ef6\u3002 \u5373 ngx_process_events \u4ec5\u5bf9 epoll_wait \u8fdb\u884c\u5904\u7406\uff0c\u4e8b\u4ef6\u7684\u6d88\u8d39\u5219\u653e\u5230accept\u9501\u91ca\u653e\u4e4b\u540e\uff0c\u6765\u6700\u5927\u9650\u5ea6\u5730\u7f29\u77ed\u5360\u6709accept\u7684\u65f6\u95f4\uff0c\u6765\u8ba9\u5176\u4ed6\u8fdb\u7a0b\u4e5f\u6709\u8db3\u591f\u7684\u65f6\u673a\u5904\u7406accept\u4e8b\u4ef6\u3002 \u90a3\u4e48\u5177\u4f53\u662f\u600e\u4e48\u5b9e\u73b0\u7684\u5462\uff1f\u5176\u5b9e\u5c31\u662f\u5728 static ngx_int_t ngx_epoll_process_events(ngx_cycle_t *cycle, ngx_msec_t timer, ngx_uint_t flags) \u7684flags\u53c2\u6570\u4e2d\u4f20\u5165\u4e00\u4e2a NGX_POST_EVENTS \u7684\u6807\u5fd7\u4f4d\uff0c\u5904\u7406\u4e8b\u4ef6\u65f6\u68c0\u67e5\u8fd9\u4e2a\u6807\u5fd7\u4f4d\u5373\u53ef\u3002 \u8fd9\u91cc\u53ea\u662f\u907f\u514d\u4e86\u4e8b\u4ef6\u7684\u6d88\u8d39\u5bf9\u4e8eaccept\u9501\u7684\u957f\u671f\u5360\u7528\uff0c\u90a3\u4e48\u4e07\u4e00epoll_wait\u672c\u8eab\u5360\u7528\u7684\u65f6\u95f4\u5f88\u957f\u5462\uff1f\u8fd9\u79cd\u4e8b\u60c5\u4e5f\u4e0d\u662f\u4e0d\u53ef\u80fd\u53d1\u751f\u3002\u8fd9\u65b9\u9762\u7684\u5904\u7406\u4e5f\u5f88\u7b80\u5355\uff0cepoll_wait\u672c\u8eab\u662f\u6709\u8d85\u65f6\u65f6\u95f4\u7684\uff0c\u9650\u5236\u4f4f\u5b83\u7684\u503c\u5c31\u53ef\u4ee5\u4e86\uff0c\u8fd9\u4e2a\u53c2\u6570\u4fdd\u5b58\u5728 ngx_accept_mutex_delay \u8fd9\u4e2a\u5168\u5c40\u53d8\u91cf\u4e2d\u3002 \u4e0b\u9762\u653e\u4e0a ngx_process_events_and_timers \u7684\u5b9e\u73b0\u4ee3\u7801\uff0c\u53ef\u4ee5\u5927\u6982\u4e00\u89c2\u76f8\u5173\u7684\u5904\u7406\uff1a void ngx_process_events_and_timers ( ngx_cycle_t * cycle ) { ngx_uint_t flags ; ngx_msec_t timer , delta ; /* \u7701\u7565\u4e00\u4e9b\u5904\u7406\u65f6\u95f4\u4e8b\u4ef6\u7684\u4ee3\u7801 */ // \u8fd9\u91cc\u662f\u5904\u7406\u8d1f\u8f7d\u5747\u8861\u9501\u548caccept\u9501\u7684\u65f6\u673a if ( ngx_use_accept_mutex ) { // \u5982\u679c\u8d1f\u8f7d\u5747\u8861token\u7684\u503c\u5927\u4e8e0, \u5219\u8bf4\u660e\u8d1f\u8f7d\u5df2\u6ee1\uff0c\u6b64\u65f6\u4e0d\u518d\u5904\u7406accept, \u540c\u65f6\u628a\u8fd9\u4e2a\u503c\u51cf\u4e00 if ( ngx_accept_disabled > 0 ) { ngx_accept_disabled -- ; } else { // \u5c1d\u8bd5\u62ff\u5230accept\u9501 if ( ngx_trylock_accept_mutex ( cycle ) == NGX_ERROR ) { return ; } // \u62ff\u5230\u9501\u4e4b\u540e\u628aflag\u52a0\u4e0apost\u6807\u5fd7\uff0c\u8ba9\u6240\u6709\u4e8b\u4ef6\u7684\u5904\u7406\u90fd\u5ef6\u540e // \u4ee5\u514d\u592a\u957f\u65f6\u95f4\u5360\u7528accept\u9501 if ( ngx_accept_mutex_held ) { flags |= NGX_POST_EVENTS ; } else { if ( timer == NGX_TIMER_INFINITE || timer > ngx_accept_mutex_delay ) { timer = ngx_accept_mutex_delay ; // \u6700\u591a\u7b49ngx_accept_mutex_delay\u4e2a\u6beb\u79d2\uff0c\u9632\u6b62\u5360\u7528\u592a\u4e45accept\u9501 } } } } delta = ngx_current_msec ; // \u8c03\u7528\u4e8b\u4ef6\u5904\u7406\u6a21\u5757\u7684process_events\uff0c\u5904\u7406\u4e00\u4e2aepoll_wait\u7684\u65b9\u6cd5 ( void ) ngx_process_events ( cycle , timer , flags ); delta = ngx_current_msec - delta ; //\u8ba1\u7b97\u5904\u7406events\u4e8b\u4ef6\u6240\u6d88\u8017\u7684\u65f6\u95f4 ngx_log_debug1 ( NGX_LOG_DEBUG_EVENT , cycle -> log , 0 , \"timer delta: %M\" , delta ); // \u5982\u679c\u6709\u5ef6\u540e\u5904\u7406\u7684accept\u4e8b\u4ef6\uff0c\u90a3\u4e48\u5ef6\u540e\u5904\u7406\u8fd9\u4e2a\u4e8b\u4ef6 if ( ngx_posted_accept_events ) { ngx_event_process_posted ( cycle , & ngx_posted_accept_events ); } // \u91ca\u653eaccept\u9501 if ( ngx_accept_mutex_held ) { ngx_shmtx_unlock ( & ngx_accept_mutex ); } // \u5904\u7406\u6240\u6709\u7684\u8d85\u65f6\u4e8b\u4ef6 if ( delta ) { ngx_event_expire_timers (); } ngx_log_debug1 ( NGX_LOG_DEBUG_EVENT , cycle -> log , 0 , \"posted events %p\" , ngx_posted_events ); if ( ngx_posted_events ) { if ( ngx_threaded ) { ngx_wakeup_worker_thread ( cycle ); } else { // \u5904\u7406\u6240\u6709\u7684\u5ef6\u540e\u4e8b\u4ef6 ngx_event_process_posted ( cycle , & ngx_posted_events ); } } } \u518d\u6765\u770b\u770b ngx_epoll_process_events \u7684\u76f8\u5173\u5904\u7406\uff1a // \u8bfb\u4e8b\u4ef6 if (( revents & EPOLLIN ) && rev -> active ) { if (( flags & NGX_POST_THREAD_EVENTS ) && ! rev -> accept ) { rev -> posted_ready = 1 ; } else { rev -> ready = 1 ; } if ( flags & NGX_POST_EVENTS ) { queue = ( ngx_event_t ** ) ( rev -> accept ? & ngx_posted_accept_events : & ngx_posted_events ); ngx_locked_post_event ( rev , queue ); } else { rev -> handler ( rev ); } } wev = c -> write ; // \u5199\u4e8b\u4ef6 if (( revents & EPOLLOUT ) && wev -> active ) { if ( flags & NGX_POST_THREAD_EVENTS ) { wev -> posted_ready = 1 ; } else { wev -> ready = 1 ; } if ( flags & NGX_POST_EVENTS ) { ngx_locked_post_event ( wev , & ngx_posted_events ); } else { wev -> handler ( wev ); } } \u5904\u7406\u4e5f\u76f8\u5bf9\u7b80\u5355\uff0c\u5982\u679c\u62ff\u5230\u4e86accept\u9501\uff0c\u5c31\u4f1a\u6709 NGX_POST_EVENTS \u6807\u5fd7\u90a3\u4e48\u5c31\u4f1a\u653e\u5230\u76f8\u5e94\u7684\u961f\u5217\u4e2d\u3002\u6ca1\u6709\u7684\u8bdd\u5c31\u4f1a\u76f4\u63a5\u5904\u7406\u4e8b\u4ef6\u3002","title":"1.3 \u4e8b\u4ef6\u5904\u7406\u673a\u5236\u4e3a\u4e0d\u957f\u65f6\u95f4\u5360\u7528accept\u9501\u4f5c\u4e86\u54ea\u4e9b\u52aa\u529b"},{"location":"Event-driven-concurrent-server/software-Nginx/Concurrency-model/blog-Inside-NGINX-How-We-Designed-for-Performance-%26-Scale/","text":"nginx blog Inside NGINX: How We Designed for Performance & Scale Setting the Scene \u2013 The NGINX Process Model How Does NGINX Work? Each worker process is single\u2011threaded and runs independently, grabbing new connections and processing them. The processes can communicate using shared memory for shared cache data, session persistence data, and other shared resources. NOTE: IPC shared memory Inside the NGINX Worker Process The NGINX worker processes begin by waiting for events on the listen sockets ( accept_mutex and kernel socket sharding ). Events are initiated by new incoming connections. These connections are assigned to a state machine \u2013 the HTTP state machine is the most commonly used, but NGINX also implements state machines for stream (raw TCP) traffic and for a number of mail protocols (SMTP, IMAP, and POP3).","title":"Introduction"},{"location":"Event-driven-concurrent-server/software-Nginx/Concurrency-model/blog-Inside-NGINX-How-We-Designed-for-Performance-%26-Scale/#nginx#blog#inside#nginx#how#we#designed#for#performance#scale","text":"","title":"nginx blog Inside NGINX: How We Designed for Performance &amp; Scale"},{"location":"Event-driven-concurrent-server/software-Nginx/Concurrency-model/blog-Inside-NGINX-How-We-Designed-for-Performance-%26-Scale/#setting#the#scene#the#nginx#process#model","text":"","title":"Setting the Scene \u2013 The NGINX Process Model"},{"location":"Event-driven-concurrent-server/software-Nginx/Concurrency-model/blog-Inside-NGINX-How-We-Designed-for-Performance-%26-Scale/#how#does#nginx#work","text":"Each worker process is single\u2011threaded and runs independently, grabbing new connections and processing them. The processes can communicate using shared memory for shared cache data, session persistence data, and other shared resources. NOTE: IPC shared memory","title":"How Does NGINX Work?"},{"location":"Event-driven-concurrent-server/software-Nginx/Concurrency-model/blog-Inside-NGINX-How-We-Designed-for-Performance-%26-Scale/#inside#the#nginx#worker#process","text":"The NGINX worker processes begin by waiting for events on the listen sockets ( accept_mutex and kernel socket sharding ). Events are initiated by new incoming connections. These connections are assigned to a state machine \u2013 the HTTP state machine is the most commonly used, but NGINX also implements state machines for stream (raw TCP) traffic and for a number of mail protocols (SMTP, IMAP, and POP3).","title":"Inside the NGINX Worker Process"},{"location":"Event-driven-concurrent-server/software-Nginx/Concurrency-model/zhihu-nginx-master-worker%E8%BF%9B%E7%A8%8B%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/","text":"zhihu nginx master-worker\u8fdb\u7a0b\u5de5\u4f5c\u539f\u7406 nginx\u7684master-worker\u8fdb\u7a0b\u6a21\u578b\u662f\u5176\u80fd\u591f\u9ad8\u6027\u80fd\u7684\u5904\u7406\u7528\u6237\u8bf7\u6c42\u7684\u539f\u56e0\u4e4b\u4e00\uff0c\u800c\u4e14\u8fd9\u91cc\u7684\u6bcf\u4e2aworker\u8fdb\u7a0b\u90fd\u53ea\u4f1a\u542f\u52a8\u4e00\u4e2a\u7ebf\u7a0b\u6765\u5904\u7406\u7528\u6237\u8bf7\u6c42\u3002\u901a\u5e38\u6211\u4eec\u4f1a\u5c06worker\u8fdb\u7a0b\u7684\u6570\u91cf\u8bbe\u7f6e\u5f97\u4e0e\u6211\u4eec\u7684CPU\u6570\u91cf\u4e00\u81f4\uff0cnginx\u4e5f\u4f1a\u5c06\u6bcf\u4e2a\u8fdb\u7a0b\u4e0e\u6bcf\u4e2aCPU\u8fdb\u884c\u7ed1\u5b9a\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u53ef\u4ee5\u5145\u5206\u5229\u7528\u64cd\u4f5c\u7cfb\u7edf\u591a\u6838\u7684\u7279\u6027\uff0c\u5e76\u4e14\u80fd\u591f\u6700\u5927\u9650\u5ea6\u7684\u51cf\u5c11\u7ebf\u7a0b\u4e4b\u95f4\u7684\u5207\u6362\u800c\u5bfc\u81f4\u7684\u8d44\u6e90\u635f\u8017\u3002\u672c\u6587\u9996\u5148\u4f1a\u5bf9nginx\u7684master-worker\u8fdb\u7a0b\u6a21\u578b\u8fdb\u884c\u8bb2\u89e3\uff0c\u7136\u540e\u4f1a\u4ece\u6e90\u7801\u7684\u89d2\u5ea6\u5bf9nginx\u7684master-worker\u8fdb\u7a0b\u6a21\u578b\u7684\u5b9e\u73b0\u539f\u7406\u8fdb\u884c\u8bb2\u89e3\u3002 1. \u5de5\u4f5c\u539f\u7406 \u5728nginx\u542f\u52a8\u8fc7\u7a0b\u4e2d\uff0c\u4e3b\u8fdb\u7a0b\u5c31\u662fmaster\u8fdb\u7a0b\uff0c\u8be5\u8fdb\u7a0b\u5728\u542f\u52a8\u5404\u4e2aworker\u8fdb\u7a0b\u4e4b\u540e\uff0c\u5c31\u4f1a\u8fdb\u5165\u4e00\u4e2a\u65e0\u9650\u5faa\u73af\u4e2d\uff0c\u4ee5\u5904\u7406\u5ba2\u6237\u7aef\u53d1\u9001\u8fc7\u6765\u7684\u63a7\u5236\u6307\u4ee4\uff1b\u800cworker\u8fdb\u7a0b\u5219\u4f1a\u8fdb\u5165\u4e00\u4e2a\u5faa\u73af\u4e2d\uff0c\u4ece\u800c\u4e0d\u65ad\u63a5\u6536\u5ba2\u6237\u7aef\u7684\u8fde\u63a5\u8bf7\u6c42\u4ee5\u53ca\u5904\u7406\u8bf7\u6c42\u3002\u5982\u4e0b\u662fmaster-worker\u8fdb\u7a0b\u6a21\u578b\u7684\u4e00\u4e2a\u539f\u7406\u793a\u610f\u56fe\uff1a \u4ece\u56fe\u4e2d\u6211\u4eec\u53ef\u4ee5\u770b\u51fanginx\u5de5\u4f5c\u7684\u4e00\u822c\u6027\u539f\u7406\uff1a master\u8fdb\u7a0b\u901a\u8fc7\u63a5\u6536\u5ba2\u6237\u7aef\u7684\u8bf7\u6c42\uff0c\u6bd4\u5982 -s reload \u3001 -s stop \u7b49\uff0c\u89e3\u6790\u8fd9\u4e9b\u547d\u4ee4\u4e4b\u540e\uff0c\u901a\u8fc7\u8fdb\u7a0b\u95f4\u901a\u4fe1\uff0c\u5c06\u76f8\u5e94\u7684\u6307\u4ee4\u53d1\u9001\u5230\u5404\u4e2aworker\u8fdb\u7a0b\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9worker\u8fdb\u7a0b\u7684\u63a7\u5236\uff1b \u6bcf\u4e2aworker\u8fdb\u7a0b\u90fd\u4f1a\u7ade\u4e89\u540c\u4e00\u4e2a\u5171\u4eab\u9501\uff0c\u53ea\u6709\u7ade\u4e89\u5230\u5171\u4eab\u9501\u7684\u8fdb\u7a0b\u624d\u80fd\u591f\u5904\u7406\u5ba2\u6237\u7aef\u8bf7\u6c42\uff1b \u5f53\u5ba2\u6237\u7aef\u8bf7\u6c42\u53d1\u9001\u8fc7\u6765\u4e4b\u540e\uff0cworker\u8fdb\u7a0b\u4f1a\u5904\u7406\u8be5\u8bf7\u6c42\u7684\u4e8b\u4ef6\uff0c\u5982\u679c\u662faccept\u4e8b\u4ef6\uff0c\u5219\u4f1a\u5c06\u5176\u6dfb\u52a0\u5230accept\u961f\u5217\u4e2d\uff0c\u5982\u679c\u662fread\u6216\u8005write\u4e8b\u4ef6\uff0c\u5219\u4f1a\u5c06\u5176\u6dfb\u52a0\u5230read-write\u961f\u5217\u4e2d\uff1b \u5728\u5c06\u4e8b\u4ef6\u6dfb\u52a0\u5230\u76f8\u5e94\u7684\u961f\u5217\u4e2d\u4e4b\u540e\uff0c\u5728\u6301\u6709\u5171\u4eab\u9501\u7684\u60c5\u51b5\u4e0b\uff0cnginx\u4f1a\u5904\u7406\u5b8caccept\u961f\u5217\u4e2d\u7684\u5ba2\u6237\u7aef\u8fde\u63a5\u8bf7\u6c42\uff0c\u800c\u5bf9\u4e8eread\u6216\u8005write\u4e8b\u4ef6\uff0c\u5219\u4f1a\u5728\u91ca\u653e\u9501\u4e4b\u540e\u76f4\u63a5\u4eceread-write\u961f\u5217\u4e2d\u53d6\u51fa\u4e8b\u4ef6\u6765\u5904\u7406\u3002","title":"Introduction"},{"location":"Event-driven-concurrent-server/software-Nginx/Concurrency-model/zhihu-nginx-master-worker%E8%BF%9B%E7%A8%8B%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/#zhihu#nginx#master-worker","text":"nginx\u7684master-worker\u8fdb\u7a0b\u6a21\u578b\u662f\u5176\u80fd\u591f\u9ad8\u6027\u80fd\u7684\u5904\u7406\u7528\u6237\u8bf7\u6c42\u7684\u539f\u56e0\u4e4b\u4e00\uff0c\u800c\u4e14\u8fd9\u91cc\u7684\u6bcf\u4e2aworker\u8fdb\u7a0b\u90fd\u53ea\u4f1a\u542f\u52a8\u4e00\u4e2a\u7ebf\u7a0b\u6765\u5904\u7406\u7528\u6237\u8bf7\u6c42\u3002\u901a\u5e38\u6211\u4eec\u4f1a\u5c06worker\u8fdb\u7a0b\u7684\u6570\u91cf\u8bbe\u7f6e\u5f97\u4e0e\u6211\u4eec\u7684CPU\u6570\u91cf\u4e00\u81f4\uff0cnginx\u4e5f\u4f1a\u5c06\u6bcf\u4e2a\u8fdb\u7a0b\u4e0e\u6bcf\u4e2aCPU\u8fdb\u884c\u7ed1\u5b9a\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u53ef\u4ee5\u5145\u5206\u5229\u7528\u64cd\u4f5c\u7cfb\u7edf\u591a\u6838\u7684\u7279\u6027\uff0c\u5e76\u4e14\u80fd\u591f\u6700\u5927\u9650\u5ea6\u7684\u51cf\u5c11\u7ebf\u7a0b\u4e4b\u95f4\u7684\u5207\u6362\u800c\u5bfc\u81f4\u7684\u8d44\u6e90\u635f\u8017\u3002\u672c\u6587\u9996\u5148\u4f1a\u5bf9nginx\u7684master-worker\u8fdb\u7a0b\u6a21\u578b\u8fdb\u884c\u8bb2\u89e3\uff0c\u7136\u540e\u4f1a\u4ece\u6e90\u7801\u7684\u89d2\u5ea6\u5bf9nginx\u7684master-worker\u8fdb\u7a0b\u6a21\u578b\u7684\u5b9e\u73b0\u539f\u7406\u8fdb\u884c\u8bb2\u89e3\u3002","title":"zhihu nginx master-worker\u8fdb\u7a0b\u5de5\u4f5c\u539f\u7406"},{"location":"Event-driven-concurrent-server/software-Nginx/Concurrency-model/zhihu-nginx-master-worker%E8%BF%9B%E7%A8%8B%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/#1","text":"\u5728nginx\u542f\u52a8\u8fc7\u7a0b\u4e2d\uff0c\u4e3b\u8fdb\u7a0b\u5c31\u662fmaster\u8fdb\u7a0b\uff0c\u8be5\u8fdb\u7a0b\u5728\u542f\u52a8\u5404\u4e2aworker\u8fdb\u7a0b\u4e4b\u540e\uff0c\u5c31\u4f1a\u8fdb\u5165\u4e00\u4e2a\u65e0\u9650\u5faa\u73af\u4e2d\uff0c\u4ee5\u5904\u7406\u5ba2\u6237\u7aef\u53d1\u9001\u8fc7\u6765\u7684\u63a7\u5236\u6307\u4ee4\uff1b\u800cworker\u8fdb\u7a0b\u5219\u4f1a\u8fdb\u5165\u4e00\u4e2a\u5faa\u73af\u4e2d\uff0c\u4ece\u800c\u4e0d\u65ad\u63a5\u6536\u5ba2\u6237\u7aef\u7684\u8fde\u63a5\u8bf7\u6c42\u4ee5\u53ca\u5904\u7406\u8bf7\u6c42\u3002\u5982\u4e0b\u662fmaster-worker\u8fdb\u7a0b\u6a21\u578b\u7684\u4e00\u4e2a\u539f\u7406\u793a\u610f\u56fe\uff1a \u4ece\u56fe\u4e2d\u6211\u4eec\u53ef\u4ee5\u770b\u51fanginx\u5de5\u4f5c\u7684\u4e00\u822c\u6027\u539f\u7406\uff1a master\u8fdb\u7a0b\u901a\u8fc7\u63a5\u6536\u5ba2\u6237\u7aef\u7684\u8bf7\u6c42\uff0c\u6bd4\u5982 -s reload \u3001 -s stop \u7b49\uff0c\u89e3\u6790\u8fd9\u4e9b\u547d\u4ee4\u4e4b\u540e\uff0c\u901a\u8fc7\u8fdb\u7a0b\u95f4\u901a\u4fe1\uff0c\u5c06\u76f8\u5e94\u7684\u6307\u4ee4\u53d1\u9001\u5230\u5404\u4e2aworker\u8fdb\u7a0b\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9worker\u8fdb\u7a0b\u7684\u63a7\u5236\uff1b \u6bcf\u4e2aworker\u8fdb\u7a0b\u90fd\u4f1a\u7ade\u4e89\u540c\u4e00\u4e2a\u5171\u4eab\u9501\uff0c\u53ea\u6709\u7ade\u4e89\u5230\u5171\u4eab\u9501\u7684\u8fdb\u7a0b\u624d\u80fd\u591f\u5904\u7406\u5ba2\u6237\u7aef\u8bf7\u6c42\uff1b \u5f53\u5ba2\u6237\u7aef\u8bf7\u6c42\u53d1\u9001\u8fc7\u6765\u4e4b\u540e\uff0cworker\u8fdb\u7a0b\u4f1a\u5904\u7406\u8be5\u8bf7\u6c42\u7684\u4e8b\u4ef6\uff0c\u5982\u679c\u662faccept\u4e8b\u4ef6\uff0c\u5219\u4f1a\u5c06\u5176\u6dfb\u52a0\u5230accept\u961f\u5217\u4e2d\uff0c\u5982\u679c\u662fread\u6216\u8005write\u4e8b\u4ef6\uff0c\u5219\u4f1a\u5c06\u5176\u6dfb\u52a0\u5230read-write\u961f\u5217\u4e2d\uff1b \u5728\u5c06\u4e8b\u4ef6\u6dfb\u52a0\u5230\u76f8\u5e94\u7684\u961f\u5217\u4e2d\u4e4b\u540e\uff0c\u5728\u6301\u6709\u5171\u4eab\u9501\u7684\u60c5\u51b5\u4e0b\uff0cnginx\u4f1a\u5904\u7406\u5b8caccept\u961f\u5217\u4e2d\u7684\u5ba2\u6237\u7aef\u8fde\u63a5\u8bf7\u6c42\uff0c\u800c\u5bf9\u4e8eread\u6216\u8005write\u4e8b\u4ef6\uff0c\u5219\u4f1a\u5728\u91ca\u653e\u9501\u4e4b\u540e\u76f4\u63a5\u4eceread-write\u961f\u5217\u4e2d\u53d6\u51fa\u4e8b\u4ef6\u6765\u5904\u7406\u3002","title":"1. \u5de5\u4f5c\u539f\u7406"},{"location":"Event-driven-concurrent-server/software-Nginx/Nginx-proxy/","text":"Nginx proxy cnblogs Nginx\u4ee3\u7406\u7684\u51e0\u79cd\u6a21\u5f0f","title":"Introduction"},{"location":"Event-driven-concurrent-server/software-Nginx/Nginx-proxy/#nginx#proxy","text":"","title":"Nginx proxy"},{"location":"Event-driven-concurrent-server/software-Nginx/Nginx-proxy/#cnblogs#nginx","text":"","title":"cnblogs Nginx\u4ee3\u7406\u7684\u51e0\u79cd\u6a21\u5f0f"},{"location":"Message-processing-system/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u8ba8\u8bbamessage/event processing system\uff0c\u5373\u5904\u7406\u6d88\u606f\u7684\u7cfb\u7edf\u3002Message\u662f\u4e00\u4e2a\u975e\u5e38\u62bd\u8c61\u7684\u6982\u5ff5\uff0c\u6211\u4eec\u5e73\u65f6\u6240\u8bf4\u7684event\u7b49\uff0c\u90fd\u53ef\u4ee5\u770b\u505a\u662fmessage\uff0c\u6b63\u56e0\u4e3a\u5982\u6b64\uff0c\u6211\u4eec\u5e73\u65f6\u6240\u78b0\u5230\u7684\u5f88\u591asoftware\u90fd\u53ef\u4ee5\u770b\u505a\u662fmessage processing system\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u5bf9message processing system\u8fdb\u884c\u6df1\u5165\u7684\u603b\u7ed3\u3001\u5206\u6790\u3002 \u6211\u4eec\u91c7\u7528\u5728\u6587\u7ae0 Abstraction and model \u4e2d\u63d0\u51fa\u7684\u601d\u60f3\uff0c\u5efa\u7acb\"message/event-driven model\"\uff0c\u4f7f\u7528\u8fd9\u4e2aabstraction/conceptual model\u6765\u62bd\u8c61\u5730\u63cf\u8ff0\"message/event processing system\"\uff1b\u7136\u540e\u6211\u4eec\u8ba8\u8bba\u5177\u4f53\u7684\u5b9e\u73b0\u65b9\u5f0f\u3002 TODO InfoQ \u817e\u8baf\u5ba3\u5e03\u5f00\u6e90 RoP\uff1aApache Pulsar \u652f\u6301\u539f\u751f RocketMQ \u534f\u8bae","title":"Introduction"},{"location":"Message-processing-system/#_1","text":"\u672c\u7ae0\u8ba8\u8bbamessage/event processing system\uff0c\u5373\u5904\u7406\u6d88\u606f\u7684\u7cfb\u7edf\u3002Message\u662f\u4e00\u4e2a\u975e\u5e38\u62bd\u8c61\u7684\u6982\u5ff5\uff0c\u6211\u4eec\u5e73\u65f6\u6240\u8bf4\u7684event\u7b49\uff0c\u90fd\u53ef\u4ee5\u770b\u505a\u662fmessage\uff0c\u6b63\u56e0\u4e3a\u5982\u6b64\uff0c\u6211\u4eec\u5e73\u65f6\u6240\u78b0\u5230\u7684\u5f88\u591asoftware\u90fd\u53ef\u4ee5\u770b\u505a\u662fmessage processing system\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u5bf9message processing system\u8fdb\u884c\u6df1\u5165\u7684\u603b\u7ed3\u3001\u5206\u6790\u3002 \u6211\u4eec\u91c7\u7528\u5728\u6587\u7ae0 Abstraction and model \u4e2d\u63d0\u51fa\u7684\u601d\u60f3\uff0c\u5efa\u7acb\"message/event-driven model\"\uff0c\u4f7f\u7528\u8fd9\u4e2aabstraction/conceptual model\u6765\u62bd\u8c61\u5730\u63cf\u8ff0\"message/event processing system\"\uff1b\u7136\u540e\u6211\u4eec\u8ba8\u8bba\u5177\u4f53\u7684\u5b9e\u73b0\u65b9\u5f0f\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Message-processing-system/#todo","text":"InfoQ \u817e\u8baf\u5ba3\u5e03\u5f00\u6e90 RoP\uff1aApache Pulsar \u652f\u6301\u539f\u751f RocketMQ \u534f\u8bae","title":"TODO"},{"location":"Message-processing-system/Distributed-log-system/","text":"\u7531\u6052\u751fdlog\uff08distributed log\uff09\u5f15\u53d1\u7684\u5bf9log\u7684\u601d\u8003\u3002 Google distributed log\uff0c\u5f97\u5230\u7684\u4e86\u5982\u4e0b\u6bd4\u8f83\u6709\u4ef7\u503c\u7684\u5185\u5bb9\uff1a http://bookkeeper.apache.org/distributedlog/ https://bravenewgeek.com/building-a-distributed-log-from-scratch-part-1-storage-mechanics/ https://engineering.fb.com/core-data/logdevice-a-distributed-data-store-for-logs/ https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying https://en.wikipedia.org/wiki/Log_file","title":"Introduction"},{"location":"Message-processing-system/MQ-library/","text":"MQ implementation MQ\u5bf9\u6bd4 csdn \u4e09\u5927\u4e3b\u6d41\u6d88\u606f\u4e2d\u95f4\u4ef6\u4f18\u7f3a\u70b9 rabbitmq\uff1a \u4f18\u70b9\uff1a\u8f7b\u91cf\uff0c\u8fc5\u6377\uff0c\u5bb9\u6613\u90e8\u7f72\u548c\u4f7f\u7528\uff0c\u62e5\u6709\u7075\u6d3b\u7684\u8def\u7531\u914d\u7f6e \u7f3a\u70b9\uff1a\u6027\u80fd\u548c\u541e\u5410\u91cf\u8f83\u5dee\uff0c\u4e0d\u6613\u8fdb\u884c\u4e8c\u6b21\u5f00\u53d1 rocketmq\uff1a \u4f18\u70b9\uff1a\u6027\u80fd\u597d\uff0c\u7a33\u5b9a\u53ef\u9760\uff0c\u6709\u6d3b\u8dc3\u7684\u4e2d\u6587\u793e\u533a\uff0c\u7279\u70b9\u54cd\u5e94\u5feb \u7f3a\u70b9\uff1a\u517c\u5bb9\u6027\u8f83\u5dee\uff0c\u4f46\u968f\u610f\u5f71\u54cd\u529b\u7684\u6269\u5927\uff0c\u8be5\u95ee\u9898\u4f1a\u6709\u6539\u5584 kafka\uff1a \u4f18\u70b9\uff1a\u62e5\u6709\u5f3a\u5927\u7684\u6027\u80fd\u53ca\u541e\u5410\u91cf\uff0c\u517c\u5bb9\u6027\u5f88\u597d \u7f3a\u70b9\uff1a\u7531\u4e8e\u201c\u6512\u4e00\u6ce2\u518d\u5904\u7406\u201d\u5bfc\u81f4\u5ef6\u8fdf\u6bd4\u8f83\u9ad8\uff0c\u6709\u53ef\u80fd\u6d88\u606f\u91cd\u590d\u6d88\u8d39 cnblogs Kafka\u5b66\u4e60\u4e4b\u8def \uff08\u4e00\uff09Kafka\u7684\u7b80\u4ecb # \u4e09\u3001\u5e38\u7528Message Queue\u5bf9\u6bd4 3.1\u3000RabbitMQ RabbitMQ\u662f\u4f7f\u7528Erlang\u7f16\u5199\u7684\u4e00\u4e2a\u5f00\u6e90\u7684\u6d88\u606f\u961f\u5217\uff0c\u672c\u8eab\u652f\u6301\u5f88\u591a\u7684\u534f\u8bae\uff1aAMQP\uff0cXMPP, SMTP, STOMP\uff0c\u4e5f\u6b63\u56e0\u5982\u6b64\uff0c\u5b83\u975e\u5e38\u91cd\u91cf\u7ea7\uff0c\u66f4\u9002\u5408\u4e8e\u4f01\u4e1a\u7ea7\u7684\u5f00\u53d1\u3002\u540c\u65f6\u5b9e\u73b0\u4e86Broker\u6784\u67b6\uff0c\u8fd9\u610f\u5473\u7740\u6d88\u606f\u5728\u53d1\u9001\u7ed9\u5ba2\u6237\u7aef\u65f6\u5148\u5728\u4e2d\u5fc3\u961f\u5217\u6392\u961f\u3002\u5bf9\u8def\u7531\uff0c\u8d1f\u8f7d\u5747\u8861\u6216\u8005\u6570\u636e\u6301\u4e45\u5316\u90fd\u6709\u5f88\u597d\u7684\u652f\u6301\u3002 3.2\u3000Redis Redis\u662f\u4e00\u4e2a\u57fa\u4e8eKey-Value\u5bf9\u7684NoSQL\u6570\u636e\u5e93\uff0c\u5f00\u53d1\u7ef4\u62a4\u5f88\u6d3b\u8dc3\u3002\u867d\u7136\u5b83\u662f\u4e00\u4e2aKey-Value\u6570\u636e\u5e93\u5b58\u50a8\u7cfb\u7edf\uff0c\u4f46\u5b83\u672c\u8eab\u652f\u6301MQ\u529f\u80fd\uff0c\u6240\u4ee5\u5b8c\u5168\u53ef\u4ee5\u5f53\u505a\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u961f\u5217\u670d\u52a1\u6765\u4f7f\u7528\u3002\u5bf9\u4e8eRabbitMQ\u548cRedis\u7684\u5165\u961f\u548c\u51fa\u961f\u64cd\u4f5c\uff0c\u5404\u6267\u884c100\u4e07\u6b21\uff0c\u6bcf10\u4e07\u6b21\u8bb0\u5f55\u4e00\u6b21\u6267\u884c\u65f6\u95f4\u3002\u6d4b\u8bd5\u6570\u636e\u5206\u4e3a128Bytes\u3001512Bytes\u30011K\u548c10K\u56db\u4e2a\u4e0d\u540c\u5927\u5c0f\u7684\u6570\u636e\u3002\u5b9e\u9a8c\u8868\u660e\uff1a\u5165\u961f\u65f6\uff0c\u5f53\u6570\u636e\u6bd4\u8f83\u5c0f\u65f6Redis\u7684\u6027\u80fd\u8981\u9ad8\u4e8eRabbitMQ\uff0c\u800c\u5982\u679c\u6570\u636e\u5927\u5c0f\u8d85\u8fc7\u4e8610K\uff0cRedis\u5219\u6162\u7684\u65e0\u6cd5\u5fcd\u53d7\uff1b\u51fa\u961f\u65f6\uff0c\u65e0\u8bba\u6570\u636e\u5927\u5c0f\uff0cRedis\u90fd\u8868\u73b0\u51fa\u975e\u5e38\u597d\u7684\u6027\u80fd\uff0c\u800cRabbitMQ\u7684\u51fa\u961f\u6027\u80fd\u5219\u8fdc\u4f4e\u4e8eRedis\u3002 3.3\u3000ZeroMQ ZeroMQ\u53f7\u79f0\u6700\u5feb\u7684\u6d88\u606f\u961f\u5217\u7cfb\u7edf\uff0c\u5c24\u5176\u9488\u5bf9\u5927\u541e\u5410\u91cf\u7684\u9700\u6c42\u573a\u666f\u3002ZeroMQ\u80fd\u591f\u5b9e\u73b0RabbitMQ\u4e0d\u64c5\u957f\u7684\u9ad8\u7ea7/\u590d\u6742\u7684\u961f\u5217\uff0c\u4f46\u662f\u5f00\u53d1\u4eba\u5458\u9700\u8981\u81ea\u5df1\u7ec4\u5408\u591a\u79cd\u6280\u672f\u6846\u67b6\uff0c\u6280\u672f\u4e0a\u7684\u590d\u6742\u5ea6\u662f\u5bf9\u8fd9MQ\u80fd\u591f\u5e94\u7528\u6210\u529f\u7684\u6311\u6218\u3002ZeroMQ\u5177\u6709\u4e00\u4e2a\u72ec\u7279\u7684\u975e\u4e2d\u95f4\u4ef6\u7684\u6a21\u5f0f\uff0c\u4f60\u4e0d\u9700\u8981\u5b89\u88c5\u548c\u8fd0\u884c\u4e00\u4e2a\u6d88\u606f\u670d\u52a1\u5668\u6216\u4e2d\u95f4\u4ef6\uff0c\u56e0\u4e3a\u4f60\u7684\u5e94\u7528\u7a0b\u5e8f\u5c06\u626e\u6f14\u8fd9\u4e2a\u670d\u52a1\u5668\u89d2\u8272\u3002\u4f60\u53ea\u9700\u8981\u7b80\u5355\u7684\u5f15\u7528ZeroMQ\u7a0b\u5e8f\u5e93\uff0c\u53ef\u4ee5\u4f7f\u7528NuGet\u5b89\u88c5\uff0c\u7136\u540e\u4f60\u5c31\u53ef\u4ee5\u6109\u5feb\u7684\u5728\u5e94\u7528\u7a0b\u5e8f\u4e4b\u95f4\u53d1\u9001\u6d88\u606f\u4e86\u3002\u4f46\u662fZeroMQ\u4ec5\u63d0\u4f9b\u975e\u6301\u4e45\u6027\u7684\u961f\u5217\uff0c\u4e5f\u5c31\u662f\u8bf4\u5982\u679c\u5b95\u673a\uff0c\u6570\u636e\u5c06\u4f1a\u4e22\u5931\u3002\u5176\u4e2d\uff0cTwitter\u7684Storm 0.9.0\u4ee5\u524d\u7684\u7248\u672c\u4e2d\u9ed8\u8ba4\u4f7f\u7528ZeroMQ\u4f5c\u4e3a\u6570\u636e\u6d41\u7684\u4f20\u8f93\uff08Storm\u4ece0.9\u7248\u672c\u5f00\u59cb\u540c\u65f6\u652f\u6301ZeroMQ\u548cNetty\u4f5c\u4e3a\u4f20\u8f93\u6a21\u5757\uff09\u3002 3.4\u3000ActiveMQ ActiveMQ\u662fApache\u4e0b\u7684\u4e00\u4e2a\u5b50\u9879\u76ee\u3002 \u7c7b\u4f3c\u4e8eZeroMQ\uff0c\u5b83\u80fd\u591f\u4ee5\u4ee3\u7406\u4eba\u548c\u70b9\u5bf9\u70b9\u7684\u6280\u672f\u5b9e\u73b0\u961f\u5217\u3002\u540c\u65f6\u7c7b\u4f3c\u4e8eRabbitMQ\uff0c\u5b83\u5c11\u91cf\u4ee3\u7801\u5c31\u53ef\u4ee5\u9ad8\u6548\u5730\u5b9e\u73b0\u9ad8\u7ea7\u5e94\u7528\u573a\u666f\u3002 3.5\u3000Kafka/Jafka Kafka\u662fApache\u4e0b\u7684\u4e00\u4e2a\u5b50\u9879\u76ee\uff0c\u662f\u4e00\u4e2a\u9ad8\u6027\u80fd\u8de8\u8bed\u8a00\u5206\u5e03\u5f0f\u53d1\u5e03/\u8ba2\u9605\u6d88\u606f\u961f\u5217\u7cfb\u7edf\uff0c\u800cJafka\u662f\u5728Kafka\u4e4b\u4e0a\u5b75\u5316\u800c\u6765\u7684\uff0c\u5373Kafka\u7684\u4e00\u4e2a\u5347\u7ea7\u7248\u3002\u5177\u6709\u4ee5\u4e0b\u7279\u6027\uff1a\u5feb\u901f\u6301\u4e45\u5316\uff0c\u53ef\u4ee5\u5728O(1)\u7684\u7cfb\u7edf\u5f00\u9500\u4e0b\u8fdb\u884c\u6d88\u606f\u6301\u4e45\u5316\uff1b\u9ad8\u541e\u5410\uff0c\u5728\u4e00\u53f0\u666e\u901a\u7684\u670d\u52a1\u5668\u4e0a\u65e2\u53ef\u4ee5\u8fbe\u523010W/s\u7684\u541e\u5410\u901f\u7387\uff1b\u5b8c\u5168\u7684\u5206\u5e03\u5f0f\u7cfb\u7edf\uff0cBroker\u3001Producer\u3001Consumer\u90fd\u539f\u751f\u81ea\u52a8\u652f\u6301\u5206\u5e03\u5f0f\uff0c\u81ea\u52a8\u5b9e\u73b0\u8d1f\u8f7d\u5747\u8861\uff1b\u652f\u6301Hadoop\u6570\u636e\u5e76\u884c\u52a0\u8f7d\uff0c\u5bf9\u4e8e\u50cfHadoop\u7684\u4e00\u6837\u7684\u65e5\u5fd7\u6570\u636e\u548c\u79bb\u7ebf\u5206\u6790\u7cfb\u7edf\uff0c\u4f46\u53c8\u8981\u6c42\u5b9e\u65f6\u5904\u7406\u7684\u9650\u5236\uff0c\u8fd9\u662f\u4e00\u4e2a\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002Kafka\u901a\u8fc7Hadoop\u7684\u5e76\u884c\u52a0\u8f7d\u673a\u5236\u7edf\u4e00\u4e86\u5728\u7ebf\u548c\u79bb\u7ebf\u7684\u6d88\u606f\u5904\u7406\u3002Apache Kafka\u76f8\u5bf9\u4e8eActiveMQ\u662f\u4e00\u4e2a\u975e\u5e38\u8f7b\u91cf\u7ea7\u7684\u6d88\u606f\u7cfb\u7edf\uff0c\u9664\u4e86\u6027\u80fd\u975e\u5e38\u597d\u4e4b\u5916\uff0c\u8fd8\u662f\u4e00\u4e2a\u5de5\u4f5c\u826f\u597d\u7684\u5206\u5e03\u5f0f\u7cfb\u7edf\u3002 4u4v \u4e2d\u95f4\u4ef6\u662f\u4ec0\u4e48\uff0c\u5e38\u7528\u7684\u4e2d\u95f4\u4ef6\u6709\u54ea\u4e9b\uff1f \u6d88\u606f\u961f\u5217\uff08Message Quequing\uff09\u662f\u5728\u6d88\u606f\u4f20\u8f93\u8fc7\u7a0b\u4e2d\u4fdd\u5b58\u6d88\u606f\u7684\u5bb9\u5668\uff0c\u6d88\u606f\u4e2d\u95f4\u4ef6\u5373\u4e3a\u6d88\u606f\u961f\u5217\u7684\u627f\u8f7d\u5f62\u5f0f \u3002\u6d88\u606f\u662f\u4e24\u53f0\u8ba1\u7b97\u673a\u95f4\u4f20\u9001\u7684\u6570\u636e\u5355\u4f4d\uff0c\u6d88\u606f\u961f\u5217\u5728\u5c06\u6d88\u606f\u4ece\u5b83\u7684\u6e90\u4e2d\u7ee7\u5230\u5b83\u7684\u76ee\u6807\u65f6\u5145\u5f53\u4e2d\u95f4\u4eba\uff0c\u4e3b\u8981\u76ee\u7684\u662f\u63d0\u4f9b\u8def\u7531\u5e76\u4fdd\u8bc1\u6d88\u606f\u7684\u4f20\u9012\uff1b\u5982\u679c\u53d1\u9001\u6d88\u606f\u65f6\u63a5\u6536\u8005\u4e0d\u53ef\u7528\uff0c\u6d88\u606f\u961f\u5217\u4f1a\u4fdd\u7559\u6d88\u606f\uff0c\u76f4\u5230\u53ef\u4ee5\u6210\u529f\u5730\u4f20\u9012\u5b83\uff0c\u4e3b\u8981\u89e3\u51b3\u4f20\u7edf\u7ed3\u6784\u8026\u5408\u6027\u95ee\u9898\u3001\u7cfb\u7edf\u5f02\u6b65\u6027\u95ee\u9898\u4ee5\u53ca\u7f13\u89e3\u5927\u6570\u636e\u91cf\u5e76\u53d1\u7684\u95ee\u9898\u7b49\u3002 \u6d88\u606f\u961f\u5217\u6709\u8f83\u591a\u7684\u578b\u53f7\uff0c\u8f83\u4e3a\u5e38\u7528\u7684\u4e3aActiveMQ\u3001Rabbit MQ\u3001RocketMQ\u548cKafk a\u3002\u7531\u4e8e\u6d88\u606f\u961f\u5217\u4f7f\u7528\u6d88\u606f\u5c06\u5e94\u7528\u7a0b\u5e8f\u8fde\u63a5\u8d77\u6765\uff0c\u8fd9\u4e9b\u6d88\u606f\u901a\u8fc7\u50cfRabbit MQ\u7684\u6d88\u606f\u4ee3\u7406\u670d\u52a1\u5668\u5728\u5e94\u7528\u7a0b\u5e8f\u4e4b\u95f4\u8def\u7531\u3002","title":"Introduction"},{"location":"Message-processing-system/MQ-library/#mq#implementation","text":"","title":"MQ implementation"},{"location":"Message-processing-system/MQ-library/#mq","text":"","title":"MQ\u5bf9\u6bd4"},{"location":"Message-processing-system/MQ-library/#csdn","text":"rabbitmq\uff1a \u4f18\u70b9\uff1a\u8f7b\u91cf\uff0c\u8fc5\u6377\uff0c\u5bb9\u6613\u90e8\u7f72\u548c\u4f7f\u7528\uff0c\u62e5\u6709\u7075\u6d3b\u7684\u8def\u7531\u914d\u7f6e \u7f3a\u70b9\uff1a\u6027\u80fd\u548c\u541e\u5410\u91cf\u8f83\u5dee\uff0c\u4e0d\u6613\u8fdb\u884c\u4e8c\u6b21\u5f00\u53d1 rocketmq\uff1a \u4f18\u70b9\uff1a\u6027\u80fd\u597d\uff0c\u7a33\u5b9a\u53ef\u9760\uff0c\u6709\u6d3b\u8dc3\u7684\u4e2d\u6587\u793e\u533a\uff0c\u7279\u70b9\u54cd\u5e94\u5feb \u7f3a\u70b9\uff1a\u517c\u5bb9\u6027\u8f83\u5dee\uff0c\u4f46\u968f\u610f\u5f71\u54cd\u529b\u7684\u6269\u5927\uff0c\u8be5\u95ee\u9898\u4f1a\u6709\u6539\u5584 kafka\uff1a \u4f18\u70b9\uff1a\u62e5\u6709\u5f3a\u5927\u7684\u6027\u80fd\u53ca\u541e\u5410\u91cf\uff0c\u517c\u5bb9\u6027\u5f88\u597d \u7f3a\u70b9\uff1a\u7531\u4e8e\u201c\u6512\u4e00\u6ce2\u518d\u5904\u7406\u201d\u5bfc\u81f4\u5ef6\u8fdf\u6bd4\u8f83\u9ad8\uff0c\u6709\u53ef\u80fd\u6d88\u606f\u91cd\u590d\u6d88\u8d39","title":"csdn \u4e09\u5927\u4e3b\u6d41\u6d88\u606f\u4e2d\u95f4\u4ef6\u4f18\u7f3a\u70b9"},{"location":"Message-processing-system/MQ-library/#cnblogs#kafka#kafka#message#queue","text":"3.1\u3000RabbitMQ RabbitMQ\u662f\u4f7f\u7528Erlang\u7f16\u5199\u7684\u4e00\u4e2a\u5f00\u6e90\u7684\u6d88\u606f\u961f\u5217\uff0c\u672c\u8eab\u652f\u6301\u5f88\u591a\u7684\u534f\u8bae\uff1aAMQP\uff0cXMPP, SMTP, STOMP\uff0c\u4e5f\u6b63\u56e0\u5982\u6b64\uff0c\u5b83\u975e\u5e38\u91cd\u91cf\u7ea7\uff0c\u66f4\u9002\u5408\u4e8e\u4f01\u4e1a\u7ea7\u7684\u5f00\u53d1\u3002\u540c\u65f6\u5b9e\u73b0\u4e86Broker\u6784\u67b6\uff0c\u8fd9\u610f\u5473\u7740\u6d88\u606f\u5728\u53d1\u9001\u7ed9\u5ba2\u6237\u7aef\u65f6\u5148\u5728\u4e2d\u5fc3\u961f\u5217\u6392\u961f\u3002\u5bf9\u8def\u7531\uff0c\u8d1f\u8f7d\u5747\u8861\u6216\u8005\u6570\u636e\u6301\u4e45\u5316\u90fd\u6709\u5f88\u597d\u7684\u652f\u6301\u3002 3.2\u3000Redis Redis\u662f\u4e00\u4e2a\u57fa\u4e8eKey-Value\u5bf9\u7684NoSQL\u6570\u636e\u5e93\uff0c\u5f00\u53d1\u7ef4\u62a4\u5f88\u6d3b\u8dc3\u3002\u867d\u7136\u5b83\u662f\u4e00\u4e2aKey-Value\u6570\u636e\u5e93\u5b58\u50a8\u7cfb\u7edf\uff0c\u4f46\u5b83\u672c\u8eab\u652f\u6301MQ\u529f\u80fd\uff0c\u6240\u4ee5\u5b8c\u5168\u53ef\u4ee5\u5f53\u505a\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u961f\u5217\u670d\u52a1\u6765\u4f7f\u7528\u3002\u5bf9\u4e8eRabbitMQ\u548cRedis\u7684\u5165\u961f\u548c\u51fa\u961f\u64cd\u4f5c\uff0c\u5404\u6267\u884c100\u4e07\u6b21\uff0c\u6bcf10\u4e07\u6b21\u8bb0\u5f55\u4e00\u6b21\u6267\u884c\u65f6\u95f4\u3002\u6d4b\u8bd5\u6570\u636e\u5206\u4e3a128Bytes\u3001512Bytes\u30011K\u548c10K\u56db\u4e2a\u4e0d\u540c\u5927\u5c0f\u7684\u6570\u636e\u3002\u5b9e\u9a8c\u8868\u660e\uff1a\u5165\u961f\u65f6\uff0c\u5f53\u6570\u636e\u6bd4\u8f83\u5c0f\u65f6Redis\u7684\u6027\u80fd\u8981\u9ad8\u4e8eRabbitMQ\uff0c\u800c\u5982\u679c\u6570\u636e\u5927\u5c0f\u8d85\u8fc7\u4e8610K\uff0cRedis\u5219\u6162\u7684\u65e0\u6cd5\u5fcd\u53d7\uff1b\u51fa\u961f\u65f6\uff0c\u65e0\u8bba\u6570\u636e\u5927\u5c0f\uff0cRedis\u90fd\u8868\u73b0\u51fa\u975e\u5e38\u597d\u7684\u6027\u80fd\uff0c\u800cRabbitMQ\u7684\u51fa\u961f\u6027\u80fd\u5219\u8fdc\u4f4e\u4e8eRedis\u3002 3.3\u3000ZeroMQ ZeroMQ\u53f7\u79f0\u6700\u5feb\u7684\u6d88\u606f\u961f\u5217\u7cfb\u7edf\uff0c\u5c24\u5176\u9488\u5bf9\u5927\u541e\u5410\u91cf\u7684\u9700\u6c42\u573a\u666f\u3002ZeroMQ\u80fd\u591f\u5b9e\u73b0RabbitMQ\u4e0d\u64c5\u957f\u7684\u9ad8\u7ea7/\u590d\u6742\u7684\u961f\u5217\uff0c\u4f46\u662f\u5f00\u53d1\u4eba\u5458\u9700\u8981\u81ea\u5df1\u7ec4\u5408\u591a\u79cd\u6280\u672f\u6846\u67b6\uff0c\u6280\u672f\u4e0a\u7684\u590d\u6742\u5ea6\u662f\u5bf9\u8fd9MQ\u80fd\u591f\u5e94\u7528\u6210\u529f\u7684\u6311\u6218\u3002ZeroMQ\u5177\u6709\u4e00\u4e2a\u72ec\u7279\u7684\u975e\u4e2d\u95f4\u4ef6\u7684\u6a21\u5f0f\uff0c\u4f60\u4e0d\u9700\u8981\u5b89\u88c5\u548c\u8fd0\u884c\u4e00\u4e2a\u6d88\u606f\u670d\u52a1\u5668\u6216\u4e2d\u95f4\u4ef6\uff0c\u56e0\u4e3a\u4f60\u7684\u5e94\u7528\u7a0b\u5e8f\u5c06\u626e\u6f14\u8fd9\u4e2a\u670d\u52a1\u5668\u89d2\u8272\u3002\u4f60\u53ea\u9700\u8981\u7b80\u5355\u7684\u5f15\u7528ZeroMQ\u7a0b\u5e8f\u5e93\uff0c\u53ef\u4ee5\u4f7f\u7528NuGet\u5b89\u88c5\uff0c\u7136\u540e\u4f60\u5c31\u53ef\u4ee5\u6109\u5feb\u7684\u5728\u5e94\u7528\u7a0b\u5e8f\u4e4b\u95f4\u53d1\u9001\u6d88\u606f\u4e86\u3002\u4f46\u662fZeroMQ\u4ec5\u63d0\u4f9b\u975e\u6301\u4e45\u6027\u7684\u961f\u5217\uff0c\u4e5f\u5c31\u662f\u8bf4\u5982\u679c\u5b95\u673a\uff0c\u6570\u636e\u5c06\u4f1a\u4e22\u5931\u3002\u5176\u4e2d\uff0cTwitter\u7684Storm 0.9.0\u4ee5\u524d\u7684\u7248\u672c\u4e2d\u9ed8\u8ba4\u4f7f\u7528ZeroMQ\u4f5c\u4e3a\u6570\u636e\u6d41\u7684\u4f20\u8f93\uff08Storm\u4ece0.9\u7248\u672c\u5f00\u59cb\u540c\u65f6\u652f\u6301ZeroMQ\u548cNetty\u4f5c\u4e3a\u4f20\u8f93\u6a21\u5757\uff09\u3002 3.4\u3000ActiveMQ ActiveMQ\u662fApache\u4e0b\u7684\u4e00\u4e2a\u5b50\u9879\u76ee\u3002 \u7c7b\u4f3c\u4e8eZeroMQ\uff0c\u5b83\u80fd\u591f\u4ee5\u4ee3\u7406\u4eba\u548c\u70b9\u5bf9\u70b9\u7684\u6280\u672f\u5b9e\u73b0\u961f\u5217\u3002\u540c\u65f6\u7c7b\u4f3c\u4e8eRabbitMQ\uff0c\u5b83\u5c11\u91cf\u4ee3\u7801\u5c31\u53ef\u4ee5\u9ad8\u6548\u5730\u5b9e\u73b0\u9ad8\u7ea7\u5e94\u7528\u573a\u666f\u3002 3.5\u3000Kafka/Jafka Kafka\u662fApache\u4e0b\u7684\u4e00\u4e2a\u5b50\u9879\u76ee\uff0c\u662f\u4e00\u4e2a\u9ad8\u6027\u80fd\u8de8\u8bed\u8a00\u5206\u5e03\u5f0f\u53d1\u5e03/\u8ba2\u9605\u6d88\u606f\u961f\u5217\u7cfb\u7edf\uff0c\u800cJafka\u662f\u5728Kafka\u4e4b\u4e0a\u5b75\u5316\u800c\u6765\u7684\uff0c\u5373Kafka\u7684\u4e00\u4e2a\u5347\u7ea7\u7248\u3002\u5177\u6709\u4ee5\u4e0b\u7279\u6027\uff1a\u5feb\u901f\u6301\u4e45\u5316\uff0c\u53ef\u4ee5\u5728O(1)\u7684\u7cfb\u7edf\u5f00\u9500\u4e0b\u8fdb\u884c\u6d88\u606f\u6301\u4e45\u5316\uff1b\u9ad8\u541e\u5410\uff0c\u5728\u4e00\u53f0\u666e\u901a\u7684\u670d\u52a1\u5668\u4e0a\u65e2\u53ef\u4ee5\u8fbe\u523010W/s\u7684\u541e\u5410\u901f\u7387\uff1b\u5b8c\u5168\u7684\u5206\u5e03\u5f0f\u7cfb\u7edf\uff0cBroker\u3001Producer\u3001Consumer\u90fd\u539f\u751f\u81ea\u52a8\u652f\u6301\u5206\u5e03\u5f0f\uff0c\u81ea\u52a8\u5b9e\u73b0\u8d1f\u8f7d\u5747\u8861\uff1b\u652f\u6301Hadoop\u6570\u636e\u5e76\u884c\u52a0\u8f7d\uff0c\u5bf9\u4e8e\u50cfHadoop\u7684\u4e00\u6837\u7684\u65e5\u5fd7\u6570\u636e\u548c\u79bb\u7ebf\u5206\u6790\u7cfb\u7edf\uff0c\u4f46\u53c8\u8981\u6c42\u5b9e\u65f6\u5904\u7406\u7684\u9650\u5236\uff0c\u8fd9\u662f\u4e00\u4e2a\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002Kafka\u901a\u8fc7Hadoop\u7684\u5e76\u884c\u52a0\u8f7d\u673a\u5236\u7edf\u4e00\u4e86\u5728\u7ebf\u548c\u79bb\u7ebf\u7684\u6d88\u606f\u5904\u7406\u3002Apache Kafka\u76f8\u5bf9\u4e8eActiveMQ\u662f\u4e00\u4e2a\u975e\u5e38\u8f7b\u91cf\u7ea7\u7684\u6d88\u606f\u7cfb\u7edf\uff0c\u9664\u4e86\u6027\u80fd\u975e\u5e38\u597d\u4e4b\u5916\uff0c\u8fd8\u662f\u4e00\u4e2a\u5de5\u4f5c\u826f\u597d\u7684\u5206\u5e03\u5f0f\u7cfb\u7edf\u3002","title":"cnblogs Kafka\u5b66\u4e60\u4e4b\u8def \uff08\u4e00\uff09Kafka\u7684\u7b80\u4ecb # \u4e09\u3001\u5e38\u7528Message Queue\u5bf9\u6bd4"},{"location":"Message-processing-system/MQ-library/#4u4v","text":"\u6d88\u606f\u961f\u5217\uff08Message Quequing\uff09\u662f\u5728\u6d88\u606f\u4f20\u8f93\u8fc7\u7a0b\u4e2d\u4fdd\u5b58\u6d88\u606f\u7684\u5bb9\u5668\uff0c\u6d88\u606f\u4e2d\u95f4\u4ef6\u5373\u4e3a\u6d88\u606f\u961f\u5217\u7684\u627f\u8f7d\u5f62\u5f0f \u3002\u6d88\u606f\u662f\u4e24\u53f0\u8ba1\u7b97\u673a\u95f4\u4f20\u9001\u7684\u6570\u636e\u5355\u4f4d\uff0c\u6d88\u606f\u961f\u5217\u5728\u5c06\u6d88\u606f\u4ece\u5b83\u7684\u6e90\u4e2d\u7ee7\u5230\u5b83\u7684\u76ee\u6807\u65f6\u5145\u5f53\u4e2d\u95f4\u4eba\uff0c\u4e3b\u8981\u76ee\u7684\u662f\u63d0\u4f9b\u8def\u7531\u5e76\u4fdd\u8bc1\u6d88\u606f\u7684\u4f20\u9012\uff1b\u5982\u679c\u53d1\u9001\u6d88\u606f\u65f6\u63a5\u6536\u8005\u4e0d\u53ef\u7528\uff0c\u6d88\u606f\u961f\u5217\u4f1a\u4fdd\u7559\u6d88\u606f\uff0c\u76f4\u5230\u53ef\u4ee5\u6210\u529f\u5730\u4f20\u9012\u5b83\uff0c\u4e3b\u8981\u89e3\u51b3\u4f20\u7edf\u7ed3\u6784\u8026\u5408\u6027\u95ee\u9898\u3001\u7cfb\u7edf\u5f02\u6b65\u6027\u95ee\u9898\u4ee5\u53ca\u7f13\u89e3\u5927\u6570\u636e\u91cf\u5e76\u53d1\u7684\u95ee\u9898\u7b49\u3002 \u6d88\u606f\u961f\u5217\u6709\u8f83\u591a\u7684\u578b\u53f7\uff0c\u8f83\u4e3a\u5e38\u7528\u7684\u4e3aActiveMQ\u3001Rabbit MQ\u3001RocketMQ\u548cKafk a\u3002\u7531\u4e8e\u6d88\u606f\u961f\u5217\u4f7f\u7528\u6d88\u606f\u5c06\u5e94\u7528\u7a0b\u5e8f\u8fde\u63a5\u8d77\u6765\uff0c\u8fd9\u4e9b\u6d88\u606f\u901a\u8fc7\u50cfRabbit MQ\u7684\u6d88\u606f\u4ee3\u7406\u670d\u52a1\u5668\u5728\u5e94\u7528\u7a0b\u5e8f\u4e4b\u95f4\u8def\u7531\u3002","title":"4u4v \u4e2d\u95f4\u4ef6\u662f\u4ec0\u4e48\uff0c\u5e38\u7528\u7684\u4e2d\u95f4\u4ef6\u6709\u54ea\u4e9b\uff1f"},{"location":"Message-processing-system/MQ-library/Apache-Kafka/","text":"Flink wikipedia Apache Kafka Apache Kafka is an open-source stream-processing software platform developed by the Apache Software Foundation, written in Scala and Java. The project aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds. Kafka can connect to external systems via Kafka Connect and provides Kafka Streams, a Java stream processing library. Kafka uses a binary TCP-based protocol that is optimized for efficiency and relies on a \"message set\" abstraction that naturally groups messages together to reduce the overhead of the network roundtrip. This \"leads to larger network packets, larger sequential disk operations, contiguous memory blocks which allows Kafka to turn a bursty stream of random message writes into linear writes.\" kafka-A distributed streaming platform INTRODUCTION Everything you need to know about Kafka in 10 minutes","title":"Introduction"},{"location":"Message-processing-system/MQ-library/Apache-Kafka/#flink","text":"","title":"Flink"},{"location":"Message-processing-system/MQ-library/Apache-Kafka/#wikipedia#apache#kafka","text":"Apache Kafka is an open-source stream-processing software platform developed by the Apache Software Foundation, written in Scala and Java. The project aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds. Kafka can connect to external systems via Kafka Connect and provides Kafka Streams, a Java stream processing library. Kafka uses a binary TCP-based protocol that is optimized for efficiency and relies on a \"message set\" abstraction that naturally groups messages together to reduce the overhead of the network roundtrip. This \"leads to larger network packets, larger sequential disk operations, contiguous memory blocks which allows Kafka to turn a bursty stream of random message writes into linear writes.\"","title":"wikipedia Apache Kafka"},{"location":"Message-processing-system/MQ-library/Apache-Kafka/#kafka-a#distributed#streaming#platform","text":"","title":"kafka-A distributed streaming platform"},{"location":"Message-processing-system/MQ-library/Apache-Kafka/#introduction","text":"Everything you need to know about Kafka in 10 minutes","title":"INTRODUCTION"},{"location":"Message-processing-system/MQ-library/library-Rabbitmq/AMQP-0-9-1-Model-Explained/","text":"AMQP 0-9-1 Model Explained About This Guide This guide provides an overview of the AMQP 0-9-1 protocol, one of the protocols supported by RabbitMQ. High-level Overview of AMQP 0-9-1 and the AMQP Model What is AMQP 0-9-1? AMQP 0-9-1 (Advanced Message Queuing Protocol) is a messaging protocol that enables conforming client applications to communicate with conforming messaging middleware brokers . Brokers and Their Role Messaging brokers receive messages from publishers (applications that publish them, also known as producers ) and route them to consumers (applications that process them). Since it is a network protocol , the publishers , consumers and the broker can all reside on different machines. SUMMARY : client application\u53ef\u4ee5\u5206\u4e3apublisher\u548cconsumer\uff1b AMQP 0-9-1 Model in Brief The AMQP 0-9-1 Model has the following view of the world: messages are published to exchanges , which are often compared to post offices or mailboxes. Exchanges then distribute message copies(\u526f\u672c) to queues using rules called bindings . Then the broker either deliver messages to consumers subscribed to queues, or consumers fetch/pull messages from queues on demand. When publishing a message, publishers may specify various message attributes (message meta-data). Some of this meta-data may be used by the broker , however, the rest of it is completely opaque to the broker and is only used by applications that receive the message. Networks are unreliable and applications may fail to process messages therefore the AMQP 0-9-1 model has a notion of message acknowledgements : when a message is delivered to a consumer , the consumer notifies the broker , either automatically or as soon as the application developer chooses to do so. When message acknowledgements are in use, a broker will only completely remove a message from a queue when it receives a notification for that message (or group of messages). In certain situations, for example, when a message cannot be routed, messages may be returned to publishers, dropped, or, if the broker implements an extension, placed into a so-called \"dead letter queue\". Publishers choose how to handle situations like this by publishing messages using certain parameters. Queues , exchanges and bindings are collectively referred to as AMQP entities . SUMMARY : \u6309\u7167\u76ee\u524d\u7684\u7406\u89e3\uff0cqueues\uff0cexchanges\uff0cbindings\u4e09\u8005\u4e00\u8d77\u6784\u6210\u4e86broker\uff1b\u6ce8\u610f\uff0cbroker\u4e0d\u662fAMQP\u7684entity\u3002 SUMMARY : \u8fd9\u4e2a\u4e09\u4e2aentity\u4e4b\u95f4\u7684\u5173\u7cfb\u662f\u4ec0\u4e48\uff1f \u5728 Default Exchange \u4e2d\u6709\u8fd9\u6837\u7684\u4e00\u6bb5\u8bdd\uff1a every queue that is created is automatically bound to it\uff08 direct exchange \uff09 with a routing key which is the same as the queue name . \u901a\u8fc7\u8fd9\u53e5\u8bdd\u53ef\u4ee5\u63a8\u6d4b\u77e5\u9053queue bind to exchange\u3002 AMQP is a Programmable Protocol AMQP 0-9-1 is a programmable protocol in the sense that AMQP 0-9-1 entities and routing schemes are primarily defined by applications themselves, not a broker administrator . Accordingly, provision is made for protocol operations that declare queues and exchanges, define bindings between them, subscribe to queues and so on. This gives application developers a lot of freedom but also requires them to be aware of potential definition conflicts . In practice, definition conflicts are rare and often indicate a misconfiguration. Applications declare the AMQP 0-9-1 entities that they need, define necessary routing schemes and may choose to delete AMQP 0-9-1 entities when they are no longer used. Exchanges and Exchange Types Exchanges are AMQP 0-9-1 entities where messages are sent. Exchanges take a message and route it into zero or more queues. The routing algorithm used depends on the exchange type and rules called bindings . AMQP 0-9-1 brokers provide four exchange types : Name Default pre-declared names Direct exchange (Empty string) and amq.direct Fanout exchange amq.fanout Topic exchange amq.topic Headers exchange amq.match (and amq.headers in RabbitMQ) Besides the exchange type , exchanges are declared with a number of attributes, the most important of which are: Name Durability (exchanges survive broker restart) Auto-delete (exchange is deleted when last queue is unbound from it) Arguments (optional, used by plugins and broker-specific features) Exchanges can be durable or transient . Durable exchanges survive broker restart whereas transient exchanges do not (they have to be redeclared when broker comes back online). Not all scenarios and use cases require exchanges to be durable. Default Exchange The default exchange is a direct exchange with no name (empty string) pre-declared by the broker. It has one special property that makes it very useful for simple applications: every queue that is created is automatically bound to it with a routing key which is the same as the queue name . For example, when you declare a queue with the name of \"search-indexing-online\", the AMQP 0-9-1 broker will bind it to the default exchange using \"search-indexing-online\" as the routing key (in this context sometimes referred to as the binding key ). Therefore, a message published to the default exchange with the routing key \"search-indexing-online\" will be routed to the queue \"search-indexing-online\". In other words, the default exchange makes it seem like it is possible to deliver messages directly to queues , even though that is not technically what is happening. Direct Exchange A direct exchange delivers messages to queues based on the message routing key . A direct exchange is ideal for the unicast\uff08\u5355\u64ad\uff09 routing of messages (although they can be used for multicast\uff08\u591a\u64ad\uff09 routing as well). Here is how it works: A queue binds to the exchange with a routing key K SUMMARY : queue name\u548crouting key\u662f\u53ef\u4ee5\u4e0d\u540c\u7684\uff1b When a new message with routing key R arrives at the direct exchange, the exchange routes it to the queue if K = R Direct exchanges are often used to distribute tasks between multiple workers (instances of the same application) in a round robin manner. When doing so, it is important to understand that, in AMQP 0-9-1, messages are load balanced between consumers and not between queues. Direct exchanges \u901a\u5e38\u7528\u4e8e\u4ee5\u5faa\u73af\u65b9\u5f0f\u5728\u591a\u4e2a\u5de5\u4f5c\u8005\uff08\u540c\u4e00\u5e94\u7528\u7a0b\u5e8f\u7684\u5b9e\u4f8b\uff09\u4e4b\u95f4\u5206\u914d\u4efb\u52a1\u3002 \u5728\u8fd9\u6837\u505a\u65f6\uff0c\u91cd\u8981\u7684\u662f\u8981\u7406\u89e3\uff0c\u5728AMQP 0-9-1\u4e2d\uff0c\u6d88\u606f\u5728\u6d88\u8d39\u8005\u4e4b\u95f4\u800c\u4e0d\u662f\u5728\u961f\u5217\u4e4b\u95f4\u8fdb\u884c\u8d1f\u8f7d\u5e73\u8861\u3002 A direct exchange can be represented graphically as follows: Fanout Exchange A fanout exchange routes messages to all of the queues that are bound to it and the routing key is ignored. If N queues are bound to a fanout exchange , when a new message is published to that exchange a copy of the message is delivered to all N queues. Fanout exchanges are ideal for the broadcast \uff08\u5e7f\u64ad\uff09 routing of messages. Because a fanout exchange delivers a copy of a message to every queue bound to it, its use cases are quite similar: Massively multi-player online (MMO) games can use it for leaderboard updates or other global events Sport news sites can use fanout exchanges for distributing score updates to mobile clients in near real-time Distributed systems can broadcast various state and configuration updates Group chats can distribute messages between participants using a fanout exchange (although AMQP does not have a built-in concept of presence, so XMPP may be a better choice) A fanout exchange can be represented graphically as follows: Topic Exchange Topic exchanges route messages to one or many queues based on matching between a message routing key and the pattern that was used to bind a queue to an exchange. The topic exchange type is often used to implement various publish/subscribe pattern variations . Topic exchanges are commonly used for the multicast routing of messages . Topic exchanges have a very broad set of use cases. Whenever a problem involves multiple consumers/applications that selectively choose which type of messages they want to receive, the use of topic exchanges should be considered. Example uses: Distributing data relevant to specific geographic location, for example, points of sale Background task processing done by multiple workers, each capable of handling specific set of tasks Stocks price updates (and updates on other kinds of financial data) News updates that involve categorization or tagging (for example, only for a particular sport or team) Orchestration of services of different kinds in the cloud Distributed architecture/OS-specific software builds or packaging where each builder can handle only one architecture or OS Headers Exchange A headers exchange is designed for routing on multiple attributes that are more easily expressed as message headers than a routing key . Headers exchanges ignore the routing key attribute . Instead, the attributes used for routing are taken from the headers attribute . A message is considered matching if the value of the header equals the value specified upon binding . It is possible to bind a queue to a headers exchange using more than one header for matching. In this case, the broker needs one more piece of information from the application developer, namely, should it consider messages with any of the headers matching, or all of them? This is what the \"x-match\" binding argument is for. When the \"x-match\" argument is set to \"any\", just one matching header value is sufficient. Alternatively, setting \"x-match\" to \"all\" mandates that all the values must match. Headers exchanges can be looked upon as \"direct exchanges on steroids\". Because they route based on header values, they can be used as direct exchanges where the routing key does not have to be a string; it could be an integer or a hash (dictionary) for example. Note that headers beginning with the string x- will not be used to evaluate matches. Queues Queues in the AMQP 0-9-1 model are very similar to queues in other message- and task-queueing systems: they store messages that are consumed by applications. Queues share some properties with exchanges , but also have some additional properties: Name Durable (the queue will survive a broker restart) Exclusive (used by only one connection and the queue will be deleted when that connection closes) Auto-delete (queue that has had at least one consumer is deleted when last consumer unsubscribes) Arguments (optional; used by plugins and broker-specific features such as message TTL , queue length limit, etc) Before a queue can be used it has to be declared. Declaring a queue will cause it to be created if it does not already exist. The declaration will have no effect if the queue does already exist and its attributes are the same as those in the declaration. When the existing queue attributes are not the same as those in the declaration a channel-level exception with code 406 (PRECONDITION_FAILED) will be raised. Queue Names Applications may pick queue names or ask the broker to generate a name for them. Queue names may be up to 255 bytes of UTF-8 characters. An AMQP 0-9-1 broker can generate a unique queue name on behalf of an app. To use this feature, pass an empty string as the queue name argument. The generated name will be returned to the client with queue declaration response. Queue names starting with \"amq.\" are reserved for internal use by the broker. Attempts to declare a queue with a name that violates this rule will result in a channel-level exception with reply code 403 (ACCESS_REFUSED). Queue Durability Durable queues are persisted to disk and thus survive broker restarts. Queues that are not durable are called transient . Not all scenarios and use cases mandate queues to be durable. Durability of a queue does not make messages that are routed to that queue durable. If broker is taken down and then brought back up, durable queue will be re-declared during broker startup, however, only persistent messages will be recovered. Bindings Bindings are rules that exchanges use (among other things) to route messages to queues . To instruct an exchange E to route messages to a queue Q , Q has to be bound to E. Bindings may have an optional routing key attribute used by some exchange types . The purpose of the routing key is to select certain messages published to an exchange to be routed to the bound queue . In other words, the routing key acts like a filter. To draw an analogy: Queue is like your destination in New York city Exchange is like JFK airport Bindings are routes from JFK to your destination. There can be zero or many ways to reach it Having this layer of indirection enables routing scenarios that are impossible or very hard to implement using publishing directly to queues and also eliminates certain amount of duplicated work application developers have to do. If AMQP message cannot be routed to any queue (for example, because there are no bindings for the exchange it was published to) it is either dropped or returned to the publisher, depending on message attributes the publisher has set. Consumers Storing messages in queues is useless unless applications can consume them. In the AMQP 0-9-1 Model, there are two ways for applications to do this: Have messages delivered to them (\"push API\") Fetch messages as needed (\"pull API\") With the \"push API\", applications have to indicate interest in consuming messages from a particular queue. When they do so, we say that they register a consumer or, simply put, subscribe to a queue . It is possible to have more than one consumer per queue or to register an exclusive consumer (excludes all other consumers from the queue while it is consuming). Each consumer (subscription) has an identifier called a consumer tag . It can be used to unsubscribe from messages. Consumer tags are just strings. SUMMARY : consumer\u4ecequeue\u4e2d\u53d6\u51fa\u6d88\u606f\uff0c\u6211\u60f3\u5230\u4e86\u4e00\u4e2a\u95ee\u9898\uff1a\u4e00\u4e2aconsumer\u53ef\u4ee5\u4ece\u591a\u4e2aqueue\u4e2d\u53d6\u51fa\u6d88\u606f\u5417\uff1f\u4ece\u4e0a\u9762\u7684\u63cf\u8ff0\u6765\u770b\uff0c\u8c8c\u4f3c\u4e00\u4e2aconsumer\u53ea\u80fd\u591f\u5bf9\u5e94\u4e00\u4e2aqueue\u3002\u4f46\u662f\u53ef\u7528\u53cd\u8fc7\u6765\uff1a\u591a\u4e2aconsumer\u5bf9\u5e94\u540c\u4e00\u4e2aqueue\uff1b Message Acknowledgements Consumer applications \u2013 applications that receive and process messages \u2013 may occasionally fail to process individual messages or will sometimes just crash. There is also the possibility of network issues causing problems. This raises a question: when should the AMQP broker remove messages from queues? The AMQP 0-9-1 specification proposes two choices: After broker sends a message to an application (using either basic.deliver or basic.get-ok AMQP methods). After the application sends back an acknowledgement (using basic.ack AMQP method). The former choice is called the automatic acknowledgement model , while the latter is called the explicit acknowledgement model . With the explicit model the application chooses when it is time to send an acknowledgement. It can be right after receiving a message, or after persisting it to a data store before processing, or after fully processing the message (for example, successfully fetching a Web page, processing and storing it into some persistent data store). If a consumer dies without sending an acknowledgement the AMQP broker will redeliver it to another consumer or, if none are available at the time, the broker will wait until at least one consumer is registered for the same queue before attempting redelivery. Rejecting Messages When a consumer application receives a message, processing of that message may or may not succeed. An application can indicate to the broker that message processing has failed (or cannot be accomplished at the time) by rejecting a message. When rejecting a message, an application can ask the broker to discard or requeue it. When there is only one consumer on a queue, make sure you do not create infinite message delivery loops by rejecting and requeueing a message from the same consumer over and over again. Negative Acknowledgements Messages are rejected with the basic.reject AMQP method. There is one limitation that basic.reject has: there is no way to reject multiple messages as you can do with acknowledgements . However, if you are using RabbitMQ, then there is a solution. RabbitMQ provides an AMQP 0-9-1 extension known as negative acknowledgements or nacks . For more information, please refer to the the help page . Prefetching Messages For cases when multiple consumers share a queue , it is useful to be able to specify how many messages each consumer can be sent at once before sending the next acknowledgement. This can be used as a simple load balancing technique or to improve throughput if messages tend to be published in batches. For example, if a producing application sends messages every minute because of the nature of the work it is doing. Note that RabbitMQ only supports channel-level prefetch-count, not connection or size based prefetching. Message Attributes and Payload Messages in the AMQP model have attributes . Some attributes are so common that the AMQP 0-9-1 specification defines them and application developers do not have to think about the exact attribute name. Some examples are Content type Content encoding Routing key Delivery mode (persistent or not) Message priority Message publishing timestamp Expiration period Publisher application id Some attributes are used by AMQP brokers, but most are open to interpretation by applications that receive them. Some attributes are optional and known as headers . They are similar to X-Headers in HTTP. Message attributes are set when a message is published. AMQP messages also have a payload (the data that they carry), which AMQP brokers treat as an opaque byte array. The broker will not inspect or modify the payload. It is possible for messages to contain only attributes and no payload. It is common to use serialisation formats like JSON, Thrift, Protocol Buffers and MessagePack to serialize structured data in order to publish it as the message payload. AMQP peers typically use the \"content-type\" and \"content-encoding\" fields to communicate this information, but this is by convention only. Messages may be published as persistent, which makes the AMQP broker persist them to disk. If the server is restarted the system ensures that received persistent messages are not lost. Simply publishing a message to a durable exchange or the fact that the queue(s) it is routed to are durable doesn't make a message persistent: it all depends on persistence mode of the message itself. Publishing messages as persistent affects performance (just like with data stores, durability comes at a certain cost in performance). Message Acknowledgements Since networks are unreliable and applications fail, it is often necessary to have some kind of processing acknowledgement. Sometimes it is only necessary to acknowledge the fact that a message has been received. Sometimes acknowledgements mean that a message was validated and processed by a consumer, for example, verified as having mandatory data and persisted to a data store or indexed. This situation is very common, so AMQP 0-9-1 has a built-in feature called message acknowledgements (sometimes referred to as acks ) that consumers use to confirm message delivery and/or processing. If an application crashes (the AMQP broker notices this when the connection is closed), if an acknowledgement for a message was expected but not received by the AMQP broker, the message is re-queued (and possibly immediately delivered to another consumer, if any exists). Having acknowledgements built into the protocol helps developers to build more robust software. AMQP 0-9-1 Methods AMQP 0-9-1 is structured as a number of methods . Methods are operations (like HTTP methods ) and have nothing in common with methods in object-oriented programming languages. AMQP methods are grouped into classes . Classes are just logical groupings of AMQP methods. The AMQP 0-9-1 reference has full details of all the AMQP methods. Let us take a look at the exchange class, a group of methods related to operations on exchanges. It includes the following operations: exchange.declare exchange.declare-ok exchange.delete exchange.delete-ok (note that the RabbitMQ site reference also includes RabbitMQ-specific extensions to the exchange class that we will not discuss in this guide). The operations above form logical pairs: exchange.declare and exchange.declare-ok , exchange.delete and exchange.delete-ok . These operations are \"requests\" (sent by clients) and \"responses\" (sent by brokers in response to the aforementioned\uff08\u4e0a\u8ff0\u7684\uff09 \"requests\"). As an example, the client asks the broker to declare a new exchange using the exchange.declare method: As shown on the diagram above, exchange.declare carries several parameters . They enable the client to specify exchange name , type , durability flag and so on. If the operation succeeds, the broker responds with the exchange.declare-ok method: exchange.declare-ok does not carry any parameters except for the channel number (channels will be described later in this guide). The sequence of events is very similar for another method pair on the AMQP queue class: queue.declare and queue.declare-ok : Not all AMQP methods have counterparts. Some ( basic.publish being the most widely used one) do not have corresponding \"response\" methods and some others ( basic.get , for example) have more than one possible \"response\". Connections AMQP 0-9-1 connections are typically long-lived. AMQP 0-9-1 is an application level protocol that uses TCP for reliable delivery. Connections use authentication and can be protected using TLS . When an application no longer needs to be connected to the server, it should gracefully close its AMQP 0-9-1 connection instead of abruptly closing the underlying TCP connection. \u200b Channels Some applications need multiple connections to the broker. However, it is undesirable to keep many TCP connections open at the same time because doing so consumes system resources and makes it more difficult to configure firewalls. AMQP 0-9-1 connections are multiplexed with channels that can be thought of as \"lightweight connections that share a single TCP connection\". Every protocol operation performed by a client happens on a channel. Communication on a particular channel is completely separate from communication on another channel, therefore every protocol method also carries a channel ID (a.k.a. channel number), an integer that both the broker and clients use to figure out which channel the method is for. A channel only exists in the context of a connection and never on its own. When a connection is closed, so are all channels on it. For applications that use multiple threads/processes for processing, it is very common to open a new channel per thread/process and not share channels between them. RabbitMQ and relationship between channel and connection SUMMARY : \u8981\u900f\u5f7b\u5730\u7406\u89e3channel\u9700\u8981\u5148\u7406\u89e3 Multiplexing \u591a\u8def\u590d\u7528\u3002 Virtual Hosts To make it possible for a single broker to host multiple isolated \"environments\" (groups of users, exchanges, queues and so on), AMQP includes the concept of virtual hosts (vhosts). They are similar to virtual hosts used by many popular Web servers and provide completely isolated environments in which AMQP entities live. AMQP clients specify what vhosts they want to use during AMQP connection negotiation. AMQP is Extensible AMQP 0-9-1 has several extension points: Custom exchange types let developers implement routing schemes that exchange types provided out-of-the-box do not cover well, for example, geodata-based routing. Declaration of exchanges and queues can include additional attributes that the broker can use. For example, per-queue message TTL in RabbitMQ is implemented this way. Broker-specific extensions to the protocol. See, for example, extensions that RabbitMQ implements . New AMQP 0-9-1 method classes can be introduced. Brokers can be extended with additional plugins , for example, the RabbitMQ management frontend and HTTP API are implemented as a plugin. These features make the AMQP 0-9-1 Model even more flexible and applicable to a very broad range of problems.","title":"AMQP-0-9-1-Model-Explained"},{"location":"Message-processing-system/MQ-library/library-Rabbitmq/AMQP-0-9-1-Model-Explained/#amqp#0-9-1#model#explained","text":"","title":"AMQP 0-9-1 Model Explained"},{"location":"Message-processing-system/MQ-library/library-Rabbitmq/AMQP-0-9-1-Model-Explained/#about#this#guide","text":"This guide provides an overview of the AMQP 0-9-1 protocol, one of the protocols supported by RabbitMQ.","title":"About This Guide"},{"location":"Message-processing-system/MQ-library/library-Rabbitmq/AMQP-0-9-1-Model-Explained/#high-level#overview#of#amqp#0-9-1#and#the#amqp#model","text":"","title":"High-level Overview of AMQP 0-9-1 and the AMQP Model"},{"location":"Message-processing-system/MQ-library/library-Rabbitmq/AMQP-0-9-1-Model-Explained/#what#is#amqp#0-9-1","text":"AMQP 0-9-1 (Advanced Message Queuing Protocol) is a messaging protocol that enables conforming client applications to communicate with conforming messaging middleware brokers .","title":"What is AMQP 0-9-1?"},{"location":"Message-processing-system/MQ-library/library-Rabbitmq/AMQP-0-9-1-Model-Explained/#brokers#and#their#role","text":"Messaging brokers receive messages from publishers (applications that publish them, also known as producers ) and route them to consumers (applications that process them). Since it is a network protocol , the publishers , consumers and the broker can all reside on different machines. SUMMARY : client application\u53ef\u4ee5\u5206\u4e3apublisher\u548cconsumer\uff1b","title":"Brokers and Their Role"},{"location":"Message-processing-system/MQ-library/library-Rabbitmq/AMQP-0-9-1-Model-Explained/#amqp#0-9-1#model#in#brief","text":"The AMQP 0-9-1 Model has the following view of the world: messages are published to exchanges , which are often compared to post offices or mailboxes. Exchanges then distribute message copies(\u526f\u672c) to queues using rules called bindings . Then the broker either deliver messages to consumers subscribed to queues, or consumers fetch/pull messages from queues on demand. When publishing a message, publishers may specify various message attributes (message meta-data). Some of this meta-data may be used by the broker , however, the rest of it is completely opaque to the broker and is only used by applications that receive the message. Networks are unreliable and applications may fail to process messages therefore the AMQP 0-9-1 model has a notion of message acknowledgements : when a message is delivered to a consumer , the consumer notifies the broker , either automatically or as soon as the application developer chooses to do so. When message acknowledgements are in use, a broker will only completely remove a message from a queue when it receives a notification for that message (or group of messages). In certain situations, for example, when a message cannot be routed, messages may be returned to publishers, dropped, or, if the broker implements an extension, placed into a so-called \"dead letter queue\". Publishers choose how to handle situations like this by publishing messages using certain parameters. Queues , exchanges and bindings are collectively referred to as AMQP entities . SUMMARY : \u6309\u7167\u76ee\u524d\u7684\u7406\u89e3\uff0cqueues\uff0cexchanges\uff0cbindings\u4e09\u8005\u4e00\u8d77\u6784\u6210\u4e86broker\uff1b\u6ce8\u610f\uff0cbroker\u4e0d\u662fAMQP\u7684entity\u3002 SUMMARY : \u8fd9\u4e2a\u4e09\u4e2aentity\u4e4b\u95f4\u7684\u5173\u7cfb\u662f\u4ec0\u4e48\uff1f \u5728 Default Exchange \u4e2d\u6709\u8fd9\u6837\u7684\u4e00\u6bb5\u8bdd\uff1a every queue that is created is automatically bound to it\uff08 direct exchange \uff09 with a routing key which is the same as the queue name . \u901a\u8fc7\u8fd9\u53e5\u8bdd\u53ef\u4ee5\u63a8\u6d4b\u77e5\u9053queue bind to exchange\u3002","title":"AMQP 0-9-1 Model in Brief"},{"location":"Message-processing-system/MQ-library/library-Rabbitmq/AMQP-0-9-1-Model-Explained/#amqp#is#a#programmable#protocol","text":"AMQP 0-9-1 is a programmable protocol in the sense that AMQP 0-9-1 entities and routing schemes are primarily defined by applications themselves, not a broker administrator . Accordingly, provision is made for protocol operations that declare queues and exchanges, define bindings between them, subscribe to queues and so on. This gives application developers a lot of freedom but also requires them to be aware of potential definition conflicts . In practice, definition conflicts are rare and often indicate a misconfiguration. Applications declare the AMQP 0-9-1 entities that they need, define necessary routing schemes and may choose to delete AMQP 0-9-1 entities when they are no longer used.","title":"AMQP is a Programmable Protocol"},{"location":"Message-processing-system/MQ-library/library-Rabbitmq/AMQP-0-9-1-Model-Explained/#exchanges#and#exchange#types","text":"Exchanges are AMQP 0-9-1 entities where messages are sent. Exchanges take a message and route it into zero or more queues. The routing algorithm used depends on the exchange type and rules called bindings . AMQP 0-9-1 brokers provide four exchange types : Name Default pre-declared names Direct exchange (Empty string) and amq.direct Fanout exchange amq.fanout Topic exchange amq.topic Headers exchange amq.match (and amq.headers in RabbitMQ) Besides the exchange type , exchanges are declared with a number of attributes, the most important of which are: Name Durability (exchanges survive broker restart) Auto-delete (exchange is deleted when last queue is unbound from it) Arguments (optional, used by plugins and broker-specific features) Exchanges can be durable or transient . Durable exchanges survive broker restart whereas transient exchanges do not (they have to be redeclared when broker comes back online). Not all scenarios and use cases require exchanges to be durable.","title":"Exchanges and Exchange Types"},{"location":"Message-processing-system/MQ-library/library-Rabbitmq/AMQP-0-9-1-Model-Explained/#default#exchange","text":"The default exchange is a direct exchange with no name (empty string) pre-declared by the broker. It has one special property that makes it very useful for simple applications: every queue that is created is automatically bound to it with a routing key which is the same as the queue name . For example, when you declare a queue with the name of \"search-indexing-online\", the AMQP 0-9-1 broker will bind it to the default exchange using \"search-indexing-online\" as the routing key (in this context sometimes referred to as the binding key ). Therefore, a message published to the default exchange with the routing key \"search-indexing-online\" will be routed to the queue \"search-indexing-online\". In other words, the default exchange makes it seem like it is possible to deliver messages directly to queues , even though that is not technically what is happening.","title":"Default Exchange"},{"location":"Message-processing-system/MQ-library/library-Rabbitmq/AMQP-0-9-1-Model-Explained/#direct#exchange","text":"A direct exchange delivers messages to queues based on the message routing key . A direct exchange is ideal for the unicast\uff08\u5355\u64ad\uff09 routing of messages (although they can be used for multicast\uff08\u591a\u64ad\uff09 routing as well). Here is how it works: A queue binds to the exchange with a routing key K SUMMARY : queue name\u548crouting key\u662f\u53ef\u4ee5\u4e0d\u540c\u7684\uff1b When a new message with routing key R arrives at the direct exchange, the exchange routes it to the queue if K = R Direct exchanges are often used to distribute tasks between multiple workers (instances of the same application) in a round robin manner. When doing so, it is important to understand that, in AMQP 0-9-1, messages are load balanced between consumers and not between queues. Direct exchanges \u901a\u5e38\u7528\u4e8e\u4ee5\u5faa\u73af\u65b9\u5f0f\u5728\u591a\u4e2a\u5de5\u4f5c\u8005\uff08\u540c\u4e00\u5e94\u7528\u7a0b\u5e8f\u7684\u5b9e\u4f8b\uff09\u4e4b\u95f4\u5206\u914d\u4efb\u52a1\u3002 \u5728\u8fd9\u6837\u505a\u65f6\uff0c\u91cd\u8981\u7684\u662f\u8981\u7406\u89e3\uff0c\u5728AMQP 0-9-1\u4e2d\uff0c\u6d88\u606f\u5728\u6d88\u8d39\u8005\u4e4b\u95f4\u800c\u4e0d\u662f\u5728\u961f\u5217\u4e4b\u95f4\u8fdb\u884c\u8d1f\u8f7d\u5e73\u8861\u3002 A direct exchange can be represented graphically as follows:","title":"Direct Exchange"},{"location":"Message-processing-system/MQ-library/library-Rabbitmq/AMQP-0-9-1-Model-Explained/#fanout#exchange","text":"A fanout exchange routes messages to all of the queues that are bound to it and the routing key is ignored. If N queues are bound to a fanout exchange , when a new message is published to that exchange a copy of the message is delivered to all N queues. Fanout exchanges are ideal for the broadcast \uff08\u5e7f\u64ad\uff09 routing of messages. Because a fanout exchange delivers a copy of a message to every queue bound to it, its use cases are quite similar: Massively multi-player online (MMO) games can use it for leaderboard updates or other global events Sport news sites can use fanout exchanges for distributing score updates to mobile clients in near real-time Distributed systems can broadcast various state and configuration updates Group chats can distribute messages between participants using a fanout exchange (although AMQP does not have a built-in concept of presence, so XMPP may be a better choice) A fanout exchange can be represented graphically as follows:","title":"Fanout Exchange"},{"location":"Message-processing-system/MQ-library/library-Rabbitmq/AMQP-0-9-1-Model-Explained/#topic#exchange","text":"Topic exchanges route messages to one or many queues based on matching between a message routing key and the pattern that was used to bind a queue to an exchange. The topic exchange type is often used to implement various publish/subscribe pattern variations . Topic exchanges are commonly used for the multicast routing of messages . Topic exchanges have a very broad set of use cases. Whenever a problem involves multiple consumers/applications that selectively choose which type of messages they want to receive, the use of topic exchanges should be considered. Example uses: Distributing data relevant to specific geographic location, for example, points of sale Background task processing done by multiple workers, each capable of handling specific set of tasks Stocks price updates (and updates on other kinds of financial data) News updates that involve categorization or tagging (for example, only for a particular sport or team) Orchestration of services of different kinds in the cloud Distributed architecture/OS-specific software builds or packaging where each builder can handle only one architecture or OS","title":"Topic Exchange"},{"location":"Message-processing-system/MQ-library/library-Rabbitmq/AMQP-0-9-1-Model-Explained/#headers#exchange","text":"A headers exchange is designed for routing on multiple attributes that are more easily expressed as message headers than a routing key . Headers exchanges ignore the routing key attribute . Instead, the attributes used for routing are taken from the headers attribute . A message is considered matching if the value of the header equals the value specified upon binding . It is possible to bind a queue to a headers exchange using more than one header for matching. In this case, the broker needs one more piece of information from the application developer, namely, should it consider messages with any of the headers matching, or all of them? This is what the \"x-match\" binding argument is for. When the \"x-match\" argument is set to \"any\", just one matching header value is sufficient. Alternatively, setting \"x-match\" to \"all\" mandates that all the values must match. Headers exchanges can be looked upon as \"direct exchanges on steroids\". Because they route based on header values, they can be used as direct exchanges where the routing key does not have to be a string; it could be an integer or a hash (dictionary) for example. Note that headers beginning with the string x- will not be used to evaluate matches.","title":"Headers Exchange"},{"location":"Message-processing-system/MQ-library/library-Rabbitmq/AMQP-0-9-1-Model-Explained/#queues","text":"Queues in the AMQP 0-9-1 model are very similar to queues in other message- and task-queueing systems: they store messages that are consumed by applications. Queues share some properties with exchanges , but also have some additional properties: Name Durable (the queue will survive a broker restart) Exclusive (used by only one connection and the queue will be deleted when that connection closes) Auto-delete (queue that has had at least one consumer is deleted when last consumer unsubscribes) Arguments (optional; used by plugins and broker-specific features such as message TTL , queue length limit, etc) Before a queue can be used it has to be declared. Declaring a queue will cause it to be created if it does not already exist. The declaration will have no effect if the queue does already exist and its attributes are the same as those in the declaration. When the existing queue attributes are not the same as those in the declaration a channel-level exception with code 406 (PRECONDITION_FAILED) will be raised.","title":"Queues"},{"location":"Message-processing-system/MQ-library/library-Rabbitmq/AMQP-0-9-1-Model-Explained/#queue#names","text":"Applications may pick queue names or ask the broker to generate a name for them. Queue names may be up to 255 bytes of UTF-8 characters. An AMQP 0-9-1 broker can generate a unique queue name on behalf of an app. To use this feature, pass an empty string as the queue name argument. The generated name will be returned to the client with queue declaration response. Queue names starting with \"amq.\" are reserved for internal use by the broker. Attempts to declare a queue with a name that violates this rule will result in a channel-level exception with reply code 403 (ACCESS_REFUSED).","title":"Queue Names"},{"location":"Message-processing-system/MQ-library/library-Rabbitmq/AMQP-0-9-1-Model-Explained/#queue#durability","text":"Durable queues are persisted to disk and thus survive broker restarts. Queues that are not durable are called transient . Not all scenarios and use cases mandate queues to be durable. Durability of a queue does not make messages that are routed to that queue durable. If broker is taken down and then brought back up, durable queue will be re-declared during broker startup, however, only persistent messages will be recovered.","title":"Queue Durability"},{"location":"Message-processing-system/MQ-library/library-Rabbitmq/AMQP-0-9-1-Model-Explained/#bindings","text":"Bindings are rules that exchanges use (among other things) to route messages to queues . To instruct an exchange E to route messages to a queue Q , Q has to be bound to E. Bindings may have an optional routing key attribute used by some exchange types . The purpose of the routing key is to select certain messages published to an exchange to be routed to the bound queue . In other words, the routing key acts like a filter. To draw an analogy: Queue is like your destination in New York city Exchange is like JFK airport Bindings are routes from JFK to your destination. There can be zero or many ways to reach it Having this layer of indirection enables routing scenarios that are impossible or very hard to implement using publishing directly to queues and also eliminates certain amount of duplicated work application developers have to do. If AMQP message cannot be routed to any queue (for example, because there are no bindings for the exchange it was published to) it is either dropped or returned to the publisher, depending on message attributes the publisher has set.","title":"Bindings"},{"location":"Message-processing-system/MQ-library/library-Rabbitmq/AMQP-0-9-1-Model-Explained/#consumers","text":"Storing messages in queues is useless unless applications can consume them. In the AMQP 0-9-1 Model, there are two ways for applications to do this: Have messages delivered to them (\"push API\") Fetch messages as needed (\"pull API\") With the \"push API\", applications have to indicate interest in consuming messages from a particular queue. When they do so, we say that they register a consumer or, simply put, subscribe to a queue . It is possible to have more than one consumer per queue or to register an exclusive consumer (excludes all other consumers from the queue while it is consuming). Each consumer (subscription) has an identifier called a consumer tag . It can be used to unsubscribe from messages. Consumer tags are just strings. SUMMARY : consumer\u4ecequeue\u4e2d\u53d6\u51fa\u6d88\u606f\uff0c\u6211\u60f3\u5230\u4e86\u4e00\u4e2a\u95ee\u9898\uff1a\u4e00\u4e2aconsumer\u53ef\u4ee5\u4ece\u591a\u4e2aqueue\u4e2d\u53d6\u51fa\u6d88\u606f\u5417\uff1f\u4ece\u4e0a\u9762\u7684\u63cf\u8ff0\u6765\u770b\uff0c\u8c8c\u4f3c\u4e00\u4e2aconsumer\u53ea\u80fd\u591f\u5bf9\u5e94\u4e00\u4e2aqueue\u3002\u4f46\u662f\u53ef\u7528\u53cd\u8fc7\u6765\uff1a\u591a\u4e2aconsumer\u5bf9\u5e94\u540c\u4e00\u4e2aqueue\uff1b","title":"Consumers"},{"location":"Message-processing-system/MQ-library/library-Rabbitmq/AMQP-0-9-1-Model-Explained/#message#acknowledgements","text":"Consumer applications \u2013 applications that receive and process messages \u2013 may occasionally fail to process individual messages or will sometimes just crash. There is also the possibility of network issues causing problems. This raises a question: when should the AMQP broker remove messages from queues? The AMQP 0-9-1 specification proposes two choices: After broker sends a message to an application (using either basic.deliver or basic.get-ok AMQP methods). After the application sends back an acknowledgement (using basic.ack AMQP method). The former choice is called the automatic acknowledgement model , while the latter is called the explicit acknowledgement model . With the explicit model the application chooses when it is time to send an acknowledgement. It can be right after receiving a message, or after persisting it to a data store before processing, or after fully processing the message (for example, successfully fetching a Web page, processing and storing it into some persistent data store). If a consumer dies without sending an acknowledgement the AMQP broker will redeliver it to another consumer or, if none are available at the time, the broker will wait until at least one consumer is registered for the same queue before attempting redelivery.","title":"Message Acknowledgements"},{"location":"Message-processing-system/MQ-library/library-Rabbitmq/AMQP-0-9-1-Model-Explained/#rejecting#messages","text":"When a consumer application receives a message, processing of that message may or may not succeed. An application can indicate to the broker that message processing has failed (or cannot be accomplished at the time) by rejecting a message. When rejecting a message, an application can ask the broker to discard or requeue it. When there is only one consumer on a queue, make sure you do not create infinite message delivery loops by rejecting and requeueing a message from the same consumer over and over again.","title":"Rejecting Messages"},{"location":"Message-processing-system/MQ-library/library-Rabbitmq/AMQP-0-9-1-Model-Explained/#negative#acknowledgements","text":"Messages are rejected with the basic.reject AMQP method. There is one limitation that basic.reject has: there is no way to reject multiple messages as you can do with acknowledgements . However, if you are using RabbitMQ, then there is a solution. RabbitMQ provides an AMQP 0-9-1 extension known as negative acknowledgements or nacks . For more information, please refer to the the help page .","title":"Negative Acknowledgements"},{"location":"Message-processing-system/MQ-library/library-Rabbitmq/AMQP-0-9-1-Model-Explained/#prefetching#messages","text":"For cases when multiple consumers share a queue , it is useful to be able to specify how many messages each consumer can be sent at once before sending the next acknowledgement. This can be used as a simple load balancing technique or to improve throughput if messages tend to be published in batches. For example, if a producing application sends messages every minute because of the nature of the work it is doing. Note that RabbitMQ only supports channel-level prefetch-count, not connection or size based prefetching.","title":"Prefetching Messages"},{"location":"Message-processing-system/MQ-library/library-Rabbitmq/AMQP-0-9-1-Model-Explained/#message#attributes#and#payload","text":"Messages in the AMQP model have attributes . Some attributes are so common that the AMQP 0-9-1 specification defines them and application developers do not have to think about the exact attribute name. Some examples are Content type Content encoding Routing key Delivery mode (persistent or not) Message priority Message publishing timestamp Expiration period Publisher application id Some attributes are used by AMQP brokers, but most are open to interpretation by applications that receive them. Some attributes are optional and known as headers . They are similar to X-Headers in HTTP. Message attributes are set when a message is published. AMQP messages also have a payload (the data that they carry), which AMQP brokers treat as an opaque byte array. The broker will not inspect or modify the payload. It is possible for messages to contain only attributes and no payload. It is common to use serialisation formats like JSON, Thrift, Protocol Buffers and MessagePack to serialize structured data in order to publish it as the message payload. AMQP peers typically use the \"content-type\" and \"content-encoding\" fields to communicate this information, but this is by convention only. Messages may be published as persistent, which makes the AMQP broker persist them to disk. If the server is restarted the system ensures that received persistent messages are not lost. Simply publishing a message to a durable exchange or the fact that the queue(s) it is routed to are durable doesn't make a message persistent: it all depends on persistence mode of the message itself. Publishing messages as persistent affects performance (just like with data stores, durability comes at a certain cost in performance).","title":"Message Attributes and Payload"},{"location":"Message-processing-system/MQ-library/library-Rabbitmq/AMQP-0-9-1-Model-Explained/#message#acknowledgements_1","text":"Since networks are unreliable and applications fail, it is often necessary to have some kind of processing acknowledgement. Sometimes it is only necessary to acknowledge the fact that a message has been received. Sometimes acknowledgements mean that a message was validated and processed by a consumer, for example, verified as having mandatory data and persisted to a data store or indexed. This situation is very common, so AMQP 0-9-1 has a built-in feature called message acknowledgements (sometimes referred to as acks ) that consumers use to confirm message delivery and/or processing. If an application crashes (the AMQP broker notices this when the connection is closed), if an acknowledgement for a message was expected but not received by the AMQP broker, the message is re-queued (and possibly immediately delivered to another consumer, if any exists). Having acknowledgements built into the protocol helps developers to build more robust software.","title":"Message Acknowledgements"},{"location":"Message-processing-system/MQ-library/library-Rabbitmq/AMQP-0-9-1-Model-Explained/#amqp#0-9-1#methods","text":"AMQP 0-9-1 is structured as a number of methods . Methods are operations (like HTTP methods ) and have nothing in common with methods in object-oriented programming languages. AMQP methods are grouped into classes . Classes are just logical groupings of AMQP methods. The AMQP 0-9-1 reference has full details of all the AMQP methods. Let us take a look at the exchange class, a group of methods related to operations on exchanges. It includes the following operations: exchange.declare exchange.declare-ok exchange.delete exchange.delete-ok (note that the RabbitMQ site reference also includes RabbitMQ-specific extensions to the exchange class that we will not discuss in this guide). The operations above form logical pairs: exchange.declare and exchange.declare-ok , exchange.delete and exchange.delete-ok . These operations are \"requests\" (sent by clients) and \"responses\" (sent by brokers in response to the aforementioned\uff08\u4e0a\u8ff0\u7684\uff09 \"requests\"). As an example, the client asks the broker to declare a new exchange using the exchange.declare method: As shown on the diagram above, exchange.declare carries several parameters . They enable the client to specify exchange name , type , durability flag and so on. If the operation succeeds, the broker responds with the exchange.declare-ok method: exchange.declare-ok does not carry any parameters except for the channel number (channels will be described later in this guide). The sequence of events is very similar for another method pair on the AMQP queue class: queue.declare and queue.declare-ok : Not all AMQP methods have counterparts. Some ( basic.publish being the most widely used one) do not have corresponding \"response\" methods and some others ( basic.get , for example) have more than one possible \"response\".","title":"AMQP 0-9-1 Methods"},{"location":"Message-processing-system/MQ-library/library-Rabbitmq/AMQP-0-9-1-Model-Explained/#connections","text":"AMQP 0-9-1 connections are typically long-lived. AMQP 0-9-1 is an application level protocol that uses TCP for reliable delivery. Connections use authentication and can be protected using TLS . When an application no longer needs to be connected to the server, it should gracefully close its AMQP 0-9-1 connection instead of abruptly closing the underlying TCP connection. \u200b","title":"Connections"},{"location":"Message-processing-system/MQ-library/library-Rabbitmq/AMQP-0-9-1-Model-Explained/#channels","text":"Some applications need multiple connections to the broker. However, it is undesirable to keep many TCP connections open at the same time because doing so consumes system resources and makes it more difficult to configure firewalls. AMQP 0-9-1 connections are multiplexed with channels that can be thought of as \"lightweight connections that share a single TCP connection\". Every protocol operation performed by a client happens on a channel. Communication on a particular channel is completely separate from communication on another channel, therefore every protocol method also carries a channel ID (a.k.a. channel number), an integer that both the broker and clients use to figure out which channel the method is for. A channel only exists in the context of a connection and never on its own. When a connection is closed, so are all channels on it. For applications that use multiple threads/processes for processing, it is very common to open a new channel per thread/process and not share channels between them. RabbitMQ and relationship between channel and connection SUMMARY : \u8981\u900f\u5f7b\u5730\u7406\u89e3channel\u9700\u8981\u5148\u7406\u89e3 Multiplexing \u591a\u8def\u590d\u7528\u3002","title":"Channels"},{"location":"Message-processing-system/MQ-library/library-Rabbitmq/AMQP-0-9-1-Model-Explained/#virtual#hosts","text":"To make it possible for a single broker to host multiple isolated \"environments\" (groups of users, exchanges, queues and so on), AMQP includes the concept of virtual hosts (vhosts). They are similar to virtual hosts used by many popular Web servers and provide completely isolated environments in which AMQP entities live. AMQP clients specify what vhosts they want to use during AMQP connection negotiation.","title":"Virtual Hosts"},{"location":"Message-processing-system/MQ-library/library-Rabbitmq/AMQP-0-9-1-Model-Explained/#amqp#is#extensible","text":"AMQP 0-9-1 has several extension points: Custom exchange types let developers implement routing schemes that exchange types provided out-of-the-box do not cover well, for example, geodata-based routing. Declaration of exchanges and queues can include additional attributes that the broker can use. For example, per-queue message TTL in RabbitMQ is implemented this way. Broker-specific extensions to the protocol. See, for example, extensions that RabbitMQ implements . New AMQP 0-9-1 method classes can be introduced. Brokers can be extended with additional plugins , for example, the RabbitMQ management frontend and HTTP API are implemented as a plugin. These features make the AMQP 0-9-1 Model even more flexible and applicable to a very broad range of problems.","title":"AMQP is Extensible"},{"location":"Message-processing-system/MQ-library/library-ZeroMQ/","text":"ZeroMQ ZeroMQ ZeroMQ (also known as \u00d8MQ, 0MQ, or zmq) looks like an embeddable networking library but acts like a concurrency framework. It gives you sockets that carry atomic messages across various transports like in-process, inter-process, TCP, and multicast. You can connect sockets N-to-N with patterns like fan-out, pub-sub, task distribution, and request-reply. It's fast enough to be the fabric for clustered products. Its asynchronous I/O model gives you scalable multicore applications, built as asynchronous message-processing tasks. It has a score of language APIs and runs on most operating systems. NOTE: \u4e00\u3001\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u57fa\u672c\u4e0a\u603b\u7ed3\u4e86ZeroMQ\u7684\u6838\u5fc3\u7279\u6027: \u4e8c\u3001\u5b83\u7684socket\u662f\u975e\u5e38\u62bd\u8c61\u7684\uff0c\u5b83\u7684\u8bbe\u8ba1\u9075\u5faa\u4e86abstraction\u539f\u5219 1\u3001\u5b83\u5141\u8bb8socket\u8fdb\u884c\u7075\u6d3b\u7684\u8fde\u63a5: \"You can connect sockets N-to-N\" 2\u3001\u5b83\u63d0\u4f9b\u4e86message passing\u7684\u5404\u79cd\u6a21\u5f0f: \"fan-out, pub-sub, task distribution, and request-reply\" 3\u3001\u5b83\u9690\u85cf\u4e86\u5404\u79cd\u53ef\u80fd\u7684transport\u7ec6\u8282\uff0c\u5728\u8fd9\u4e9btransport\u4e4b\u4e0a\u5efa\u7acb\u8d77\u4e86consistent API\uff0c\u4f7f\u5f97programmer\u80fd\u591f\"program to an abstraction and polymorphism\"\uff1b\u5176\u5b9e\u5b83\u7684\u8fd9\u79cd\u62bd\u8c61\u601d\u8def\u662f\u975e\u5e38\u5bb9\u6613\u7406\u89e3\u7684\uff0c\u56e0\u4e3a: a\u3001\"Everything is a file\" abstraction\uff0cZeroMQ\u7684socket\u5176\u5b9e\u5c31\u76f8\u5f53\u4e8e\u524d\u9762\u6240\u8bf4\u7684\"file\"\uff0c\u56e0\u6b64\uff0c\u5bf9\u4e8e\u6bcf\u4e2aZeroMQ\u7684socket\uff0c\u90fd\u80fd\u591f\u5b9e\u73b0read\u3001write\u57fa\u672c\u64cd\u4f5c \u603b\u7684\u6765\u8bf4: \u901a\u8fc7\u4e0a\u8ff0\u8fd9\u79cd\u8bbe\u8ba1\u601d\u8def\uff0cZeroMQ\u7ed9\u4e88\u4e86programmer\u7075\u6d3b\u7684message passing \u529f\u80fd \u4e09\u3001\u5b83\u7684\u4ef7\u503c: \u4e0a\u8ff0\u63d0\u53ca\u7684\u8fd9\u4e9b\uff0c\u5bf9\u4e8e\u5b9e\u73b0\u5404\u79cd\u57fa\u4e8emessage\u7684application\u90fd\u662f\u5177\u6709\u975e\u5e38\u91cd\u8981\u5f97\u591a\u610f\u4e49\u7684 \u56db\u3001ZeroMQ\u4f53\u73b0\u4e86abstraction\u7684\u673a\u5236\uff1a simplify github libzmq","title":"ZeroMQ"},{"location":"Message-processing-system/MQ-library/library-ZeroMQ/#zeromq","text":"","title":"ZeroMQ"},{"location":"Message-processing-system/MQ-library/library-ZeroMQ/#zeromq_1","text":"ZeroMQ (also known as \u00d8MQ, 0MQ, or zmq) looks like an embeddable networking library but acts like a concurrency framework. It gives you sockets that carry atomic messages across various transports like in-process, inter-process, TCP, and multicast. You can connect sockets N-to-N with patterns like fan-out, pub-sub, task distribution, and request-reply. It's fast enough to be the fabric for clustered products. Its asynchronous I/O model gives you scalable multicore applications, built as asynchronous message-processing tasks. It has a score of language APIs and runs on most operating systems. NOTE: \u4e00\u3001\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u57fa\u672c\u4e0a\u603b\u7ed3\u4e86ZeroMQ\u7684\u6838\u5fc3\u7279\u6027: \u4e8c\u3001\u5b83\u7684socket\u662f\u975e\u5e38\u62bd\u8c61\u7684\uff0c\u5b83\u7684\u8bbe\u8ba1\u9075\u5faa\u4e86abstraction\u539f\u5219 1\u3001\u5b83\u5141\u8bb8socket\u8fdb\u884c\u7075\u6d3b\u7684\u8fde\u63a5: \"You can connect sockets N-to-N\" 2\u3001\u5b83\u63d0\u4f9b\u4e86message passing\u7684\u5404\u79cd\u6a21\u5f0f: \"fan-out, pub-sub, task distribution, and request-reply\" 3\u3001\u5b83\u9690\u85cf\u4e86\u5404\u79cd\u53ef\u80fd\u7684transport\u7ec6\u8282\uff0c\u5728\u8fd9\u4e9btransport\u4e4b\u4e0a\u5efa\u7acb\u8d77\u4e86consistent API\uff0c\u4f7f\u5f97programmer\u80fd\u591f\"program to an abstraction and polymorphism\"\uff1b\u5176\u5b9e\u5b83\u7684\u8fd9\u79cd\u62bd\u8c61\u601d\u8def\u662f\u975e\u5e38\u5bb9\u6613\u7406\u89e3\u7684\uff0c\u56e0\u4e3a: a\u3001\"Everything is a file\" abstraction\uff0cZeroMQ\u7684socket\u5176\u5b9e\u5c31\u76f8\u5f53\u4e8e\u524d\u9762\u6240\u8bf4\u7684\"file\"\uff0c\u56e0\u6b64\uff0c\u5bf9\u4e8e\u6bcf\u4e2aZeroMQ\u7684socket\uff0c\u90fd\u80fd\u591f\u5b9e\u73b0read\u3001write\u57fa\u672c\u64cd\u4f5c \u603b\u7684\u6765\u8bf4: \u901a\u8fc7\u4e0a\u8ff0\u8fd9\u79cd\u8bbe\u8ba1\u601d\u8def\uff0cZeroMQ\u7ed9\u4e88\u4e86programmer\u7075\u6d3b\u7684message passing \u529f\u80fd \u4e09\u3001\u5b83\u7684\u4ef7\u503c: \u4e0a\u8ff0\u63d0\u53ca\u7684\u8fd9\u4e9b\uff0c\u5bf9\u4e8e\u5b9e\u73b0\u5404\u79cd\u57fa\u4e8emessage\u7684application\u90fd\u662f\u5177\u6709\u975e\u5e38\u91cd\u8981\u5f97\u591a\u610f\u4e49\u7684 \u56db\u3001ZeroMQ\u4f53\u73b0\u4e86abstraction\u7684\u673a\u5236\uff1a simplify","title":"ZeroMQ"},{"location":"Message-processing-system/MQ-library/library-ZeroMQ/#github#libzmq","text":"","title":"github  libzmq"},{"location":"Message-processing-system/Message-disorder/","text":"Message disorder \u5206\u5e03\u5f0f\u6d88\u606f\u961f\u5217\u4e2d\u7684message disorder \u5728csdn \u8fd9\u4e09\u5e74\u88ab\u5206\u5e03\u5f0f\u5751\u60e8\u4e86\uff0c\u66dd\u5149\u5341\u5927\u5751 \u4e2d\uff0c\u5bf9\u6b64\u8fdb\u884c\u4e86\u603b\u7ed3\u3002 TODO https://www.cs.uic.edu/~ajayk/Chapter6.pdf https://www.ics.uci.edu/~cs230/reading/time.pdf Distributed Computing: Principles, Algorithms, and Systems https://pdfs.semanticscholar.org/cd38/c9976ce685260608abfc961fff4c67a8788f.pdf https://www.researchgate.net/publication/3727910_An_efficient_causal_order_algorithm_for_message_delivery_in_distributed_system https://www.researchgate.net/publication/262212678_Detecting_problematic_message_sequences_and_frequencies_in_distributed_systems https://users.ece.cmu.edu/~koopman/lost_messages/index.html https://pdfs.semanticscholar.org/cd38/c9976ce685260608abfc961fff4c67a8788f.pdf https://www.ics.uci.edu/~cs230/reading/time.pdf","title":"Introduction"},{"location":"Message-processing-system/Message-disorder/#message#disorder","text":"","title":"Message disorder"},{"location":"Message-processing-system/Message-disorder/#message#disorder_1","text":"\u5728csdn \u8fd9\u4e09\u5e74\u88ab\u5206\u5e03\u5f0f\u5751\u60e8\u4e86\uff0c\u66dd\u5149\u5341\u5927\u5751 \u4e2d\uff0c\u5bf9\u6b64\u8fdb\u884c\u4e86\u603b\u7ed3\u3002","title":"\u5206\u5e03\u5f0f\u6d88\u606f\u961f\u5217\u4e2d\u7684message disorder"},{"location":"Message-processing-system/Message-disorder/#todo","text":"https://www.cs.uic.edu/~ajayk/Chapter6.pdf https://www.ics.uci.edu/~cs230/reading/time.pdf Distributed Computing: Principles, Algorithms, and Systems https://pdfs.semanticscholar.org/cd38/c9976ce685260608abfc961fff4c67a8788f.pdf https://www.researchgate.net/publication/3727910_An_efficient_causal_order_algorithm_for_message_delivery_in_distributed_system https://www.researchgate.net/publication/262212678_Detecting_problematic_message_sequences_and_frequencies_in_distributed_systems https://users.ece.cmu.edu/~koopman/lost_messages/index.html https://pdfs.semanticscholar.org/cd38/c9976ce685260608abfc961fff4c67a8788f.pdf https://www.ics.uci.edu/~cs230/reading/time.pdf","title":"TODO"},{"location":"Message-processing-system/Messaging-pattern/","text":"Message pattern Message scope Inter-process and inner-process event-driven model\u7ae0\u8282\u7684\u4fa7\u91cd\u70b9\u662finter-process\uff1b wikipedia Messaging pattern In software architecture , a messaging pattern is a network-oriented architectural pattern which describes how two different parts of a message passing system connect and communicate with each other. NOTE : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u5bf9message\u7684\u5b9a\u4e49\u662f\u5c40\u9650\u4e8enetwork\u7684\uff0c\u5176\u5b9e\u6211\u89c9\u5f97\u6211\u4eec\u9700\u8981\u4ece\u66f4\u52a0\u5bbd\u6cdb\u7684\u89d2\u5ea6\u6765\u8ba4\u8bc6message\uff0c\u800c\u4e0d\u662f\u5c40\u9650\u5728network\uff1b \u5728\u6211\u4eec\u7684programming\u4e2d\uff0c\u5176\u5b9e\u5f88\u591a\u65f6\u5019\u90fd\u6d89\u53ca\u5230\u4e86\u901a\u8fc7message\u6765\u901a\u77e5\uff0c\u4f20\u9012\uff0c\u6709\u7684\u65f6\u5019\uff0c\u5b83\u4eec\u5e76\u4e0d\u662fnetwork\u95f4\uff0c\u53ef\u80fd\u5c31\u662f\u5728process\u5185\uff08\u5982condition variable\uff08\u53c2\u89c1youdao notebook\u7684\u300a cppreference-std-condition_variable.md \u300b\uff09\uff0c\u6216\u8005 message queue \uff09\uff1b\u6240\u4ee5\u5bf9message\u7684\u8ba4\u77e5\u8981\u62d3\u5bbd\uff1b\u5728youdao notebook\u7684\u300a event-and-message-passing-summary.md \u300b\u4e2d\u5bf9message\u8fdb\u884c\u4e86\u7efc\u8ff0\uff1b\u5176\u5b9e\u672c\u6587\u5219\u4ece\u53e6\u5916\u4e00\u4e2a\u89d2\u5ea6\u6765\u8fdb\u884c\u9610\u8ff0\uff1amessage\u7684pattern\uff1b\u7684\u786e\uff0c\u4ec5\u4ec5\u8ba4\u77e5message\u662f\u5b8c\u5168\u4e0d\u80fd\u591f\u5c06\u5176\u5e94\u7528\u8d77\u6765\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\u7684\uff0c\u8fd8\u9700\u8981\u6839\u636e\u95ee\u9898\u91c7\u53d6\u5408\u9002\u7684\u67b6\u6784\uff0c\u8fd9\u5c31\u6d89\u53camessage pattern\u4e86\uff1b\u6240\u4ee5\u8ba4\u77e5\u5230messaging pattern\u5bf9\u4e8e\u7406\u89e3\u4e00\u4e9b\u5e94\u7528\u662f\u975e\u5e38\u91cd\u8981\u7684\uff1b In telecommunications , a message exchange pattern ( MEP ) describes the pattern of messages required by a communications protocol to establish or use a communication channel . There are two major message exchange patterns \u2014 a request\u2013response pattern, and a one-way \uff08\u5355\u7a0b\u7684\uff09 pattern. For example, HTTP is a request\u2013response pattern protocol, and UDP is a one-way pattern.[ 1] NOTE: \u672c\u6587\u6240\u63cf\u8ff0\u7684messaging pattern\u662f\u5efa\u7acb\u5728network protocol\u7684\u57fa\u7840\u4e0a\u800c\u6784\u5efa\u7684\u66f4\u52a0\u9ad8\u7ea7\u7684\u529f\u80fd\uff0c\u9700\u8981\u5c06\u5b83\u548c\u6bd4\u8f83\u5e95\u5c42\u7684network protocol\u533a\u5206\u5f00\u6765\uff1b \u00d8MQ The \u00d8MQ message queueing library provides so-called sockets (a kind of generalization over the traditional IP and Unix sockets ) which require indicating a messaging pattern to be used, and are optimized for each pattern. The basic \u00d8MQ patterns are:[ 4] 1\u3001 Request\u2013reply connects a set of clients to a set of services. This is a remote procedure call and task distribution pattern.[ clarification needed ] 2\u3001 Publish\u2013subscribe connects a set of publishers to a set of subscribers. This is a data distribution pattern.[ clarification needed ] 3\u3001 Push\u2013pull connects nodes in a fan-out / fan-in pattern that can have multiple steps, and loops. This is a parallel task distribution and collection pattern.[ clarification needed ] NOTE: \u5e76\u6ca1\u6709\u641e\u61c2 4\u3001 Exclusive pair connects two sockets in an exclusive pair. This is a low-level pattern for specific, advanced use cases. NOTE: \u5e76\u6ca1\u6709\u641e\u61c2 Each pattern defines a particular network topology . Request-reply defines so-called \"service bus\", publish-subscribe defines \"data distribution tree\", push-pull defines \"parallelised pipeline\". All the patterns are deliberately designed in such a way as to be infinitely scalable and thus usable on Internet scale.[ 5] NOTE: \u6700\u540e\u4e00\u6bb5\u8bdd\u975e\u5e38\u597d\uff1b","title":"Introduction"},{"location":"Message-processing-system/Messaging-pattern/#message#pattern","text":"","title":"Message pattern"},{"location":"Message-processing-system/Messaging-pattern/#message#scope","text":"","title":"Message scope"},{"location":"Message-processing-system/Messaging-pattern/#inter-process#and#inner-process","text":"event-driven model\u7ae0\u8282\u7684\u4fa7\u91cd\u70b9\u662finter-process\uff1b","title":"Inter-process and inner-process"},{"location":"Message-processing-system/Messaging-pattern/#wikipedia#messaging#pattern","text":"In software architecture , a messaging pattern is a network-oriented architectural pattern which describes how two different parts of a message passing system connect and communicate with each other. NOTE : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u5bf9message\u7684\u5b9a\u4e49\u662f\u5c40\u9650\u4e8enetwork\u7684\uff0c\u5176\u5b9e\u6211\u89c9\u5f97\u6211\u4eec\u9700\u8981\u4ece\u66f4\u52a0\u5bbd\u6cdb\u7684\u89d2\u5ea6\u6765\u8ba4\u8bc6message\uff0c\u800c\u4e0d\u662f\u5c40\u9650\u5728network\uff1b \u5728\u6211\u4eec\u7684programming\u4e2d\uff0c\u5176\u5b9e\u5f88\u591a\u65f6\u5019\u90fd\u6d89\u53ca\u5230\u4e86\u901a\u8fc7message\u6765\u901a\u77e5\uff0c\u4f20\u9012\uff0c\u6709\u7684\u65f6\u5019\uff0c\u5b83\u4eec\u5e76\u4e0d\u662fnetwork\u95f4\uff0c\u53ef\u80fd\u5c31\u662f\u5728process\u5185\uff08\u5982condition variable\uff08\u53c2\u89c1youdao notebook\u7684\u300a cppreference-std-condition_variable.md \u300b\uff09\uff0c\u6216\u8005 message queue \uff09\uff1b\u6240\u4ee5\u5bf9message\u7684\u8ba4\u77e5\u8981\u62d3\u5bbd\uff1b\u5728youdao notebook\u7684\u300a event-and-message-passing-summary.md \u300b\u4e2d\u5bf9message\u8fdb\u884c\u4e86\u7efc\u8ff0\uff1b\u5176\u5b9e\u672c\u6587\u5219\u4ece\u53e6\u5916\u4e00\u4e2a\u89d2\u5ea6\u6765\u8fdb\u884c\u9610\u8ff0\uff1amessage\u7684pattern\uff1b\u7684\u786e\uff0c\u4ec5\u4ec5\u8ba4\u77e5message\u662f\u5b8c\u5168\u4e0d\u80fd\u591f\u5c06\u5176\u5e94\u7528\u8d77\u6765\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\u7684\uff0c\u8fd8\u9700\u8981\u6839\u636e\u95ee\u9898\u91c7\u53d6\u5408\u9002\u7684\u67b6\u6784\uff0c\u8fd9\u5c31\u6d89\u53camessage pattern\u4e86\uff1b\u6240\u4ee5\u8ba4\u77e5\u5230messaging pattern\u5bf9\u4e8e\u7406\u89e3\u4e00\u4e9b\u5e94\u7528\u662f\u975e\u5e38\u91cd\u8981\u7684\uff1b In telecommunications , a message exchange pattern ( MEP ) describes the pattern of messages required by a communications protocol to establish or use a communication channel . There are two major message exchange patterns \u2014 a request\u2013response pattern, and a one-way \uff08\u5355\u7a0b\u7684\uff09 pattern. For example, HTTP is a request\u2013response pattern protocol, and UDP is a one-way pattern.[ 1] NOTE: \u672c\u6587\u6240\u63cf\u8ff0\u7684messaging pattern\u662f\u5efa\u7acb\u5728network protocol\u7684\u57fa\u7840\u4e0a\u800c\u6784\u5efa\u7684\u66f4\u52a0\u9ad8\u7ea7\u7684\u529f\u80fd\uff0c\u9700\u8981\u5c06\u5b83\u548c\u6bd4\u8f83\u5e95\u5c42\u7684network protocol\u533a\u5206\u5f00\u6765\uff1b","title":"wikipedia Messaging pattern"},{"location":"Message-processing-system/Messaging-pattern/#mq","text":"The \u00d8MQ message queueing library provides so-called sockets (a kind of generalization over the traditional IP and Unix sockets ) which require indicating a messaging pattern to be used, and are optimized for each pattern. The basic \u00d8MQ patterns are:[ 4] 1\u3001 Request\u2013reply connects a set of clients to a set of services. This is a remote procedure call and task distribution pattern.[ clarification needed ] 2\u3001 Publish\u2013subscribe connects a set of publishers to a set of subscribers. This is a data distribution pattern.[ clarification needed ] 3\u3001 Push\u2013pull connects nodes in a fan-out / fan-in pattern that can have multiple steps, and loops. This is a parallel task distribution and collection pattern.[ clarification needed ] NOTE: \u5e76\u6ca1\u6709\u641e\u61c2 4\u3001 Exclusive pair connects two sockets in an exclusive pair. This is a low-level pattern for specific, advanced use cases. NOTE: \u5e76\u6ca1\u6709\u641e\u61c2 Each pattern defines a particular network topology . Request-reply defines so-called \"service bus\", publish-subscribe defines \"data distribution tree\", push-pull defines \"parallelised pipeline\". All the patterns are deliberately designed in such a way as to be infinitely scalable and thus usable on Internet scale.[ 5] NOTE: \u6700\u540e\u4e00\u6bb5\u8bdd\u975e\u5e38\u597d\uff1b","title":"\u00d8MQ"},{"location":"Message-processing-system/Messaging-pattern/Message-queue/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u8ba8\u8bbamessage queue\uff0c\u53c8\u79f0\u4e3amessage broker\uff0c\u53e6\u5916\u4e00\u4e2a\u7c7b\u4f3c\u7684\u6982\u5ff5\u662ftask queue\u3002 wikipedia Message broker fullstackpython Task queues","title":"Introduction"},{"location":"Message-processing-system/Messaging-pattern/Message-queue/#_1","text":"\u672c\u7ae0\u8ba8\u8bbamessage queue\uff0c\u53c8\u79f0\u4e3amessage broker\uff0c\u53e6\u5916\u4e00\u4e2a\u7c7b\u4f3c\u7684\u6982\u5ff5\u662ftask queue\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Message-processing-system/Messaging-pattern/Message-queue/#wikipedia#message#broker","text":"","title":"wikipedia Message broker"},{"location":"Message-processing-system/Messaging-pattern/Message-queue/#fullstackpython#task#queues","text":"","title":"fullstackpython Task queues"},{"location":"Message-processing-system/Messaging-pattern/Message-queue/AMQP/","text":"AMQP \u5b98\u7f51 https://www.amqp.org/ \u534f\u8bae\u6587\u6863\u4e0b\u8f7d\u7f51\u5740\uff1a https://www.amqp.org/resources/download wikipedia Advanced Message Queuing Protocol The Advanced Message Queuing Protocol ( AMQP ) is an open standard (\u5f00\u653e\u6807\u51c6) application layer protocol(\u5e94\u7528\u5c42\u534f\u8bae) for message-oriented middleware . The defining features of AMQP are message orientation, queuing, routing (including point-to-point and publish-and-subscribe ), reliability and security. NOTE: AMQP\u662f\u7c7b\u4f3c\u4e8eHTTP\u4e00\u6837\u7684\u5c5e\u4e8eOSI\u7684application layer protocol\u3002 \u601d\u8003\uff1a\u4f5c\u4e3a\u4e00\u79cd\u534f\u8bae\uff0c\u5176\u5b9e\u6211\u89c9\u5f97\u5e94\u8be5\u8981\u4ece\u5b83\u6240\u63d0\u4f9b\u7684\u7279\u6027\u6765\u5bf9\u5176\u8fdb\u884c\u628a\u63e1\uff0c\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u5c31\u5df2\u7ecf\u6982\u62ec\u4e86\u5176\u5173\u952e\u7279\u6027\uff1b\u4ee5\u53ca\u4f7f\u7528\u8fd9\u4e9b\u7279\u6027\uff0c\u80fd\u591f\u4e3a\u6211\u4eec\u89e3\u51b3\u54ea\u4e9b\u95ee\u9898\uff1b AMQP mandates\uff08\u8981\u6c42\uff09 the behavior of the messaging provider and client to the extent that implementations from different vendors are interoperable (AMQP\u8981\u6c42\u6d88\u606f\u4f20\u9012\u63d0\u4f9b\u8005\u548c\u5ba2\u6237\u7aef\u7684\u884c\u4e3a\u8fbe\u5230\u4e0d\u540c\u4f9b\u5e94\u5546\u7684\u5b9e\u73b0\u53ef\u4e92\u64cd\u4f5c\u7684\u7a0b\u5ea6), in the same way as SMTP , HTTP , FTP , etc. have created interoperable systems . Previous standardizations of middleware have happened at the API level (e.g. JMS ) and were focused on standardizing (\u6807\u51c6\u5316) programmer interaction with different middleware implementations, rather than on providing interoperability (\u53ef\u4ea4\u4e92\u6027) between multiple implementations.[ 2] Unlike JMS, which defines an API and a set of behaviors that a messaging implementation must provide, AMQP is a wire-level protocol . A wire-level protocol is a description of the format of the data that is sent across the network as a stream of bytes (\u5b57\u8282\u6d41\uff09. Consequently, any tool that can create and interpret messages that conform to this data format can interoperate with any other compliant tool irrespective of \uff08\u4e0d\u7ba1\uff09 implementation language. NOTE: \u534f\u8bae\u5b9a\u4e49\u4e86data format\uff0c\u6545\u53ea\u8981\u9075\u5faa\u8fd9\u4e2a\u534f\u8bae\uff0c\u90a3\u4e48\u4e0d\u540c\u7684\u5b9e\u73b0\u4e4b\u95f4\u5c31\u53ef\u4ee5\u8fdb\u884cinteroperate \u4e86\uff1b\u5176\u5b9e\u770b\u4e86\u4e0a\u9762\u7684\u8fd9\u6bb5\u8bdd\uff0c\u6211\u624d\u60f3\u8d77\u6765\uff0cprotocol\u5c31\u5e94\u5f53\u5982\u6b64\uff0c\u6bd4\u5982HTTP\u534f\u8bae\uff0c\u663e\u7136HTTP\u534f\u8bae\u7684\u5b9e\u73b0\u662f\u975e\u5e38\u4e4b\u591a\u7684\uff0c\u6bd4\u5982\u6211\u5728python\u4e2d\uff0c\u5728java\u4e2d\u662f\u53ef\u4ee5\u4f7f\u7528\u4e0d\u540c\u534f\u8bae\u7684\uff0c\u4f46\u662f\u4ed6\u4eec\u4e4b\u95f4\u662f\u53ef\u4ee5\u8fdb\u884c\u76f8\u4e92\u6c9f\u901a\u7684\uff1b Overview AMQP is a binary , application layer protocol\uff08\u4e8c\u8fdb\u5236\uff0c\u5e94\u7528\u5c42\u534f\u8bae\uff09, designed to efficiently support a wide variety of messaging applications and communication patterns. It provides flow controlled,[ 3] message-oriented communication with message-delivery guarantees such as at-most-once (where each message is delivered once or never), at-least-once (where each message is certain to be delivered, but may do so multiple times) and exactly-once (where the message will always certainly arrive and do so only once),[ 4] and authentication and/or encryption\uff08\u52a0\u5bc6\uff09 based on SASL and/or TLS .[ 5] It assumes an underlying reliable transport layer protocol such as Transmission Control Protocol (TCP).[ 6] The AMQP specification is defined in several layers: (i) a type system, (ii) a symmetric\uff08\u5bf9\u79f0\u7684\uff09, asynchronous protocol for the transfer of messages from one process to another, (iii) a standard, extensible message format and (iv) a set of standardised but extensible 'messaging capabilities.' Description of AMQP 1.0 Type system AMQP defines a self-describing encoding scheme allowing interoperable representation of a wide range of commonly used types. It also allows typed data to be annotated with additional meaning,[ 17] for example a particular string value might be annotated so that it could be understood as a URL . Likewise a map value containing key-value pairs for 'name', 'address' etc., might be annotated as being a representation of a 'customer' type. The type-system is used to define a message format allowing standard and extended meta-data to be expressed and understood by processing entities. It is also used to define the communication primitives through which messages are exchanged between such entities, i.e. the AMQP frame bodies . Implementations Specification","title":"Advanced-Message-Queuing-Protocol"},{"location":"Message-processing-system/Messaging-pattern/Message-queue/AMQP/#amqp","text":"","title":"AMQP"},{"location":"Message-processing-system/Messaging-pattern/Message-queue/AMQP/#_1","text":"https://www.amqp.org/ \u534f\u8bae\u6587\u6863\u4e0b\u8f7d\u7f51\u5740\uff1a https://www.amqp.org/resources/download","title":"\u5b98\u7f51"},{"location":"Message-processing-system/Messaging-pattern/Message-queue/AMQP/#wikipedia#advanced#message#queuing#protocol","text":"The Advanced Message Queuing Protocol ( AMQP ) is an open standard (\u5f00\u653e\u6807\u51c6) application layer protocol(\u5e94\u7528\u5c42\u534f\u8bae) for message-oriented middleware . The defining features of AMQP are message orientation, queuing, routing (including point-to-point and publish-and-subscribe ), reliability and security. NOTE: AMQP\u662f\u7c7b\u4f3c\u4e8eHTTP\u4e00\u6837\u7684\u5c5e\u4e8eOSI\u7684application layer protocol\u3002 \u601d\u8003\uff1a\u4f5c\u4e3a\u4e00\u79cd\u534f\u8bae\uff0c\u5176\u5b9e\u6211\u89c9\u5f97\u5e94\u8be5\u8981\u4ece\u5b83\u6240\u63d0\u4f9b\u7684\u7279\u6027\u6765\u5bf9\u5176\u8fdb\u884c\u628a\u63e1\uff0c\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u5c31\u5df2\u7ecf\u6982\u62ec\u4e86\u5176\u5173\u952e\u7279\u6027\uff1b\u4ee5\u53ca\u4f7f\u7528\u8fd9\u4e9b\u7279\u6027\uff0c\u80fd\u591f\u4e3a\u6211\u4eec\u89e3\u51b3\u54ea\u4e9b\u95ee\u9898\uff1b AMQP mandates\uff08\u8981\u6c42\uff09 the behavior of the messaging provider and client to the extent that implementations from different vendors are interoperable (AMQP\u8981\u6c42\u6d88\u606f\u4f20\u9012\u63d0\u4f9b\u8005\u548c\u5ba2\u6237\u7aef\u7684\u884c\u4e3a\u8fbe\u5230\u4e0d\u540c\u4f9b\u5e94\u5546\u7684\u5b9e\u73b0\u53ef\u4e92\u64cd\u4f5c\u7684\u7a0b\u5ea6), in the same way as SMTP , HTTP , FTP , etc. have created interoperable systems . Previous standardizations of middleware have happened at the API level (e.g. JMS ) and were focused on standardizing (\u6807\u51c6\u5316) programmer interaction with different middleware implementations, rather than on providing interoperability (\u53ef\u4ea4\u4e92\u6027) between multiple implementations.[ 2] Unlike JMS, which defines an API and a set of behaviors that a messaging implementation must provide, AMQP is a wire-level protocol . A wire-level protocol is a description of the format of the data that is sent across the network as a stream of bytes (\u5b57\u8282\u6d41\uff09. Consequently, any tool that can create and interpret messages that conform to this data format can interoperate with any other compliant tool irrespective of \uff08\u4e0d\u7ba1\uff09 implementation language. NOTE: \u534f\u8bae\u5b9a\u4e49\u4e86data format\uff0c\u6545\u53ea\u8981\u9075\u5faa\u8fd9\u4e2a\u534f\u8bae\uff0c\u90a3\u4e48\u4e0d\u540c\u7684\u5b9e\u73b0\u4e4b\u95f4\u5c31\u53ef\u4ee5\u8fdb\u884cinteroperate \u4e86\uff1b\u5176\u5b9e\u770b\u4e86\u4e0a\u9762\u7684\u8fd9\u6bb5\u8bdd\uff0c\u6211\u624d\u60f3\u8d77\u6765\uff0cprotocol\u5c31\u5e94\u5f53\u5982\u6b64\uff0c\u6bd4\u5982HTTP\u534f\u8bae\uff0c\u663e\u7136HTTP\u534f\u8bae\u7684\u5b9e\u73b0\u662f\u975e\u5e38\u4e4b\u591a\u7684\uff0c\u6bd4\u5982\u6211\u5728python\u4e2d\uff0c\u5728java\u4e2d\u662f\u53ef\u4ee5\u4f7f\u7528\u4e0d\u540c\u534f\u8bae\u7684\uff0c\u4f46\u662f\u4ed6\u4eec\u4e4b\u95f4\u662f\u53ef\u4ee5\u8fdb\u884c\u76f8\u4e92\u6c9f\u901a\u7684\uff1b","title":"wikipedia Advanced Message Queuing Protocol"},{"location":"Message-processing-system/Messaging-pattern/Message-queue/AMQP/#overview","text":"AMQP is a binary , application layer protocol\uff08\u4e8c\u8fdb\u5236\uff0c\u5e94\u7528\u5c42\u534f\u8bae\uff09, designed to efficiently support a wide variety of messaging applications and communication patterns. It provides flow controlled,[ 3] message-oriented communication with message-delivery guarantees such as at-most-once (where each message is delivered once or never), at-least-once (where each message is certain to be delivered, but may do so multiple times) and exactly-once (where the message will always certainly arrive and do so only once),[ 4] and authentication and/or encryption\uff08\u52a0\u5bc6\uff09 based on SASL and/or TLS .[ 5] It assumes an underlying reliable transport layer protocol such as Transmission Control Protocol (TCP).[ 6] The AMQP specification is defined in several layers: (i) a type system, (ii) a symmetric\uff08\u5bf9\u79f0\u7684\uff09, asynchronous protocol for the transfer of messages from one process to another, (iii) a standard, extensible message format and (iv) a set of standardised but extensible 'messaging capabilities.'","title":"Overview"},{"location":"Message-processing-system/Messaging-pattern/Message-queue/AMQP/#description#of#amqp#10","text":"","title":"Description of AMQP 1.0"},{"location":"Message-processing-system/Messaging-pattern/Message-queue/AMQP/#type#system","text":"AMQP defines a self-describing encoding scheme allowing interoperable representation of a wide range of commonly used types. It also allows typed data to be annotated with additional meaning,[ 17] for example a particular string value might be annotated so that it could be understood as a URL . Likewise a map value containing key-value pairs for 'name', 'address' etc., might be annotated as being a representation of a 'customer' type. The type-system is used to define a message format allowing standard and extended meta-data to be expressed and understood by processing entities. It is also used to define the communication primitives through which messages are exchanged between such entities, i.e. the AMQP frame bodies .","title":"Type system"},{"location":"Message-processing-system/Messaging-pattern/Message-queue/AMQP/#implementations","text":"","title":"Implementations"},{"location":"Message-processing-system/Messaging-pattern/Message-queue/AMQP/#specification","text":"","title":"Specification"},{"location":"Message-processing-system/Messaging-pattern/Message-queue/AMQP/Fan-out/","text":"Fan-out (software) In message-oriented middleware solutions, fan-out is a messaging pattern used to model an information exchange that implies the delivery (or spreading) of a message to one or multiple destinations possibly in parallel, and not halting the process that executes the messaging to wait for any response to that message.[ 1] [ 2] [ 3] Also fan-out in software construction means the number of classes used by a certain class or the number of methods called by a certain method.[ 4] References \"AMQP 0-9-1 Model Explained\" . RabbitMQ . \"Writing Request/Response Clients and Servers: Sending Asynchronous Messages\" . Oracle Tuxedo Documentation . TODO https://www.rabbitmq.com/tutorials/amqp-concepts.html \u601d\u8003\uff1aPublish\u2013subscribe pattern\u548cfan-out\u4e4b\u95f4\u7684\u5dee\u5f02\u6240\u5728\uff1b","title":"Introduction"},{"location":"Message-processing-system/Messaging-pattern/Message-queue/AMQP/Fan-out/#fan-out#software","text":"In message-oriented middleware solutions, fan-out is a messaging pattern used to model an information exchange that implies the delivery (or spreading) of a message to one or multiple destinations possibly in parallel, and not halting the process that executes the messaging to wait for any response to that message.[ 1] [ 2] [ 3] Also fan-out in software construction means the number of classes used by a certain class or the number of methods called by a certain method.[ 4]","title":"Fan-out (software)"},{"location":"Message-processing-system/Messaging-pattern/Message-queue/AMQP/Fan-out/#references","text":"\"AMQP 0-9-1 Model Explained\" . RabbitMQ . \"Writing Request/Response Clients and Servers: Sending Asynchronous Messages\" . Oracle Tuxedo Documentation .","title":"References"},{"location":"Message-processing-system/Messaging-pattern/Message-queue/AMQP/Fan-out/#todo","text":"https://www.rabbitmq.com/tutorials/amqp-concepts.html \u601d\u8003\uff1aPublish\u2013subscribe pattern\u548cfan-out\u4e4b\u95f4\u7684\u5dee\u5f02\u6240\u5728\uff1b","title":"TODO"},{"location":"Message-processing-system/Messaging-pattern/Pub-sub/","text":"Publish\u2013subscribe pattern https://aws.amazon.com/pub-sub-messaging/?nc1=h_ls https://cloud.google.com/pubsub/docs/overview https://cloud.google.com/pubsub/docs/admin wikipedia Publish\u2013subscribe pattern In software architecture , publish\u2013subscribe is a messaging pattern where senders of messages , called publishers , do not program the messages to be sent directly to specific receivers, called subscribers , but instead categorize published messages into classes without knowledge of which subscribers, if any, there may be. Similarly, subscribers express interest in one or more classes and only receive messages that are of interest, without knowledge of which publishers, if any, there are. Publish\u2013subscribe is a sibling\uff08\u5144\u59b9\uff09 of the message queue paradigm, and is typically one part of a larger message-oriented middleware system. Most messaging systems support both the pub/sub and message queue models in their API , e.g. Java Message Service (JMS). This pattern provides greater network scalability and a more dynamic network topology , with a resulting decreased flexibility to modify the publisher and the structure of the published data. Message filtering In the publish-subscribe model, subscribers typically receive only a subset of the total messages published. The process of selecting messages for reception and processing is called filtering . There are two common forms of filtering: topic-based and content-based . In a topic-based system, messages are published to \"topics\" or named logical channels . Subscribers in a topic-based system will receive all messages published to the topics to which they subscribe, and all subscribers to a topic will receive the same messages. The publisher is responsible for defining the classes of messages to which subscribers can subscribe. In a content-based system, messages are only delivered to a subscriber if the attributes or content of those messages matches constraints defined by the subscriber. The subscriber is responsible for classifying the messages. Some systems support a hybrid of the two; publishers post messages to a topic while subscribers register content-based subscriptions to one or more topics. Topologies In many pub/sub systems, publishers post messages to an intermediary message broker or event bus , and subscribers register subscriptions with that broker , letting the broker perform the filtering . The broker normally performs a store and forward function to route messages from publishers to subscribers. In addition, the broker may prioritize \uff08\u6309\u7167\u4f18\u5148\u7ea7\u987a\u5e8f\u5217\u51fa\uff09 messages in a queue before routing. Subscribers may register for specific messages at build time, initialization time or runtime. In GUI systems, subscribers can be coded to handle user commands (e.g., click of a button), which corresponds to build time registration. Some frameworks and software products use XML configuration files to register subscribers. These configuration files are read at initialization time. The most sophisticated alternative is when subscribers can be added or removed at runtime. This latter approach is used, for example, in database triggers , mailing lists , and RSS . The Data Distribution Service (DDS) middleware does not use a broker in the middle. Instead, each publisher and subscriber in the pub/sub system shares meta-data about each other via IP multicast . The publisher and the subscribers cache this information locally and route messages based on the discovery of each other in the shared cognizance.","title":"Introduction"},{"location":"Message-processing-system/Messaging-pattern/Pub-sub/#publishsubscribe#pattern","text":"https://aws.amazon.com/pub-sub-messaging/?nc1=h_ls https://cloud.google.com/pubsub/docs/overview https://cloud.google.com/pubsub/docs/admin","title":"Publish\u2013subscribe pattern"},{"location":"Message-processing-system/Messaging-pattern/Pub-sub/#wikipedia#publishsubscribe#pattern","text":"In software architecture , publish\u2013subscribe is a messaging pattern where senders of messages , called publishers , do not program the messages to be sent directly to specific receivers, called subscribers , but instead categorize published messages into classes without knowledge of which subscribers, if any, there may be. Similarly, subscribers express interest in one or more classes and only receive messages that are of interest, without knowledge of which publishers, if any, there are. Publish\u2013subscribe is a sibling\uff08\u5144\u59b9\uff09 of the message queue paradigm, and is typically one part of a larger message-oriented middleware system. Most messaging systems support both the pub/sub and message queue models in their API , e.g. Java Message Service (JMS). This pattern provides greater network scalability and a more dynamic network topology , with a resulting decreased flexibility to modify the publisher and the structure of the published data.","title":"wikipedia Publish\u2013subscribe pattern"},{"location":"Message-processing-system/Messaging-pattern/Pub-sub/#message#filtering","text":"In the publish-subscribe model, subscribers typically receive only a subset of the total messages published. The process of selecting messages for reception and processing is called filtering . There are two common forms of filtering: topic-based and content-based . In a topic-based system, messages are published to \"topics\" or named logical channels . Subscribers in a topic-based system will receive all messages published to the topics to which they subscribe, and all subscribers to a topic will receive the same messages. The publisher is responsible for defining the classes of messages to which subscribers can subscribe. In a content-based system, messages are only delivered to a subscriber if the attributes or content of those messages matches constraints defined by the subscriber. The subscriber is responsible for classifying the messages. Some systems support a hybrid of the two; publishers post messages to a topic while subscribers register content-based subscriptions to one or more topics.","title":"Message filtering"},{"location":"Message-processing-system/Messaging-pattern/Pub-sub/#topologies","text":"In many pub/sub systems, publishers post messages to an intermediary message broker or event bus , and subscribers register subscriptions with that broker , letting the broker perform the filtering . The broker normally performs a store and forward function to route messages from publishers to subscribers. In addition, the broker may prioritize \uff08\u6309\u7167\u4f18\u5148\u7ea7\u987a\u5e8f\u5217\u51fa\uff09 messages in a queue before routing. Subscribers may register for specific messages at build time, initialization time or runtime. In GUI systems, subscribers can be coded to handle user commands (e.g., click of a button), which corresponds to build time registration. Some frameworks and software products use XML configuration files to register subscribers. These configuration files are read at initialization time. The most sophisticated alternative is when subscribers can be added or removed at runtime. This latter approach is used, for example, in database triggers , mailing lists , and RSS . The Data Distribution Service (DDS) middleware does not use a broker in the middle. Instead, each publisher and subscriber in the pub/sub system shares meta-data about each other via IP multicast . The publisher and the subscribers cache this information locally and route messages based on the discovery of each other in the shared cognizance.","title":"Topologies"},{"location":"Message-processing-system/Stream-based/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u8ba8\u8bbaStream-based-message-processing-system\uff0c\u5373\u57fa\u4e8estream\u6784\u5efa\u7684message processing system\uff0c\u4e0b\u9762\u662f\u6211\u4eec\u7ecf\u5e38\u4f1a\u770b\u5230\u7684stream-based message processing system\u7684\u53e6\u5916\u4e00\u4e9b\u63d0\u6cd5: 1) Distributed Stream Processing \u5728\u6587\u7ae0linkedin Spark Streaming vs Flink vs Storm vs Kafka Streams vs Samza : Choose Your Stream Processing Framework : This is why Distributed Stream Processing has become very popular in Big Data world. \u5728\u6587\u7ae0scottlogic Comparing Apache Spark, Storm, Flink and Samza stream processing engines - Part 1 : Distributed stream processing engines have been on the rise in the last few years, first Hadoop became popular as a batch processing engine, then focus shifted towards stream processing engines. What is stream-based-message-processing-system? \u9996\u5148\u9700\u8981\u7406\u89e3stream\u7684\u6982\u5ff5\uff0c\u5728\u5de5\u7a0bdiscrete\u7684 Relation-structure-computation\\Model\\Stream \u7ae0\u8282\u5bf9stream\u6982\u5ff5\u8fdb\u884c\u4e86\u603b\u7ed3\u3002\u5176\u6b21\u9700\u8981\u7406\u89e3message/event stream\u7684\u542b\u4e49: kafka What is event streaming? Event streaming is the digital equivalent of the human body's central nervous system (\u4e2d\u67a2\u795e\u7ecf\u7cfb\u7edf). It is the technological foundation for the 'always-on' world where businesses are increasingly software-defined and automated, and where the user of software is more software. Technically speaking, event streaming is the practice of capturing data in real-time from event sources like databases, sensors, mobile devices, cloud services, and software applications in the form of streams of events ; storing these event streams durably for later retrieval; manipulating, processing, and reacting to the event streams in real-time as well as retrospectively(\u56de\u987e); and routing the event streams to different destination technologies as needed. Event streaming thus ensures a continuous flow and interpretation of data so that the right information is at the right place, at the right time. wikipedia Event stream processing \u65f6\u4ee3\u80cc\u666f \u5728linkedin Spark Streaming vs Flink vs Storm vs Kafka Streams vs Samza : Choose Your Stream Processing Framework \u4e2d\u5bf9stream-based message processing framework\u5174\u8d77\u7684\u65f6\u4ee3\u80cc\u666f\u8fdb\u884c\u4e86\u975e\u5e38\u597d\u7684\u603b\u7ed3: According to a recent report by IBM Marketing cloud , \u201c 90 percent of the data in the world today has been created in the last two years alone, creating 2.5 quintillion bytes of data every day \u2013 and with new devices, sensors and technologies emerging, the data growth rate will likely accelerate even more \u201d. Technically this means our Big Data Processing world is going to be more complex and more challenging. And a lot of use cases (e.g. mobile app ads, fraud detection, cab booking, patient monitoring,etc) need data processing in real-time, as and when data arrives, to make quick actionable decisions. This is why Distributed Stream Processing has become very popular in Big Data world. Implementations \u5173\u4e8estream-based message processing system\uff0c\u53c2\u89c1\u4e0b\u9762\u7684\u8f83\u597d\u7684\u6587\u7ae0: 1) linkedin Spark Streaming vs Flink vs Storm vs Kafka Streams vs Samza : Choose Your Stream Processing Framework 2) scottlogic Comparing Apache Spark, Storm, Flink and Samza stream processing engines - Part 1 3) upsolver 7 Popular Stream Processing Frameworks Compared \u3002 4) zhuanlan Apache \u4e24\u4e2a\u5f00\u6e90\u9879\u76ee\u6bd4\u8f83\uff1aFlink vs Spark 5) Flink\u53ca\u4e3b\u6d41\u6d41\u6846\u67b6\u6bd4\u8f83","title":"Introduction"},{"location":"Message-processing-system/Stream-based/#_1","text":"\u672c\u7ae0\u8ba8\u8bbaStream-based-message-processing-system\uff0c\u5373\u57fa\u4e8estream\u6784\u5efa\u7684message processing system\uff0c\u4e0b\u9762\u662f\u6211\u4eec\u7ecf\u5e38\u4f1a\u770b\u5230\u7684stream-based message processing system\u7684\u53e6\u5916\u4e00\u4e9b\u63d0\u6cd5: 1) Distributed Stream Processing \u5728\u6587\u7ae0linkedin Spark Streaming vs Flink vs Storm vs Kafka Streams vs Samza : Choose Your Stream Processing Framework : This is why Distributed Stream Processing has become very popular in Big Data world. \u5728\u6587\u7ae0scottlogic Comparing Apache Spark, Storm, Flink and Samza stream processing engines - Part 1 : Distributed stream processing engines have been on the rise in the last few years, first Hadoop became popular as a batch processing engine, then focus shifted towards stream processing engines.","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Message-processing-system/Stream-based/#what#is#stream-based-message-processing-system","text":"\u9996\u5148\u9700\u8981\u7406\u89e3stream\u7684\u6982\u5ff5\uff0c\u5728\u5de5\u7a0bdiscrete\u7684 Relation-structure-computation\\Model\\Stream \u7ae0\u8282\u5bf9stream\u6982\u5ff5\u8fdb\u884c\u4e86\u603b\u7ed3\u3002\u5176\u6b21\u9700\u8981\u7406\u89e3message/event stream\u7684\u542b\u4e49:","title":"What is stream-based-message-processing-system?"},{"location":"Message-processing-system/Stream-based/#kafka#what#is#event#streaming","text":"Event streaming is the digital equivalent of the human body's central nervous system (\u4e2d\u67a2\u795e\u7ecf\u7cfb\u7edf). It is the technological foundation for the 'always-on' world where businesses are increasingly software-defined and automated, and where the user of software is more software. Technically speaking, event streaming is the practice of capturing data in real-time from event sources like databases, sensors, mobile devices, cloud services, and software applications in the form of streams of events ; storing these event streams durably for later retrieval; manipulating, processing, and reacting to the event streams in real-time as well as retrospectively(\u56de\u987e); and routing the event streams to different destination technologies as needed. Event streaming thus ensures a continuous flow and interpretation of data so that the right information is at the right place, at the right time.","title":"kafka What is event streaming?"},{"location":"Message-processing-system/Stream-based/#wikipedia#event#stream#processing","text":"","title":"wikipedia Event stream processing"},{"location":"Message-processing-system/Stream-based/#_2","text":"\u5728linkedin Spark Streaming vs Flink vs Storm vs Kafka Streams vs Samza : Choose Your Stream Processing Framework \u4e2d\u5bf9stream-based message processing framework\u5174\u8d77\u7684\u65f6\u4ee3\u80cc\u666f\u8fdb\u884c\u4e86\u975e\u5e38\u597d\u7684\u603b\u7ed3: According to a recent report by IBM Marketing cloud , \u201c 90 percent of the data in the world today has been created in the last two years alone, creating 2.5 quintillion bytes of data every day \u2013 and with new devices, sensors and technologies emerging, the data growth rate will likely accelerate even more \u201d. Technically this means our Big Data Processing world is going to be more complex and more challenging. And a lot of use cases (e.g. mobile app ads, fraud detection, cab booking, patient monitoring,etc) need data processing in real-time, as and when data arrives, to make quick actionable decisions. This is why Distributed Stream Processing has become very popular in Big Data world.","title":"\u65f6\u4ee3\u80cc\u666f"},{"location":"Message-processing-system/Stream-based/#implementations","text":"\u5173\u4e8estream-based message processing system\uff0c\u53c2\u89c1\u4e0b\u9762\u7684\u8f83\u597d\u7684\u6587\u7ae0: 1) linkedin Spark Streaming vs Flink vs Storm vs Kafka Streams vs Samza : Choose Your Stream Processing Framework 2) scottlogic Comparing Apache Spark, Storm, Flink and Samza stream processing engines - Part 1 3) upsolver 7 Popular Stream Processing Frameworks Compared \u3002 4) zhuanlan Apache \u4e24\u4e2a\u5f00\u6e90\u9879\u76ee\u6bd4\u8f83\uff1aFlink vs Spark 5) Flink\u53ca\u4e3b\u6d41\u6d41\u6846\u67b6\u6bd4\u8f83","title":"Implementations"},{"location":"Message-processing-system/Stream-based/Apache-Flink/","text":"Flink wikipedia Apache Flink Apache Flink is an open-source, unified stream-processing and batch-processing framework developed by the Apache Software Foundation. The core of Apache Flink is a distributed streaming data-flow engine written in Java and Scala. Flink executes arbitrary dataflow programs in a data-parallel and pipelined manner. Flink's pipelined runtime system enables the execution of bulk/batch and stream processing programs. Furthermore, Flink's runtime supports the execution of iterative algorithms natively. Apache Flink\u00ae - Stateful Computations over Data Streams","title":"Introduction"},{"location":"Message-processing-system/Stream-based/Apache-Flink/#flink","text":"","title":"Flink"},{"location":"Message-processing-system/Stream-based/Apache-Flink/#wikipedia#apache#flink","text":"Apache Flink is an open-source, unified stream-processing and batch-processing framework developed by the Apache Software Foundation. The core of Apache Flink is a distributed streaming data-flow engine written in Java and Scala. Flink executes arbitrary dataflow programs in a data-parallel and pipelined manner. Flink's pipelined runtime system enables the execution of bulk/batch and stream processing programs. Furthermore, Flink's runtime supports the execution of iterative algorithms natively.","title":"wikipedia Apache Flink"},{"location":"Message-processing-system/Stream-based/Apache-Flink/#apache#flink#-#stateful#computations#over#data#streams","text":"","title":"Apache Flink\u00ae - Stateful Computations over Data Streams"},{"location":"Message-processing-system/library-Celery/","text":"Celery : Distributed Task Queue","title":"Introduction"},{"location":"Message-processing-system/library-Celery/#celery#distributed#task#queue","text":"","title":"Celery: Distributed Task Queue"},{"location":"Model/","text":"Model \u5efa\u7acb\u62bd\u8c61\u7684model\u6765\u5bf9parallel computing\u3001concurrent computing\u3001distributed computing\u4e2d\u6d89\u53ca\u7684\u95ee\u9898\u8fdb\u884c\u63cf\u8ff0\u3002 \u4e32\u884c\u4e0e\u5e76\u884c \u4e32\u884c\u7684\u82f1\u8bed\u662fserial\uff1b \u663e\u7136\uff0c\u4e32\u884c\u548c\u5e76\u884c\u662f\u76f8\u53cd\u7684\u3002","title":"Introduction"},{"location":"Model/#model","text":"\u5efa\u7acb\u62bd\u8c61\u7684model\u6765\u5bf9parallel computing\u3001concurrent computing\u3001distributed computing\u4e2d\u6d89\u53ca\u7684\u95ee\u9898\u8fdb\u884c\u63cf\u8ff0\u3002","title":"Model"},{"location":"Model/#_1","text":"\u4e32\u884c\u7684\u82f1\u8bed\u662fserial\uff1b \u663e\u7136\uff0c\u4e32\u884c\u548c\u5e76\u884c\u662f\u76f8\u53cd\u7684\u3002","title":"\u4e32\u884c\u4e0e\u5e76\u884c"},{"location":"Model/Communicating-Sequential-Processes/","text":"Communicating Sequential Processes \u662f\u5728\u4e86\u89e3Golang\u65f6\uff0c\u53d1\u73b0\u7684\u8fd9\u4e2a\u6982\u5ff5: golangprograms Golang Concurrency : Communicating Sequential Processes, or CSP for short, is used to describe how systems that feature multiple concurrent models should interact with one another. It typically relies heavily on using channels as a medium for passing messages between two or more concurrent processes, and is the underlying mantra of Golang. NOTE: \u4e0a\u8ff0 \"multiple concurrent models\"\u5176\u5b9e\u5bf9\u5e94\u7684\u5c31\u662fmultiple model\u3002 wikipedia Communicating Sequential Processes In computer science, communicating sequential processes (CSP) is a formal language for describing patterns of interaction in concurrent systems. It is a member of the family of mathematical theories of concurrency known as process algebras, or process calculi, based on message passing via channels. CSP was highly influential in the design of the occam programming language and also influenced the design of programming languages such as Limbo, RaftLib, Go, Crystal, and Clojure's core.async. CSP-style concurrency Python: zh217 / aiochan","title":"Introduction"},{"location":"Model/Communicating-Sequential-Processes/#communicating#sequential#processes","text":"\u662f\u5728\u4e86\u89e3Golang\u65f6\uff0c\u53d1\u73b0\u7684\u8fd9\u4e2a\u6982\u5ff5:","title":"Communicating Sequential Processes"},{"location":"Model/Communicating-Sequential-Processes/#golangprograms#golang#concurrency","text":"Communicating Sequential Processes, or CSP for short, is used to describe how systems that feature multiple concurrent models should interact with one another. It typically relies heavily on using channels as a medium for passing messages between two or more concurrent processes, and is the underlying mantra of Golang. NOTE: \u4e0a\u8ff0 \"multiple concurrent models\"\u5176\u5b9e\u5bf9\u5e94\u7684\u5c31\u662fmultiple model\u3002","title":"golangprograms Golang Concurrency:"},{"location":"Model/Communicating-Sequential-Processes/#wikipedia#communicating#sequential#processes","text":"In computer science, communicating sequential processes (CSP) is a formal language for describing patterns of interaction in concurrent systems. It is a member of the family of mathematical theories of concurrency known as process algebras, or process calculi, based on message passing via channels. CSP was highly influential in the design of the occam programming language and also influenced the design of programming languages such as Limbo, RaftLib, Go, Crystal, and Clojure's core.async.","title":"wikipedia Communicating Sequential Processes"},{"location":"Model/Communicating-Sequential-Processes/#csp-style#concurrency","text":"Python: zh217 / aiochan","title":"CSP-style concurrency"},{"location":"Model/Fork%E2%80%93join-model/","text":"Fork\u2013join model wikipedia Fork\u2013join model In parallel computing , the fork\u2013join model is a way of setting up and executing parallel programs, such that execution branches off in parallel at designated points in the program, to \"join\" (merge) at a subsequent point and resume sequential execution. Parallel sections may fork recursively until a certain task granularity is reached. Fork\u2013join can be considered a parallel design pattern .[ 1] :209 ff. It was formulated as early as 1963.[ 2] [ 3] NOTE: \u4e0a\u9762\u63d0\u5230\u7684 \" to \"join\" (merge) \" \u975e\u5e38\u9002\u5408\u4f7f\u7528barrier\u6765\u5b9e\u73b0\u3002 By nesting fork\u2013join computations recursively, one obtains a parallel version of the divide and conquer paradigm, expressed by the following generic pseudocode :[ 4] solve ( problem ) : if problem is small enough : solve problem directly ( sequential algorithm ) else : for part in subdivide ( problem ) fork subtask to solve ( part ) join all subtasks spawned in previous loop return combined results NOTE: 1\u3001\u5728APUE\u4e2d\uff0c\u6709\u8fd9\u6837\u7684\u4f8b\u5b50\uff0c\u5176\u4e2d\u662f\u4f7f\u7528\u7684 pthread_barrier 2\u3001\u540e\u7eed\u5c06\u8fd9\u79cd\u7528\u6cd5\u79f0\u4e3afork-join-parallel-divide-and-conquer Summary Fork-join model in OS Process/thread \u5728 parallel computing \u4e2d\uff0c\u666e\u904d\u91c7\u7528fork-join model\uff0c\u5f53entity\u662fprocess\u3001thread\u65f6\uff0cOS\u63d0\u4f9b\u4e86system call\u6216api\u6765\u5b9e\u73b0\u8fd9\u79cdmodel\uff0c\u5173\u4e8e\u6b64\uff0c\u5728\u5de5\u7a0b Linux-OS \u7684 Programming\\Process \u7ae0\u8282\u4e2d\u8fdb\u884c\u4e86\u603b\u7ed3\uff0c\u5728apue\u5728\u4e5f\u8fdb\u884c\u4e86\u603b\u7ed3\u3002 Barrier barrier \u90fd\u53ef\u4ee5\u770b\u505a\u662f\u8fd9\u79cd\u6a21\u578b\u3002 Fork-join model\u3001divide-conquer and merge algorithm Divide-conquer \u662f\u4e00\u79cd\u7b97\u6cd5\u8bbe\u8ba1\u601d\u60f3\uff0c merge algorithm \u662f\u4e00\u79cd\u4f7f\u7528divide conquer\u601d\u60f3\u7684\u7b97\u6cd5\uff0c\u5b83\u4e3b\u8981\u5e94\u7528\u4e8esorting\u9886\u57df\uff1bFork-join model\u4e3b\u8981\u662f\u63cf\u8ff0\u7684parallel computing\u7684\u6a21\u578b\uff0c\u5229\u7528\u5b83\u6765\u5b9e\u73b0\u4e00\u4e9bdivide-conquer\u7b97\u6cd5\u7684concurrent\u7248\u672c\uff0cAPUE 11.6.8\u8282\u7684\u4f8b\u5b50\u5c31\u662ffork-join model\u6765concurrent\u5730\u6765\u5b9e\u73b0merge-algorithm\u3002 Fork-join model\u548cdivide-conquer\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5728\u7ef4\u57fa\u767e\u79d1 Fork\u2013join model \u4e2d\u8fdb\u884c\u4e86\u603b\u7ed3\u3002","title":"Introduction"},{"location":"Model/Fork%E2%80%93join-model/#forkjoin#model","text":"","title":"Fork\u2013join model"},{"location":"Model/Fork%E2%80%93join-model/#wikipedia#forkjoin#model","text":"In parallel computing , the fork\u2013join model is a way of setting up and executing parallel programs, such that execution branches off in parallel at designated points in the program, to \"join\" (merge) at a subsequent point and resume sequential execution. Parallel sections may fork recursively until a certain task granularity is reached. Fork\u2013join can be considered a parallel design pattern .[ 1] :209 ff. It was formulated as early as 1963.[ 2] [ 3] NOTE: \u4e0a\u9762\u63d0\u5230\u7684 \" to \"join\" (merge) \" \u975e\u5e38\u9002\u5408\u4f7f\u7528barrier\u6765\u5b9e\u73b0\u3002 By nesting fork\u2013join computations recursively, one obtains a parallel version of the divide and conquer paradigm, expressed by the following generic pseudocode :[ 4] solve ( problem ) : if problem is small enough : solve problem directly ( sequential algorithm ) else : for part in subdivide ( problem ) fork subtask to solve ( part ) join all subtasks spawned in previous loop return combined results NOTE: 1\u3001\u5728APUE\u4e2d\uff0c\u6709\u8fd9\u6837\u7684\u4f8b\u5b50\uff0c\u5176\u4e2d\u662f\u4f7f\u7528\u7684 pthread_barrier 2\u3001\u540e\u7eed\u5c06\u8fd9\u79cd\u7528\u6cd5\u79f0\u4e3afork-join-parallel-divide-and-conquer","title":"wikipedia Fork\u2013join model"},{"location":"Model/Fork%E2%80%93join-model/#summary","text":"","title":"Summary"},{"location":"Model/Fork%E2%80%93join-model/#fork-join#model#in#os","text":"","title":"Fork-join model in OS"},{"location":"Model/Fork%E2%80%93join-model/#processthread","text":"\u5728 parallel computing \u4e2d\uff0c\u666e\u904d\u91c7\u7528fork-join model\uff0c\u5f53entity\u662fprocess\u3001thread\u65f6\uff0cOS\u63d0\u4f9b\u4e86system call\u6216api\u6765\u5b9e\u73b0\u8fd9\u79cdmodel\uff0c\u5173\u4e8e\u6b64\uff0c\u5728\u5de5\u7a0b Linux-OS \u7684 Programming\\Process \u7ae0\u8282\u4e2d\u8fdb\u884c\u4e86\u603b\u7ed3\uff0c\u5728apue\u5728\u4e5f\u8fdb\u884c\u4e86\u603b\u7ed3\u3002","title":"Process/thread"},{"location":"Model/Fork%E2%80%93join-model/#barrier","text":"barrier \u90fd\u53ef\u4ee5\u770b\u505a\u662f\u8fd9\u79cd\u6a21\u578b\u3002","title":"Barrier"},{"location":"Model/Fork%E2%80%93join-model/#fork-join#modeldivide-conquer#and#merge#algorithm","text":"Divide-conquer \u662f\u4e00\u79cd\u7b97\u6cd5\u8bbe\u8ba1\u601d\u60f3\uff0c merge algorithm \u662f\u4e00\u79cd\u4f7f\u7528divide conquer\u601d\u60f3\u7684\u7b97\u6cd5\uff0c\u5b83\u4e3b\u8981\u5e94\u7528\u4e8esorting\u9886\u57df\uff1bFork-join model\u4e3b\u8981\u662f\u63cf\u8ff0\u7684parallel computing\u7684\u6a21\u578b\uff0c\u5229\u7528\u5b83\u6765\u5b9e\u73b0\u4e00\u4e9bdivide-conquer\u7b97\u6cd5\u7684concurrent\u7248\u672c\uff0cAPUE 11.6.8\u8282\u7684\u4f8b\u5b50\u5c31\u662ffork-join model\u6765concurrent\u5730\u6765\u5b9e\u73b0merge-algorithm\u3002 Fork-join model\u548cdivide-conquer\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5728\u7ef4\u57fa\u767e\u79d1 Fork\u2013join model \u4e2d\u8fdb\u884c\u4e86\u603b\u7ed3\u3002","title":"Fork-join model\u3001divide-conquer and merge algorithm"},{"location":"Model/Multiple-model/","text":"Multiple model \u672c\u6587\u4f7f\u7528\u4e00\u4e2a\u7edf\u4e00\u7684\u6a21\u578b\u6765\u5bf9\u5bf9parallel computing\u3001concurrent computing\u3001distributed computing\u8fdb\u884c\u63cf\u8ff0\uff0c\u8fd9\u4e2a\u6a21\u578b\u547d\u540d\u4e3a\u201c multiple model \u201d\u3002 What is multiple model? \u5728\u6709\u4e86parallel computing\u3001concurrent computing\u3001distributed computing\u7684\u4e00\u4e9b\u7406\u8bba\u77e5\u8bc6\u540e\uff0c\u6211\u4eec\u4f1a\u53d1\u73b0\u5b83\u4eec\u6d89\u53ca\u5230\u4e86\u975e\u5e38\u591a\u7c7b\u4f3c\u7684\u6982\u5ff5\uff08\u95ee\u9898\uff09\uff1a race condition atomic communication consistency ...... \u8fd9\u662f\u56e0\u4e3aparallel computing\u3001concurrent computing\u3001distributed computing\u90fd\u6d89\u53ca\u201c \u591a\u4e2a \u201delement\u6216entity\uff08\u8fd9\u4e9belement\u5e76\u53d1\u6216\u5e76\u884c\u5730\u8fd0\u884c\u4e2d\uff09\uff0c\u5b83\u4eec\u53ef\u80fd\u540c\u65f6\u64cd\u4f5cshared data(shared memory)\uff0c\u6240\u4ee5\u5b83\u4eec\u90fd\u4f1a\u6d89\u53ca\u4e0e**\u591a\u4e2a**(multiple)\u76f8\u5173\u7684\u95ee\u9898\uff0c\u6211\u4eec\u5c06\u4f7f\u7528\u201c multiple model \u201d\u6765\u5bf9\u5b83\u4eec\u8fdb\u884c\u7edf\u4e00\u7684\u63cf\u8ff0\u3002 Concurrency/parallel element/unit \u672c\u8282\u603b\u7ed3\u5728\u5404\u79cd\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5concurrent/parallel\u6267\u884c/\u53d1\u751f\u7684element\uff0c\u5176\u4e2d\u90e8\u5206\u53c2\u8003\u4e86wikipedia Parallel computing \u4e2d\u603b\u7ed3\u7684\u53ef\u80fd\u7684element\uff1a 1\u3001machine/server \u4e3b\u8981\u662fdistributed computing\u9886\u57df\u3002 2\u3001 Process \u4e3b\u8981\u662fmultiple process\u3002 3\u3001 Thread \u4e3b\u8981\u662fmultiple thread\u3002 4\u3001 Fiber \u4e3b\u8981\u662fcoroutine\u3002 5\u3001 Instruction window NOTE: \u7ef4\u57fa\u767e\u79d1\u8fd9\u91cc\u4f7f\u7528\u7684\u662f Instruction window \uff0c\u6211\u6709\u4e9b\u4e0d\u61c2\uff0c\u6211\u89c9\u5f97\u4f7f\u7528 instruction \u4f1a\u66f4\u52a0\u51c6\u786e\u3002 6\u3001network connection \u4e3b\u8981\u662fconcurrent server\u3002 7\u3001transaction \u4e3b\u8981\u662fDBMS\u4e2d\u3002 8\u3001...... Shared data \u53c2\u89c1 Shared-data \u7ae0\u8282\u3002 Operation on shared data \u5bf9shared data\u7684\u64cd\u4f5c\u53ef\u4ee5\u6982\u62ec\u4e3a\u5982\u4e0b\u4e24\u79cd: 1\u3001read 2\u3001write \u5173\u4e8eread\u3001write\uff0c\u5728 ./Read-and-write \u4e2d\uff0c\u4f1a\u8fdb\u884c\u4e13\u95e8\u603b\u7ed3\u3002 Relation: many-to-one Many: multiple entity One: shared data Problems **\u591a\u4e2a**\u6bd4**\u5355\u4e2a**\u7684\u590d\u6742\u5ea6\u8981\u9ad8\u5f88\u591a\uff0c\u5728**multiple model**\u4e2d\u5b58\u5728\u7740\u4e00\u7cfb\u5217\u95ee\u9898\uff1a \u591a\u4e2aelement\u4e4b\u95f4\u7684race \u591a\u4e2aelement\u4e4b\u95f4\u5982\u4f55\u8fdb\u884ccommunication\uff08\u6d88\u606f\u4f20\u9012\uff0c\u8fdb\u884c\u6c9f\u901a\uff09 \u591a\u4e2aelement\u4e4b\u95f4\u5982\u4f55\u8fdb\u884ccoordination\uff08\u534f\u4f5c\u3001\u534f\u8c03\uff09 \u591a\u4e2aentity\u4e4b\u95f4\u5982\u4f55\u8fbe\u6210consensus\uff08\u5171\u8bc6\uff09 ...... \u4e0b\u9762\u662f\u4e00\u4e9b\u7528\u4e8e\u8bf4\u660eproblem\u7684\u8bcd\u8bed: 1\u3001memory corruption \u6765\u6e90: microsoft Lockless Programming Considerations for Xbox 360 and Microsoft Windows From single machine to multiple machine NOTE: \u53c2\u89c1\u300aOne-to-many\u300b Single machine \u5355\u4e2aprocess\uff1a\u5b83\u7684element\u5c31\u662fthread \u591a\u4e2aprocess\uff1a\u5b83\u7684element\u5c31\u662fprocess Multiple machine \u591a\u4e2aprocess\uff1a\u5b83\u7684element\u5c31\u662fprocess Nature \u4ece\u4e0a\u5230\u4e0b\uff0c\u95ee\u9898\u8d8a\u6765\u8d8a\u590d\u6742\uff0c\u5728\u6bcf\u4e00\u5c42\u7ea7\uff0c\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\u7684\u6280\u672f\u624b\u6bb5\u90fd\u662f\u4e0d\u540c\u7684\uff0c\u672c\u8d28\u4e0a\u5b83\u4eec\u6240\u89e3\u51b3\u7684\u90fd\u662f\u76f8\u540c\u7c7b\u578b\u7684\u95ee\u9898\uff0c\u8fd9\u662f\u6211\u4eec\u5efa\u7acb\u5176multiple model\u7684\u4ef7\u503c\u6240\u5728\u3002 \u5f53\u7136\uff0c\u968f\u7740\u8d8a\u6765\u8d8a\u590d\u6742\uff0c\u6bcf\u4e00\u79cd\u4e5f\u90fd\u4f1a\u6d89\u53ca\u5230\u7b80\u5355\u95ee\u9898\u4e2d\u4e0d\u5b58\u5728\u7684\u95ee\u9898\u3002 Concurrency control \u5728multiple model\u4e2d\uff0c\u8fdb\u884cConcurrency control\u662f\u975e\u5e38\u91cd\u8981\u7684\u3002 Communication \u5176\u5b9e\u5c31\u662fentity\u4e4b\u95f4\u7684communication\u3002 Consistency \u5173\u4e8e\u6b64\u7684\u975e\u5e38\u597d\u7684\u8bba\u8ff0: zhuanlan.zhihu \u9ad8\u5e76\u53d1\u7f16\u7a0b--\u591a\u5904\u7406\u5668\u7f16\u7a0b\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898(\u4e0a) \u5728\u5206\u5e03\u5f0f\u5b58\u50a8\u7cfb\u7edf\u4e2d\uff0c\u4e3a\u4e86\u63d0\u9ad8**\u53ef\u9760\u6027**\uff0c\u901a\u5e38\u4f1a\u5f15\u5165\u591a\u4e2a\u526f\u672c\uff0c\u591a\u4e2a\u526f\u672c\u9700\u8981\u5411\u7528\u6237\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u5185\u5bb9\u3002\u8fd9\u5f88\u81ea\u7136\u7684\u8ba9\u4eba\u60f3\u5230\u5982\u4f55\u786e\u4fdd\u591a\u526f\u672c\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u3002\u4e3a\u6b64\u4e5f\u6709\u4e86paxos\u548craft\u7b49\u4fdd\u8bc1\u591a\u526f\u672c\u4e4b\u95f4\u4e00\u81f4\u6027\u7684\u534f\u8bae\u3002\u5f53\u6211\u4eec\u5728\u4e00\u4e2a\u591a\u5904\u7406\u5668\u673a\u5668\u4e0a\u7f16\u7a0b\u65f6\u6211\u4eec\u901a\u5e38\u4f1a\u5ffd\u7565\u5728\u591a\u5904\u7406\u5668\u73af\u5883\u4e0b\u7684\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u56e0\u4e3a\u7cfb\u7edf\u5df2\u7ecf\u4e3a\u6211\u4eec\u505a\u597d\u4e86\u57fa\u672c\u7684**\u4e00\u81f4\u6027**\u4fdd\u8bc1\uff0c\u5f53\u5b58\u5728\u4e00\u81f4\u6027\u95ee\u9898\u7684\u65f6\u5019\u4e0a\u5c42\u7f16\u7a0b\u8bed\u8a00\u4e5f\u63d0\u4f9b\u4e86\u5177\u5907\u4e00\u81f4\u6027\u8bed\u610f\u7684\u63a5\u53e3\uff0c\u53ea\u662f\u6211\u4eec\u5728\u7f16\u7a0b\u4e2d\u5e76\u6ca1\u6709\u610f\u8bc6\u5230\u8fd9\u4e9b**\u63a5\u53e3**\u4e0e**\u4e00\u81f4\u6027**\u7684\u5173\u7cfb\u3002\u65e0\u8bba\u662f\u5206\u5e03\u5f0f\u5b58\u50a8\u8fd8\u662f\u591a\u5904\u7406\u5668\u7f16\u7a0b\u90fd\u6709\u4e00\u4e2a\u5171\u540c\u70b9\uff0c\u5c31\u662f\u4f1a\u6d89\u53ca\u5171\u4eab\u5bf9\u8c61\u7684\u64cd\u4f5c\u3002 NOTE: 1\u3001\"\u53ef\u9760\u6027\"\uff0c\u5176\u5b9e\u5c31\u662fHA \"\u591a\u4e2a\u526f\u672c\"\uff0c\u5176\u5b9e\u5c31\u662fmaster-slave \"\u4e00\u81f4\u7684\u5185\u5bb9\"\uff0c\u5176\u5b9e\u5c31\u662fconsistency \"\u5171\u4eab\u5bf9\u8c61\"\u5176\u5b9e\u5c31\u662fmultiple model\u4e2d\u7684shared data 2\u3001\u4e0a\u8ff0\u5176\u5b9e\u53ef\u4ee5\u4f7f\u7528multiple model-shared data\u6765\u8fdb\u884c\u63cf\u8ff0 \u4e00\u65e6\u51fa\u73b0\u5171\u4eab\uff0c\u5c31\u4f1a\u51fa\u73b0\u6b63\u786e\u6027\u7684\u95ee\u9898\uff0c\u90a3\u4e48\u5982\u4f55\u5b9a\u4e49\u5728\u5e76\u53d1\u4e2d\u64cd\u4f5c\u5171\u4eab\u5bf9\u8c61\u7684\u6b63\u786e\u6027\uff0c\u8fd9\u5c31\u662f\u4e00\u81f4\u6027\u534f\u8bae\u7684\u4efb\u52a1\u4e86\u3002 \u672c\u6587\u4e3b\u8981\u9488\u5bf9\u591a\u5904\u7406\u5668\u7cfb\u7edf\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\u8fdb\u884c\u4e86\u4e00\u4e9b\u603b\u7ed3\uff0c\u5bf9\u4e8e\u5206\u5e03\u5f0f\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\u4f1a\u5728\u540e\u9762\u6587\u7ae0\u4e2d\u603b\u7ed3\u3002 \u591a\u5904\u7406\u5668\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\u6e90\u4e8e\u5e76\u53d1\uff0c\u6e90\u4e8e\u5171\u4eab\u3002\u5bf9\u4e8e\u5171\u4eab\u5185\u5b58\u5e76\u53d1\u64cd\u4f5c\u4e0b\u7684\u6b63\u786e\u6027\u4fdd\u8bc1\u662f\u786c\u4ef6\u8bbe\u8ba1\u8005\u9700\u8981\u63d0\u4f9b\u7ed9\u4e0a\u5c42\u5f00\u53d1\u4eba\u5458\u6700\u91cd\u8981\u7684\u4fdd\u8bc1\u3002\u5bf9\u4e8e\u4e0a\u5c42\u5f00\u53d1\u4eba\u5458\u6765\u8bf4\uff0c\u7cfb\u7edf\u5185\u90e8\u7684\u4e00\u81f4\u6027\u662f\u900f\u660e\u7684\uff0c\u4f46\u662f\u53bb\u4e86\u89e3\u7cfb\u7edf\u5185\u90e8\u4e00\u81f4\u6027\u7684\u8bbe\u8ba1\u53ca\u539f\u7406\u6709\u5229\u4e8e\u6211\u4eec\u66f4\u80fd\u591f\u9762\u5411\u673a\u5668\u7f16\u7a0b\uff0c\u5199\u51fa\u6b63\u786e\u7684\u4ee3\u7801\u3002","title":"Introduction"},{"location":"Model/Multiple-model/#multiple#model","text":"\u672c\u6587\u4f7f\u7528\u4e00\u4e2a\u7edf\u4e00\u7684\u6a21\u578b\u6765\u5bf9\u5bf9parallel computing\u3001concurrent computing\u3001distributed computing\u8fdb\u884c\u63cf\u8ff0\uff0c\u8fd9\u4e2a\u6a21\u578b\u547d\u540d\u4e3a\u201c multiple model \u201d\u3002","title":"Multiple model"},{"location":"Model/Multiple-model/#what#is#multiple#model","text":"\u5728\u6709\u4e86parallel computing\u3001concurrent computing\u3001distributed computing\u7684\u4e00\u4e9b\u7406\u8bba\u77e5\u8bc6\u540e\uff0c\u6211\u4eec\u4f1a\u53d1\u73b0\u5b83\u4eec\u6d89\u53ca\u5230\u4e86\u975e\u5e38\u591a\u7c7b\u4f3c\u7684\u6982\u5ff5\uff08\u95ee\u9898\uff09\uff1a race condition atomic communication consistency ...... \u8fd9\u662f\u56e0\u4e3aparallel computing\u3001concurrent computing\u3001distributed computing\u90fd\u6d89\u53ca\u201c \u591a\u4e2a \u201delement\u6216entity\uff08\u8fd9\u4e9belement\u5e76\u53d1\u6216\u5e76\u884c\u5730\u8fd0\u884c\u4e2d\uff09\uff0c\u5b83\u4eec\u53ef\u80fd\u540c\u65f6\u64cd\u4f5cshared data(shared memory)\uff0c\u6240\u4ee5\u5b83\u4eec\u90fd\u4f1a\u6d89\u53ca\u4e0e**\u591a\u4e2a**(multiple)\u76f8\u5173\u7684\u95ee\u9898\uff0c\u6211\u4eec\u5c06\u4f7f\u7528\u201c multiple model \u201d\u6765\u5bf9\u5b83\u4eec\u8fdb\u884c\u7edf\u4e00\u7684\u63cf\u8ff0\u3002","title":"What is multiple model?"},{"location":"Model/Multiple-model/#concurrencyparallel#elementunit","text":"\u672c\u8282\u603b\u7ed3\u5728\u5404\u79cd\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5concurrent/parallel\u6267\u884c/\u53d1\u751f\u7684element\uff0c\u5176\u4e2d\u90e8\u5206\u53c2\u8003\u4e86wikipedia Parallel computing \u4e2d\u603b\u7ed3\u7684\u53ef\u80fd\u7684element\uff1a 1\u3001machine/server \u4e3b\u8981\u662fdistributed computing\u9886\u57df\u3002 2\u3001 Process \u4e3b\u8981\u662fmultiple process\u3002 3\u3001 Thread \u4e3b\u8981\u662fmultiple thread\u3002 4\u3001 Fiber \u4e3b\u8981\u662fcoroutine\u3002 5\u3001 Instruction window NOTE: \u7ef4\u57fa\u767e\u79d1\u8fd9\u91cc\u4f7f\u7528\u7684\u662f Instruction window \uff0c\u6211\u6709\u4e9b\u4e0d\u61c2\uff0c\u6211\u89c9\u5f97\u4f7f\u7528 instruction \u4f1a\u66f4\u52a0\u51c6\u786e\u3002 6\u3001network connection \u4e3b\u8981\u662fconcurrent server\u3002 7\u3001transaction \u4e3b\u8981\u662fDBMS\u4e2d\u3002 8\u3001......","title":"Concurrency/parallel element/unit"},{"location":"Model/Multiple-model/#shared#data","text":"\u53c2\u89c1 Shared-data \u7ae0\u8282\u3002","title":"Shared data"},{"location":"Model/Multiple-model/#operation#on#shared#data","text":"\u5bf9shared data\u7684\u64cd\u4f5c\u53ef\u4ee5\u6982\u62ec\u4e3a\u5982\u4e0b\u4e24\u79cd: 1\u3001read 2\u3001write \u5173\u4e8eread\u3001write\uff0c\u5728 ./Read-and-write \u4e2d\uff0c\u4f1a\u8fdb\u884c\u4e13\u95e8\u603b\u7ed3\u3002","title":"Operation on shared data"},{"location":"Model/Multiple-model/#relation#many-to-one","text":"Many: multiple entity One: shared data","title":"Relation:  many-to-one"},{"location":"Model/Multiple-model/#problems","text":"**\u591a\u4e2a**\u6bd4**\u5355\u4e2a**\u7684\u590d\u6742\u5ea6\u8981\u9ad8\u5f88\u591a\uff0c\u5728**multiple model**\u4e2d\u5b58\u5728\u7740\u4e00\u7cfb\u5217\u95ee\u9898\uff1a \u591a\u4e2aelement\u4e4b\u95f4\u7684race \u591a\u4e2aelement\u4e4b\u95f4\u5982\u4f55\u8fdb\u884ccommunication\uff08\u6d88\u606f\u4f20\u9012\uff0c\u8fdb\u884c\u6c9f\u901a\uff09 \u591a\u4e2aelement\u4e4b\u95f4\u5982\u4f55\u8fdb\u884ccoordination\uff08\u534f\u4f5c\u3001\u534f\u8c03\uff09 \u591a\u4e2aentity\u4e4b\u95f4\u5982\u4f55\u8fbe\u6210consensus\uff08\u5171\u8bc6\uff09 ...... \u4e0b\u9762\u662f\u4e00\u4e9b\u7528\u4e8e\u8bf4\u660eproblem\u7684\u8bcd\u8bed: 1\u3001memory corruption \u6765\u6e90: microsoft Lockless Programming Considerations for Xbox 360 and Microsoft Windows","title":"Problems"},{"location":"Model/Multiple-model/#from#single#machine#to#multiple#machine","text":"NOTE: \u53c2\u89c1\u300aOne-to-many\u300b","title":"From single machine to multiple machine"},{"location":"Model/Multiple-model/#single#machine","text":"\u5355\u4e2aprocess\uff1a\u5b83\u7684element\u5c31\u662fthread \u591a\u4e2aprocess\uff1a\u5b83\u7684element\u5c31\u662fprocess","title":"Single machine"},{"location":"Model/Multiple-model/#multiple#machine","text":"\u591a\u4e2aprocess\uff1a\u5b83\u7684element\u5c31\u662fprocess","title":"Multiple machine"},{"location":"Model/Multiple-model/#nature","text":"\u4ece\u4e0a\u5230\u4e0b\uff0c\u95ee\u9898\u8d8a\u6765\u8d8a\u590d\u6742\uff0c\u5728\u6bcf\u4e00\u5c42\u7ea7\uff0c\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\u7684\u6280\u672f\u624b\u6bb5\u90fd\u662f\u4e0d\u540c\u7684\uff0c\u672c\u8d28\u4e0a\u5b83\u4eec\u6240\u89e3\u51b3\u7684\u90fd\u662f\u76f8\u540c\u7c7b\u578b\u7684\u95ee\u9898\uff0c\u8fd9\u662f\u6211\u4eec\u5efa\u7acb\u5176multiple model\u7684\u4ef7\u503c\u6240\u5728\u3002 \u5f53\u7136\uff0c\u968f\u7740\u8d8a\u6765\u8d8a\u590d\u6742\uff0c\u6bcf\u4e00\u79cd\u4e5f\u90fd\u4f1a\u6d89\u53ca\u5230\u7b80\u5355\u95ee\u9898\u4e2d\u4e0d\u5b58\u5728\u7684\u95ee\u9898\u3002","title":"Nature"},{"location":"Model/Multiple-model/#concurrency#control","text":"\u5728multiple model\u4e2d\uff0c\u8fdb\u884cConcurrency control\u662f\u975e\u5e38\u91cd\u8981\u7684\u3002","title":"Concurrency control"},{"location":"Model/Multiple-model/#communication","text":"\u5176\u5b9e\u5c31\u662fentity\u4e4b\u95f4\u7684communication\u3002","title":"Communication"},{"location":"Model/Multiple-model/#consistency","text":"\u5173\u4e8e\u6b64\u7684\u975e\u5e38\u597d\u7684\u8bba\u8ff0:","title":"Consistency"},{"location":"Model/Multiple-model/#zhuanlanzhihu#--","text":"\u5728\u5206\u5e03\u5f0f\u5b58\u50a8\u7cfb\u7edf\u4e2d\uff0c\u4e3a\u4e86\u63d0\u9ad8**\u53ef\u9760\u6027**\uff0c\u901a\u5e38\u4f1a\u5f15\u5165\u591a\u4e2a\u526f\u672c\uff0c\u591a\u4e2a\u526f\u672c\u9700\u8981\u5411\u7528\u6237\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u5185\u5bb9\u3002\u8fd9\u5f88\u81ea\u7136\u7684\u8ba9\u4eba\u60f3\u5230\u5982\u4f55\u786e\u4fdd\u591a\u526f\u672c\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u3002\u4e3a\u6b64\u4e5f\u6709\u4e86paxos\u548craft\u7b49\u4fdd\u8bc1\u591a\u526f\u672c\u4e4b\u95f4\u4e00\u81f4\u6027\u7684\u534f\u8bae\u3002\u5f53\u6211\u4eec\u5728\u4e00\u4e2a\u591a\u5904\u7406\u5668\u673a\u5668\u4e0a\u7f16\u7a0b\u65f6\u6211\u4eec\u901a\u5e38\u4f1a\u5ffd\u7565\u5728\u591a\u5904\u7406\u5668\u73af\u5883\u4e0b\u7684\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u56e0\u4e3a\u7cfb\u7edf\u5df2\u7ecf\u4e3a\u6211\u4eec\u505a\u597d\u4e86\u57fa\u672c\u7684**\u4e00\u81f4\u6027**\u4fdd\u8bc1\uff0c\u5f53\u5b58\u5728\u4e00\u81f4\u6027\u95ee\u9898\u7684\u65f6\u5019\u4e0a\u5c42\u7f16\u7a0b\u8bed\u8a00\u4e5f\u63d0\u4f9b\u4e86\u5177\u5907\u4e00\u81f4\u6027\u8bed\u610f\u7684\u63a5\u53e3\uff0c\u53ea\u662f\u6211\u4eec\u5728\u7f16\u7a0b\u4e2d\u5e76\u6ca1\u6709\u610f\u8bc6\u5230\u8fd9\u4e9b**\u63a5\u53e3**\u4e0e**\u4e00\u81f4\u6027**\u7684\u5173\u7cfb\u3002\u65e0\u8bba\u662f\u5206\u5e03\u5f0f\u5b58\u50a8\u8fd8\u662f\u591a\u5904\u7406\u5668\u7f16\u7a0b\u90fd\u6709\u4e00\u4e2a\u5171\u540c\u70b9\uff0c\u5c31\u662f\u4f1a\u6d89\u53ca\u5171\u4eab\u5bf9\u8c61\u7684\u64cd\u4f5c\u3002 NOTE: 1\u3001\"\u53ef\u9760\u6027\"\uff0c\u5176\u5b9e\u5c31\u662fHA \"\u591a\u4e2a\u526f\u672c\"\uff0c\u5176\u5b9e\u5c31\u662fmaster-slave \"\u4e00\u81f4\u7684\u5185\u5bb9\"\uff0c\u5176\u5b9e\u5c31\u662fconsistency \"\u5171\u4eab\u5bf9\u8c61\"\u5176\u5b9e\u5c31\u662fmultiple model\u4e2d\u7684shared data 2\u3001\u4e0a\u8ff0\u5176\u5b9e\u53ef\u4ee5\u4f7f\u7528multiple model-shared data\u6765\u8fdb\u884c\u63cf\u8ff0 \u4e00\u65e6\u51fa\u73b0\u5171\u4eab\uff0c\u5c31\u4f1a\u51fa\u73b0\u6b63\u786e\u6027\u7684\u95ee\u9898\uff0c\u90a3\u4e48\u5982\u4f55\u5b9a\u4e49\u5728\u5e76\u53d1\u4e2d\u64cd\u4f5c\u5171\u4eab\u5bf9\u8c61\u7684\u6b63\u786e\u6027\uff0c\u8fd9\u5c31\u662f\u4e00\u81f4\u6027\u534f\u8bae\u7684\u4efb\u52a1\u4e86\u3002 \u672c\u6587\u4e3b\u8981\u9488\u5bf9\u591a\u5904\u7406\u5668\u7cfb\u7edf\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\u8fdb\u884c\u4e86\u4e00\u4e9b\u603b\u7ed3\uff0c\u5bf9\u4e8e\u5206\u5e03\u5f0f\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\u4f1a\u5728\u540e\u9762\u6587\u7ae0\u4e2d\u603b\u7ed3\u3002 \u591a\u5904\u7406\u5668\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\u6e90\u4e8e\u5e76\u53d1\uff0c\u6e90\u4e8e\u5171\u4eab\u3002\u5bf9\u4e8e\u5171\u4eab\u5185\u5b58\u5e76\u53d1\u64cd\u4f5c\u4e0b\u7684\u6b63\u786e\u6027\u4fdd\u8bc1\u662f\u786c\u4ef6\u8bbe\u8ba1\u8005\u9700\u8981\u63d0\u4f9b\u7ed9\u4e0a\u5c42\u5f00\u53d1\u4eba\u5458\u6700\u91cd\u8981\u7684\u4fdd\u8bc1\u3002\u5bf9\u4e8e\u4e0a\u5c42\u5f00\u53d1\u4eba\u5458\u6765\u8bf4\uff0c\u7cfb\u7edf\u5185\u90e8\u7684\u4e00\u81f4\u6027\u662f\u900f\u660e\u7684\uff0c\u4f46\u662f\u53bb\u4e86\u89e3\u7cfb\u7edf\u5185\u90e8\u4e00\u81f4\u6027\u7684\u8bbe\u8ba1\u53ca\u539f\u7406\u6709\u5229\u4e8e\u6211\u4eec\u66f4\u80fd\u591f\u9762\u5411\u673a\u5668\u7f16\u7a0b\uff0c\u5199\u51fa\u6b63\u786e\u7684\u4ee3\u7801\u3002","title":"zhuanlan.zhihu \u9ad8\u5e76\u53d1\u7f16\u7a0b--\u591a\u5904\u7406\u5668\u7f16\u7a0b\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898(\u4e0a)"},{"location":"Model/Multiple-model/Contention-and-race/","text":"Contention and race \u5173\u4e8erace condition\uff0c\u53c2\u89c1\u5982\u4e0b\u7ae0\u8282: 1\u3001\u5de5\u7a0b Linux-OS \u7684 Race-condition \u7ae0\u8282 2\u3001\u5de5\u7a0b parallel-computing \u7684 Concurrent-computing\\Multithread\\Thread-safety\\What-cause-unsafety\\Race \u7ae0\u8282","title":"Introduction"},{"location":"Model/Multiple-model/Contention-and-race/#contention#and#race","text":"\u5173\u4e8erace condition\uff0c\u53c2\u89c1\u5982\u4e0b\u7ae0\u8282: 1\u3001\u5de5\u7a0b Linux-OS \u7684 Race-condition \u7ae0\u8282 2\u3001\u5de5\u7a0b parallel-computing \u7684 Concurrent-computing\\Multithread\\Thread-safety\\What-cause-unsafety\\Race \u7ae0\u8282","title":"Contention and race"},{"location":"Model/Multiple-model/Contention-and-race/Resource-contention/","text":"Resource contention wikipedia Resource contention","title":"Introduction"},{"location":"Model/Multiple-model/Contention-and-race/Resource-contention/#resource#contention","text":"","title":"Resource contention"},{"location":"Model/Multiple-model/Contention-and-race/Resource-contention/#wikipedia#resource#contention","text":"","title":"wikipedia Resource contention"},{"location":"Model/Multiple-model/Read-and-write/","text":"Read and write \u5728multiple model\u4e2d\uff0c\u5bf9shared data\u7684\u64cd\u4f5c\u6700\u7ec8\u53ef\u4ee5\u6982\u62ec\u4e3a\u5982\u4e0b\u4e24\u79cd: 1\u3001read 2\u3001write \u56e0\u6b64\uff0c\u901a\u8fc7read\u3001write\u53ef\u4ee5\u5bf9multiple model\u4e2d\u7684\u5f88\u591a\u95ee\u9898\u8fdb\u884c\u63cf\u8ff0\u3002 DBMS \u5728DBMS\u4e2d\uff0c\u5bf9\u6b64\u6709\u7740\u8f83\u597d\u7684\u603b\u7ed3\uff0c\u6211\u4eec\u53ef\u4ee5\u501f\u7528\u5176\u4e2d\u7684\u7406\u8bba: read write read no conflict conflict write conflict conflict \u53c2\u89c1\u5de5\u7a0bDB\u7684 Theory\\Concurrency-Control-in-DBMS\\Problem \u7ae0\u8282\u3002 Concurrent computing concurrent computing\u9886\u57df\u4e2d\u7684 Readers\u2013writers problem \u5176\u5b9e\u5c31\u662fread and write\uff0c\u53c2\u89c1 Concurrent-computing\\Classic-problem\\Readers\u2013writers-problem \u7ae0\u8282\u3002 Concurrency control \u5728 Concurrent-computing\\Concurrency-control \u7ae0\u8282\u8fdb\u884c\u4e86\u603b\u7ed3\u3002 Consistency model consistent model\u4e5f\u662f\u57fa\u4e8eread\u3001write\u800c\u5efa\u7acb\u7684: 1\u3001\u5728CPU\u4e2d\uff0c\u4f7f\u7528load\u3001store\u6765\u8868\u793aread\u3001write Memory access \u53c2\u89c1 \u5de5\u7a0bhardware\u7684 CPU-memory-access \u7ae0\u8282\u3002 1\u3001memory ordering\u662f\u57fa\u4e8eread\u3001write\u6765\u5efa\u7acb\u7684 Read and write tradeoff \u6839\u636eRead and write\u6765\u8fdb\u884ctradeoff\uff0c\u4ece\u800c\u9009\u62e9\u5408\u9002\u7684concurrency\u6280\u672f\uff0c\u53c2\u89c1 Concurrent-computing\\Concurrency-control\\How-to-optimize \u7ae0\u8282\u3002","title":"Introduction"},{"location":"Model/Multiple-model/Read-and-write/#read#and#write","text":"\u5728multiple model\u4e2d\uff0c\u5bf9shared data\u7684\u64cd\u4f5c\u6700\u7ec8\u53ef\u4ee5\u6982\u62ec\u4e3a\u5982\u4e0b\u4e24\u79cd: 1\u3001read 2\u3001write \u56e0\u6b64\uff0c\u901a\u8fc7read\u3001write\u53ef\u4ee5\u5bf9multiple model\u4e2d\u7684\u5f88\u591a\u95ee\u9898\u8fdb\u884c\u63cf\u8ff0\u3002","title":"Read and write"},{"location":"Model/Multiple-model/Read-and-write/#dbms","text":"\u5728DBMS\u4e2d\uff0c\u5bf9\u6b64\u6709\u7740\u8f83\u597d\u7684\u603b\u7ed3\uff0c\u6211\u4eec\u53ef\u4ee5\u501f\u7528\u5176\u4e2d\u7684\u7406\u8bba: read write read no conflict conflict write conflict conflict \u53c2\u89c1\u5de5\u7a0bDB\u7684 Theory\\Concurrency-Control-in-DBMS\\Problem \u7ae0\u8282\u3002","title":"DBMS"},{"location":"Model/Multiple-model/Read-and-write/#concurrent#computing","text":"concurrent computing\u9886\u57df\u4e2d\u7684 Readers\u2013writers problem \u5176\u5b9e\u5c31\u662fread and write\uff0c\u53c2\u89c1 Concurrent-computing\\Classic-problem\\Readers\u2013writers-problem \u7ae0\u8282\u3002","title":"Concurrent computing"},{"location":"Model/Multiple-model/Read-and-write/#concurrency#control","text":"\u5728 Concurrent-computing\\Concurrency-control \u7ae0\u8282\u8fdb\u884c\u4e86\u603b\u7ed3\u3002","title":"Concurrency control"},{"location":"Model/Multiple-model/Read-and-write/#consistency#model","text":"consistent model\u4e5f\u662f\u57fa\u4e8eread\u3001write\u800c\u5efa\u7acb\u7684: 1\u3001\u5728CPU\u4e2d\uff0c\u4f7f\u7528load\u3001store\u6765\u8868\u793aread\u3001write","title":"Consistency model"},{"location":"Model/Multiple-model/Read-and-write/#memory#access","text":"\u53c2\u89c1 \u5de5\u7a0bhardware\u7684 CPU-memory-access \u7ae0\u8282\u3002 1\u3001memory ordering\u662f\u57fa\u4e8eread\u3001write\u6765\u5efa\u7acb\u7684","title":"Memory access"},{"location":"Model/Multiple-model/Read-and-write/#read#and#write#tradeoff","text":"\u6839\u636eRead and write\u6765\u8fdb\u884ctradeoff\uff0c\u4ece\u800c\u9009\u62e9\u5408\u9002\u7684concurrency\u6280\u672f\uff0c\u53c2\u89c1 Concurrent-computing\\Concurrency-control\\How-to-optimize \u7ae0\u8282\u3002","title":"Read and write tradeoff"},{"location":"Model/Multiple-model/Shared-data/","text":"Shared data \u663e\u7136\u5bf9\u4e8e\u4e0d\u540c\u7684concurrency/parallel element/unit\uff0c\u5b83\u4eec\u7684shared data\u662f\u53ef\u80fd\u4e0d\u540c\u7684\uff0c\u4e0b\u9762\u662f\u4e00\u4e9b\u7b80\u5355\u7684\u4f8b\u5b50: 1\u3001DBMS table 2\u3001file 3\u3001memory 4\u3001global variable \u53c2\u89c1: wikipedia Reentrancy (computing) Data has a characteristic called scope , which describes where in a program the data may be used. Data scope is either global (outside the scope of any function and with an indefinite extent) or local (created each time a function is called and destroyed upon exit). Local data is not shared by any routines, re-entering or not; therefore, it does not affect re-entrance. Global data is defined outside functions and can be accessed by more than one function, either in the form of global variables (data shared between all functions), or as static variables (data shared by all functions of the same name). 5\u3001...... NOTE: \u540e\u7eed\u8fdb\u884c\u8865\u5145","title":"Introduction"},{"location":"Model/Multiple-model/Shared-data/#shared#data","text":"\u663e\u7136\u5bf9\u4e8e\u4e0d\u540c\u7684concurrency/parallel element/unit\uff0c\u5b83\u4eec\u7684shared data\u662f\u53ef\u80fd\u4e0d\u540c\u7684\uff0c\u4e0b\u9762\u662f\u4e00\u4e9b\u7b80\u5355\u7684\u4f8b\u5b50: 1\u3001DBMS table 2\u3001file 3\u3001memory 4\u3001global variable \u53c2\u89c1: wikipedia Reentrancy (computing) Data has a characteristic called scope , which describes where in a program the data may be used. Data scope is either global (outside the scope of any function and with an indefinite extent) or local (created each time a function is called and destroyed upon exit). Local data is not shared by any routines, re-entering or not; therefore, it does not affect re-entrance. Global data is defined outside functions and can be accessed by more than one function, either in the form of global variables (data shared between all functions), or as static variables (data shared by all functions of the same name). 5\u3001...... NOTE: \u540e\u7eed\u8fdb\u884c\u8865\u5145","title":"Shared data"},{"location":"Model/Multiple-model/Shared-data/Shared-resource/","text":"Shared resource wikipedia Shared resource","title":"Introduction"},{"location":"Model/Multiple-model/Shared-data/Shared-resource/#shared#resource","text":"","title":"Shared resource"},{"location":"Model/Multiple-model/Shared-data/Shared-resource/#wikipedia#shared#resource","text":"","title":"wikipedia Shared resource"},{"location":"Model/Multiple-model/Shared-data/TODO-Shared-memory/","text":"Shared memory \u5305\u62ec: 1\u3001 shared memory [ 1] multiprocessors 2\u3001.... wikipedia Shared memory NOTE: 1\u3001\u603b\u7ed3\u5f97\u975e\u5e38\u597d In hardware In software","title":"Introduction"},{"location":"Model/Multiple-model/Shared-data/TODO-Shared-memory/#shared#memory","text":"\u5305\u62ec: 1\u3001 shared memory [ 1] multiprocessors 2\u3001....","title":"Shared memory"},{"location":"Model/Multiple-model/Shared-data/TODO-Shared-memory/#wikipedia#shared#memory","text":"NOTE: 1\u3001\u603b\u7ed3\u5f97\u975e\u5e38\u597d","title":"wikipedia Shared memory"},{"location":"Model/Multiple-model/Shared-data/TODO-Shared-memory/#in#hardware","text":"","title":"In hardware"},{"location":"Model/Multiple-model/Shared-data/TODO-Shared-memory/#in#software","text":"","title":"In software"},{"location":"Model/Stream-model/","text":"Stream model 1\u3001\u5728\u5de5\u7a0b discrete \u7684 Relation-structure-computation\\Model\\Stream-model \u7ae0\u8282\u4e2d\uff0c\u4e5f\u5bf9\"Stream model\"\u8fdb\u884c\u4e86\u8bf4\u660e\u3002 wikipedia Stream processing Stream processing is a computer programming paradigm, equivalent to dataflow programming , event stream processing , and reactive programming ,[ 1] that allows some applications to more easily exploit a limited form of parallel processing . Such applications can use multiple computational units, such as the floating point unit on a graphics processing unit or field-programmable gate arrays (FPGAs),[ 2] without explicitly managing allocation, synchronization, or communication among those units.","title":"Introduction"},{"location":"Model/Stream-model/#stream#model","text":"1\u3001\u5728\u5de5\u7a0b discrete \u7684 Relation-structure-computation\\Model\\Stream-model \u7ae0\u8282\u4e2d\uff0c\u4e5f\u5bf9\"Stream model\"\u8fdb\u884c\u4e86\u8bf4\u660e\u3002","title":"Stream model"},{"location":"Model/Stream-model/#wikipedia#stream#processing","text":"Stream processing is a computer programming paradigm, equivalent to dataflow programming , event stream processing , and reactive programming ,[ 1] that allows some applications to more easily exploit a limited form of parallel processing . Such applications can use multiple computational units, such as the floating point unit on a graphics processing unit or field-programmable gate arrays (FPGAs),[ 2] without explicitly managing allocation, synchronization, or communication among those units.","title":"wikipedia Stream processing"},{"location":"Multithread/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u8ba8\u8bba\u591a\u7ebf\u7a0b\u3002 TODO consistency in multithread \u5728\u5206\u5e03\u5f0f\u4e2d\uff0cconsistency\u662fmaster-slave\u4e4b\u95f4\u7684memory\u7684consistency\uff0c\u4f46\u662f\u5728multithread\u4e2d\uff0cconsistency\u662f\u4ec0\u4e48\u5462\uff1f \u8fd9\u4e2a\u95ee\u9898\u5728\u4e0b\u9762\u7ae0\u8282\u7684\u56de\u7b54\u4e2d\u89e3\u7b54\u4e86\u3002 \u73b0\u5728\u53ef\u4ee5\u518d\u603b\u7ed3\u4e00\u4e0b: 1\u3001multithread\u4e2d\u7684entity\u662fthread 2\u3001\u5404\u4e2athread\u4e4b\u95f4\u4e5f\u662f\u5b58\u5728consistency\u7684: \u5404\u4e2athread\u6240\u770b\u5230\u7684data\u7684consistency thread-safe and data consistency \u8fd9\u662f\u5728\u9605\u8bfb wikipedia Multiversion concurrency control \u65f6\uff0c\u5176\u4e2d\u7684\u4e00\u6bb5\u8bdd: Without concurrency control, if someone is reading from a database at the same time as someone else is writing to it, it is possible that the reader will see a half-written or inconsistent piece of data. \u8ba9\u6211\u60f3\u5230\u4e86\u4ecedata consistency\u7684\u89d2\u5ea6\u6765\u5bf9thread safe\u8fdb\u884c\u5206\u6790: 1\u3001\u4e00\u4e2athread\u6240\u5199\u7684\u548c\u53e6\u5916\u4e00\u4e2athread\u6240\u8bfb\u7684\uff0c\u4e0d\u4e00\u81f4(half-written or inconsistent piece of data) 2\u3001\u4e24\u4e2athread\u770b\u5230\u7684data\u4e0d\u4e00\u81f4 thread-safe and atomic thread safe\u4e2d\u7684data race\u4e2d\u7684\u770b\u5230\u4e2d\u95f4\u72b6\u6001\uff0c\u4f7f\u7528**\u539f\u5b50\u64cd\u4f5c**\u6765\u8fdb\u884c\u89e3\u51b3","title":"Introduction"},{"location":"Multithread/#_1","text":"\u672c\u7ae0\u8ba8\u8bba\u591a\u7ebf\u7a0b\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Multithread/#todo","text":"","title":"TODO"},{"location":"Multithread/#consistency#in#multithread","text":"\u5728\u5206\u5e03\u5f0f\u4e2d\uff0cconsistency\u662fmaster-slave\u4e4b\u95f4\u7684memory\u7684consistency\uff0c\u4f46\u662f\u5728multithread\u4e2d\uff0cconsistency\u662f\u4ec0\u4e48\u5462\uff1f \u8fd9\u4e2a\u95ee\u9898\u5728\u4e0b\u9762\u7ae0\u8282\u7684\u56de\u7b54\u4e2d\u89e3\u7b54\u4e86\u3002 \u73b0\u5728\u53ef\u4ee5\u518d\u603b\u7ed3\u4e00\u4e0b: 1\u3001multithread\u4e2d\u7684entity\u662fthread 2\u3001\u5404\u4e2athread\u4e4b\u95f4\u4e5f\u662f\u5b58\u5728consistency\u7684: \u5404\u4e2athread\u6240\u770b\u5230\u7684data\u7684consistency","title":"consistency in multithread"},{"location":"Multithread/#thread-safe#and#data#consistency","text":"\u8fd9\u662f\u5728\u9605\u8bfb wikipedia Multiversion concurrency control \u65f6\uff0c\u5176\u4e2d\u7684\u4e00\u6bb5\u8bdd: Without concurrency control, if someone is reading from a database at the same time as someone else is writing to it, it is possible that the reader will see a half-written or inconsistent piece of data. \u8ba9\u6211\u60f3\u5230\u4e86\u4ecedata consistency\u7684\u89d2\u5ea6\u6765\u5bf9thread safe\u8fdb\u884c\u5206\u6790: 1\u3001\u4e00\u4e2athread\u6240\u5199\u7684\u548c\u53e6\u5916\u4e00\u4e2athread\u6240\u8bfb\u7684\uff0c\u4e0d\u4e00\u81f4(half-written or inconsistent piece of data) 2\u3001\u4e24\u4e2athread\u770b\u5230\u7684data\u4e0d\u4e00\u81f4","title":"thread-safe and data consistency"},{"location":"Multithread/#thread-safe#and#atomic","text":"thread safe\u4e2d\u7684data race\u4e2d\u7684\u770b\u5230\u4e2d\u95f4\u72b6\u6001\uff0c\u4f7f\u7528**\u539f\u5b50\u64cd\u4f5c**\u6765\u8fdb\u884c\u89e3\u51b3","title":"thread-safe and atomic"},{"location":"Multithread/Expert-Herb-Sutter/","text":"Effective Concurrency Serial http://www.gotw.ca/publications/ \u6587\u7ae0 NOTE: 1\u3001\u4e0b\u9762\u8fd9\u4e9b\u5185\u5bb9\u662f\u4ece Effective Concurrency: Prefer Using Active Objects Instead of Naked Threads \u4e2d\u6458\u6284\u8fc7\u6765\u7684 2\u3001\u53ef\u4ee5\u770b\u5230\uff0cHerb Sutter\u7684Effective Concurrency Serial\u8986\u76d6\u4e86concurrency programming\u7684\u65b9\u65b9\u9762\u9762 3\u3001\u603b\u5f97\u6765\u8bf4\uff0c\u6709\u5982\u4e0b\u539f\u5219: a\u3001\u4f7f\u7528good abstraction I hope you enjoy it. Finally, here are links to previous Effective Concurrency columns: 1 The Pillars of Concurrency (Aug 2007) \u7ed9\u51fa\u4e86framework\u3001model\uff0c\u975e\u5e38\u597d\u7684guide\u3002 2 How Much Scalability Do You Have or Need? (Sep 2007) \u5982\u4f55\u8bbe\u8ba1thread model\u4ee5\u5145\u5206\u53d1\u6325scalability\uff0c\u8fd9\u662f\u5bf9 \"Pillar 2: Throughput and Scalability Via Concurrent Collections\"\u7684\u5c55\u5f00\u3002 3 Use Critical Sections (Preferably Locks) to Eliminate Races (Oct 2007) 4 Apply Critical Sections Consistently (Nov 2007) 5 Avoid Calling Unknown Code While Inside a Critical Section (Dec 2007) 6 Use Lock Hierarchies to Avoid Deadlock (Jan 2008) \u524d\u9762\u90fd\u662f\u63cf\u8ff0\u5982\u4f55lock\u7684\uff0c\u5176\u5b9e\u662f\u5bf9\"Pillar 3: Consistency Via Safely Shared Resources\"\u7684\u8be6\u7ec6\u8bf4\u660e\u3002 7 Break Amdahl\u2019s Law! (Feb 2008) 8 Going Superlinear (Mar 2008) 9 Super Linearity and the Bigger Machine (Apr 2008) 10 Interrupt Politely (May 2008) 11 Maximize Locality, Minimize Contention (Jun 2008) 12 Choose Concurrency-Friendly Data Structures (Jul 2008) 13 The Many Faces of Deadlock (Aug 2008) 14 Lock-Free Code: A False Sense of Security (Sep 2008) 15 Writing Lock-Free Code: A Corrected Queue (Oct 2008) 16 Writing a Generalized Concurrent Queue (Nov 2008) 17 Understanding Parallel Performance (Dec 2008) 18 Measuring Parallel Performance: Optimizing a Concurrent Queue (Jan 2009) 19 volatile vs. volatile (Feb 2009) 20 Sharing Is the Root of All Contention (Mar 2009) shared data 21 Use Threads Correctly = Isolation + Asynchronous Messages (Apr 2009) 22 Use Thread Pools Correctly: Keep Tasks Short and Nonblocking (Apr 2009) 23 Eliminate False Sharing (May 2009) 24 Break Up and Interleave Work to Keep Threads Responsive (Jun 2009) 25 The Power of \u201cIn Progress\u201d (Jul 2009) 26 Design for Manycore Systems (Aug 2009) 27 Avoid Exposing Concurrency \u2013 Hide It Inside Synchronous Methods (Oct 2009) 28 Prefer structured lifetimes \u2013 local, nested, bounded, deterministic (Nov 2009) 29 Prefer Futures to Baked-In \u201cAsync APIs\u201d (Jan 2010) 30 Associate Mutexes with Data to Prevent Races (May 2010) 31 Prefer Using Active Objects Instead of Naked Threads (June 2010)","title":"Introduction"},{"location":"Multithread/Expert-Herb-Sutter/#effective#concurrency#serial","text":"http://www.gotw.ca/publications/","title":"Effective Concurrency Serial"},{"location":"Multithread/Expert-Herb-Sutter/#_1","text":"NOTE: 1\u3001\u4e0b\u9762\u8fd9\u4e9b\u5185\u5bb9\u662f\u4ece Effective Concurrency: Prefer Using Active Objects Instead of Naked Threads \u4e2d\u6458\u6284\u8fc7\u6765\u7684 2\u3001\u53ef\u4ee5\u770b\u5230\uff0cHerb Sutter\u7684Effective Concurrency Serial\u8986\u76d6\u4e86concurrency programming\u7684\u65b9\u65b9\u9762\u9762 3\u3001\u603b\u5f97\u6765\u8bf4\uff0c\u6709\u5982\u4e0b\u539f\u5219: a\u3001\u4f7f\u7528good abstraction I hope you enjoy it. Finally, here are links to previous Effective Concurrency columns: 1 The Pillars of Concurrency (Aug 2007) \u7ed9\u51fa\u4e86framework\u3001model\uff0c\u975e\u5e38\u597d\u7684guide\u3002 2 How Much Scalability Do You Have or Need? (Sep 2007) \u5982\u4f55\u8bbe\u8ba1thread model\u4ee5\u5145\u5206\u53d1\u6325scalability\uff0c\u8fd9\u662f\u5bf9 \"Pillar 2: Throughput and Scalability Via Concurrent Collections\"\u7684\u5c55\u5f00\u3002 3 Use Critical Sections (Preferably Locks) to Eliminate Races (Oct 2007) 4 Apply Critical Sections Consistently (Nov 2007) 5 Avoid Calling Unknown Code While Inside a Critical Section (Dec 2007) 6 Use Lock Hierarchies to Avoid Deadlock (Jan 2008) \u524d\u9762\u90fd\u662f\u63cf\u8ff0\u5982\u4f55lock\u7684\uff0c\u5176\u5b9e\u662f\u5bf9\"Pillar 3: Consistency Via Safely Shared Resources\"\u7684\u8be6\u7ec6\u8bf4\u660e\u3002 7 Break Amdahl\u2019s Law! (Feb 2008) 8 Going Superlinear (Mar 2008) 9 Super Linearity and the Bigger Machine (Apr 2008) 10 Interrupt Politely (May 2008) 11 Maximize Locality, Minimize Contention (Jun 2008) 12 Choose Concurrency-Friendly Data Structures (Jul 2008) 13 The Many Faces of Deadlock (Aug 2008) 14 Lock-Free Code: A False Sense of Security (Sep 2008) 15 Writing Lock-Free Code: A Corrected Queue (Oct 2008) 16 Writing a Generalized Concurrent Queue (Nov 2008) 17 Understanding Parallel Performance (Dec 2008) 18 Measuring Parallel Performance: Optimizing a Concurrent Queue (Jan 2009) 19 volatile vs. volatile (Feb 2009) 20 Sharing Is the Root of All Contention (Mar 2009) shared data 21 Use Threads Correctly = Isolation + Asynchronous Messages (Apr 2009) 22 Use Thread Pools Correctly: Keep Tasks Short and Nonblocking (Apr 2009) 23 Eliminate False Sharing (May 2009) 24 Break Up and Interleave Work to Keep Threads Responsive (Jun 2009) 25 The Power of \u201cIn Progress\u201d (Jul 2009) 26 Design for Manycore Systems (Aug 2009) 27 Avoid Exposing Concurrency \u2013 Hide It Inside Synchronous Methods (Oct 2009) 28 Prefer structured lifetimes \u2013 local, nested, bounded, deterministic (Nov 2009) 29 Prefer Futures to Baked-In \u201cAsync APIs\u201d (Jan 2010) 30 Associate Mutexes with Data to Prevent Races (May 2010) 31 Prefer Using Active Objects Instead of Naked Threads (June 2010)","title":"\u6587\u7ae0"},{"location":"Multithread/Expert-Herb-Sutter/01-The-Pillars-of-Concurrency/","text":"drdobbs The Pillars of Concurrency In his inaugural column, Herb makes the case that we must build a consistent mental model before talking about concurrency. NOTE: 1\u3001\"Pillar\"\u7684\u542b\u4e49\u662f\"\u652f\u67f1\" 2\u3001\u8fd9\u7bc7\u6587\u7ae0\u4f5c\u8005\u7ed9\u51fa\u4e86\u7406\u89e3concurrency\u7684\u4e00\u4e2a\u7406\u8bba\u6846\u67b6 Callahan's Pillars My colleague David Callahan leads a team within Visual Studio that is working on programming models for concurrency . He has pointed out that fundamental concurrency requirements and techniques fall into three basic categories, or pillars. They are summarized in Table 1 [1]. Understanding these pillars gives us a framework for reasoning clearly about all aspects of concurrency \u2014 from the concurrency requirements and tradeoffs that matter to our current project, to why specific design patterns and implementation techniques are applicable to getting specific results and how they are liable(\u6709\u8d23\u4efb\u3001\u6709\u4e49\u52a1) to interact, and even to evaluating how future tools and technologies will fit with our needs. NOTE: 1\u3001\u603b\u7ed3\u51fa programming models for concurrency \u3001 framework \u662f\u975e\u5e38\u91cd\u8981\u7684\u4e00\u6b65 Let's consider an overview of each pillar in turn, note why techniques in different pillars compose well, and see how this framework helps to clarify our vocabulary. Table 1: The Pillars of Concurrency. NOTE: 1\u3001\u4e0a\u8ff0\u603b\u7ed3\u5f97\u975e\u5e38\u597d Pillar 1: Responsiveness and Isolation Via Asynchronous Agents NOTE: \u4e00\u3001\u7b80\u800c\u8a00\u4e4b: \u901a\u8fc7asynchronous agent\u6765\u5b9e\u73b0Responsiveness and Isolation \u4e8c\u3001\u6211\u7684\u5b9e\u8df5\u662f: \u884c\u60c5\u63d2\u4ef6 \u5c06\u91c7\u96c6\u5230\u7684\u884c\u60c5\u653e\u5230 \u53d1\u9001\u961f\u5217\uff0c\u7136\u540e\u7531\u53d1\u9001\u7ebf\u7a0b\u8fdb\u884c\u53d1\u9001\uff0c\u5373\"\u5f02\u6b65\u53d1\u9001\" Pillar 1 is all about running separate tasks, or agents, independently and letting them communicate via asynchronous messages. We particularly want to avoid blocking, especially on user-facing and other time-sensitive threads, by running expensive work asynchronously. Also, isolating separable tasks makes them easier to test separately and then deploy into various parallel contexts with confidence. Here we use key terms like \"interactive\" and \"responsive\" and \"background\"; \"message\" and \"dialogue\"; and \"timeout\" and \"cancel.\" Responsiveness NOTE: \u4e00\u3001\u8fd9\u91cc\u4f5c\u8005\u4ee5GUI application\u4e3a\u4f8b\u6765\u8bf4\u660eresponsiveness A typical Pillar 1 technique is to move expensive work off an interactive application's main GUI pump thread. NOTE: 1\u3001\u5c06\u8017\u65f6\u64cd\u4f5c\u653e\u5230\u4e00\u4e2a\u5355\u72ec\u7684thread\uff0c\u8ba9\u5b83\u5f02\u6b65\u5730\u6267\u884c\uff0cmain GUI pump thread\u5c31\u80fd\u591f\u7ee7\u7eed\u8fd0\u884c\u800c\u4e0d\u963b\u585e\uff0c\u4ece\u800c\u4fdd\u8bc1\u4e86 responsiveness We never want to freeze our display for seconds or longer; users should still be able to keep clicking away and interacting with a responsive GUI while the hard work churns away in the background. It's okay for users to experience a change in the application while the work is being performed (for example, some buttons or menu items might be disabled, or an animated icon or progress bar might indicate status of the background work), but they should never experience a \"white screen of death\"\u2014a GUI thread that stops responding to basic messages like \"repaint\" for a while because the new messages pile up behind one that's taking a long time to process synchronously. What kind of work to ship out of responsiveness-sensitive threads? NOTE: 1\u3001\u4e3a\u4e86\u4fdd\u6301responsiveness\uff0c\u5e94\u8be5\u5c06\u90a3\u4e9b\u4eceresponsiveness-sensitive threads\u4e2d\u79fb\u51fa\uff1f\u663e\u7136\u662f\u90a3\u4e9b\u8017\u65f6\u7684\u3002 So what kind of work do we want to ship out of(\u4ece...\u4e2d\u79fb\u51fa) responsiveness-sensitive threads? It can be work that performs an expensive or high-latency computation (background compilation or print rendering, for instance) or actual blocking (idle waiting for a lock, a database result, or a web service reply). Some of these tasks merely want to return a value; others will interact more to provide intermediate results or accept additional input as they make progress on their work. How should the independent tasks communicate? Finally, how should the independent tasks communicate? A key is to have the communication itself be asynchronous, preferably using asynchronous messages where possible because messages are nearly always preferable to sharing objects in memory (which is Pillar 3's territory(\u9886\u57df)). In the case of a GUI thread, this is an easy fit because GUIs already use message-based event-driven models. Implementation NOTE: 1\u3001\u672c\u6bb5\u4e3b\u8981\u63cf\u8ff0\u5982\u4f55\u5b9e\u73b0Asynchronous Agents Today, we typically express Pillar 1 by running the background work on its own thread or as a work item on a thread pool; the foreground task that wants to stay responsive is typically long-running and is usually a thread; and communication happens through message queues and message-like abstractions like futures (Java Future , .NET IAsyncResult ). In coming years, we'll get new tools and abstractions in this pillar, where potential candidates include active objects/services (objects that conceptually run on their own thread, and calling a method is an asynchronous message); channels of communication between two or more tasks; and contracts that let us explicitly express, enforce, and validate the expected order of messages. This pillar is not about keeping hundreds of cores busy; that job belongs to Pillar 2. Pillar 1 is all about responsiveness, asynchrony, and independence; but it may keep some number of cores busy purely as a side effect, because it still expresses work that can be done independently, and therefore, in parallel. Pillar 2: Throughput and Scalability Via Concurrent Collections NOTE: \u4e00\u3001\u672c\u6bb5\u4e3b\u8981\u63cf\u8ff0\u7684\u662f\u5982\u4f55\u5145\u5206\u53d1\u6325hardware\u7684computation power\uff0c\u4ece\u800c\"re-enabling the \"free lunch\"\" \u4e8c\u3001\"Collections\"\u7684\u542b\u4e49\u662f\u4ec0\u4e48\u5462\uff1f\u4e0b\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u8fdb\u884c\u4e86\u89e3\u91ca: \"We particularly want to target operations performed on collections (any group of things, not just containers) and exploit parallelism in data and algorithm structures\" \u5728 2 How Much Scalability Do You Have or Need? (Sep 2007) \u4e2d\uff0c\u5c06\u8fd9\u4e00\u6761\u603b\u7ed3\u4e3a: Pillar 2, scalable throughput by exploiting parallelism in algorithms and data structures; \u663e\u7136\uff0c\u4e0a\u8ff0\u662f\u66f4\u52a0\u6613\u4e8e\u7406\u89e3\u7684\u3002 Pillar 2, on the other hand, is about keeping hundreds of cores busy to compute results faster, thereby re-enabling the \"free lunch\"[2]. We particularly want to target operations performed on collections (any group of things, not just containers) and exploit parallelism in data and algorithm structures. Here, we use key terms like \"scalability\" and \"throughput\"; \"data-driven\" and \"fine-grained\" and \"schedule\"; and \"side effect\" and \"reduction.\" \u5f53\u4ecahardware\u7684\u8ba1\u7b97\u7279\u6027 New hardware no longer delivers the \"free lunch\" of automatically running single-threaded code faster to the degree it did historically. Instead, it provides increasing capacity to run more tasks concurrently on more CPU cores and hardware threads. How can we write applications that will regain the free lunch, that we can ship today and know they will naturally execute faster on future machines having ever greater parallelism. The key to scalability NOTE: \u8fd9\u5728 2 How Much Scalability Do You Have or Need? (Sep 2007) \u4e2d\uff0c\u4f1a\u8fdb\u4e00\u6b65\u5c55\u5f00 The key to scalability is not to divide the computation-intensive work across some fixed number of explicit threads hard-coded into the structure of the application (for instance, when a game might try to divide its computation work among a physics thread, a rendering thread, and an everything-else thread). As we'll see next month, that path leads to an application that prefers to run on some constant number K of cores, which can penalize a system with fewer than K cores and doesn't scale on a system with more than K cores. That's fine if you're targeting a known fixed hardware configuration, like a particular game console whose architecture isn't going to change until the next console generation, but it isn't scalable to hardware that supports greater parallelism. Rather, the key to scalability is to express lots of latent concurrency in the program that scales to match its inputs (number of messages, size of data). We do this in two main ways. The first way The first way is to use libraries and abstractions that let you say what you want to do rather than specifically how to do it. Today, we may use tools like OpenMP to ask to execute a loop's iterations in parallel and let the runtime system decide how finely to subdivide the work to fit the number of cores available. Tomorrow, tools like parallel STL and parallel LINQ [5] will let us express queries like \"select the names of all undergraduate students sorted by grade\" that can be executed in parallel against an in-memory container as easily as they are routinely executed in parallel by a SQL database server. The second way The second way, is to explicitly express work that can be done in parallel. Today, we can do this by explicitly running work items on a thread pool (for instance, using Java ThreadPoolExecutor or .NET BackgroundWorker ). Just remember that there is overhead to moving the work over to a pool, so the onus(\u8d23\u4efb) is on us to make sure the work is big enough to make that worthwhile. For example, we might implement a recursive algorithm like quicksort to at each step sort the left and right subranges in parallel if the subranges are large enough, or serially if they are small. NOTE: 1\u3001\"fork-join-parallel-divide-and-conquer-and-merge\" Future runtime systems based on work stealing will make this style even easier by letting us simply express all possible parallelism without worrying if it's big enough, and rely on the runtime system to dynamically decide not to actually perform the work in parallel if it isn't worth it on a given user's machine (or with the load of other work in the system at a given time), with an acceptably low cost for the unrealized parallelism (for example, if the system decides to run it serially, we would want the performance penalty compared to if we had just written the recursive call purely serially in the first place to be similar to the overhead of calling an empty function). Pillar 3: Consistency Via Safely Shared Resources Pillar 3 is all about dealing with shared resources, especially shared memory state, without either corruption or deadlock. Here we use key terms like acquire and release; read and write; and atomic and consistent and transaction. In these columns, I'll mostly focus on dealing with mutable objects in shared memory. Lock Today's status quo for synchronizing access to mutable shared objects is locks. Locks are known to be inadequate (see [3] and [4]), but they are nevertheless the best general-purpose tools we have. Some frameworks provide selected lock-free data structures (hash tables) that are internally synchronized using atomic variables so that they can be used safely without taking locks either internally inside the data structure implementation or externally in your calling code; these are useful, but they are not a way to avoid locking in general because they are few and many common data structures have no known lock-free implementations at all. Future work In the future, we can look forward to improved support for locks (for example, being able to express lock levels/hierarchies in a portable way, and what data is protected by what lock) and probably transactional memory (where the idea is to automatically version memory, so that the programmer can just write \"begin transaction; do work; end transaction\" like we do with databases and let the system handle synchronization and contention automatically). Until we have those, though, learn to love locks. Composability: More Than The Sum of the Parts Because the pillars address independent issues, they also compose well, so that a given technique or pattern can apply elements from more than one category. For example, an application can move an expensive tree traversal from the main GUI thread to run in the background to keep the GUI free to pump new messages (responsiveness, Pillar 1), while the tree traversal task itself can internally exploit the parallelism in the tree to traverse it in parallel and compute the result faster (throughput, Pillar 2). The two techniques are independent of each other and target different goals using different patterns and techniques, but can be used effectively together: The user has an application that is responsive no matter how long the computation takes on a less-powerful machine; he also has a scalable application that runs faster on more powerful hardware. Conversely, you can use this framework as a tool to decompose concurrency tools, requirements, and techniques into their fundamental parts. By better understanding the parts and how they relate, we can get a more accurate understanding of exactly what the whole is trying to achieve and evaluate whether it makes sense, whether it's a good approach, or how it can be improved by changing one of the fundamental pieces while leaving the others intact. Summary Have a consistent mental model for reasoning about concurrency \u2014 including requirements, tradeoffs, patterns, techniques, and technologies both current and future. Distinguish among the goals of responsiveness (by doing work asynchronously), throughput (by minimizing time to solution), and consistency (by avoiding corruption due to races and deadlocks). In future columns, I'll dig into various specific aspects of these three pillars. Next month, we'll answer the question, \"how much concurrency does your application have or need?\" and distinguish between O(1) , O(K) , and O(N) concurrency. Stay tuned. Notes [1] The elephant analogy and the pillar segmentation were created by David Callahan ( www.microsoft.com/presspass/exec/de/Callahan/ default.mspx) in an unpublished work. [2] H. Sutter. \"The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software\" ( www.ddj.com/dept/architect/184405990 ). [3] H. Sutter. \"The Trouble With Locks\" ( www.ddj.com/dept/cpp/184401930 ) [4] H. Sutter and J. Larus. \"Software and the Concurrency Revolution\" ( ACM Queue , September 2005). (gotw.ca/publications/concurrency-acm.htm). [5] J. Duffy, http://www.bluebytesoftware.com/ blog/PermaLink,guid,81ca9c00-b43e-4860-b96b-4fd2bd735c9f.aspx.","title":"Introduction"},{"location":"Multithread/Expert-Herb-Sutter/01-The-Pillars-of-Concurrency/#drdobbs#the#pillars#of#concurrency","text":"In his inaugural column, Herb makes the case that we must build a consistent mental model before talking about concurrency. NOTE: 1\u3001\"Pillar\"\u7684\u542b\u4e49\u662f\"\u652f\u67f1\" 2\u3001\u8fd9\u7bc7\u6587\u7ae0\u4f5c\u8005\u7ed9\u51fa\u4e86\u7406\u89e3concurrency\u7684\u4e00\u4e2a\u7406\u8bba\u6846\u67b6","title":"drdobbs The Pillars of Concurrency"},{"location":"Multithread/Expert-Herb-Sutter/01-The-Pillars-of-Concurrency/#callahans#pillars","text":"My colleague David Callahan leads a team within Visual Studio that is working on programming models for concurrency . He has pointed out that fundamental concurrency requirements and techniques fall into three basic categories, or pillars. They are summarized in Table 1 [1]. Understanding these pillars gives us a framework for reasoning clearly about all aspects of concurrency \u2014 from the concurrency requirements and tradeoffs that matter to our current project, to why specific design patterns and implementation techniques are applicable to getting specific results and how they are liable(\u6709\u8d23\u4efb\u3001\u6709\u4e49\u52a1) to interact, and even to evaluating how future tools and technologies will fit with our needs. NOTE: 1\u3001\u603b\u7ed3\u51fa programming models for concurrency \u3001 framework \u662f\u975e\u5e38\u91cd\u8981\u7684\u4e00\u6b65 Let's consider an overview of each pillar in turn, note why techniques in different pillars compose well, and see how this framework helps to clarify our vocabulary.","title":"Callahan's Pillars"},{"location":"Multithread/Expert-Herb-Sutter/01-The-Pillars-of-Concurrency/#table#1#the#pillars#of#concurrency","text":"NOTE: 1\u3001\u4e0a\u8ff0\u603b\u7ed3\u5f97\u975e\u5e38\u597d","title":"Table 1: The Pillars of Concurrency."},{"location":"Multithread/Expert-Herb-Sutter/01-The-Pillars-of-Concurrency/#pillar#1#responsiveness#and#isolation#via#asynchronous#agents","text":"NOTE: \u4e00\u3001\u7b80\u800c\u8a00\u4e4b: \u901a\u8fc7asynchronous agent\u6765\u5b9e\u73b0Responsiveness and Isolation \u4e8c\u3001\u6211\u7684\u5b9e\u8df5\u662f: \u884c\u60c5\u63d2\u4ef6 \u5c06\u91c7\u96c6\u5230\u7684\u884c\u60c5\u653e\u5230 \u53d1\u9001\u961f\u5217\uff0c\u7136\u540e\u7531\u53d1\u9001\u7ebf\u7a0b\u8fdb\u884c\u53d1\u9001\uff0c\u5373\"\u5f02\u6b65\u53d1\u9001\" Pillar 1 is all about running separate tasks, or agents, independently and letting them communicate via asynchronous messages. We particularly want to avoid blocking, especially on user-facing and other time-sensitive threads, by running expensive work asynchronously. Also, isolating separable tasks makes them easier to test separately and then deploy into various parallel contexts with confidence. Here we use key terms like \"interactive\" and \"responsive\" and \"background\"; \"message\" and \"dialogue\"; and \"timeout\" and \"cancel.\"","title":"Pillar 1: Responsiveness and Isolation Via Asynchronous Agents"},{"location":"Multithread/Expert-Herb-Sutter/01-The-Pillars-of-Concurrency/#responsiveness","text":"NOTE: \u4e00\u3001\u8fd9\u91cc\u4f5c\u8005\u4ee5GUI application\u4e3a\u4f8b\u6765\u8bf4\u660eresponsiveness A typical Pillar 1 technique is to move expensive work off an interactive application's main GUI pump thread. NOTE: 1\u3001\u5c06\u8017\u65f6\u64cd\u4f5c\u653e\u5230\u4e00\u4e2a\u5355\u72ec\u7684thread\uff0c\u8ba9\u5b83\u5f02\u6b65\u5730\u6267\u884c\uff0cmain GUI pump thread\u5c31\u80fd\u591f\u7ee7\u7eed\u8fd0\u884c\u800c\u4e0d\u963b\u585e\uff0c\u4ece\u800c\u4fdd\u8bc1\u4e86 responsiveness We never want to freeze our display for seconds or longer; users should still be able to keep clicking away and interacting with a responsive GUI while the hard work churns away in the background. It's okay for users to experience a change in the application while the work is being performed (for example, some buttons or menu items might be disabled, or an animated icon or progress bar might indicate status of the background work), but they should never experience a \"white screen of death\"\u2014a GUI thread that stops responding to basic messages like \"repaint\" for a while because the new messages pile up behind one that's taking a long time to process synchronously.","title":"Responsiveness"},{"location":"Multithread/Expert-Herb-Sutter/01-The-Pillars-of-Concurrency/#what#kind#of#work#to#ship#out#of#responsiveness-sensitive#threads","text":"NOTE: 1\u3001\u4e3a\u4e86\u4fdd\u6301responsiveness\uff0c\u5e94\u8be5\u5c06\u90a3\u4e9b\u4eceresponsiveness-sensitive threads\u4e2d\u79fb\u51fa\uff1f\u663e\u7136\u662f\u90a3\u4e9b\u8017\u65f6\u7684\u3002 So what kind of work do we want to ship out of(\u4ece...\u4e2d\u79fb\u51fa) responsiveness-sensitive threads? It can be work that performs an expensive or high-latency computation (background compilation or print rendering, for instance) or actual blocking (idle waiting for a lock, a database result, or a web service reply). Some of these tasks merely want to return a value; others will interact more to provide intermediate results or accept additional input as they make progress on their work.","title":"What kind of work to ship out of responsiveness-sensitive threads?"},{"location":"Multithread/Expert-Herb-Sutter/01-The-Pillars-of-Concurrency/#how#should#the#independent#tasks#communicate","text":"Finally, how should the independent tasks communicate? A key is to have the communication itself be asynchronous, preferably using asynchronous messages where possible because messages are nearly always preferable to sharing objects in memory (which is Pillar 3's territory(\u9886\u57df)). In the case of a GUI thread, this is an easy fit because GUIs already use message-based event-driven models.","title":"How should the independent tasks communicate?"},{"location":"Multithread/Expert-Herb-Sutter/01-The-Pillars-of-Concurrency/#implementation","text":"NOTE: 1\u3001\u672c\u6bb5\u4e3b\u8981\u63cf\u8ff0\u5982\u4f55\u5b9e\u73b0Asynchronous Agents Today, we typically express Pillar 1 by running the background work on its own thread or as a work item on a thread pool; the foreground task that wants to stay responsive is typically long-running and is usually a thread; and communication happens through message queues and message-like abstractions like futures (Java Future , .NET IAsyncResult ). In coming years, we'll get new tools and abstractions in this pillar, where potential candidates include active objects/services (objects that conceptually run on their own thread, and calling a method is an asynchronous message); channels of communication between two or more tasks; and contracts that let us explicitly express, enforce, and validate the expected order of messages. This pillar is not about keeping hundreds of cores busy; that job belongs to Pillar 2. Pillar 1 is all about responsiveness, asynchrony, and independence; but it may keep some number of cores busy purely as a side effect, because it still expresses work that can be done independently, and therefore, in parallel.","title":"Implementation"},{"location":"Multithread/Expert-Herb-Sutter/01-The-Pillars-of-Concurrency/#pillar#2#throughput#and#scalability#via#concurrent#collections","text":"NOTE: \u4e00\u3001\u672c\u6bb5\u4e3b\u8981\u63cf\u8ff0\u7684\u662f\u5982\u4f55\u5145\u5206\u53d1\u6325hardware\u7684computation power\uff0c\u4ece\u800c\"re-enabling the \"free lunch\"\" \u4e8c\u3001\"Collections\"\u7684\u542b\u4e49\u662f\u4ec0\u4e48\u5462\uff1f\u4e0b\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u8fdb\u884c\u4e86\u89e3\u91ca: \"We particularly want to target operations performed on collections (any group of things, not just containers) and exploit parallelism in data and algorithm structures\" \u5728 2 How Much Scalability Do You Have or Need? (Sep 2007) \u4e2d\uff0c\u5c06\u8fd9\u4e00\u6761\u603b\u7ed3\u4e3a: Pillar 2, scalable throughput by exploiting parallelism in algorithms and data structures; \u663e\u7136\uff0c\u4e0a\u8ff0\u662f\u66f4\u52a0\u6613\u4e8e\u7406\u89e3\u7684\u3002 Pillar 2, on the other hand, is about keeping hundreds of cores busy to compute results faster, thereby re-enabling the \"free lunch\"[2]. We particularly want to target operations performed on collections (any group of things, not just containers) and exploit parallelism in data and algorithm structures. Here, we use key terms like \"scalability\" and \"throughput\"; \"data-driven\" and \"fine-grained\" and \"schedule\"; and \"side effect\" and \"reduction.\"","title":"Pillar 2: Throughput and Scalability Via Concurrent Collections"},{"location":"Multithread/Expert-Herb-Sutter/01-The-Pillars-of-Concurrency/#hardware","text":"New hardware no longer delivers the \"free lunch\" of automatically running single-threaded code faster to the degree it did historically. Instead, it provides increasing capacity to run more tasks concurrently on more CPU cores and hardware threads. How can we write applications that will regain the free lunch, that we can ship today and know they will naturally execute faster on future machines having ever greater parallelism.","title":"\u5f53\u4ecahardware\u7684\u8ba1\u7b97\u7279\u6027"},{"location":"Multithread/Expert-Herb-Sutter/01-The-Pillars-of-Concurrency/#the#key#to#scalability","text":"NOTE: \u8fd9\u5728 2 How Much Scalability Do You Have or Need? (Sep 2007) \u4e2d\uff0c\u4f1a\u8fdb\u4e00\u6b65\u5c55\u5f00 The key to scalability is not to divide the computation-intensive work across some fixed number of explicit threads hard-coded into the structure of the application (for instance, when a game might try to divide its computation work among a physics thread, a rendering thread, and an everything-else thread). As we'll see next month, that path leads to an application that prefers to run on some constant number K of cores, which can penalize a system with fewer than K cores and doesn't scale on a system with more than K cores. That's fine if you're targeting a known fixed hardware configuration, like a particular game console whose architecture isn't going to change until the next console generation, but it isn't scalable to hardware that supports greater parallelism. Rather, the key to scalability is to express lots of latent concurrency in the program that scales to match its inputs (number of messages, size of data). We do this in two main ways.","title":"The key to scalability"},{"location":"Multithread/Expert-Herb-Sutter/01-The-Pillars-of-Concurrency/#the#first#way","text":"The first way is to use libraries and abstractions that let you say what you want to do rather than specifically how to do it. Today, we may use tools like OpenMP to ask to execute a loop's iterations in parallel and let the runtime system decide how finely to subdivide the work to fit the number of cores available. Tomorrow, tools like parallel STL and parallel LINQ [5] will let us express queries like \"select the names of all undergraduate students sorted by grade\" that can be executed in parallel against an in-memory container as easily as they are routinely executed in parallel by a SQL database server.","title":"The first way"},{"location":"Multithread/Expert-Herb-Sutter/01-The-Pillars-of-Concurrency/#the#second#way","text":"The second way, is to explicitly express work that can be done in parallel. Today, we can do this by explicitly running work items on a thread pool (for instance, using Java ThreadPoolExecutor or .NET BackgroundWorker ). Just remember that there is overhead to moving the work over to a pool, so the onus(\u8d23\u4efb) is on us to make sure the work is big enough to make that worthwhile. For example, we might implement a recursive algorithm like quicksort to at each step sort the left and right subranges in parallel if the subranges are large enough, or serially if they are small. NOTE: 1\u3001\"fork-join-parallel-divide-and-conquer-and-merge\" Future runtime systems based on work stealing will make this style even easier by letting us simply express all possible parallelism without worrying if it's big enough, and rely on the runtime system to dynamically decide not to actually perform the work in parallel if it isn't worth it on a given user's machine (or with the load of other work in the system at a given time), with an acceptably low cost for the unrealized parallelism (for example, if the system decides to run it serially, we would want the performance penalty compared to if we had just written the recursive call purely serially in the first place to be similar to the overhead of calling an empty function).","title":"The second way"},{"location":"Multithread/Expert-Herb-Sutter/01-The-Pillars-of-Concurrency/#pillar#3#consistency#via#safely#shared#resources","text":"Pillar 3 is all about dealing with shared resources, especially shared memory state, without either corruption or deadlock. Here we use key terms like acquire and release; read and write; and atomic and consistent and transaction. In these columns, I'll mostly focus on dealing with mutable objects in shared memory.","title":"Pillar 3: Consistency Via Safely Shared Resources"},{"location":"Multithread/Expert-Herb-Sutter/01-The-Pillars-of-Concurrency/#lock","text":"Today's status quo for synchronizing access to mutable shared objects is locks. Locks are known to be inadequate (see [3] and [4]), but they are nevertheless the best general-purpose tools we have. Some frameworks provide selected lock-free data structures (hash tables) that are internally synchronized using atomic variables so that they can be used safely without taking locks either internally inside the data structure implementation or externally in your calling code; these are useful, but they are not a way to avoid locking in general because they are few and many common data structures have no known lock-free implementations at all.","title":"Lock"},{"location":"Multithread/Expert-Herb-Sutter/01-The-Pillars-of-Concurrency/#future#work","text":"In the future, we can look forward to improved support for locks (for example, being able to express lock levels/hierarchies in a portable way, and what data is protected by what lock) and probably transactional memory (where the idea is to automatically version memory, so that the programmer can just write \"begin transaction; do work; end transaction\" like we do with databases and let the system handle synchronization and contention automatically). Until we have those, though, learn to love locks.","title":"Future work"},{"location":"Multithread/Expert-Herb-Sutter/01-The-Pillars-of-Concurrency/#composability#more#than#the#sum#of#the#parts","text":"Because the pillars address independent issues, they also compose well, so that a given technique or pattern can apply elements from more than one category. For example, an application can move an expensive tree traversal from the main GUI thread to run in the background to keep the GUI free to pump new messages (responsiveness, Pillar 1), while the tree traversal task itself can internally exploit the parallelism in the tree to traverse it in parallel and compute the result faster (throughput, Pillar 2). The two techniques are independent of each other and target different goals using different patterns and techniques, but can be used effectively together: The user has an application that is responsive no matter how long the computation takes on a less-powerful machine; he also has a scalable application that runs faster on more powerful hardware. Conversely, you can use this framework as a tool to decompose concurrency tools, requirements, and techniques into their fundamental parts. By better understanding the parts and how they relate, we can get a more accurate understanding of exactly what the whole is trying to achieve and evaluate whether it makes sense, whether it's a good approach, or how it can be improved by changing one of the fundamental pieces while leaving the others intact.","title":"Composability: More Than The Sum of the Parts"},{"location":"Multithread/Expert-Herb-Sutter/01-The-Pillars-of-Concurrency/#summary","text":"Have a consistent mental model for reasoning about concurrency \u2014 including requirements, tradeoffs, patterns, techniques, and technologies both current and future. Distinguish among the goals of responsiveness (by doing work asynchronously), throughput (by minimizing time to solution), and consistency (by avoiding corruption due to races and deadlocks). In future columns, I'll dig into various specific aspects of these three pillars. Next month, we'll answer the question, \"how much concurrency does your application have or need?\" and distinguish between O(1) , O(K) , and O(N) concurrency. Stay tuned.","title":"Summary"},{"location":"Multithread/Expert-Herb-Sutter/01-The-Pillars-of-Concurrency/#notes","text":"[1] The elephant analogy and the pillar segmentation were created by David Callahan ( www.microsoft.com/presspass/exec/de/Callahan/ default.mspx) in an unpublished work. [2] H. Sutter. \"The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software\" ( www.ddj.com/dept/architect/184405990 ). [3] H. Sutter. \"The Trouble With Locks\" ( www.ddj.com/dept/cpp/184401930 ) [4] H. Sutter and J. Larus. \"Software and the Concurrency Revolution\" ( ACM Queue , September 2005). (gotw.ca/publications/concurrency-acm.htm). [5] J. Duffy, http://www.bluebytesoftware.com/ blog/PermaLink,guid,81ca9c00-b43e-4860-b96b-4fd2bd735c9f.aspx.","title":"Notes"},{"location":"Multithread/Expert-Herb-Sutter/02-How-Much-Scalability-Do-You-Have-or-Need/","text":"drdobbs How Much Scalability Do You Have or Need? NOTE: 1\u3001\u5982\u4f55\u83b7\u5f97 Scalability How many cores (or hardware threads) can your code harness to get its answers faster? In your application, how many independent pieces of work are ready to run at any given time? Put another way, how many cores (or hardware threads, or nodes) can your code harness to get its answers faster? And when should the answers to these questions not be \"as many as possible\"? Last month [2], we looked at three \"pillars\" of concurrency: Pillar 1, isolation by structuring work asynchronously and communicating through messages; Pillar 2, scalable throughput by exploiting parallelism in algorithms and data structures; and Pillar 3, dealing with mutable shared state. This month's topic delves into throughput-oriented techniques using techniques in Pillar 1 and Pillar 2. Order O(1): Single-Core O(K): Fixed O(N): Scalable Tagline One thing at a time Explicit threading Re-enable the free lunch Summary Sequential applications, and bottlenecked parallel applications Explicitly express how much work can be done in parallel Express lots of latent(\u6f5c\u5728\u7684) concurrency in a way that can be efficiently mapped to N cores Examples Multithreaded code convoyed on a global lock or message queue, occasional or intermittent background work Pipelining, hardwired division of tasks, regular or continuous background computation Tree traversal, quicksort, compilation Applicability Single-core hardware, single-threaded OS, or nonCPU-bound app Hardware with fixed concurrency, or app whose CPU-bound parts have limited scalability Hardware with variable (esp. growing) concurrency, and app with CPU-bound parts that are scalably parallelizable Examples Code targeting legacy hardware, small embedded systems, single-core game consoles; simple text processor Game targeting one multicore game console generation; code whose key operations are order-sensitive (e.g., can be pipelined but not fully parallelized) Mainstream desktop or server software with CPU-bound features and targeting commodity hardware or future upgradeable game consoles Pillar (see Notes [2]), and today's mainstream tools Pillar 1: Threads, message queues, futures Pillar 2: Thread pools, futures, OpenMP O(1): Sequential Code(\u4e32\u884c) O(1) means that the program typically has one CPU-intensive piece of work available to be actively executed at any given time. It may occasionally perform multiple concurrent operations, such as occasional background work in addition to the main foreground work, but the extra work is not ongoing and/or doesn't keep more than a single core busy. This category includes not only all sequential code , but also every concurrent application with throughput that is as good as sequential because its threads execute serially, such as by being convoyed on a global lock or message queue. The free throughput lunch is over for both these kinds of O(1) code, but the fully serial option tends to have the additional liability of poorer responsiveness, while a concurrent application tends to be better structured to do background work asynchronously[1, 2]. NOTE: 1\u3001\u5e76\u53d1\u4e32\u884c\u662f\u7ecf\u5e38\u4f1a\u51fa\u73b0\u7684: \"concurrent application with throughput that is as good as sequential because its threads execute serially, such as by being convoyed on a global lock or message queue\" In all O(1) cases, if we want better throughput for CPU-bound operations, essentially, our only option is to try to optimize our code in traditional ways because adding more cores won't help. NOTE: 1\u3001\u8fd9\u662f\u975e\u5e38\u91cd\u8981\u7684\u4e00\u4e2a\u4e8b\u5b9e O(K): Explicitly Threaded Code O(K) means that the system typically has some constant number of things it can do to keep a constant number of cores busy at any given time. That number is hardwired into the structure of the program and will be the same regardless of the amount of hardware concurrency available at execution time. Example: first-person action game NOTE: 1\u3001\u6211\u4eec\u4f1a\u7ecf\u5e38\u91c7\u7528\u8fd9\u79cd\u8bbe\u8ba1 For example, consider a first-person action game. To take some advantage of additional cores, let's say we divide the game's compute-bound work into three threads: Thread 1 does game physics, thread 2 does rendering, and thread 3 does nonplayer character AI. For simplicity, assume that all three threads are equally busy and interdependent. The game runs fine on a single-core machine; the operating system just interleaves the threads on the one core. When the user upgrades to a two-core machine, the game runs faster\u2014but not twice as fast, because if we schedule thread 1 on core 1, and thread 2 on core 2, we have to put thread 3 somewhere. If we put thread 3 on the same core as thread 1, then thread 2 (which depends on 1 and/or 3) will be idle half the time. But if we schedule thread 3 on both core 1 and core 2, we incur cache sloshing and other overhead every time we move it from one core to the other. So the game runs faster on a two-core system, just not twice as fast. When the user upgrades to a four-core machine, the game runs faster still\u2014but only three times as fast as on a single core, or maybe slightly better if spyware and other applications can be moved to the fourth core. When the user upgrades to an eight-core machine, nothing happens. When he upgrades to a 16-core machine, more nothing happens. The O(3) application is hardwired to prefer three cores regardless of the input or execution environment. Example: pipelining NOTE: 1\u3001\u8fd9\u662f\u4e00\u79cd\u5e76\u884c\u5316\u7684\u6280\u5de7 2\u3001\"4\" A closely related technique is pipelining, which is useful when we have a collection of things to operate on, but we can't apply O(N) techniques to fully parallelize them because of ordering dependencies. For example, consider a communications subsystem that packets data to be sent over the network. The input to the subsystem is a stream of raw packets; before sending a given packet, it must first be decorated with header information, then compressed, and then encrypted. Those three operations can't easily be run in parallel for a given packet, in part because they must be applied in that order. A solution is to have three agents that communicate using message queues: The Decorator agent (typically a thread) decorates each raw packet and sends it to the Compressor agent; the Compressor accepts the decorated packet, compresses it, and sends it to the Encryptor ; and the Encryptor encrypts it and sends it to the network. Like the game example, this subsystem also has O(3) parallelism if the pipeline stages are equally busy. We are using Pillar 1 techniques (isolated agents, messaging; see [2] for details) because it's a natural fit for expressing pipelines: The pipeline stages are relatively isolated, and using messaging lets us tolerate any difference in execution rates between the stages. NOTE: 1\u3001\u5178\u578b\u7684\"protocol interceptor\u62e6\u622a\u5668decorator-pattern-assemble ability\" \u7ed3\u8bba An O(K) application is explicitly structured to prefer K cores for a given input workload. O(K) code can't scale to take advantage of environments with more than K cores, and it can penalize execution environments with fewer than K cores by interfering with load balancing and causing some of the cores to be only partly utilized. Comparing O(1) and O(K) Alert readers may already have noticed that in other computer science contexts we usually don't distinguish between O(1) and O(K) complexity because big-Oh notation typically ignores the constant factor; after all, K =1 is just a special case. That's exactly true, and similarly here O(K) is far closer to O(1) than it is to O(N). Both O(1) and O(K) hardwire concurrency into the explicit structure of the code, and neither is scalable to arbitrary numbers of cores. Even so, it's well worth distinguishing between O(1) and O(K ). O(1) is an important special case because it targets three significant situations: 1\u3001Single-core hardware, including legacy hardware and some game consoles, such as Nintendo Wii. 2\u3001Operating systems that don't support concurrency, including some that have no concept of a thread. 3\u3001Applications that aren't CPU-bound, where their current feature set won't drive more than one core's worth of computation no matter how you slice them (a simple text processor, for instance). NOTE: 1\u3001\"simple text processor\" \"aren't CPU-bound\" \u662f\u975e\u5e38\u5bb9\u6613\u7406\u89e3\u7684\uff0c\u56e0\u4e3a\u5b83\u9700\u8981\u7b49\u5f85\u7528\u6237\u8f93\u5165\uff0c\u8fd9\u8ba9\u6211\u60f3\u8d77\u4e86Redis is not CPU-bound\u7684\u8bba\u65ad\uff0cRedis\u548c\"simple text processor\" \u4e00\u6837\uff0c\u662f\"IO-bound\"\u7684 If you have no reason or capability to escape one of those constraints, it probably doesn't make sense to invest(\u6295\u8d44) heavily in making your code O(K) or better because the extra concurrency will never be exercised on such systems. But don't forget that, even in the O(1) world, concurrency is a primary path to better responsiveness, and some basic techniques like using an event-driven program structure will work even in the most constrained case of running on an OS that has no support for actual threads. O(K) is appropriate especially for code that targets domains with fixed concurrency: 1\u3001Fixed hardware targets: A notable situation is when you're targeting one multicore game console generation. For example, when developing a game for XBox 360, it makes sense to write an O(6) application, because that generation of the console is fixed at six hardware threads (three cores with two hardware threads each); similarly, PlayStation 3 naturally lends itself to O(8) or O(9) applications because it is fixed at 1+8 cores (one general-purpose core, and eight special-purpose cores). These configurations will not change until each console's next major hardware generation. 2\u3001Applications whose key operations cannot be fully parallelized: The earlier packet-sending example illustrates a case where a CPU-intensive operation has internal ordering dependencies; its parts are not sufficiently independent to let them be run fully in parallel. As we saw, pipelining is a classic approach to extract(\u63d0\u53d6) some parallelism out of an otherwise inherently(\u5185\u5728\u5730) serial(\u4e32\u884c) operation; but basic pipelining is at best O(K) , for a pipeline with K stages of relatively equal cost. Although O(K) is not scalable, it can be deployed tactically(\u7b56\u7565\u9ad8\u660e\u5730) to temporarily extend the free lunch. NOTE: \"async method invocat-message queue-mailbox-pipeline parallel \u4e32\u8054\u5e76\u884c\u6d41\u7a0b\" In O(K) in particular, there are some fixed costs of making the code concurrent that we will incur at runtime on even a single-core machine, including work to manage messages, perform context switches , and lock shared data . However, by applying concurrency, we can get some limited scalability, and often also better responsiveness. Note that O(1) and O(K) situations are mostly described by what you can't do. If you can target hardware that has variable parallelism (especially increasing parallelism), and your operating system supports concurrency, and you can find enough independence inside your application's CPU-intensive operations to make them scale, prefer aiming for O(N ). O(N): Scalable Throughput And the Free Lunch The key to scalable throughput is to express lots of latent concurrency that isn't explicitly coded in the program and that scales to match its inputs (number of messages, size of data, and so on) and that can be efficiently mapped down at execution time to the variable number N cores available on a given machine. We find the scalable concurrency opportunities principally by exploiting natural parallelism : 1\u3001Exploit parallelism in algorithm structures: For example, recursive sorting can exploit the natural parallelism in its divide-and-conquer structure. 2\u3001Exploit parallelism in data structures: For example, tree traversal can often exploit the independence in each node's subtrees. Compilation can exploit independence at several levels in the structure of source code, from coarse-grained independence among source files to finer-grained independence among classes or methods within a file. NOTE: 1\u3001divide-and-conquer This lets us decompose the application into a \"sea of chores\"\u2014expressing independent chunks of work that are big or small, blocking or nonblocking, structured subdivisions or independent; but we often want to look first for the CPU-bound work\u2014and rely on the runtime system to \"rightsize\" the application by assigning those chores efficiently to whatever hardware parallelism is available on a given user's system. Implementation OpenMP supports some constrained O(N) styles, but it is primarily intended for use with integer-indexed loops over arrays and doesn't work well with iteration-based containers in STL, .NET, or Java. Instead, as mentioned in last month's column [2], today, one common idiom for expressing such a sea of work items is to explicitly schedule chores for execution on a thread pool (for example, using Java ThreadPoolExecutor or .NET BackgroundWorker ). Unfortunately, this incurs significant context-switch overhead and so you have to ensure that a work item will be worth shipping over to a worker thread. In the future, this constraint will be relaxed as languages and runtimes improve. Re-enabling the free lunch O(N) is the key to re-enabling the free lunch and keeping lots of cores busy, because it lets us express applications that can run on yesterday's single-core machine, run better on today's four-core machine, run better still on tomorrow's 64-core machine, and so on until we exhaust the inherent limit of CPU-boundedness in the application. For a thread pool-driven program, on a single-core machine the system can spin up a single worker thread that runs the program by serially pulling chore after chore from the sea and executing it; on an eight-core machine, it can spin up eight threads; and so on [4]. As with O(K) , O(N) can have costs at runtime on even a single-core machine. In addition to the costs mentioned for O(K) , there can be extra work inherent in divide-and-conquer techniques (e.g., reductions such as piecing together a grand total from intermediate results), and the cost of locking shared data can now also increase. However, by applying concurrency, we can often get good scalability that far offsets the overhead. Using O(N) is highly desirable for software that expects to run on a variety of current and future hardware having variable amounts of hardware parallelism \u2014which happens to now be the target for all mainstream desktop and server software. If your application doesn't currently have key CPU-bound operations that are amenable to full O(N) parallelization, don't give up: Consider finding new desirable features that are amenable to O(N) , or at least more O(K ), and you will still be able to deliver software that runs well on today's hardware, better on tomorrow's hardware, and better still on future systems. Notes [1] I use \"cores\" as a simple shorthand measure of execution hardware parallelism. For applications running on one local machine the appropriate measure is usually \"total hardware threads,\" meaning #sockets\u00d7 #cores/socket\u00d7#threads/core; and for distributed applications the appropriate measure is usually \"nodes.\" [2] H. Sutter. \" The Pillars of Concurrency \" ( DDJ , 32(7), August 2007, http://ddj.com/dept/64bit/200001985 ). [3] H. Sutter. \" The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software \" ( DDJ , 30(3), March 2005, http://ddj.com/dept/webservices/184405990 ). [4] The details are more complex. For example, a pool should spin up extra worker threads to keep the system busy whenever one chore blocks to wait for some event and so temporarily idles its worker thread.","title":"Introduction"},{"location":"Multithread/Expert-Herb-Sutter/02-How-Much-Scalability-Do-You-Have-or-Need/#drdobbs#how#much#scalability#do#you#have#or#need","text":"NOTE: 1\u3001\u5982\u4f55\u83b7\u5f97 Scalability How many cores (or hardware threads) can your code harness to get its answers faster? In your application, how many independent pieces of work are ready to run at any given time? Put another way, how many cores (or hardware threads, or nodes) can your code harness to get its answers faster? And when should the answers to these questions not be \"as many as possible\"? Last month [2], we looked at three \"pillars\" of concurrency: Pillar 1, isolation by structuring work asynchronously and communicating through messages; Pillar 2, scalable throughput by exploiting parallelism in algorithms and data structures; and Pillar 3, dealing with mutable shared state. This month's topic delves into throughput-oriented techniques using techniques in Pillar 1 and Pillar 2. Order O(1): Single-Core O(K): Fixed O(N): Scalable Tagline One thing at a time Explicit threading Re-enable the free lunch Summary Sequential applications, and bottlenecked parallel applications Explicitly express how much work can be done in parallel Express lots of latent(\u6f5c\u5728\u7684) concurrency in a way that can be efficiently mapped to N cores Examples Multithreaded code convoyed on a global lock or message queue, occasional or intermittent background work Pipelining, hardwired division of tasks, regular or continuous background computation Tree traversal, quicksort, compilation Applicability Single-core hardware, single-threaded OS, or nonCPU-bound app Hardware with fixed concurrency, or app whose CPU-bound parts have limited scalability Hardware with variable (esp. growing) concurrency, and app with CPU-bound parts that are scalably parallelizable Examples Code targeting legacy hardware, small embedded systems, single-core game consoles; simple text processor Game targeting one multicore game console generation; code whose key operations are order-sensitive (e.g., can be pipelined but not fully parallelized) Mainstream desktop or server software with CPU-bound features and targeting commodity hardware or future upgradeable game consoles Pillar (see Notes [2]), and today's mainstream tools Pillar 1: Threads, message queues, futures Pillar 2: Thread pools, futures, OpenMP","title":"drdobbs How Much Scalability Do You Have or Need?"},{"location":"Multithread/Expert-Herb-Sutter/02-How-Much-Scalability-Do-You-Have-or-Need/#o1#sequential#code","text":"O(1) means that the program typically has one CPU-intensive piece of work available to be actively executed at any given time. It may occasionally perform multiple concurrent operations, such as occasional background work in addition to the main foreground work, but the extra work is not ongoing and/or doesn't keep more than a single core busy. This category includes not only all sequential code , but also every concurrent application with throughput that is as good as sequential because its threads execute serially, such as by being convoyed on a global lock or message queue. The free throughput lunch is over for both these kinds of O(1) code, but the fully serial option tends to have the additional liability of poorer responsiveness, while a concurrent application tends to be better structured to do background work asynchronously[1, 2]. NOTE: 1\u3001\u5e76\u53d1\u4e32\u884c\u662f\u7ecf\u5e38\u4f1a\u51fa\u73b0\u7684: \"concurrent application with throughput that is as good as sequential because its threads execute serially, such as by being convoyed on a global lock or message queue\" In all O(1) cases, if we want better throughput for CPU-bound operations, essentially, our only option is to try to optimize our code in traditional ways because adding more cores won't help. NOTE: 1\u3001\u8fd9\u662f\u975e\u5e38\u91cd\u8981\u7684\u4e00\u4e2a\u4e8b\u5b9e","title":"O(1): Sequential Code(\u4e32\u884c)"},{"location":"Multithread/Expert-Herb-Sutter/02-How-Much-Scalability-Do-You-Have-or-Need/#ok#explicitly#threaded#code","text":"O(K) means that the system typically has some constant number of things it can do to keep a constant number of cores busy at any given time. That number is hardwired into the structure of the program and will be the same regardless of the amount of hardware concurrency available at execution time.","title":"O(K): Explicitly Threaded Code"},{"location":"Multithread/Expert-Herb-Sutter/02-How-Much-Scalability-Do-You-Have-or-Need/#example#first-person#action#game","text":"NOTE: 1\u3001\u6211\u4eec\u4f1a\u7ecf\u5e38\u91c7\u7528\u8fd9\u79cd\u8bbe\u8ba1 For example, consider a first-person action game. To take some advantage of additional cores, let's say we divide the game's compute-bound work into three threads: Thread 1 does game physics, thread 2 does rendering, and thread 3 does nonplayer character AI. For simplicity, assume that all three threads are equally busy and interdependent. The game runs fine on a single-core machine; the operating system just interleaves the threads on the one core. When the user upgrades to a two-core machine, the game runs faster\u2014but not twice as fast, because if we schedule thread 1 on core 1, and thread 2 on core 2, we have to put thread 3 somewhere. If we put thread 3 on the same core as thread 1, then thread 2 (which depends on 1 and/or 3) will be idle half the time. But if we schedule thread 3 on both core 1 and core 2, we incur cache sloshing and other overhead every time we move it from one core to the other. So the game runs faster on a two-core system, just not twice as fast. When the user upgrades to a four-core machine, the game runs faster still\u2014but only three times as fast as on a single core, or maybe slightly better if spyware and other applications can be moved to the fourth core. When the user upgrades to an eight-core machine, nothing happens. When he upgrades to a 16-core machine, more nothing happens. The O(3) application is hardwired to prefer three cores regardless of the input or execution environment.","title":"Example: first-person action game"},{"location":"Multithread/Expert-Herb-Sutter/02-How-Much-Scalability-Do-You-Have-or-Need/#example#pipelining","text":"NOTE: 1\u3001\u8fd9\u662f\u4e00\u79cd\u5e76\u884c\u5316\u7684\u6280\u5de7 2\u3001\"4\" A closely related technique is pipelining, which is useful when we have a collection of things to operate on, but we can't apply O(N) techniques to fully parallelize them because of ordering dependencies. For example, consider a communications subsystem that packets data to be sent over the network. The input to the subsystem is a stream of raw packets; before sending a given packet, it must first be decorated with header information, then compressed, and then encrypted. Those three operations can't easily be run in parallel for a given packet, in part because they must be applied in that order. A solution is to have three agents that communicate using message queues: The Decorator agent (typically a thread) decorates each raw packet and sends it to the Compressor agent; the Compressor accepts the decorated packet, compresses it, and sends it to the Encryptor ; and the Encryptor encrypts it and sends it to the network. Like the game example, this subsystem also has O(3) parallelism if the pipeline stages are equally busy. We are using Pillar 1 techniques (isolated agents, messaging; see [2] for details) because it's a natural fit for expressing pipelines: The pipeline stages are relatively isolated, and using messaging lets us tolerate any difference in execution rates between the stages. NOTE: 1\u3001\u5178\u578b\u7684\"protocol interceptor\u62e6\u622a\u5668decorator-pattern-assemble ability\"","title":"Example: pipelining"},{"location":"Multithread/Expert-Herb-Sutter/02-How-Much-Scalability-Do-You-Have-or-Need/#_1","text":"An O(K) application is explicitly structured to prefer K cores for a given input workload. O(K) code can't scale to take advantage of environments with more than K cores, and it can penalize execution environments with fewer than K cores by interfering with load balancing and causing some of the cores to be only partly utilized.","title":"\u7ed3\u8bba"},{"location":"Multithread/Expert-Herb-Sutter/02-How-Much-Scalability-Do-You-Have-or-Need/#comparing#o1#and#ok","text":"Alert readers may already have noticed that in other computer science contexts we usually don't distinguish between O(1) and O(K) complexity because big-Oh notation typically ignores the constant factor; after all, K =1 is just a special case. That's exactly true, and similarly here O(K) is far closer to O(1) than it is to O(N). Both O(1) and O(K) hardwire concurrency into the explicit structure of the code, and neither is scalable to arbitrary numbers of cores. Even so, it's well worth distinguishing between O(1) and O(K ). O(1) is an important special case because it targets three significant situations: 1\u3001Single-core hardware, including legacy hardware and some game consoles, such as Nintendo Wii. 2\u3001Operating systems that don't support concurrency, including some that have no concept of a thread. 3\u3001Applications that aren't CPU-bound, where their current feature set won't drive more than one core's worth of computation no matter how you slice them (a simple text processor, for instance). NOTE: 1\u3001\"simple text processor\" \"aren't CPU-bound\" \u662f\u975e\u5e38\u5bb9\u6613\u7406\u89e3\u7684\uff0c\u56e0\u4e3a\u5b83\u9700\u8981\u7b49\u5f85\u7528\u6237\u8f93\u5165\uff0c\u8fd9\u8ba9\u6211\u60f3\u8d77\u4e86Redis is not CPU-bound\u7684\u8bba\u65ad\uff0cRedis\u548c\"simple text processor\" \u4e00\u6837\uff0c\u662f\"IO-bound\"\u7684 If you have no reason or capability to escape one of those constraints, it probably doesn't make sense to invest(\u6295\u8d44) heavily in making your code O(K) or better because the extra concurrency will never be exercised on such systems. But don't forget that, even in the O(1) world, concurrency is a primary path to better responsiveness, and some basic techniques like using an event-driven program structure will work even in the most constrained case of running on an OS that has no support for actual threads. O(K) is appropriate especially for code that targets domains with fixed concurrency: 1\u3001Fixed hardware targets: A notable situation is when you're targeting one multicore game console generation. For example, when developing a game for XBox 360, it makes sense to write an O(6) application, because that generation of the console is fixed at six hardware threads (three cores with two hardware threads each); similarly, PlayStation 3 naturally lends itself to O(8) or O(9) applications because it is fixed at 1+8 cores (one general-purpose core, and eight special-purpose cores). These configurations will not change until each console's next major hardware generation. 2\u3001Applications whose key operations cannot be fully parallelized: The earlier packet-sending example illustrates a case where a CPU-intensive operation has internal ordering dependencies; its parts are not sufficiently independent to let them be run fully in parallel. As we saw, pipelining is a classic approach to extract(\u63d0\u53d6) some parallelism out of an otherwise inherently(\u5185\u5728\u5730) serial(\u4e32\u884c) operation; but basic pipelining is at best O(K) , for a pipeline with K stages of relatively equal cost. Although O(K) is not scalable, it can be deployed tactically(\u7b56\u7565\u9ad8\u660e\u5730) to temporarily extend the free lunch. NOTE: \"async method invocat-message queue-mailbox-pipeline parallel \u4e32\u8054\u5e76\u884c\u6d41\u7a0b\" In O(K) in particular, there are some fixed costs of making the code concurrent that we will incur at runtime on even a single-core machine, including work to manage messages, perform context switches , and lock shared data . However, by applying concurrency, we can get some limited scalability, and often also better responsiveness. Note that O(1) and O(K) situations are mostly described by what you can't do. If you can target hardware that has variable parallelism (especially increasing parallelism), and your operating system supports concurrency, and you can find enough independence inside your application's CPU-intensive operations to make them scale, prefer aiming for O(N ).","title":"Comparing O(1) and O(K)"},{"location":"Multithread/Expert-Herb-Sutter/02-How-Much-Scalability-Do-You-Have-or-Need/#on#scalable#throughput#and#the#free#lunch","text":"The key to scalable throughput is to express lots of latent concurrency that isn't explicitly coded in the program and that scales to match its inputs (number of messages, size of data, and so on) and that can be efficiently mapped down at execution time to the variable number N cores available on a given machine. We find the scalable concurrency opportunities principally by exploiting natural parallelism : 1\u3001Exploit parallelism in algorithm structures: For example, recursive sorting can exploit the natural parallelism in its divide-and-conquer structure. 2\u3001Exploit parallelism in data structures: For example, tree traversal can often exploit the independence in each node's subtrees. Compilation can exploit independence at several levels in the structure of source code, from coarse-grained independence among source files to finer-grained independence among classes or methods within a file. NOTE: 1\u3001divide-and-conquer This lets us decompose the application into a \"sea of chores\"\u2014expressing independent chunks of work that are big or small, blocking or nonblocking, structured subdivisions or independent; but we often want to look first for the CPU-bound work\u2014and rely on the runtime system to \"rightsize\" the application by assigning those chores efficiently to whatever hardware parallelism is available on a given user's system.","title":"O(N): Scalable Throughput And the Free Lunch"},{"location":"Multithread/Expert-Herb-Sutter/02-How-Much-Scalability-Do-You-Have-or-Need/#implementation","text":"OpenMP supports some constrained O(N) styles, but it is primarily intended for use with integer-indexed loops over arrays and doesn't work well with iteration-based containers in STL, .NET, or Java. Instead, as mentioned in last month's column [2], today, one common idiom for expressing such a sea of work items is to explicitly schedule chores for execution on a thread pool (for example, using Java ThreadPoolExecutor or .NET BackgroundWorker ). Unfortunately, this incurs significant context-switch overhead and so you have to ensure that a work item will be worth shipping over to a worker thread. In the future, this constraint will be relaxed as languages and runtimes improve.","title":"Implementation"},{"location":"Multithread/Expert-Herb-Sutter/02-How-Much-Scalability-Do-You-Have-or-Need/#re-enabling#the#free#lunch","text":"O(N) is the key to re-enabling the free lunch and keeping lots of cores busy, because it lets us express applications that can run on yesterday's single-core machine, run better on today's four-core machine, run better still on tomorrow's 64-core machine, and so on until we exhaust the inherent limit of CPU-boundedness in the application. For a thread pool-driven program, on a single-core machine the system can spin up a single worker thread that runs the program by serially pulling chore after chore from the sea and executing it; on an eight-core machine, it can spin up eight threads; and so on [4]. As with O(K) , O(N) can have costs at runtime on even a single-core machine. In addition to the costs mentioned for O(K) , there can be extra work inherent in divide-and-conquer techniques (e.g., reductions such as piecing together a grand total from intermediate results), and the cost of locking shared data can now also increase. However, by applying concurrency, we can often get good scalability that far offsets the overhead. Using O(N) is highly desirable for software that expects to run on a variety of current and future hardware having variable amounts of hardware parallelism \u2014which happens to now be the target for all mainstream desktop and server software. If your application doesn't currently have key CPU-bound operations that are amenable to full O(N) parallelization, don't give up: Consider finding new desirable features that are amenable to O(N) , or at least more O(K ), and you will still be able to deliver software that runs well on today's hardware, better on tomorrow's hardware, and better still on future systems.","title":"Re-enabling the free lunch"},{"location":"Multithread/Expert-Herb-Sutter/02-How-Much-Scalability-Do-You-Have-or-Need/#notes","text":"[1] I use \"cores\" as a simple shorthand measure of execution hardware parallelism. For applications running on one local machine the appropriate measure is usually \"total hardware threads,\" meaning #sockets\u00d7 #cores/socket\u00d7#threads/core; and for distributed applications the appropriate measure is usually \"nodes.\" [2] H. Sutter. \" The Pillars of Concurrency \" ( DDJ , 32(7), August 2007, http://ddj.com/dept/64bit/200001985 ). [3] H. Sutter. \" The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software \" ( DDJ , 30(3), March 2005, http://ddj.com/dept/webservices/184405990 ). [4] The details are more complex. For example, a pool should spin up extra worker threads to keep the system busy whenever one chore blocks to wait for some event and so temporarily idles its worker thread.","title":"Notes"},{"location":"Multithread/Expert-Herb-Sutter/06-Use-Lock-Hierarchies-to-Avoid-Deadlock/","text":"drdobbs Use Lock Hierarchies to Avoid Deadlock NOTE: \u4e00\u3001\u672c\u6587\u8ba8\u8bba\u7684\u5176\u5b9e\u662flock multiple\uff0c\u5176\u5b9e\u8fd9\u5728APUE\u4e2d\u5df2\u7ecf\u8ba8\u8bba\u4e86\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86lock hierarchy\u7684\u6982\u5ff5\u3001\u65b9\u6cd5\uff0c\u5b83\u662f\u4e00\u79cd\u66f4\u52a0\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u4fdd\u8bc1consistent global order\u3002 \u4e8c\u3001deadlock\u662f\u53ef\u4ee5\u4f7f\u7528dependency model\u6765\u8fdb\u884c\u63cf\u8ff0\u7684 \u4e09\u3001\u8fd9\u79cd\u7b97\u6cd5\u5176\u5b9e\u662f Dijkstra \u63d0\u51fa\u7684\uff0c \u53c2\u89c1 \"wikipedia Dining philosophers problem # Resource hierarchy solution \"\u7ae0\u8282\uff0c\u5176\u4e2d\u6709\u7740\u975e\u5e38\u597d\u7684\u5206\u6790\u8bba\u8ff0\u3002 Need to avoid deadlock in the code you control? Try using lock hierarchies. In the first two Effective Concurrency columns\u2014\" The Pillars of Concurrency \" ( DDJ , August 2007) and \" How Much Scalability Do You Have or Need? \" ( DDJ , September 2007)\u2014we saw the three pillars of concurrency and what kinds of concurrency they express: 1\u3001Pillar 1: Isolation via asynchronous agents. This is all about doing work asynchronously to gain isolation and responsiveness, using techniques like messaging. Some Pillar 1 techniques, such as pipelining, also enable limited scalability, but this category principally targets isolation and responsiveness. 2\u3001Pillar 2: Scalability via concurrent collections. This is all about reenabling the free lunch of being able to ship applications that get faster on machines with more cores, using more cores to get the answer faster by exploiting parallelism in algorithms and data. 3\u3001Pillar 3: Consistency via safely shared resources. While doing all of the above, we need to avoid races and deadlocks when using shared objects in memory or any other shared resources. My last few columns have focused on Pillar 3, and I've referred to lock levels or lock hierarchies as a way to control deadlock . A number of readers have asked me to explain what those are and how they work, so I'll write one more column about Pillar 3 (for now). The Problem: Anatomy(\u89e3\u5256) of a Deadlock Your program contains a potential deadlock if: 1\u3001One part of your program tries to acquire exclusive use of two shared resources (such as mutexes) a and b at the same time by acquiring first a and then b. 2\u3001Some other part of your program tries to do the same by acquiring b and then a in the reverse order. 3\u3001The two pieces of code could ever execute concurrently. NOTE: 1\u3001 For convenience, from now on I'm going to talk about just locks, but the issues and techniques apply to any shared resource that needs to be used exclusively by one piece of code at a time. The following code shows the simplest example of a potential deadlock: // Thread 1: a then b // Thread 2: b then a a . lock (); b . lock (); b . lock (); a . lock (); ... ... // unlock a and b // unlock a and b // in either order // in either order The only way to eliminate such a potential deadlock is to make sure that all mutexes ever held at the same time are acquired in a consistent order. But how can we ensure this in a way that will be both usable and correct? For example, we could try to figure out which groups of mutexes might ever be held at the same time, and then try to define pairwise ordering rules that cover each possible combination. But that approach by itself is prone to accidentally missing unexpected combinations of locks; and even if we did it perfectly, the result would still be at best \"DAG spaghetti\"\u2014a directed acyclic graph (DAG) that nobody could comprehend(\u7406\u89e3) as a whole. And every time we want to add a new mutex to the system, we would have to find a way fit it into the DAG without creating any cycles. NOTE: 1\u3001\u4e0a\u9762\u7ed9\u51fa\u4e86\u4e00\u79cd\u5f62\u5f0f\u5316\u7684\u5206\u6790\u65b9\u6cd5 DAG We can do better by directly exploiting the knowledge we already have about the structure of the program to regularize the mess and make it understandable. A Solution: Lock Hierarchies and Layering NOTE: 1\u3001\u8fd9\u662f\u4f5c\u8005\u7ed9\u51fa\u7684\u89e3\u51b3\u65b9\u6cd5 The idea of a lock hierarchy is to assign a numeric level to every mutex in the system, and then consistently follow two simple rules: 1\u3001Rule 1: While holding a lock on a mutex at level N , you may only acquire new locks on mutexes at lower levels <N . 2\u3001Rule 2: Multiple locks at the same level must be acquired at the same time, which means we need a \"lock-multiple\" operation such as lock( mut1, mut2, mut3, ... ). This operation internally has the smarts to make sure it always takes the requested locks in some consistent global order . [1] Note that any consistent order will do; for example, one typical strategy is to acquire mutexes at the same level in increasing address order. If the entire program follows these rules, then there can be no deadlock among the mutex acquire operations, because no two pieces of code can ever try to acquire two mutexes a and b in opposite orders: Either a and b are at different levels and so the one at the higher level must be taken first; or else they are at the same level and they must be requested at the same time, and the system will automatically acquire them in the same order. The two simple rules have provided a convenient and understandable way to conveniently express a total order on all locking performed in the system. Where do we find the levels? But where do we find the levels? The answer is: You probably already have them. Mutexes protect data, and the data is already in layers. Lock levels should directly leverage and mirror the layering already in place in the modular structure of your application. Figure 1 illustrates a typical example of layering (or \"hierarchical decomposition\" and \"into a directed acyclic graph,\" if you prefer five-dollar words), a time-tested technique to control the dependencies in your software. The idea is to group your code into modules and the modules into layers, where code at a given layer can only call code at the same or lower layers, and should avoid calling upward into higher layers. Figure 1: Sample module/layer decomposition. If that sounds a lot like the Two Rules of lock hierarchies, that's no coincidence. After all, both the layering and the mutexes are driven by the same goal: to protect and control access to the encapsulated data that is owned by each piece of code, and to keep it free from corruption by maintaining its invariants correctly. As in Figure 1, the levels you assign to mutexes will normally closely follow the levels in your program's layered structure . A direct consequence of Rule 1 is that locks held on mutexes at lower levels have a shorter duration than locks held at higher levels; this is just what we expect of calls into code at lower layers of a layered software system. Software can't always be perfectly layered, but exceptions should be rare. After all, if you can't define such layers, it means that there is a cycle among the modules somewhere that includes code in what should be a lower level subsystem calling into higher level code somewhere, such as via a callback, and you have the potential for reentrancy even in single-threaded code. And remember, reentrancy is a form of concurrency , so the program can observe corrupt state even in single-threaded code. If higher level code is in the middle of taking the system from one valid state to another, thus temporarily breaking some invariant, and calls into lower level code, the trouble is that if that call could ultimately call back into the higher level code it might see the broken invariant. Layering helps to solve this single-threaded concurrency problem for the same reasons it helps to solve the more general multithreaded version. Frameworks and Lock Hierarchies NOTE: \u4e00\u3001\u8fd9\u4e00\u8282\u4f5c\u8005\u7ed9\u51fa\u4e86\u5b9e\u73b0Lock Hierarchies\u7684wrapper class It is a curious thing that major frameworks that supply mutexes and locks do nothing to offer any direct support for lock hierarchies. Everyone is taught that lock hierarchies are a best practice, but then are generally told to go roll their own. The frameworks vendors will undoubtedly fix this little embarrassment in the future, but for now, here's a useful recipe to follow as you do roll your own level-aware mutex wrapper. You can adapt this simple sketch to your project's specific needs (for example, to suit details such as whether your lock operation is a method or a separate class): 1\u3001Write a wrapper around each of your favorite language- or platform-specific mutex types, and let the wrapper's constructor(s) take a level number parameter that it saves in a myLevel member. Use these wrappers everywhere. (Where practical, save time by making the wrapper generic\u2014as a C++ template, or a Java or .NET generic\u2014so that it can be instantiated to wrap arbitrary mutex types that have similar lock/unlock features. You might only have to write it once.) 2\u3001Give the wrapper class a thread-local static variable called currentLevel , initialized to a value higher than any valid lock level. 3\u3001In the wrapper's lock method (or similar), assert that currentLevel is greater than myLevel , the level of the mutex that you're about to try to acquire. Remember, if the previous value of currentLevel is using another member variable, then set currentLevel = myLevel ; and acquire the lock. 4\u3001In the wrapper's unlock method (or similar), restore the previous value of currentLevel . 5\u3001As needed, also wrap other necessary methods you need to be able to use, such as try_lock . Any of these methods that might try to acquire the lock should do the same things as lock does. 6\u3001Finally, write a \"lock-multiple\" method lock( m1, m2, ... ) that takes a variable number of lockable objects, asserts that they are all at the same level, and locks them in their address order (or their GUID order , or some other globally consistent order). Using assertions in the lock methods NOTE: 1\u3001\u4f7f\u7528assertions \uff0c\u662f\u5178\u578b\u7684\"design by contract\" The reason for using assertions in the lock methods is so that, in a debug build, we force any errors to be exposed the first time we execute the code path that violates the lock hierarchy rules. That way, we can expect to find violations at test time and have high confidence that the program is deadlock-free based on code path coverage. Enabling such deterministic test-time failures is a great improvement over the way concurrency errors usually manifest, namely as nondeterministic runtime failures that can't be thoroughly(\u5f7b\u5e95\u7684) tested using code path coverage alone. But often our test-time code path coverage isn't complete, either because it's impossible to cover all possible code path combinations or because we might forget a few cases; so prefer to also perform the tests in release builds, recording violations in a log or diagnostic dump that you can review later if a problem does occur. A Word About Composability Although lock hierarchies address many of the flaws of locks, including the possibility of deadlock, they still share the same Achilles Heel (\u81f4\u547d\u8981\u5bb3): Like locks themselves, lock hierarchies are not in general composable without some extra discipline and effort. After all, just because you use a lock hierarchy discipline correctly within the code you control, a separately authored module or plug-in that you link with won't necessarily know anything about your lock hierarchy unless you somehow inform them about your layers and how they should fit into the hierarchy. For example, look at the sample application architecture in Figure 1 again, and consider: What if the application wants to allow plugins that are called from the 5000s layer? First, of course, the program and all the plug-ins should make every effort to avoid calling unknown code (in this case, each other) while holding a lock. But, as we saw last month (\"Avoid Calling Unknown Code While Inside a Critical Section\", DDJ , December 2007), we can encounter trouble even if a plug-in takes no locks of its own but may call back into the main program and create a code path that unexpectedly calls up into higher level code that takes a higher level lock. So the plug-ins must be aware of the layering of the application and be told that they are to operate at (say) level 4999, and may only call APIs below level 4999. Summary Keep it on the level: Use lock hierarchies to avoid deadlock in the code you control. Assign each shared resource a level that corresponds to its architectural layer in your application, and follow the two rules: While holding a resource at a higher level, acquire only resources at lower levels; and acquire multiple resources at the same level all at once. If your program will call external code, especially plug-ins, then document your public API sufficiently for plug-in authors to see what level their plug-in is expected to operate at, and therefore the API calls they can and can't make (those below their level and those at or above their level, respectively). Next month, we'll start to look into issues and techniques for writing scalable manycore applications. Stay tuned. Notes [1] Or otherwise gets the same effect. For example, it is possible to just start trying to acquire the locks in some randomly selected order using try_lock operations, and if we can't acquire them all just back-off (unlock the ones already acquired) and try a different order until we find one that works. Surprisingly, this can be more efficient than taking the locks in a hard-coded global order, although any backoff-and-retry strategy has to take care that it doesn't end up prone to livelock problems in-stead. But we can leave all this to the implementer; the key is that the programmer simply writes lock( /* whatever */ ) and is insulated from the details of determining the best way to keep the order consistent.","title":"Introduction"},{"location":"Multithread/Expert-Herb-Sutter/06-Use-Lock-Hierarchies-to-Avoid-Deadlock/#drdobbs#use#lock#hierarchies#to#avoid#deadlock","text":"NOTE: \u4e00\u3001\u672c\u6587\u8ba8\u8bba\u7684\u5176\u5b9e\u662flock multiple\uff0c\u5176\u5b9e\u8fd9\u5728APUE\u4e2d\u5df2\u7ecf\u8ba8\u8bba\u4e86\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86lock hierarchy\u7684\u6982\u5ff5\u3001\u65b9\u6cd5\uff0c\u5b83\u662f\u4e00\u79cd\u66f4\u52a0\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u4fdd\u8bc1consistent global order\u3002 \u4e8c\u3001deadlock\u662f\u53ef\u4ee5\u4f7f\u7528dependency model\u6765\u8fdb\u884c\u63cf\u8ff0\u7684 \u4e09\u3001\u8fd9\u79cd\u7b97\u6cd5\u5176\u5b9e\u662f Dijkstra \u63d0\u51fa\u7684\uff0c \u53c2\u89c1 \"wikipedia Dining philosophers problem # Resource hierarchy solution \"\u7ae0\u8282\uff0c\u5176\u4e2d\u6709\u7740\u975e\u5e38\u597d\u7684\u5206\u6790\u8bba\u8ff0\u3002 Need to avoid deadlock in the code you control? Try using lock hierarchies. In the first two Effective Concurrency columns\u2014\" The Pillars of Concurrency \" ( DDJ , August 2007) and \" How Much Scalability Do You Have or Need? \" ( DDJ , September 2007)\u2014we saw the three pillars of concurrency and what kinds of concurrency they express: 1\u3001Pillar 1: Isolation via asynchronous agents. This is all about doing work asynchronously to gain isolation and responsiveness, using techniques like messaging. Some Pillar 1 techniques, such as pipelining, also enable limited scalability, but this category principally targets isolation and responsiveness. 2\u3001Pillar 2: Scalability via concurrent collections. This is all about reenabling the free lunch of being able to ship applications that get faster on machines with more cores, using more cores to get the answer faster by exploiting parallelism in algorithms and data. 3\u3001Pillar 3: Consistency via safely shared resources. While doing all of the above, we need to avoid races and deadlocks when using shared objects in memory or any other shared resources. My last few columns have focused on Pillar 3, and I've referred to lock levels or lock hierarchies as a way to control deadlock . A number of readers have asked me to explain what those are and how they work, so I'll write one more column about Pillar 3 (for now).","title":"drdobbs Use Lock Hierarchies to Avoid Deadlock"},{"location":"Multithread/Expert-Herb-Sutter/06-Use-Lock-Hierarchies-to-Avoid-Deadlock/#the#problem#anatomy#of#a#deadlock","text":"Your program contains a potential deadlock if: 1\u3001One part of your program tries to acquire exclusive use of two shared resources (such as mutexes) a and b at the same time by acquiring first a and then b. 2\u3001Some other part of your program tries to do the same by acquiring b and then a in the reverse order. 3\u3001The two pieces of code could ever execute concurrently. NOTE: 1\u3001 For convenience, from now on I'm going to talk about just locks, but the issues and techniques apply to any shared resource that needs to be used exclusively by one piece of code at a time. The following code shows the simplest example of a potential deadlock: // Thread 1: a then b // Thread 2: b then a a . lock (); b . lock (); b . lock (); a . lock (); ... ... // unlock a and b // unlock a and b // in either order // in either order The only way to eliminate such a potential deadlock is to make sure that all mutexes ever held at the same time are acquired in a consistent order. But how can we ensure this in a way that will be both usable and correct? For example, we could try to figure out which groups of mutexes might ever be held at the same time, and then try to define pairwise ordering rules that cover each possible combination. But that approach by itself is prone to accidentally missing unexpected combinations of locks; and even if we did it perfectly, the result would still be at best \"DAG spaghetti\"\u2014a directed acyclic graph (DAG) that nobody could comprehend(\u7406\u89e3) as a whole. And every time we want to add a new mutex to the system, we would have to find a way fit it into the DAG without creating any cycles. NOTE: 1\u3001\u4e0a\u9762\u7ed9\u51fa\u4e86\u4e00\u79cd\u5f62\u5f0f\u5316\u7684\u5206\u6790\u65b9\u6cd5 DAG We can do better by directly exploiting the knowledge we already have about the structure of the program to regularize the mess and make it understandable.","title":"The Problem: Anatomy(\u89e3\u5256) of a Deadlock"},{"location":"Multithread/Expert-Herb-Sutter/06-Use-Lock-Hierarchies-to-Avoid-Deadlock/#a#solution#lock#hierarchies#and#layering","text":"NOTE: 1\u3001\u8fd9\u662f\u4f5c\u8005\u7ed9\u51fa\u7684\u89e3\u51b3\u65b9\u6cd5 The idea of a lock hierarchy is to assign a numeric level to every mutex in the system, and then consistently follow two simple rules: 1\u3001Rule 1: While holding a lock on a mutex at level N , you may only acquire new locks on mutexes at lower levels <N . 2\u3001Rule 2: Multiple locks at the same level must be acquired at the same time, which means we need a \"lock-multiple\" operation such as lock( mut1, mut2, mut3, ... ). This operation internally has the smarts to make sure it always takes the requested locks in some consistent global order . [1] Note that any consistent order will do; for example, one typical strategy is to acquire mutexes at the same level in increasing address order. If the entire program follows these rules, then there can be no deadlock among the mutex acquire operations, because no two pieces of code can ever try to acquire two mutexes a and b in opposite orders: Either a and b are at different levels and so the one at the higher level must be taken first; or else they are at the same level and they must be requested at the same time, and the system will automatically acquire them in the same order. The two simple rules have provided a convenient and understandable way to conveniently express a total order on all locking performed in the system.","title":"A Solution: Lock Hierarchies and Layering"},{"location":"Multithread/Expert-Herb-Sutter/06-Use-Lock-Hierarchies-to-Avoid-Deadlock/#where#do#we#find#the#levels","text":"But where do we find the levels? The answer is: You probably already have them. Mutexes protect data, and the data is already in layers. Lock levels should directly leverage and mirror the layering already in place in the modular structure of your application. Figure 1 illustrates a typical example of layering (or \"hierarchical decomposition\" and \"into a directed acyclic graph,\" if you prefer five-dollar words), a time-tested technique to control the dependencies in your software. The idea is to group your code into modules and the modules into layers, where code at a given layer can only call code at the same or lower layers, and should avoid calling upward into higher layers.","title":"Where do we find the levels?"},{"location":"Multithread/Expert-Herb-Sutter/06-Use-Lock-Hierarchies-to-Avoid-Deadlock/#figure#1#sample#modulelayer#decomposition","text":"If that sounds a lot like the Two Rules of lock hierarchies, that's no coincidence. After all, both the layering and the mutexes are driven by the same goal: to protect and control access to the encapsulated data that is owned by each piece of code, and to keep it free from corruption by maintaining its invariants correctly. As in Figure 1, the levels you assign to mutexes will normally closely follow the levels in your program's layered structure . A direct consequence of Rule 1 is that locks held on mutexes at lower levels have a shorter duration than locks held at higher levels; this is just what we expect of calls into code at lower layers of a layered software system. Software can't always be perfectly layered, but exceptions should be rare. After all, if you can't define such layers, it means that there is a cycle among the modules somewhere that includes code in what should be a lower level subsystem calling into higher level code somewhere, such as via a callback, and you have the potential for reentrancy even in single-threaded code. And remember, reentrancy is a form of concurrency , so the program can observe corrupt state even in single-threaded code. If higher level code is in the middle of taking the system from one valid state to another, thus temporarily breaking some invariant, and calls into lower level code, the trouble is that if that call could ultimately call back into the higher level code it might see the broken invariant. Layering helps to solve this single-threaded concurrency problem for the same reasons it helps to solve the more general multithreaded version.","title":"Figure 1: Sample module/layer decomposition."},{"location":"Multithread/Expert-Herb-Sutter/06-Use-Lock-Hierarchies-to-Avoid-Deadlock/#frameworks#and#lock#hierarchies","text":"NOTE: \u4e00\u3001\u8fd9\u4e00\u8282\u4f5c\u8005\u7ed9\u51fa\u4e86\u5b9e\u73b0Lock Hierarchies\u7684wrapper class It is a curious thing that major frameworks that supply mutexes and locks do nothing to offer any direct support for lock hierarchies. Everyone is taught that lock hierarchies are a best practice, but then are generally told to go roll their own. The frameworks vendors will undoubtedly fix this little embarrassment in the future, but for now, here's a useful recipe to follow as you do roll your own level-aware mutex wrapper. You can adapt this simple sketch to your project's specific needs (for example, to suit details such as whether your lock operation is a method or a separate class): 1\u3001Write a wrapper around each of your favorite language- or platform-specific mutex types, and let the wrapper's constructor(s) take a level number parameter that it saves in a myLevel member. Use these wrappers everywhere. (Where practical, save time by making the wrapper generic\u2014as a C++ template, or a Java or .NET generic\u2014so that it can be instantiated to wrap arbitrary mutex types that have similar lock/unlock features. You might only have to write it once.) 2\u3001Give the wrapper class a thread-local static variable called currentLevel , initialized to a value higher than any valid lock level. 3\u3001In the wrapper's lock method (or similar), assert that currentLevel is greater than myLevel , the level of the mutex that you're about to try to acquire. Remember, if the previous value of currentLevel is using another member variable, then set currentLevel = myLevel ; and acquire the lock. 4\u3001In the wrapper's unlock method (or similar), restore the previous value of currentLevel . 5\u3001As needed, also wrap other necessary methods you need to be able to use, such as try_lock . Any of these methods that might try to acquire the lock should do the same things as lock does. 6\u3001Finally, write a \"lock-multiple\" method lock( m1, m2, ... ) that takes a variable number of lockable objects, asserts that they are all at the same level, and locks them in their address order (or their GUID order , or some other globally consistent order).","title":"Frameworks and Lock Hierarchies"},{"location":"Multithread/Expert-Herb-Sutter/06-Use-Lock-Hierarchies-to-Avoid-Deadlock/#using#assertions#in#the#lock#methods","text":"NOTE: 1\u3001\u4f7f\u7528assertions \uff0c\u662f\u5178\u578b\u7684\"design by contract\" The reason for using assertions in the lock methods is so that, in a debug build, we force any errors to be exposed the first time we execute the code path that violates the lock hierarchy rules. That way, we can expect to find violations at test time and have high confidence that the program is deadlock-free based on code path coverage. Enabling such deterministic test-time failures is a great improvement over the way concurrency errors usually manifest, namely as nondeterministic runtime failures that can't be thoroughly(\u5f7b\u5e95\u7684) tested using code path coverage alone. But often our test-time code path coverage isn't complete, either because it's impossible to cover all possible code path combinations or because we might forget a few cases; so prefer to also perform the tests in release builds, recording violations in a log or diagnostic dump that you can review later if a problem does occur.","title":"Using assertions in the lock methods"},{"location":"Multithread/Expert-Herb-Sutter/06-Use-Lock-Hierarchies-to-Avoid-Deadlock/#a#word#about#composability","text":"Although lock hierarchies address many of the flaws of locks, including the possibility of deadlock, they still share the same Achilles Heel (\u81f4\u547d\u8981\u5bb3): Like locks themselves, lock hierarchies are not in general composable without some extra discipline and effort. After all, just because you use a lock hierarchy discipline correctly within the code you control, a separately authored module or plug-in that you link with won't necessarily know anything about your lock hierarchy unless you somehow inform them about your layers and how they should fit into the hierarchy. For example, look at the sample application architecture in Figure 1 again, and consider: What if the application wants to allow plugins that are called from the 5000s layer? First, of course, the program and all the plug-ins should make every effort to avoid calling unknown code (in this case, each other) while holding a lock. But, as we saw last month (\"Avoid Calling Unknown Code While Inside a Critical Section\", DDJ , December 2007), we can encounter trouble even if a plug-in takes no locks of its own but may call back into the main program and create a code path that unexpectedly calls up into higher level code that takes a higher level lock. So the plug-ins must be aware of the layering of the application and be told that they are to operate at (say) level 4999, and may only call APIs below level 4999.","title":"A Word About Composability"},{"location":"Multithread/Expert-Herb-Sutter/06-Use-Lock-Hierarchies-to-Avoid-Deadlock/#summary","text":"Keep it on the level: Use lock hierarchies to avoid deadlock in the code you control. Assign each shared resource a level that corresponds to its architectural layer in your application, and follow the two rules: While holding a resource at a higher level, acquire only resources at lower levels; and acquire multiple resources at the same level all at once. If your program will call external code, especially plug-ins, then document your public API sufficiently for plug-in authors to see what level their plug-in is expected to operate at, and therefore the API calls they can and can't make (those below their level and those at or above their level, respectively). Next month, we'll start to look into issues and techniques for writing scalable manycore applications. Stay tuned.","title":"Summary"},{"location":"Multithread/Expert-Herb-Sutter/06-Use-Lock-Hierarchies-to-Avoid-Deadlock/#notes","text":"[1] Or otherwise gets the same effect. For example, it is possible to just start trying to acquire the locks in some randomly selected order using try_lock operations, and if we can't acquire them all just back-off (unlock the ones already acquired) and try a different order until we find one that works. Surprisingly, this can be more efficient than taking the locks in a hard-coded global order, although any backoff-and-retry strategy has to take care that it doesn't end up prone to livelock problems in-stead. But we can leave all this to the implementer; the key is that the programmer simply writes lock( /* whatever */ ) and is insulated from the details of determining the best way to keep the order consistent.","title":"Notes"},{"location":"Multithread/Expert-Herb-Sutter/07-TODO-Break-Amdahl%27s-Law/","text":"drdobbs Break Amdahl's Law!","title":"Introduction"},{"location":"Multithread/Expert-Herb-Sutter/07-TODO-Break-Amdahl%27s-Law/#drdobbs#break#amdahls#law","text":"","title":"drdobbs Break Amdahl's Law!"},{"location":"Multithread/Expert-Herb-Sutter/11-Maximize-Locality-Minimize-Contention/","text":"drdobbs Maximize Locality, Minimize Contention NOTE: \u4e00\u672c\u6587\u4e2d\u7684\"locality\"\u5bf9\u5e94\u7684\u662flocal to core cache\uff1b\u66f4\u5927\u7684locality\uff0c\u610f\u5473\u7740\u66f4\u5c11\u7684contention Want to kill your parallel application's scalability? Easy: Just add a dash of contention. In the concurrent world, locality is a first-order issue that trumps(\u738b\u724c\u3001\u9996\u8981\u8003\u8651\u56e0\u7d20) most other performance considerations. Now locality is no longer just about fitting well into cache and RAM, but to avoid scalability busters(\u904f\u5236) by keeping tightly coupled data physically close together and separately used data far, far apart. NOTE: \u7ffb\u8bd1\u5982\u4e0b: \"\u5728\u5e76\u53d1\u4e16\u754c\u4e2d\uff0c\u5c40\u90e8\u6027\u662f\u4e00\u4e2a\u4e00\u9636\u95ee\u9898\uff0c\u6bd4\u5927\u591a\u6570\u5176\u4ed6\u6027\u80fd\u8003\u8651\u56e0\u7d20\u90fd\u91cd\u8981\u3002 \u73b0\u5728\uff0c\u5c40\u90e8\u6027\u4e0d\u518d\u4ec5\u4ec5\u662f\u4e3a\u4e86\u5f88\u597d\u5730\u9002\u5e94\u7f13\u5b58\u548cRAM\uff0c\u800c\u662f\u4e3a\u4e86\u907f\u514d\u53ef\u4f38\u7f29\u6027\u95ee\u9898\uff0c\u5c06\u7d27\u5bc6\u8026\u5408\u7684\u6570\u636e\u4fdd\u6301\u7269\u7406\u4e0a\u5728\u4e00\u8d77\uff0c\u800c\u5c06\u5355\u72ec\u4f7f\u7528\u7684\u6570\u636e\u4fdd\u6301\u5728\u5f88\u8fdc\u5f88\u8fdc\u7684\u5730\u65b9\u3002\" \u6700\u540e\u4e00\u53e5\u8bdd\u7684\u542b\u4e49\u662f: locality \u4e0d\u518d\u4ec5\u4ec5\u662f\"fitting well into cache and RAM\"\uff0c\u8fd8\u5305\u62ec\u4e86: \u4e3a\u4e86\u907f\u514d\"scalability busters(\u904f\u5236)\"\uff0c\u9700\u8981: 1\u3001keeping tightly coupled data physically close together 2\u3001separately used data far, far apart \u8fd9\u662f\u4e3a\u4e86\u907f\u514dfalse sharing Of Course, You'd Never Convoy(\u62a4\u822a) On a Global Lock Example Nobody would ever willingly write code like this in a tight loop. // Threads 1-N while ( ... ) { globalMutex . lock (); DoWork (); globalMutex . unlock (); } This is clearly foolish, because all the work is being done while holding a global lock and so only one thread can make progress at a time. We end up with a classic lock convoy: At any given time, all of the threads save one are piled up behind the lock as each waits idly for its turn to come. Convoys are a classic way to kill parallel scalability. In this example, we'll get no parallel speedup at all because this is just a fancy way to write sequential code. In fact, we'll probably get a minor performance hit because of taking and releasing the lock many times and incurring context switches, and so we would be better off just putting the lock around the whole loop and making the convoy more obvious. \u964d\u4f4e\u9501\u7684\u7c92\u5ea6 True, we sometimes still gain some parallel benefit when only part of each thread's work is done while holding the lock: // Threads 1-N while ( ... ) { DoParallelWork (); // p = time spent here, // parallel portion globalMutex . lock (); DoSequentialWork (); // s = time spent here, // sequential portion globalMutex . unlock (); } Now at least some of the threads' work can be done in parallel. NOTE: \u4e00\u3001Global Lock\u9501\u7684\u7c92\u5ea6\u592a\u5927\u4e86\uff0c\u901a\u8fc7\u964d\u4f4e\u9501\u7684\u7c92\u5ea6\uff0c\u80fd\u591f\u63d0\u5347\u4e00\u4e9b\u5e76\u53d1\u6027\uff0c\u4f46\u662f\u4ece\u4e0b\u9762\u7684\"Amdahl's Law\"\u53ef\u77e5\uff0c\u8fd9\u79cd\u63d0\u5347\u662f\u6709\u4e0a\u9650\u7684\u3002 Amdahl's Law Of course, we still hit Amdahl's Law [1]: Even on infinitely parallel hardware with an infinite number of workers, the maximum speedup is merely (s+p)/s, minus whatever overhead we incur to perform the locking and context switches. But if we fail to measure (ahem) and the time spent in p is much less than in s , we're really gaining little and we've again written a convoy. The ugly truth: \u65e0\u5904\u4e0d\u5728\u7684exclusive lock NOTE: 1\u3001\u65e0\u5904\u4e0d\u5728\u7684lock\uff0c\u6700\u7ec8\u90fd\u4f1a\u5bfc\u81f4 \"hit Amdahl's Law\" We'd never willingly do this. But the ugly truth is that we do it all the time: Sometimes it happens unintentionally; for example, when some function we call might be taking a lock on a popular mutex unbeknownst(\u4e0d\u4e3a\u6240\u77e5\u7684) to us. But often it happens invisibly(\u770b\u4e0d\u89c1\u7684), when hardware will helpfully take exactly such an exclusive lock automatically, and silently, on our behalf. Let's see how, why, and what we can do about it. Recap: Chunky Memory NOTE: 1\u3001\"Recap\"\u7684\u610f\u601d\u662f \"\u56de\u987e\" 2\u3001\"chunky\" \u5bf9\u5e94\u662f\u540e\u9762\u4f1a\u4ecb\u7ecd\u7684\"chunk\"\uff0c\u5bf9\u5e94\u7684OS\u7684page\u3001cache\u7684cache line For high-performance code, we've always had to be aware of paging and caching effects. Now, hardware concurrency adds a whole new layer to consider. When you ask for a byte of memory, the system never retrieves just one byte. We probably all know that nearly all computer systems keep track\u2014not of bytes, but of chunks of memory. There are two major levels at which chunking occurs: 1\u3001the operating system chunks virtual memory into pages, each of which is managed as a unit, and 2\u3001the cache hardware further chunks memory into cache lines, which again are each handled as a unit. Figure 1 shows a simplified view. (In a previous article, we considered some issues that arise from nonshared caches, where only subsets of processors share caches in common [2].) Figure 1: Chunking in the memory hierarchy. First, consider memory pages: How big is a page? That's up to the OS and varies by platform, but on mainstream systems the page size is typically 4K or more (see Figure 2). So when you ask for just one byte on a page that's not currently in memory, you incur two main costs: 1\u3001Speed: A page fault where the OS has to load the entire page from disk. 2\u3001Space: Memory overhead for storing the entire page in memory, even if you only ever touch one byte from the page. Second, consider cache lines: How big is a line? That's up to the cache hardware and again varies, but on mainstream systems the line size is typically 64 bytes (see Figure 2). So when you ask for just one byte on a line that's not currently in cache, you incur two main costs: 1\u3001Speed: A cache miss where the cache hardware has to load the entire line from memory. 2\u3001Space: Cache overhead for storing the entire line in cache, even if you only ever touch one byte from the line. And now comes the fun part. On multicore hardware, if one core writes to a byte of memory, then typically, as part of the hardware's cache coherency protocol, that core will automatically (read: invisibly) take an exclusive write lock on that cache line. The good news is that this prevents other cores from causing trouble by trying to perform conflicting writes. The sad news is that it also means, well, taking a lock. Figure 2: Load a byte = load a line + load a page. Sharing and False Sharing (Ping-Pong) Consider the following code where two threads update two distinct global integers x and y , and assume we've disabled optimizations to prevent the optimizer from eliminating the loops entirely in this toy example: // Thread 1 for ( i = 0 ; i < MAX ; ++ i ) { ++ x ; } // Thread 2 for ( i = 0 ; i < MAX ; ++ i ) { ++ y ; } Question: What relative performance would you expect if running Thread 1 in isolation versus running both threads: 1\u3001On a machine with one core? 2\u3001On a machine with two or more cores? On a machine with one core, the program would probably take twice as long to run, as we'd probably get the same throughput (additions/sec), maybe with a little overhead for context switches as the operating system schedules the two threads interleaved on the single core. On a machine with two or more cores, we'd probably expect to get a 2x throughput improvement as the two threads each run at full speed on their own cores. And that is what in fact will happen...but only if x and y are on different cache lines. x and y are on the same cache line If x and y are on the same cache line, however, only one core can be updating the cache line at a time, because only one core can have exclusive access at a time\u2014it's as if the cache line is a token being passed between the threads/cores to say who is currently allowed to run. So the situation is exactly as if we had explicitly written: // Thread 1 for ( i = 0 ; i < MAX ; ++ i ) { lightweightMutexForXandY . lock (); ++ x ; lightweightMutexForXandY . unlock (); } // Thread 2 for ( i = 0 ; i < MAX ; ++ i ) { lightweightMutexForXandY . lock (); ++ y ; lightweightMutexForXandY . unlock (); } Which of course is exactly what we said we would never willingly do: Only one thread can make progress at a time. This effect is called \"false sharing\" because, even though the cores are trying to update different parts of the cache line , that doesn't matter; the unit of sharing is the whole line, and so the performance effect is the same as if the two threads were trying to share the same variable. It's also called ping-ponging because that's an apt description of how the cache line ownership keeps hopping back and forth madly between the two cores. How to ensure x and y are on different cache lines? Even in this simple example, what could we do to ensure x and y are on different cache lines? First, we can rearrange the data: For example, if x and y are data values inside the same object, perhaps we can rearrange the object's members so that x and y are sufficiently further apart. Second, we can add padding: If we have no other data that's easy to put adjacent to x, we can ensure x is alone on its cache line by allocating extra space, such as by allocating a larger object with x as a member (preceded/followed by appropriate padding to fill the cache line) instead of allocating x by itself as a naked integer. This is a great example of how to deliberately waste space to improve performance. False sharing example False sharing arises in lots of hard-to-see places. For example: 1\u3001Two independent variables or objects are too close together, as in the above examples. 2\u3001Two node-based containers (lists or trees, for example) interleave in memory, so that the same cache line contains nodes from two containers. Cache-Conscious Design NOTE: 1\u3001\u6211\u4eec\u8981\u610f\u8bc6\u5230CPU cache\u7684\u91cd\u8981\u6027\uff0c\u56e0\u6b64\u5728\u8fdb\u884c\u8bbe\u8ba1\u7684\u65f6\u5019\uff0c\u9700\u8981\u5408\u7406\u7684\u5b89\u6392\u6570\u636e Locality is a first-order issue that trumps much of our existing understanding of performance optimization. Most traditional optimization techniques come after \"locality\" on parallel hardware (although a few are still equally or more important than locality, such as big-Oh algorithmic complexity for example). Arrange your data carefully by following these three guidelines, starting with the most important: First First: Keep data that are not used together apart in memory. If variables A and B are not protected by the same mutex and are liable(\u5e94\u8be5) to be used by two different threads, keep them on separate cache lines. (Add padding, if necessary; it's a great way to \"waste\" memory to make your code run faster.) This avoids the invisible convoying of false sharing (ping-pong) where in the worst case only one contending thread can make progress at all, and so typically trumps other important cache considerations. Second Second: Keep data that is frequently used together close together in memory. If a thread that uses variable A frequently also needs variable B, try to put A and B in the same cache line if possible. For example, A and B might be two fields in the same object that are frequently used together. This is a traditional technique to improve memory performance in both sequential and concurrent code, which now comes second to keeping separate data apart. Third Third: Keep \"hot\" (frequently accessed) and \"cold\" (infrequently accessed) data apart. This is true even if the data is conceptually in the same logical object. This helps both to fit \"hot\" data into the fewest possible cache lines and memory pages and to avoid needlessly loading the \"colder\" parts. Together these effects reduce (a) the cache footprint and cache misses, and (b) the memory footprint and virtual memory paging. Next Steps Achieving parallel scalability involves two things: 1\u3001Express it: Find the work that can be parallelized effectively. 2\u3001Then don't lose it: Avoid visible and invisible scalability busters like the ones noted in this article. We've seen some ways to avoid losing scalability unwittingly by invisibly adding contention. Next time, we'll consider one other important way we need to avoid invisibly adding contention and losing scalability: Choosing concurrency-friendly data structures, and avoiding concurrency-hostile ones. Stay tuned.","title":"Introduction"},{"location":"Multithread/Expert-Herb-Sutter/11-Maximize-Locality-Minimize-Contention/#drdobbs#maximize#locality#minimize#contention","text":"NOTE: \u4e00\u672c\u6587\u4e2d\u7684\"locality\"\u5bf9\u5e94\u7684\u662flocal to core cache\uff1b\u66f4\u5927\u7684locality\uff0c\u610f\u5473\u7740\u66f4\u5c11\u7684contention Want to kill your parallel application's scalability? Easy: Just add a dash of contention. In the concurrent world, locality is a first-order issue that trumps(\u738b\u724c\u3001\u9996\u8981\u8003\u8651\u56e0\u7d20) most other performance considerations. Now locality is no longer just about fitting well into cache and RAM, but to avoid scalability busters(\u904f\u5236) by keeping tightly coupled data physically close together and separately used data far, far apart. NOTE: \u7ffb\u8bd1\u5982\u4e0b: \"\u5728\u5e76\u53d1\u4e16\u754c\u4e2d\uff0c\u5c40\u90e8\u6027\u662f\u4e00\u4e2a\u4e00\u9636\u95ee\u9898\uff0c\u6bd4\u5927\u591a\u6570\u5176\u4ed6\u6027\u80fd\u8003\u8651\u56e0\u7d20\u90fd\u91cd\u8981\u3002 \u73b0\u5728\uff0c\u5c40\u90e8\u6027\u4e0d\u518d\u4ec5\u4ec5\u662f\u4e3a\u4e86\u5f88\u597d\u5730\u9002\u5e94\u7f13\u5b58\u548cRAM\uff0c\u800c\u662f\u4e3a\u4e86\u907f\u514d\u53ef\u4f38\u7f29\u6027\u95ee\u9898\uff0c\u5c06\u7d27\u5bc6\u8026\u5408\u7684\u6570\u636e\u4fdd\u6301\u7269\u7406\u4e0a\u5728\u4e00\u8d77\uff0c\u800c\u5c06\u5355\u72ec\u4f7f\u7528\u7684\u6570\u636e\u4fdd\u6301\u5728\u5f88\u8fdc\u5f88\u8fdc\u7684\u5730\u65b9\u3002\" \u6700\u540e\u4e00\u53e5\u8bdd\u7684\u542b\u4e49\u662f: locality \u4e0d\u518d\u4ec5\u4ec5\u662f\"fitting well into cache and RAM\"\uff0c\u8fd8\u5305\u62ec\u4e86: \u4e3a\u4e86\u907f\u514d\"scalability busters(\u904f\u5236)\"\uff0c\u9700\u8981: 1\u3001keeping tightly coupled data physically close together 2\u3001separately used data far, far apart \u8fd9\u662f\u4e3a\u4e86\u907f\u514dfalse sharing","title":"drdobbs Maximize Locality, Minimize Contention"},{"location":"Multithread/Expert-Herb-Sutter/11-Maximize-Locality-Minimize-Contention/#of#course#youd#never#convoy#on#a#global#lock","text":"","title":"Of Course, You'd Never Convoy(\u62a4\u822a) On a Global Lock"},{"location":"Multithread/Expert-Herb-Sutter/11-Maximize-Locality-Minimize-Contention/#example","text":"Nobody would ever willingly write code like this in a tight loop. // Threads 1-N while ( ... ) { globalMutex . lock (); DoWork (); globalMutex . unlock (); } This is clearly foolish, because all the work is being done while holding a global lock and so only one thread can make progress at a time. We end up with a classic lock convoy: At any given time, all of the threads save one are piled up behind the lock as each waits idly for its turn to come. Convoys are a classic way to kill parallel scalability. In this example, we'll get no parallel speedup at all because this is just a fancy way to write sequential code. In fact, we'll probably get a minor performance hit because of taking and releasing the lock many times and incurring context switches, and so we would be better off just putting the lock around the whole loop and making the convoy more obvious.","title":"Example"},{"location":"Multithread/Expert-Herb-Sutter/11-Maximize-Locality-Minimize-Contention/#_1","text":"True, we sometimes still gain some parallel benefit when only part of each thread's work is done while holding the lock: // Threads 1-N while ( ... ) { DoParallelWork (); // p = time spent here, // parallel portion globalMutex . lock (); DoSequentialWork (); // s = time spent here, // sequential portion globalMutex . unlock (); } Now at least some of the threads' work can be done in parallel. NOTE: \u4e00\u3001Global Lock\u9501\u7684\u7c92\u5ea6\u592a\u5927\u4e86\uff0c\u901a\u8fc7\u964d\u4f4e\u9501\u7684\u7c92\u5ea6\uff0c\u80fd\u591f\u63d0\u5347\u4e00\u4e9b\u5e76\u53d1\u6027\uff0c\u4f46\u662f\u4ece\u4e0b\u9762\u7684\"Amdahl's Law\"\u53ef\u77e5\uff0c\u8fd9\u79cd\u63d0\u5347\u662f\u6709\u4e0a\u9650\u7684\u3002","title":"\u964d\u4f4e\u9501\u7684\u7c92\u5ea6"},{"location":"Multithread/Expert-Herb-Sutter/11-Maximize-Locality-Minimize-Contention/#amdahls#law","text":"Of course, we still hit Amdahl's Law [1]: Even on infinitely parallel hardware with an infinite number of workers, the maximum speedup is merely (s+p)/s, minus whatever overhead we incur to perform the locking and context switches. But if we fail to measure (ahem) and the time spent in p is much less than in s , we're really gaining little and we've again written a convoy.","title":"Amdahl's Law"},{"location":"Multithread/Expert-Herb-Sutter/11-Maximize-Locality-Minimize-Contention/#the#ugly#truth#exclusive#lock","text":"NOTE: 1\u3001\u65e0\u5904\u4e0d\u5728\u7684lock\uff0c\u6700\u7ec8\u90fd\u4f1a\u5bfc\u81f4 \"hit Amdahl's Law\" We'd never willingly do this. But the ugly truth is that we do it all the time: Sometimes it happens unintentionally; for example, when some function we call might be taking a lock on a popular mutex unbeknownst(\u4e0d\u4e3a\u6240\u77e5\u7684) to us. But often it happens invisibly(\u770b\u4e0d\u89c1\u7684), when hardware will helpfully take exactly such an exclusive lock automatically, and silently, on our behalf. Let's see how, why, and what we can do about it.","title":"The ugly truth: \u65e0\u5904\u4e0d\u5728\u7684exclusive lock"},{"location":"Multithread/Expert-Herb-Sutter/11-Maximize-Locality-Minimize-Contention/#recap#chunky#memory","text":"NOTE: 1\u3001\"Recap\"\u7684\u610f\u601d\u662f \"\u56de\u987e\" 2\u3001\"chunky\" \u5bf9\u5e94\u662f\u540e\u9762\u4f1a\u4ecb\u7ecd\u7684\"chunk\"\uff0c\u5bf9\u5e94\u7684OS\u7684page\u3001cache\u7684cache line For high-performance code, we've always had to be aware of paging and caching effects. Now, hardware concurrency adds a whole new layer to consider. When you ask for a byte of memory, the system never retrieves just one byte. We probably all know that nearly all computer systems keep track\u2014not of bytes, but of chunks of memory. There are two major levels at which chunking occurs: 1\u3001the operating system chunks virtual memory into pages, each of which is managed as a unit, and 2\u3001the cache hardware further chunks memory into cache lines, which again are each handled as a unit. Figure 1 shows a simplified view. (In a previous article, we considered some issues that arise from nonshared caches, where only subsets of processors share caches in common [2].) Figure 1: Chunking in the memory hierarchy. First, consider memory pages: How big is a page? That's up to the OS and varies by platform, but on mainstream systems the page size is typically 4K or more (see Figure 2). So when you ask for just one byte on a page that's not currently in memory, you incur two main costs: 1\u3001Speed: A page fault where the OS has to load the entire page from disk. 2\u3001Space: Memory overhead for storing the entire page in memory, even if you only ever touch one byte from the page. Second, consider cache lines: How big is a line? That's up to the cache hardware and again varies, but on mainstream systems the line size is typically 64 bytes (see Figure 2). So when you ask for just one byte on a line that's not currently in cache, you incur two main costs: 1\u3001Speed: A cache miss where the cache hardware has to load the entire line from memory. 2\u3001Space: Cache overhead for storing the entire line in cache, even if you only ever touch one byte from the line. And now comes the fun part. On multicore hardware, if one core writes to a byte of memory, then typically, as part of the hardware's cache coherency protocol, that core will automatically (read: invisibly) take an exclusive write lock on that cache line. The good news is that this prevents other cores from causing trouble by trying to perform conflicting writes. The sad news is that it also means, well, taking a lock. Figure 2: Load a byte = load a line + load a page.","title":"Recap: Chunky Memory"},{"location":"Multithread/Expert-Herb-Sutter/11-Maximize-Locality-Minimize-Contention/#sharing#and#false#sharing#ping-pong","text":"Consider the following code where two threads update two distinct global integers x and y , and assume we've disabled optimizations to prevent the optimizer from eliminating the loops entirely in this toy example: // Thread 1 for ( i = 0 ; i < MAX ; ++ i ) { ++ x ; } // Thread 2 for ( i = 0 ; i < MAX ; ++ i ) { ++ y ; } Question: What relative performance would you expect if running Thread 1 in isolation versus running both threads: 1\u3001On a machine with one core? 2\u3001On a machine with two or more cores? On a machine with one core, the program would probably take twice as long to run, as we'd probably get the same throughput (additions/sec), maybe with a little overhead for context switches as the operating system schedules the two threads interleaved on the single core. On a machine with two or more cores, we'd probably expect to get a 2x throughput improvement as the two threads each run at full speed on their own cores. And that is what in fact will happen...but only if x and y are on different cache lines.","title":"Sharing and False Sharing (Ping-Pong)"},{"location":"Multithread/Expert-Herb-Sutter/11-Maximize-Locality-Minimize-Contention/#x#and#y#are#on#the#same#cache#line","text":"If x and y are on the same cache line, however, only one core can be updating the cache line at a time, because only one core can have exclusive access at a time\u2014it's as if the cache line is a token being passed between the threads/cores to say who is currently allowed to run. So the situation is exactly as if we had explicitly written: // Thread 1 for ( i = 0 ; i < MAX ; ++ i ) { lightweightMutexForXandY . lock (); ++ x ; lightweightMutexForXandY . unlock (); } // Thread 2 for ( i = 0 ; i < MAX ; ++ i ) { lightweightMutexForXandY . lock (); ++ y ; lightweightMutexForXandY . unlock (); } Which of course is exactly what we said we would never willingly do: Only one thread can make progress at a time. This effect is called \"false sharing\" because, even though the cores are trying to update different parts of the cache line , that doesn't matter; the unit of sharing is the whole line, and so the performance effect is the same as if the two threads were trying to share the same variable. It's also called ping-ponging because that's an apt description of how the cache line ownership keeps hopping back and forth madly between the two cores.","title":"x and y are on the same cache line"},{"location":"Multithread/Expert-Herb-Sutter/11-Maximize-Locality-Minimize-Contention/#how#to#ensure#x#and#y#are#on#different#cache#lines","text":"Even in this simple example, what could we do to ensure x and y are on different cache lines? First, we can rearrange the data: For example, if x and y are data values inside the same object, perhaps we can rearrange the object's members so that x and y are sufficiently further apart. Second, we can add padding: If we have no other data that's easy to put adjacent to x, we can ensure x is alone on its cache line by allocating extra space, such as by allocating a larger object with x as a member (preceded/followed by appropriate padding to fill the cache line) instead of allocating x by itself as a naked integer. This is a great example of how to deliberately waste space to improve performance.","title":"How to ensure x and y are on different cache lines?"},{"location":"Multithread/Expert-Herb-Sutter/11-Maximize-Locality-Minimize-Contention/#false#sharing#example","text":"False sharing arises in lots of hard-to-see places. For example: 1\u3001Two independent variables or objects are too close together, as in the above examples. 2\u3001Two node-based containers (lists or trees, for example) interleave in memory, so that the same cache line contains nodes from two containers.","title":"False sharing example"},{"location":"Multithread/Expert-Herb-Sutter/11-Maximize-Locality-Minimize-Contention/#cache-conscious#design","text":"NOTE: 1\u3001\u6211\u4eec\u8981\u610f\u8bc6\u5230CPU cache\u7684\u91cd\u8981\u6027\uff0c\u56e0\u6b64\u5728\u8fdb\u884c\u8bbe\u8ba1\u7684\u65f6\u5019\uff0c\u9700\u8981\u5408\u7406\u7684\u5b89\u6392\u6570\u636e Locality is a first-order issue that trumps much of our existing understanding of performance optimization. Most traditional optimization techniques come after \"locality\" on parallel hardware (although a few are still equally or more important than locality, such as big-Oh algorithmic complexity for example). Arrange your data carefully by following these three guidelines, starting with the most important:","title":"Cache-Conscious Design"},{"location":"Multithread/Expert-Herb-Sutter/11-Maximize-Locality-Minimize-Contention/#first","text":"First: Keep data that are not used together apart in memory. If variables A and B are not protected by the same mutex and are liable(\u5e94\u8be5) to be used by two different threads, keep them on separate cache lines. (Add padding, if necessary; it's a great way to \"waste\" memory to make your code run faster.) This avoids the invisible convoying of false sharing (ping-pong) where in the worst case only one contending thread can make progress at all, and so typically trumps other important cache considerations.","title":"First"},{"location":"Multithread/Expert-Herb-Sutter/11-Maximize-Locality-Minimize-Contention/#second","text":"Second: Keep data that is frequently used together close together in memory. If a thread that uses variable A frequently also needs variable B, try to put A and B in the same cache line if possible. For example, A and B might be two fields in the same object that are frequently used together. This is a traditional technique to improve memory performance in both sequential and concurrent code, which now comes second to keeping separate data apart.","title":"Second"},{"location":"Multithread/Expert-Herb-Sutter/11-Maximize-Locality-Minimize-Contention/#third","text":"Third: Keep \"hot\" (frequently accessed) and \"cold\" (infrequently accessed) data apart. This is true even if the data is conceptually in the same logical object. This helps both to fit \"hot\" data into the fewest possible cache lines and memory pages and to avoid needlessly loading the \"colder\" parts. Together these effects reduce (a) the cache footprint and cache misses, and (b) the memory footprint and virtual memory paging.","title":"Third"},{"location":"Multithread/Expert-Herb-Sutter/11-Maximize-Locality-Minimize-Contention/#next#steps","text":"Achieving parallel scalability involves two things: 1\u3001Express it: Find the work that can be parallelized effectively. 2\u3001Then don't lose it: Avoid visible and invisible scalability busters like the ones noted in this article. We've seen some ways to avoid losing scalability unwittingly by invisibly adding contention. Next time, we'll consider one other important way we need to avoid invisibly adding contention and losing scalability: Choosing concurrency-friendly data structures, and avoiding concurrency-hostile ones. Stay tuned.","title":"Next Steps"},{"location":"Multithread/Expert-Herb-Sutter/12-Choose-Concurrency-Friendly-Data-Structures/","text":"drdobbs Choose Concurrency-Friendly Data Structures NOTE: 1\u3001\u5982\u4f55\u9009\u578b\uff1f\u9009\u62e9Concurrency-Friendly Data Structures\uff0c\u5b83\u4eec\u8fd0\u884cfine-grained lock\uff1b\u672c\u6587\u4e00linked list \u548c balanced search tree \u6765\u8bf4\u660e\u8fd9\u4e2a\u95ee\u9898\u3002\u8fd9\u4f4d\u6211\u4eec\u9009\u578b\u63d0\u4f9b\u4e86\u5f88\u597d\u7684\u601d\u8def\u3002 Linked Lists and Balanced Search Trees are familiar data structures, but can they make the leap to parallelized environments? What is a high-performance data structure? To answer that question, we're used to applying normal considerations like Big-Oh complexity, and memory overhead, locality, and traversal order. All of those apply to both sequential and concurrent software. Concurrency-friendly data structure But in concurrent code, we need to consider two additional things to help us pick a data structure that is also sufficiently concurrency-friendly: 1\u3001In parallel code, your performance needs likely include the ability to allow multiple threads to use the data at the same time. If this is (or may become) a high-contention data structure, does it allow for concurrent readers and/or writers in different parts of the data structure at the same time? If the answer is, \"No,\" then you may be designing an inherent bottleneck into your system and be just asking for lock convoys as threads wait, only one being able to use the data structure at a time. 2\u3001On parallel hardware, you may also care about minimizing the cost of memory synchronization . When one thread updates one part of the data structure, how much memory needs to be moved to make the change visible to another thread? If the answer is, \"More than just the part that has ostensibly(\u8868\u9762\u4e0a) changed,\" then again you're asking for a potential performance penalty, this time due to cache sloshing(\u6643\u8361) as more data has to move from the core that performed the update to the core that is reading the result. Locality It turns out that both of these answers are directly influenced by whether the data structure allows truly localized updates . If making what appears to be a small change in one part of the data structure actually ends up reading or writing other parts of the structure, then we lose locality ; those other parts need to be locked, too, and all of the memory that has changed needs to be synchronized. To illustrate, let's consider two common data structures: linked lists and balanced trees. Linked Lists Linked lists are wonderfully concurrency-friendly data structures because they support highly localized updates. In particular, as illustrated in Figure 1, to insert a new node into a doubly linked list , you only need to touch two existing nodes; namely, the ones immediately adjacent to the position the new node will occupy to splice the new node into the list. To erase a node, you only need to touch three nodes: the one that is being erased, and its two immediately adjacent nodes. This locality enables the option of using fine-grained locking: We can allow a potentially large number of threads to be actively working inside the same list, knowing that they won't conflict as long as they are manipulating different parts of the list. Each operation only needs to lock enough of the list to cover the nodes it actually uses. For example, consider Figure 2, which illustrates the technique of hand-over-hand locking . The basic idea is this: Each segment of the list, or even each individual node, is protected by its own mutex. Each thread that may add or remove nodes from the list takes a lock on the first node, then while still holding that, takes a lock on the next node; then it lets go of the first node and while still holding a lock on the second node, it takes a lock on the third node; and so on. (To delete a node requires locking three nodes.) While traversing the list, each such thread always holds at least two locks\u2014and the locks are always taken in the same order. Figure 1: Localized insertion into a linked list. Figure 2: Hand-over-hand locking in a linked list. This technique delivers a number of benefits, including the following: 1\u3001Multiple readers and writers can be actively doing work in the same list. 2\u3001Readers and writers that are traversing the list in the same order will not pass each other. This can be useful to get deterministic results in concurrent code. In particular, the list's semantics will be the same as if each thread acquired complete exclusion on the list and performed its complete pass in isolation, which is easy to reason about. 3\u3001The locks taken on parts of the list won't deadlock with each other, because multiple locks are acquired in the same order. 4\u3001We can readily tune(\u8c03\u6574) the code for better concurrency vs. lower locking overhead by choosing a suitable locking granularity: one lock for the whole list (no concurrency), a lock for each node in the list (maximum concurrency), or a lock for each chunk of some fixed or variable length (something in between). NOTE: 1\u3001\u8c03\u6574\u9501\u7c92\u5ea6 Aside: If we always traverse the list in the same order, why does the figure show a doubly linked list? Because not all operations need to take multiple locks; those that use individual segments or nodes in-place one at a time without taking more than one node's or chunk's lock at a time can traverse the list in any order without deadlock. (For more on avoiding deadlock, see [1].) Besides being well suited for concurrent traversal and update, linked lists also are cache-friendly on parallel hardware. When one thread removes a node, for example, the only memory that needs to be transferred to every other core that subsequently reads the list is the memory containing the two adjacent nodes. If the rest of the list hasn't been changed, multiple cores can happily store read-only copies of the list in their caches without expensive memory fetches and synchronization. (Remember, writes are always more expensive than reads because writes need to be broadcast. In turn, \"lots of writes\" are always more expensive than \"limited writes.\") Clearly, one benefit lists enjoy is that they are node-based containers: Each element is stored in its own node, unlike an array or vector where elements are contiguous and inserting or erasing typically involves copying an arbitrary number of elements to one side or the other of the inserted or erased value. We might therefore anticipate(\u63a8\u6d4b) that perhaps all node-based containers will be good for concurrency. Unfortunately, we would be wrong. Balanced Search Trees The story isn't nearly as good for another popular data structure: the balanced search tree. (Important note: This section refers only to balanced trees; unbalanced trees that support localized updates don't suffer from the problems we'll describe next.) Consider a red-black tree : The tree stays balanced by marking each node as either \"red\" or \"black,\" and applying rules that call for optionally rebalancing the tree on each insert or erase to avoid having different branches of the tree become too uneven. In particular, rebalancing is done by rotating subtrees , which involves touching an inserted or erased node's parent and/or uncle node, that node's own parent and/or uncle, and so on to the grandparents and granduncles up the tree, possibly as far as the root. For example, consider Figure 3. To start with, the tree contains three nodes with the values 1 , 2 , and 3. To insert the value 4 , we simply make it a child of node 3 , as we would in a nonbalanced binary search tree. Clearly, that involves writing to node 3 , to set its right-child pointer. However, to satisfy the red-black tree mechanics, we must also change node 3 's and node 1 's color to black. That adds overhead and loses some concurrency; for example, inserting 4 would conflict with adding 1.5 concurrently, because both inserts would need to touch both nodes 1 and 3 . Figure 3: Nonlocalized insertion into a red-black tree. Next, to insert the value 5 , we need to touch all but one of the nodes in the tree: We first make node 4 point to the new node 5 as its right child, then recolor both node 4 and node 3 , and then because the tree is out of balance we also rotate 3-4-5 to make node 4 the root of that subtree, which means also touching node 2 to install node 4 as its new right child. So red-black trees cause some problems for concurrent code: 1\u3001It's hard to run updates truly concurrently because updates arbitrarily far apart in the tree can touch the same nodes\u2014especially the root, but also other higher-level nodes to lesser degrees\u2014and therefore contend with each other. We have lost the ability to make truly localized changes. 2\u3001The tree performs extra internal housekeeping writes. This increases the amount of shared data that needs to be written and synchronized across caches to publish what would be a small update in another data structure. \"But wait,\" I can hear some people saying, \"why can't we just put a mutex inside each node and take the locks in a single direction (up the tree) like we could do with the linked list and hand-over-hand locking? Wouldn't that let us regain the ability to have concurrent use of the data structure at least?\" Short answer: That's easy to do, but hard to do right. Unlike the linked list case, however: (a) you may need to take many more locks, even all the way up to the root; and (b) the higher-level nodes will still end up being high-contention resources that bottleneck scalability. Also, the code to do this is much more complicated. As Fraser noted in 2004: \"One superficially attractive solution is to read-lock down the tree and then write-lock on the way back up, just as far as rebalancing operations are required. This scheme would acquire exclusive access to the minimal number of nodes (those that are actually modified), but can result in deadlock with search operations (which are locking down the tree).\" [2] He also proposed a fine-grained locking technique that does allow some concurrency, but notes that it \"is significantly more complicated.\" There are easy answers, but few easy and correct answers. To get around these limitations, researchers have worked on alternative structures such as skip lists [4], and on variants of red-black trees that can be more amenable to concurrency, such as by doing relaxed balancing instead of rebalancing immediately when needed after each update. Some of these are significantly more complex, which incurs its own costs in both performance and correctness/maintainability (for example, relaxed balancing was first suggested in 1978 but not implemented successfully until five years later). For more information and some relative performance measurements showing how even concurrent versions can still limit scalability, see [3]. Conclusions Concurrency-friendliness alone doesn't singlehandedly trump other performance requirements. The usual performance considerations of Big-Oh complexity, and memory overhead, locality, and traversal order all still apply. Even when writing parallel code, you shouldn't choose a data structure only because it's concurrency-friendly; you should choose the right one that meets all your performance needs. Lists may be more concurrency-friendly than balanced trees, but trees are faster to search, and \"individual searches are fast\" can outbalance \"multiple searches can run in parallel.\" (If you need both, try an alternative like skip lists.) Remember: 1\u3001In parallel code, your performance needs likely include the ability to allow multiple threads to use the data at the same time. 2\u3001On parallel hardware, you may also care about minimizing the cost of memory synchronization. In those situations, prefer concurrency-friendly data structures. The more a container supports truly localized updates, the more concurrency you can have as multiple threads can actively use different parts of the data structure at the same time, and (secondarily but still sometimes importantly) the more you can avoid invisible memory synchronization overhead in your high-performance code. Notes [1] H. Sutter. \" Use Lock Hierarchies to Avoid Deadlock \" ( Dr. Dobb's Journal, January 2008). [2] K. Fraser. \"Practical lock-freedom\" ( University of Cambridge Computer Laboratory Technical Report #579 , February 2004). [3] S. Hanke. \"The Performance of Concurrent Red-Black Tree Algorithms\" ( Lecture Notes in Computer Science , 1668:286-300, Springer, 1999). [4] M. Fomitchev and E. Ruppert. \"Lock-Free Linked Lists and Skip Lists\" ( PODC '04 , July 2004).","title":"Introduction"},{"location":"Multithread/Expert-Herb-Sutter/12-Choose-Concurrency-Friendly-Data-Structures/#drdobbs#choose#concurrency-friendly#data#structures","text":"NOTE: 1\u3001\u5982\u4f55\u9009\u578b\uff1f\u9009\u62e9Concurrency-Friendly Data Structures\uff0c\u5b83\u4eec\u8fd0\u884cfine-grained lock\uff1b\u672c\u6587\u4e00linked list \u548c balanced search tree \u6765\u8bf4\u660e\u8fd9\u4e2a\u95ee\u9898\u3002\u8fd9\u4f4d\u6211\u4eec\u9009\u578b\u63d0\u4f9b\u4e86\u5f88\u597d\u7684\u601d\u8def\u3002 Linked Lists and Balanced Search Trees are familiar data structures, but can they make the leap to parallelized environments? What is a high-performance data structure? To answer that question, we're used to applying normal considerations like Big-Oh complexity, and memory overhead, locality, and traversal order. All of those apply to both sequential and concurrent software.","title":"drdobbs Choose Concurrency-Friendly Data Structures"},{"location":"Multithread/Expert-Herb-Sutter/12-Choose-Concurrency-Friendly-Data-Structures/#concurrency-friendly#data#structure","text":"But in concurrent code, we need to consider two additional things to help us pick a data structure that is also sufficiently concurrency-friendly: 1\u3001In parallel code, your performance needs likely include the ability to allow multiple threads to use the data at the same time. If this is (or may become) a high-contention data structure, does it allow for concurrent readers and/or writers in different parts of the data structure at the same time? If the answer is, \"No,\" then you may be designing an inherent bottleneck into your system and be just asking for lock convoys as threads wait, only one being able to use the data structure at a time. 2\u3001On parallel hardware, you may also care about minimizing the cost of memory synchronization . When one thread updates one part of the data structure, how much memory needs to be moved to make the change visible to another thread? If the answer is, \"More than just the part that has ostensibly(\u8868\u9762\u4e0a) changed,\" then again you're asking for a potential performance penalty, this time due to cache sloshing(\u6643\u8361) as more data has to move from the core that performed the update to the core that is reading the result.","title":"Concurrency-friendly data structure"},{"location":"Multithread/Expert-Herb-Sutter/12-Choose-Concurrency-Friendly-Data-Structures/#locality","text":"It turns out that both of these answers are directly influenced by whether the data structure allows truly localized updates . If making what appears to be a small change in one part of the data structure actually ends up reading or writing other parts of the structure, then we lose locality ; those other parts need to be locked, too, and all of the memory that has changed needs to be synchronized. To illustrate, let's consider two common data structures: linked lists and balanced trees.","title":"Locality"},{"location":"Multithread/Expert-Herb-Sutter/12-Choose-Concurrency-Friendly-Data-Structures/#linked#lists","text":"Linked lists are wonderfully concurrency-friendly data structures because they support highly localized updates. In particular, as illustrated in Figure 1, to insert a new node into a doubly linked list , you only need to touch two existing nodes; namely, the ones immediately adjacent to the position the new node will occupy to splice the new node into the list. To erase a node, you only need to touch three nodes: the one that is being erased, and its two immediately adjacent nodes. This locality enables the option of using fine-grained locking: We can allow a potentially large number of threads to be actively working inside the same list, knowing that they won't conflict as long as they are manipulating different parts of the list. Each operation only needs to lock enough of the list to cover the nodes it actually uses. For example, consider Figure 2, which illustrates the technique of hand-over-hand locking . The basic idea is this: Each segment of the list, or even each individual node, is protected by its own mutex. Each thread that may add or remove nodes from the list takes a lock on the first node, then while still holding that, takes a lock on the next node; then it lets go of the first node and while still holding a lock on the second node, it takes a lock on the third node; and so on. (To delete a node requires locking three nodes.) While traversing the list, each such thread always holds at least two locks\u2014and the locks are always taken in the same order.","title":"Linked Lists"},{"location":"Multithread/Expert-Herb-Sutter/12-Choose-Concurrency-Friendly-Data-Structures/#figure#1#localized#insertion#into#a#linked#list","text":"","title":"Figure 1: Localized insertion into a linked list."},{"location":"Multithread/Expert-Herb-Sutter/12-Choose-Concurrency-Friendly-Data-Structures/#figure#2#hand-over-hand#locking#in#a#linked#list","text":"This technique delivers a number of benefits, including the following: 1\u3001Multiple readers and writers can be actively doing work in the same list. 2\u3001Readers and writers that are traversing the list in the same order will not pass each other. This can be useful to get deterministic results in concurrent code. In particular, the list's semantics will be the same as if each thread acquired complete exclusion on the list and performed its complete pass in isolation, which is easy to reason about. 3\u3001The locks taken on parts of the list won't deadlock with each other, because multiple locks are acquired in the same order. 4\u3001We can readily tune(\u8c03\u6574) the code for better concurrency vs. lower locking overhead by choosing a suitable locking granularity: one lock for the whole list (no concurrency), a lock for each node in the list (maximum concurrency), or a lock for each chunk of some fixed or variable length (something in between). NOTE: 1\u3001\u8c03\u6574\u9501\u7c92\u5ea6 Aside: If we always traverse the list in the same order, why does the figure show a doubly linked list? Because not all operations need to take multiple locks; those that use individual segments or nodes in-place one at a time without taking more than one node's or chunk's lock at a time can traverse the list in any order without deadlock. (For more on avoiding deadlock, see [1].) Besides being well suited for concurrent traversal and update, linked lists also are cache-friendly on parallel hardware. When one thread removes a node, for example, the only memory that needs to be transferred to every other core that subsequently reads the list is the memory containing the two adjacent nodes. If the rest of the list hasn't been changed, multiple cores can happily store read-only copies of the list in their caches without expensive memory fetches and synchronization. (Remember, writes are always more expensive than reads because writes need to be broadcast. In turn, \"lots of writes\" are always more expensive than \"limited writes.\") Clearly, one benefit lists enjoy is that they are node-based containers: Each element is stored in its own node, unlike an array or vector where elements are contiguous and inserting or erasing typically involves copying an arbitrary number of elements to one side or the other of the inserted or erased value. We might therefore anticipate(\u63a8\u6d4b) that perhaps all node-based containers will be good for concurrency. Unfortunately, we would be wrong.","title":"Figure 2: Hand-over-hand locking in a linked list."},{"location":"Multithread/Expert-Herb-Sutter/12-Choose-Concurrency-Friendly-Data-Structures/#balanced#search#trees","text":"The story isn't nearly as good for another popular data structure: the balanced search tree. (Important note: This section refers only to balanced trees; unbalanced trees that support localized updates don't suffer from the problems we'll describe next.) Consider a red-black tree : The tree stays balanced by marking each node as either \"red\" or \"black,\" and applying rules that call for optionally rebalancing the tree on each insert or erase to avoid having different branches of the tree become too uneven. In particular, rebalancing is done by rotating subtrees , which involves touching an inserted or erased node's parent and/or uncle node, that node's own parent and/or uncle, and so on to the grandparents and granduncles up the tree, possibly as far as the root. For example, consider Figure 3. To start with, the tree contains three nodes with the values 1 , 2 , and 3. To insert the value 4 , we simply make it a child of node 3 , as we would in a nonbalanced binary search tree. Clearly, that involves writing to node 3 , to set its right-child pointer. However, to satisfy the red-black tree mechanics, we must also change node 3 's and node 1 's color to black. That adds overhead and loses some concurrency; for example, inserting 4 would conflict with adding 1.5 concurrently, because both inserts would need to touch both nodes 1 and 3 .","title":"Balanced Search Trees"},{"location":"Multithread/Expert-Herb-Sutter/12-Choose-Concurrency-Friendly-Data-Structures/#figure#3#nonlocalized#insertion#into#a#red-black#tree","text":"Next, to insert the value 5 , we need to touch all but one of the nodes in the tree: We first make node 4 point to the new node 5 as its right child, then recolor both node 4 and node 3 , and then because the tree is out of balance we also rotate 3-4-5 to make node 4 the root of that subtree, which means also touching node 2 to install node 4 as its new right child. So red-black trees cause some problems for concurrent code: 1\u3001It's hard to run updates truly concurrently because updates arbitrarily far apart in the tree can touch the same nodes\u2014especially the root, but also other higher-level nodes to lesser degrees\u2014and therefore contend with each other. We have lost the ability to make truly localized changes. 2\u3001The tree performs extra internal housekeeping writes. This increases the amount of shared data that needs to be written and synchronized across caches to publish what would be a small update in another data structure. \"But wait,\" I can hear some people saying, \"why can't we just put a mutex inside each node and take the locks in a single direction (up the tree) like we could do with the linked list and hand-over-hand locking? Wouldn't that let us regain the ability to have concurrent use of the data structure at least?\" Short answer: That's easy to do, but hard to do right. Unlike the linked list case, however: (a) you may need to take many more locks, even all the way up to the root; and (b) the higher-level nodes will still end up being high-contention resources that bottleneck scalability. Also, the code to do this is much more complicated. As Fraser noted in 2004: \"One superficially attractive solution is to read-lock down the tree and then write-lock on the way back up, just as far as rebalancing operations are required. This scheme would acquire exclusive access to the minimal number of nodes (those that are actually modified), but can result in deadlock with search operations (which are locking down the tree).\" [2] He also proposed a fine-grained locking technique that does allow some concurrency, but notes that it \"is significantly more complicated.\" There are easy answers, but few easy and correct answers. To get around these limitations, researchers have worked on alternative structures such as skip lists [4], and on variants of red-black trees that can be more amenable to concurrency, such as by doing relaxed balancing instead of rebalancing immediately when needed after each update. Some of these are significantly more complex, which incurs its own costs in both performance and correctness/maintainability (for example, relaxed balancing was first suggested in 1978 but not implemented successfully until five years later). For more information and some relative performance measurements showing how even concurrent versions can still limit scalability, see [3].","title":"Figure 3: Nonlocalized insertion into a red-black tree."},{"location":"Multithread/Expert-Herb-Sutter/12-Choose-Concurrency-Friendly-Data-Structures/#conclusions","text":"Concurrency-friendliness alone doesn't singlehandedly trump other performance requirements. The usual performance considerations of Big-Oh complexity, and memory overhead, locality, and traversal order all still apply. Even when writing parallel code, you shouldn't choose a data structure only because it's concurrency-friendly; you should choose the right one that meets all your performance needs. Lists may be more concurrency-friendly than balanced trees, but trees are faster to search, and \"individual searches are fast\" can outbalance \"multiple searches can run in parallel.\" (If you need both, try an alternative like skip lists.) Remember: 1\u3001In parallel code, your performance needs likely include the ability to allow multiple threads to use the data at the same time. 2\u3001On parallel hardware, you may also care about minimizing the cost of memory synchronization. In those situations, prefer concurrency-friendly data structures. The more a container supports truly localized updates, the more concurrency you can have as multiple threads can actively use different parts of the data structure at the same time, and (secondarily but still sometimes importantly) the more you can avoid invisible memory synchronization overhead in your high-performance code.","title":"Conclusions"},{"location":"Multithread/Expert-Herb-Sutter/12-Choose-Concurrency-Friendly-Data-Structures/#notes","text":"[1] H. Sutter. \" Use Lock Hierarchies to Avoid Deadlock \" ( Dr. Dobb's Journal, January 2008). [2] K. Fraser. \"Practical lock-freedom\" ( University of Cambridge Computer Laboratory Technical Report #579 , February 2004). [3] S. Hanke. \"The Performance of Concurrent Red-Black Tree Algorithms\" ( Lecture Notes in Computer Science , 1668:286-300, Springer, 1999). [4] M. Fomitchev and E. Ruppert. \"Lock-Free Linked Lists and Skip Lists\" ( PODC '04 , July 2004).","title":"Notes"},{"location":"Multithread/Expert-Herb-Sutter/15-TODO-Writing-Lock-Free-Code-A-Corrected-Queue/","text":"drdobbs Writing Lock-Free Code: A Corrected Queue Herb continues his exploration of lock-free code--this time focusing on creating a lock-free queue. As we saw last month [1], lock-free coding is hard even for experts. There, I dissected(\u5256\u6790) a published lock-free queue implementation [2] and examined why the code was quite broken. This month, let's see how to do it right.","title":"Introduction"},{"location":"Multithread/Expert-Herb-Sutter/15-TODO-Writing-Lock-Free-Code-A-Corrected-Queue/#drdobbs#writing#lock-free#code#a#corrected#queue","text":"Herb continues his exploration of lock-free code--this time focusing on creating a lock-free queue. As we saw last month [1], lock-free coding is hard even for experts. There, I dissected(\u5256\u6790) a published lock-free queue implementation [2] and examined why the code was quite broken. This month, let's see how to do it right.","title":"drdobbs Writing Lock-Free Code: A Corrected Queue"},{"location":"Multithread/Expert-Herb-Sutter/20-TODO-Sharing-Is-the-Root-of-All-Contention/","text":"drdobbs Sharing Is the Root of All Contention","title":"Introduction"},{"location":"Multithread/Expert-Herb-Sutter/20-TODO-Sharing-Is-the-Root-of-All-Contention/#drdobbs#sharing#is#the#root#of#all#contention","text":"","title":"drdobbs Sharing Is the Root of All Contention"},{"location":"Multithread/Expert-Herb-Sutter/31-Prefer-Using-Active-Objects-Instead-of-Naked-Thread/","text":"Effective Concurrency: Prefer Using Active Objects Instead of Naked Threads This month\u2019s Effective Concurrency column, \u201c Prefer Using Active Objects Instead of Naked Threads ,\u201d is now live on DDJ\u2019s website. From the article: \u2026 Active objects dramatically improve our ability to reason about our thread\u2019s code and operation by giving us higher-level abstractions and idioms that raise the semantic level of our program and let us express our intent more directly. As with all good patterns, we also get better vocabulary to talk about our design. Note that active objects aren\u2019t a novelty: UML and various libraries have provided support for active classes. Some actor-based languages already have variations of this pattern baked into the language itself; but fortunately, we aren\u2019t limited to using only such languages to get the benefits of active objects. This article will show how to implement the pattern, including a reusable helper to automate the common parts, in any of the popular mainstream languages and threading environments, including C++ , C#/.NET, Java, and C/Pthreads. NOTE: tag-active object is a good abstraction COMMENTS Kjell Hedstr\u00f6m Thanks for yet another great article Herb. I think that instead of using Message objects (for pre C++0x developers) it could be benificial to follow the idiom of generic function callbacks ( your Gotw: 83 et.al ). This way you can easily make the Active object and Backgrounder without message objects focus on the core with cleaner code as a result. Inspired by your Effective Concurrency Europe 2009 seminar I ended up with something like this :) I.e. : (cut-down & pseudo-ish version) Class Active { private : \u2026 . void run () { while ( ! done ){ smart_ptr msg = mq . pop (); msg (); // executes msg } } public : \u2026 . ~ Active () { send ( bind ( this , & Active :: DoDone )); thd -> join (); } \u2026 . }; class Background { \u2026 public : void Save ( smart_ptr data ) { active -> send ( bind ( this , & Background :: DoSave , data ); } \u2026 }; Standard pre C++0x but still with the clean look thanks to some template touch :) Cheers","title":"Introduction"},{"location":"Multithread/Expert-Herb-Sutter/31-Prefer-Using-Active-Objects-Instead-of-Naked-Thread/#effective#concurrency#prefer#using#active#objects#instead#of#naked#threads","text":"This month\u2019s Effective Concurrency column, \u201c Prefer Using Active Objects Instead of Naked Threads ,\u201d is now live on DDJ\u2019s website. From the article: \u2026 Active objects dramatically improve our ability to reason about our thread\u2019s code and operation by giving us higher-level abstractions and idioms that raise the semantic level of our program and let us express our intent more directly. As with all good patterns, we also get better vocabulary to talk about our design. Note that active objects aren\u2019t a novelty: UML and various libraries have provided support for active classes. Some actor-based languages already have variations of this pattern baked into the language itself; but fortunately, we aren\u2019t limited to using only such languages to get the benefits of active objects. This article will show how to implement the pattern, including a reusable helper to automate the common parts, in any of the popular mainstream languages and threading environments, including C++ , C#/.NET, Java, and C/Pthreads. NOTE: tag-active object is a good abstraction","title":"Effective Concurrency: Prefer Using Active Objects Instead of Naked Threads"},{"location":"Multithread/Expert-Herb-Sutter/31-Prefer-Using-Active-Objects-Instead-of-Naked-Thread/#comments","text":"","title":"COMMENTS"},{"location":"Multithread/Expert-Herb-Sutter/31-Prefer-Using-Active-Objects-Instead-of-Naked-Thread/#kjell#hedstrom","text":"Thanks for yet another great article Herb. I think that instead of using Message objects (for pre C++0x developers) it could be benificial to follow the idiom of generic function callbacks ( your Gotw: 83 et.al ). This way you can easily make the Active object and Backgrounder without message objects focus on the core with cleaner code as a result. Inspired by your Effective Concurrency Europe 2009 seminar I ended up with something like this :) I.e. : (cut-down & pseudo-ish version) Class Active { private : \u2026 . void run () { while ( ! done ){ smart_ptr msg = mq . pop (); msg (); // executes msg } } public : \u2026 . ~ Active () { send ( bind ( this , & Active :: DoDone )); thd -> join (); } \u2026 . }; class Background { \u2026 public : void Save ( smart_ptr data ) { active -> send ( bind ( this , & Background :: DoSave , data ); } \u2026 }; Standard pre C++0x but still with the clean look thanks to some template touch :) Cheers","title":"Kjell Hedstr\u00f6m"},{"location":"Multithread/Inter-thread-communication/","text":"Inter-thread communication \u6807\u9898\u7684\u542b\u4e49\u662f: \"\u7ebf\u7a0b\u95f4\u901a\u4fe1\"\u3002\u8fd9\u4e2a\u540d\u79f0\u662f\u8fd9\u6837\u5f97\u6765\u7684: 1\u3001\u5728\u9605\u8bfb cppreference std::condition_variable \u65f6\uff0c\u5176\u4e2d\u63d0\u53ca\u4e86 \" inter-thread communication \" 2\u3001\u7531 Inter-process communication \u7c7b\u63a8\u800c\u5f97\u51fa\u7684\u540d\u79f0 \u5b9e\u73b0\u65b9\u5f0f 1\u3001condition variable 2\u3001promise-future channel 3\u3001Semaphore \u6bd4\u8f83\u597d\u7684\u6587\u7ae0: 1\u3001modernescpp Synchronization with Atomics in C++20 2\u3001modernescpp Performance Comparison of Condition Variables and Atomics in C++20","title":"Introduction"},{"location":"Multithread/Inter-thread-communication/#inter-thread#communication","text":"\u6807\u9898\u7684\u542b\u4e49\u662f: \"\u7ebf\u7a0b\u95f4\u901a\u4fe1\"\u3002\u8fd9\u4e2a\u540d\u79f0\u662f\u8fd9\u6837\u5f97\u6765\u7684: 1\u3001\u5728\u9605\u8bfb cppreference std::condition_variable \u65f6\uff0c\u5176\u4e2d\u63d0\u53ca\u4e86 \" inter-thread communication \" 2\u3001\u7531 Inter-process communication \u7c7b\u63a8\u800c\u5f97\u51fa\u7684\u540d\u79f0","title":"Inter-thread communication"},{"location":"Multithread/Inter-thread-communication/#_1","text":"1\u3001condition variable 2\u3001promise-future channel 3\u3001Semaphore \u6bd4\u8f83\u597d\u7684\u6587\u7ae0: 1\u3001modernescpp Synchronization with Atomics in C++20 2\u3001modernescpp Performance Comparison of Condition Variables and Atomics in C++20","title":"\u5b9e\u73b0\u65b9\u5f0f"},{"location":"Multithread/Inter-thread-communication/Condition-variables-VS-mutex/","text":"condition variables VS mutex stackoverflow Advantages of using condition variables over mutex I was wondering what is the performance benefit of using condition variables over mutex locks in pthreads. What I found is : \"Without condition variables, the programmer would need to have threads continually polling (possibly in a critical section), to check if the condition is met. This can be very resource consuming since the thread would be continuously busy in this activity. A condition variable is a way to achieve the same goal without polling.\" ( https://computing.llnl.gov/tutorials/pthreads ) Summary :\u4f5c\u8005\u5728\u4e0a\u9762\u8fd9\u4e00\u6bb5\u6240\u63cf\u8ff0\u7684\u662f\u4e8b\u5b9e\uff0c\u56e0\u4e3acondition variable\u662f\u4e00\u79cdinter-thread communication\uff0c\u5b83\u5141\u8bb8\u7531programmer\u6765\u51b3\u5b9a\u4f55\u65f6\uff0c\u57fa\u4e8e\u4f55\u79cd\u6761\u4ef6\uff0c\u6765\u901a\u77e5\uff08\u5524\u9192\uff09\u88ab\u963b\u585e\u7684\u7ebf\u7a0b\u3002\u800cmutex\u5219\u5e76\u4e0d\u5177\u5907\u6b64\u529f\u80fd\u3002 But it also seems that mutex calls are blocking (unlike spin-locks). Hence if a thread ( T1 ) fails to get a lock because some other thread ( T2 ) has the lock, T1 is put to sleep by the OS, and is woken up only when T2 releases the lock and the OS gives T1 the lock. The thread T1 does not really poll to get the lock. From this description, it seems that there is no performance benefit of using condition variables. In either case, there is no polling involved. The OS anyway provides the benefit that the condition-variable paradigm can provide. SUMMARY : \u5173\u4e8e\u4e0a\u9762\u8fd9\u6bb5\u4e2d\u7684**blocking**\uff0c\u53c2\u89c1APUE 14.2 Nonblocking I/O\uff0c\u5176\u4e2dnonblocking I/O\u7684\u4f8b\u5b50\u975e\u5e38\u597d\uff0c\u5e76\u4e14\uff0c\u90a3\u4e2a\u4f8b\u5b50\u4e5f\u7ed3\u5408\u4e86**polling** \uff0c\u548c\u672c\u95ee\u9898\u6b63\u597d\u76f8\u5bf9\u5e94\uff0c\u672c\u95ee\u9898\u662f\uff1ablocking + polling\u6a21\u5f0f\u3002 Can you please explain what actually happens. A A condition variable allows a thread to be signaled when something of interest to that thread occurs. SUMMARY : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u63cf\u8ff0\u4e86condition variable\u7684semantic\u3002 By itself, a mutex doesn't do this. If you just need mutual exclusion, then condition variables don't do anything for you. However, if you need to know when something happens, then condition variables can help. For example, if you have a queue of items to work on, you'll have a mutex to ensure the queue's internals are consistent when accessed by the various producer and consumer threads. However, when the queue is empty, how will a consumer thread know when something is in there for it to work on? Without something like a condition variable it would need to poll the queue, taking and releasing the mutex on each poll (otherwise a producer thread could never put something on the queue). Using a condition variable lets the consumer find that when the queue is empty it can just wait on the condition variable indicating that the queue has had something put into it. No polling - that thread does nothing until a producer puts something in the queue, then signals the condition that the queue has a new item. SUMMARY : \u5982\u679cconsumer thread\u7684\u5904\u7406\u80fd\u529b\u6bd4producer thread\u7684\u5f3a\uff0c\u90a3\u4e48\u6b64\u65f6consumer thread\u5c31\u65e0\u9700\u518dmutex\u4e0a\u963b\u585e\uff0c\u90a3\u4e48\u6b64\u65f6\u5b83\u5c31\u4f1a\u4e00\u76f4\u8fdb\u884cpoll\u4e86\uff0c\u663e\u7136\u8fd9\u662f\u5bf9\u63d0\u95ee\u8005\u95ee\u9898\u4e2d\u6ca1\u6709\u8003\u8651\u7684\u4e00\u79cd\u60c5\u51b5\u7684\u8865\u51fa\uff0c\u663e\u7136\u5728\u8fd9\u79cd \u60c5\u51b5\u4e0b\uff0ccondition variable\u662f\u660e\u663e\u4f18\u4e8emutex\u7684\uff1b\u603b\u7684\u6765\u8bf4\uff0ccondition variable\u662f\u4e0d\u540c\u4e8emutex\u7684\u4e00\u79cd\u673a\u5236\uff1b comments \uff1a Right. A mutex only allows you to wait until the lock is available; a condition variable allows you to wait until some application-defined condition has changed. \u2013 caf Jan 20 '11 at 5:09 I don't see why you can't drop the condition variable and just use the mutex in some simple cases. Make acquiring the mutex the \"work is available\" condition. Then have the producer thread acquire the mutex and the worker thread try to acquire it. When work is available, the producer unlocks the mutex. Then the OS will wake the worker with the mutex acquired and the worker will run to completion and then sleep (deadlock itself) by trying to re-acquire it's own mutex. Once the producer has more work, it can unlock the mutex. Care may need to be taken to check that the mutex is locked first. \u2013 Eloff Apr 6 '13 at 20:34 A If you are looking for performance, then start reading about \"non blocking / non locking\" thread synchronization algorithms. They are based upon atomic operations , which gcc is kind enough to provide. Lookup gcc atomic operations. Our tests showed we could increment a global value with multiple threads using atomic operation magnitudes faster than locking with a mutex. Here is some sample code that shows how to add items to and from a linked list from multiple threads at the same time without locking. For sleeping and waking threads, signals are much faster than conditions . You use pthread_kill to send the signal, and sigwait to sleep the thread. We tested this too with the same kind of performance benefits. Here is some example code. COMMENTS : Thanks a lot for pointing out the area of non-locking synchronization! I am looking into it now! Nice tip about using signals instead of condition variables. \u2013 Eloff Apr 6 '13 at 18:16 A You're looking for too much overlap in two separate but related things: a mutex and a condition variable. A common implementation approach for a mutex is to use a flag and a queue. The flag indicates whether the mutex is held by anyone (a single-count semaphore would work too), and the queue tracks which threads are in line waiting to acquire the mutex exclusively. A condition variable is then implemented as another queue bolted onto that mutex. Threads that got in line to wait to acquire the mutex can\u2014usually once they have acquired it\u2014volunteer to get out of the front of the line and get into the condition queue instead. At this point, you have two separate set of waiters: Those waiting to acquire the mutex exclusively Those waiting for the condition variable to be signaled When a thread holding the mutex exclusively signals the condition variable, for which we'll assume for now that it's a singular signal (unleashing no more than one waiting thread) and not a broadcast (unleashing all the waiting threads), the first thread in the condition variable queue gets shunted back over into the front (usually) of the mutex queue. Once the thread currently holding the mutex\u2014usually the thread that signaled the condition variable\u2014relinquishes the mutex, the next thread in the mutex queue can acquire it. That next thread in line will have been the one that was at the head of the condition variable queue. There are many complicated details that come into play, but this sketch should give you a feel for the structures and operations in play. mutex and condition variable condition variable\u662f\u4e00\u79cdinter-thread communication\u7684\u65b9\u5f0f\uff0c\u800cmutex\u5e76\u4e0d\u5177\u5907\u6b64\u529f\u80fd\u3002mutex\u4ec5\u4ec5\u662f\u4e00\u79cd\u4e92\u65a5\u3002","title":"Introduction"},{"location":"Multithread/Inter-thread-communication/Condition-variables-VS-mutex/#condition#variables#vs#mutex","text":"","title":"condition variables VS mutex"},{"location":"Multithread/Inter-thread-communication/Condition-variables-VS-mutex/#stackoverflow#advantages#of#using#condition#variables#over#mutex","text":"I was wondering what is the performance benefit of using condition variables over mutex locks in pthreads. What I found is : \"Without condition variables, the programmer would need to have threads continually polling (possibly in a critical section), to check if the condition is met. This can be very resource consuming since the thread would be continuously busy in this activity. A condition variable is a way to achieve the same goal without polling.\" ( https://computing.llnl.gov/tutorials/pthreads ) Summary :\u4f5c\u8005\u5728\u4e0a\u9762\u8fd9\u4e00\u6bb5\u6240\u63cf\u8ff0\u7684\u662f\u4e8b\u5b9e\uff0c\u56e0\u4e3acondition variable\u662f\u4e00\u79cdinter-thread communication\uff0c\u5b83\u5141\u8bb8\u7531programmer\u6765\u51b3\u5b9a\u4f55\u65f6\uff0c\u57fa\u4e8e\u4f55\u79cd\u6761\u4ef6\uff0c\u6765\u901a\u77e5\uff08\u5524\u9192\uff09\u88ab\u963b\u585e\u7684\u7ebf\u7a0b\u3002\u800cmutex\u5219\u5e76\u4e0d\u5177\u5907\u6b64\u529f\u80fd\u3002 But it also seems that mutex calls are blocking (unlike spin-locks). Hence if a thread ( T1 ) fails to get a lock because some other thread ( T2 ) has the lock, T1 is put to sleep by the OS, and is woken up only when T2 releases the lock and the OS gives T1 the lock. The thread T1 does not really poll to get the lock. From this description, it seems that there is no performance benefit of using condition variables. In either case, there is no polling involved. The OS anyway provides the benefit that the condition-variable paradigm can provide. SUMMARY : \u5173\u4e8e\u4e0a\u9762\u8fd9\u6bb5\u4e2d\u7684**blocking**\uff0c\u53c2\u89c1APUE 14.2 Nonblocking I/O\uff0c\u5176\u4e2dnonblocking I/O\u7684\u4f8b\u5b50\u975e\u5e38\u597d\uff0c\u5e76\u4e14\uff0c\u90a3\u4e2a\u4f8b\u5b50\u4e5f\u7ed3\u5408\u4e86**polling** \uff0c\u548c\u672c\u95ee\u9898\u6b63\u597d\u76f8\u5bf9\u5e94\uff0c\u672c\u95ee\u9898\u662f\uff1ablocking + polling\u6a21\u5f0f\u3002 Can you please explain what actually happens.","title":"stackoverflow Advantages of using condition variables over mutex"},{"location":"Multithread/Inter-thread-communication/Condition-variables-VS-mutex/#a","text":"A condition variable allows a thread to be signaled when something of interest to that thread occurs. SUMMARY : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u63cf\u8ff0\u4e86condition variable\u7684semantic\u3002 By itself, a mutex doesn't do this. If you just need mutual exclusion, then condition variables don't do anything for you. However, if you need to know when something happens, then condition variables can help. For example, if you have a queue of items to work on, you'll have a mutex to ensure the queue's internals are consistent when accessed by the various producer and consumer threads. However, when the queue is empty, how will a consumer thread know when something is in there for it to work on? Without something like a condition variable it would need to poll the queue, taking and releasing the mutex on each poll (otherwise a producer thread could never put something on the queue). Using a condition variable lets the consumer find that when the queue is empty it can just wait on the condition variable indicating that the queue has had something put into it. No polling - that thread does nothing until a producer puts something in the queue, then signals the condition that the queue has a new item. SUMMARY : \u5982\u679cconsumer thread\u7684\u5904\u7406\u80fd\u529b\u6bd4producer thread\u7684\u5f3a\uff0c\u90a3\u4e48\u6b64\u65f6consumer thread\u5c31\u65e0\u9700\u518dmutex\u4e0a\u963b\u585e\uff0c\u90a3\u4e48\u6b64\u65f6\u5b83\u5c31\u4f1a\u4e00\u76f4\u8fdb\u884cpoll\u4e86\uff0c\u663e\u7136\u8fd9\u662f\u5bf9\u63d0\u95ee\u8005\u95ee\u9898\u4e2d\u6ca1\u6709\u8003\u8651\u7684\u4e00\u79cd\u60c5\u51b5\u7684\u8865\u51fa\uff0c\u663e\u7136\u5728\u8fd9\u79cd \u60c5\u51b5\u4e0b\uff0ccondition variable\u662f\u660e\u663e\u4f18\u4e8emutex\u7684\uff1b\u603b\u7684\u6765\u8bf4\uff0ccondition variable\u662f\u4e0d\u540c\u4e8emutex\u7684\u4e00\u79cd\u673a\u5236\uff1b comments \uff1a Right. A mutex only allows you to wait until the lock is available; a condition variable allows you to wait until some application-defined condition has changed. \u2013 caf Jan 20 '11 at 5:09 I don't see why you can't drop the condition variable and just use the mutex in some simple cases. Make acquiring the mutex the \"work is available\" condition. Then have the producer thread acquire the mutex and the worker thread try to acquire it. When work is available, the producer unlocks the mutex. Then the OS will wake the worker with the mutex acquired and the worker will run to completion and then sleep (deadlock itself) by trying to re-acquire it's own mutex. Once the producer has more work, it can unlock the mutex. Care may need to be taken to check that the mutex is locked first. \u2013 Eloff Apr 6 '13 at 20:34","title":"A"},{"location":"Multithread/Inter-thread-communication/Condition-variables-VS-mutex/#a_1","text":"If you are looking for performance, then start reading about \"non blocking / non locking\" thread synchronization algorithms. They are based upon atomic operations , which gcc is kind enough to provide. Lookup gcc atomic operations. Our tests showed we could increment a global value with multiple threads using atomic operation magnitudes faster than locking with a mutex. Here is some sample code that shows how to add items to and from a linked list from multiple threads at the same time without locking. For sleeping and waking threads, signals are much faster than conditions . You use pthread_kill to send the signal, and sigwait to sleep the thread. We tested this too with the same kind of performance benefits. Here is some example code. COMMENTS : Thanks a lot for pointing out the area of non-locking synchronization! I am looking into it now! Nice tip about using signals instead of condition variables. \u2013 Eloff Apr 6 '13 at 18:16","title":"A"},{"location":"Multithread/Inter-thread-communication/Condition-variables-VS-mutex/#a_2","text":"You're looking for too much overlap in two separate but related things: a mutex and a condition variable. A common implementation approach for a mutex is to use a flag and a queue. The flag indicates whether the mutex is held by anyone (a single-count semaphore would work too), and the queue tracks which threads are in line waiting to acquire the mutex exclusively. A condition variable is then implemented as another queue bolted onto that mutex. Threads that got in line to wait to acquire the mutex can\u2014usually once they have acquired it\u2014volunteer to get out of the front of the line and get into the condition queue instead. At this point, you have two separate set of waiters: Those waiting to acquire the mutex exclusively Those waiting for the condition variable to be signaled When a thread holding the mutex exclusively signals the condition variable, for which we'll assume for now that it's a singular signal (unleashing no more than one waiting thread) and not a broadcast (unleashing all the waiting threads), the first thread in the condition variable queue gets shunted back over into the front (usually) of the mutex queue. Once the thread currently holding the mutex\u2014usually the thread that signaled the condition variable\u2014relinquishes the mutex, the next thread in the mutex queue can acquire it. That next thread in line will have been the one that was at the head of the condition variable queue. There are many complicated details that come into play, but this sketch should give you a feel for the structures and operations in play.","title":"A"},{"location":"Multithread/Inter-thread-communication/Condition-variables-VS-mutex/#mutex#and#condition#variable","text":"condition variable\u662f\u4e00\u79cdinter-thread communication\u7684\u65b9\u5f0f\uff0c\u800cmutex\u5e76\u4e0d\u5177\u5907\u6b64\u529f\u80fd\u3002mutex\u4ec5\u4ec5\u662f\u4e00\u79cd\u4e92\u65a5\u3002","title":"mutex and condition variable"},{"location":"Multithread/Lifetime/","text":"","title":"Introduction"},{"location":"Multithread/Lifetime/Creation/","text":"Thread Creation APUE 11.4 Thread Creation pthread_create Example #include <stdio.h> // printf #include <stdlib.h> // exit #include <pthread.h> // pthread_create\u3001pthread_t #include <unistd.h> // getpid\u3001pid_t #include <errno.h> /* for definition of errno */ #include <stdarg.h> /* ISO C variable aruments */ #include <stddef.h> /* for offsetof */ #include <string.h> /* for string */ /*\u6253\u5370\u9519\u8bef\u65e5\u5fd7\u8f85\u52a9\u51fd\u6570*/ #define MAXLINE 4096 /* max line length */ void err_exit ( int , const char * , ...) __attribute__ (( noreturn )); static void err_doit ( int , int , const char * , va_list ); pthread_t ntid ; void printids ( const char * s ) { pid_t pid ; pthread_t tid ; pid = getpid (); tid = pthread_self (); printf ( \"%s pid %lu tid %lu (0x%lx) \\n \" , s , ( unsigned long ) pid , ( unsigned long ) tid , ( unsigned long ) tid ); } void * thr_fn ( void * arg ) { printids ( \"new thread: \" ); return (( void * ) 0 ); } int main ( void ) { int err ; err = pthread_create ( & ntid , NULL , thr_fn , NULL ); if ( err != 0 ) err_exit ( err , \"can\u2019t create thread\" ); printids ( \"main thread:\" ); sleep ( 1 ); exit ( 0 ); } /* * Fatal error unrelated to a system call. * Error code passed as explict parameter. * Print a message and terminate. */ void err_exit ( int error , const char * fmt , ...) { va_list ap ; va_start ( ap , fmt ); err_doit ( 1 , error , fmt , ap ); va_end ( ap ); exit ( 1 ); } /* * Print a message and return to caller. * Caller specifies \"errnoflag\". */ static void err_doit ( int errnoflag , int error , const char * fmt , va_list ap ) { char buf [ MAXLINE ]; vsnprintf ( buf , MAXLINE - 1 , fmt , ap ); if ( errnoflag ) snprintf ( buf + strlen ( buf ), MAXLINE - strlen ( buf ) - 1 , \": %s\" , strerror ( error )); strcat ( buf , \" \\n \" ); fflush ( stdout ); /* in case stdout and stderr are the same */ fputs ( buf , stderr ); fflush ( NULL ); /* flushes all stdio output streams */ } // gcc test.cpp -lpthread","title":"Introduction"},{"location":"Multithread/Lifetime/Creation/#thread#creation","text":"","title":"Thread Creation"},{"location":"Multithread/Lifetime/Creation/#apue#114#thread#creation","text":"","title":"APUE 11.4 Thread Creation"},{"location":"Multithread/Lifetime/Creation/#pthread_create","text":"","title":"pthread_create"},{"location":"Multithread/Lifetime/Creation/#example","text":"#include <stdio.h> // printf #include <stdlib.h> // exit #include <pthread.h> // pthread_create\u3001pthread_t #include <unistd.h> // getpid\u3001pid_t #include <errno.h> /* for definition of errno */ #include <stdarg.h> /* ISO C variable aruments */ #include <stddef.h> /* for offsetof */ #include <string.h> /* for string */ /*\u6253\u5370\u9519\u8bef\u65e5\u5fd7\u8f85\u52a9\u51fd\u6570*/ #define MAXLINE 4096 /* max line length */ void err_exit ( int , const char * , ...) __attribute__ (( noreturn )); static void err_doit ( int , int , const char * , va_list ); pthread_t ntid ; void printids ( const char * s ) { pid_t pid ; pthread_t tid ; pid = getpid (); tid = pthread_self (); printf ( \"%s pid %lu tid %lu (0x%lx) \\n \" , s , ( unsigned long ) pid , ( unsigned long ) tid , ( unsigned long ) tid ); } void * thr_fn ( void * arg ) { printids ( \"new thread: \" ); return (( void * ) 0 ); } int main ( void ) { int err ; err = pthread_create ( & ntid , NULL , thr_fn , NULL ); if ( err != 0 ) err_exit ( err , \"can\u2019t create thread\" ); printids ( \"main thread:\" ); sleep ( 1 ); exit ( 0 ); } /* * Fatal error unrelated to a system call. * Error code passed as explict parameter. * Print a message and terminate. */ void err_exit ( int error , const char * fmt , ...) { va_list ap ; va_start ( ap , fmt ); err_doit ( 1 , error , fmt , ap ); va_end ( ap ); exit ( 1 ); } /* * Print a message and return to caller. * Caller specifies \"errnoflag\". */ static void err_doit ( int errnoflag , int error , const char * fmt , va_list ap ) { char buf [ MAXLINE ]; vsnprintf ( buf , MAXLINE - 1 , fmt , ap ); if ( errnoflag ) snprintf ( buf + strlen ( buf ), MAXLINE - strlen ( buf ) - 1 , \": %s\" , strerror ( error )); strcat ( buf , \" \\n \" ); fflush ( stdout ); /* in case stdout and stderr are the same */ fputs ( buf , stderr ); fflush ( NULL ); /* flushes all stdio output streams */ } // gcc test.cpp -lpthread","title":"Example"},{"location":"Multithread/Lifetime/Termination/","text":"Thread Termination APUE 11.5 Thread Termination pthread_exit #include <pthread.h> void pthread_exit ( void * rval_ptr ); The rval_ptr argument is a typeless pointer, similar to the single argument passed to the start routine. This pointer is available to other threads in the process by calling the pthread_join function. pthread_join #include <pthread.h> int pthread_join ( pthread_t thread , void ** rval_ptr ); // Returns: 0 if OK, error number on failure NOTE: rval_ptr is typeless pointer \u7ebf\u7a0b\u6267\u884c\u51fd\u6570\u7684\u539f\u578b\u5982\u4e0b: void * ( * start_rtn )( void * ) pthread_exit \u7684\u539f\u578b\u5982\u4e0b\uff1a void pthread_exit ( void * rval_ptr ); pthread_join \u7684\u539f\u578b\u5982\u4e0b\uff1a int pthread_join ( pthread_t thread , void ** rval_ptr ); \u53ef\u4ee5\u770b\u5230\uff0c rval_ptr \u7684\u7c7b\u578b\u662f void * \uff0c\u5373typeless pointer\uff0c\u8fd9\u5c31\u4f7f\u5f97\u6211\u4eec\u5728\u4f7f\u7528\u5b83\u7684\u65f6\u5019\uff0c\u9700\u8981\u8fdb\u884ccast\uff1b NOTE: Double pointer \u7528\u6237\u901a\u8fc7\u8c03\u7528 pthread_exit \u6765\u544a\u77e5OS \u672cthread\u7684\u8fd4\u56de\u503c\uff0c\u5373\u901a\u8fc7\u5c06\u8fd4\u56de\u503c\u7684\u5730\u5740\u4f20\u5165\u5230 pthread_exit \u51fd\u6570\uff0c\u6240\u4ee5 pthread_exit \u7684\u5165\u53c2\u7684\u7c7b\u578b\u662ftypeless pointer\uff1a void *retval \uff0c\u663e\u7136\u58f0\u660e\u4e3a void * \u662f\u4e3a\u4e86\u5b9e\u73b0generic\uff1b\u663e\u7136\u5728\u5b9e\u73b0\u5c42\uff0c\u80af\u5b9a\u4f1a\u6709\u4e00\u4e2aglobal variable\u6216\u8005thread local variable\u6765\u4fdd\u5b58\u5165\u53c2 retval \u7684\u503c\uff0c\u8fd9\u4e2avariable\u5e94\u8be5\u662fdouble pointer\u7c7b\u578b\u7684\uff1b \u8ba9 retval \u6307\u5411\u8fd4\u56de\u503c\uff1b pthread_join \u8981\u53bb\u53d6\u8fd9\u4e2a\u8fd4\u56de\u503c\uff0c\u5219\u9700\u8981\u4f7f\u7528\u4e00\u4e2apointer\u6307\u5411 \u8ba9\u4e00\u4e2a\u6307\u9488\u6307\u5411\uff0c\u5219\u9700\u8981\u4f20\u5165\u8fd9\u4e2a\u6307\u9488\u7684\u5730\u5740\uff0c\u8ba9\u540e\u5411\u8fd9\u4e2a\u5730\u5740\u4e2d\u5199\u5165\uff1b \u5173\u4e8edouble pointer\uff0c\u53c2\u89c1\u5de5\u7a0bprogramming-language\uff1b NOTE: Promise-future\u6a21\u578b \u4e0a\u8ff0 pthread_exit \u7684\u5165\u53c2 rval_ptr \u4e0e pthread_join \u7684\u5165\u53c2 rval_ptr \u540d\u79f0\u76f8\u540c\uff0c\u5b83\u6240\u8868\u8fbe\u7684\u542b\u4e49\u662f\uff1a\u901a\u8fc7\u8c03\u7528 pthread_join \u6765\u53d6\u5f97return value\uff1b pthread_exit \u548c pthread_join \u8ba9\u6211\u60f3\u8d77\u6765promise-future\u6a21\u578b\uff1a pthread_exit \u8fd4\u56defuture\uff0c pthread_join \u53d6\u5f97future\uff1b \u5173\u4e8ePromise-future\u6a21\u578b\uff0c\u53c2\u89c1\u5de5\u7a0bParallel-computing\u3002 Promise-future\u6a21\u578b\u548c\u540e\u9762\u4f1a\u4ecb\u7ecd\u7684\u201cFork-join model\u201d\u5bc6\u5207\u76f8\u5173\uff1b The calling thread will block until the specified thread calls pthread_exit , returns from its start routine, or is canceled. If the thread simply returned from its start routine, rval_ptr will contain the return code . If the thread was canceled, the memory location specified by rval_ptr is set to PTHREAD_CANCELED . By calling pthread_join , we automatically place the thread with which we\u2019re joining in the detached state (discussed shortly) so that its resources can be recovered. If the thread was already in the detached state , pthread_join can fail, returning EINVAL , although this behavior is implementation-specific. NOTE: Joinable state and detached state pthread_join \u4f1a\u5c06\u4e00\u4e2athread\u4ecejoinalbe state\u8f6c\u6362\u4e3adetached state\u3002 \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u6700\u540e\u4e00\u53e5\u544a\u8bc9\u6211\u4eec\uff0c pthread_join \u7684\u7528\u6cd5\u5e94\u8be5\u5982\u4e0b\uff1a if ( is_joinable ( thread_id )) { pthread_join ( thread_id ); } c++ thread library\u5c31\u662f\u4f7f\u7528\u7684\u8fd9\u79cd\u6a21\u5f0f\uff0c\u6211\u4eec\u5c06\u8fd9\u79cd\u6a21\u5f0f\u6210\u4e3a\uff1aFork-join model\uff0c\u5728\u4e0b\u9762\u7684\u201cFork-join model\u201d\u4e2d\u5bf9\u5b83\u8fdb\u884c\u4e86\u8be6\u7ec6\u8bf4\u660e\u3002 \u5982\u4e0b\u4e09\u79cd\u65b9\u5f0f\u53ef\u4ee5\u4f7f\u4e00\u4e2athread\u5904\u4e8edetached state\uff1a pthread_join pthread_detach create a thread that is already in the detached state by modifying the thread attributes we pass to pthread_create . \u5173\u4e8edetached state\uff0c\u5728\u672c\u7ae0\u6700\u540e\u4e00\u6bb5\u4e5f\u8fdb\u884c\u4e86\u8bf4\u660e\u3002 If we\u2019re not interested in a thread\u2019s return value , we can set rval_ptr to NULL. In this case, calling pthread_join allows us to wait for the specified thread, but does not retrieve the thread\u2019s termination status . Example 11.3 Figure 11.3 shows how to fetch the exit code from a thread that has terminated. #include <stdio.h> // printf #include <stdlib.h> // exit #include <pthread.h> // pthread_create\u3001pthread_t #include <unistd.h> // getpid\u3001pid_t #include <errno.h> /* for definition of errno */ #include <stdarg.h> /* ISO C variable aruments */ #include <stddef.h> /* for offsetof */ #include <string.h> /* for string */ /*\u6253\u5370\u9519\u8bef\u65e5\u5fd7\u8f85\u52a9\u51fd\u6570*/ #define MAXLINE 4096 /* max line length */ void err_exit ( int , const char * , ...) __attribute__ (( noreturn )); static void err_doit ( int , int , const char * , va_list ); void * thr_fn1 ( void * arg ) { printf ( \"thread 1 returning \\n \" ); return (( void * ) 1 ); // \u76f4\u63a5\u901a\u8fc7\u7ebf\u7a0b\u6267\u884c\u51fd\u6570\u8fd4\u56de } void * thr_fn2 ( void * arg ) { printf ( \"thread 2 exiting \\n \" ); pthread_exit (( void * ) 2 ); // \u76f4\u63a5\u901a\u8fc7\u7ebf\u7a0b\u6267\u884c\u51fd\u6570\u8fd4\u56de } int main ( void ) { int err ; pthread_t tid1 , tid2 ; void * tret ; // \u7ebf\u7a0b\u7684\u8fd4\u56de\u503c err = pthread_create ( & tid1 , NULL , thr_fn1 , NULL ); if ( err != 0 ) { err_exit ( err , \"can\u2019t create thread 1\" ); } err = pthread_create ( & tid2 , NULL , thr_fn2 , NULL ); if ( err != 0 ) { err_exit ( err , \"can\u2019t create thread 2\" ); } err = pthread_join ( tid1 , & tret ); if ( err != 0 ) { err_exit ( err , \"can\u2019t join with thread 1\" ); } printf ( \"thread 1 exit code %ld \\n \" , ( long ) tret ); err = pthread_join ( tid2 , & tret ); if ( err != 0 ) { err_exit ( err , \"can\u2019t join with thread 2\" ); } printf ( \"thread 2 exit code %ld \\n \" , ( long ) tret ); exit ( 0 ); } /* * Fatal error unrelated to a system call. * Error code passed as explict parameter. * Print a message and terminate. */ void err_exit ( int error , const char * fmt , ...) { va_list ap ; va_start ( ap , fmt ); err_doit ( 1 , error , fmt , ap ); va_end ( ap ); exit ( 1 ); } /* * Print a message and return to caller. * Caller specifies \"errnoflag\". */ static void err_doit ( int errnoflag , int error , const char * fmt , va_list ap ) { char buf [ MAXLINE ]; vsnprintf ( buf , MAXLINE - 1 , fmt , ap ); if ( errnoflag ) snprintf ( buf + strlen ( buf ), MAXLINE - strlen ( buf ) - 1 , \": %s\" , strerror ( error )); strcat ( buf , \" \\n \" ); fflush ( stdout ); /* in case stdout and stderr are the same */ fputs ( buf , stderr ); fflush ( NULL ); /* flushes all stdio output streams */ } // gcc test.cpp -lpthread NOTE: \u8fd0\u884c\u7ed3\u679c\u5982\u4e0b\uff1a thread 1 returning thread 1 exit code 1 thread 2 exiting thread 2 exit code 2 As we can see, when a thread exits by calling pthread_exit or by simply returning from the start routine, the exit status can be obtained by another thread by calling pthread_join . The typeless pointer passed to pthread_create and pthread_exit can be used to pass more than a single value. The pointer can be used to pass the address of a structure containing more complex information. Be careful that the memory used for the structure is still valid when the caller has completed. If the structure was allocated on the caller \u2019s stack, for example, the memory contents might have changed by the time the structure is used. If a thread allocates a structure on its stack and passes a pointer to this structure to pthread_exit , then the stack might be destroyed and its memory reused for something else by the time the caller of pthread_join tries to use it. NOTE: \u4e0a\u9762\u6240\u63cf\u8ff0\u7684\u95ee\u9898\u5c31\u662fdangling pointer\u95ee\u9898\uff0c\u5173\u4e8edangling pointer\uff0c\u53c2\u89c1 Programming\\Computer-errors\\Memory-access-error\\Dangling-and-wild-pointer Example 11.4: using an automatic variable (allocated on the stack) as the argument to pthread_exit The program in Figure 11.4 shows the problem with using an automatic variable (allocated on the stack) as the argument to pthread_exit . #include <stdio.h> // printf #include <stdlib.h> // exit #include <pthread.h> // pthread_create\u3001pthread_t #include <unistd.h> // getpid\u3001pid_t #include <errno.h> /* for definition of errno */ #include <stdarg.h> /* ISO C variable aruments */ #include <stddef.h> /* for offsetof */ #include <string.h> /* for string */ /*\u6253\u5370\u9519\u8bef\u65e5\u5fd7\u8f85\u52a9\u51fd\u6570*/ #define MAXLINE 4096 /* max line length */ void err_exit ( int , const char * , ...) __attribute__ (( noreturn )); static void err_doit ( int , int , const char * , va_list ); struct foo { int a , b , c , d ; }; void printfoo ( const char * s , const struct foo * fp ) { printf ( \"%s\" , s ); printf ( \" structure at 0x%lx \\n \" , ( unsigned long ) fp ); printf ( \" foo.a = %d \\n \" , fp -> a ); printf ( \" foo.b = %d \\n \" , fp -> b ); printf ( \" foo.c = %d \\n \" , fp -> c ); printf ( \" foo.d = %d \\n \" , fp -> d ); } void * thr_fn1 ( void * arg ) { struct foo foo = { 1 , 2 , 3 , 4 }; printfoo ( \"thread 1: \\n \" , & foo ); pthread_exit (( void * ) & foo ); } void * thr_fn2 ( void * arg ) { printf ( \"thread 2: ID is %lu \\n \" , ( unsigned long ) pthread_self ()); pthread_exit (( void * ) 0 ); } int main ( void ) { int err ; pthread_t tid1 , tid2 ; struct foo * fp ; err = pthread_create ( & tid1 , NULL , thr_fn1 , NULL ); if ( err != 0 ) { err_exit ( err , \"can\u2019t create thread 1\" ); } err = pthread_join ( tid1 , ( void ** ) & fp ); if ( err != 0 ) { err_exit ( err , \"can\u2019t join with thread 1\" ); } sleep ( 1 ); printf ( \"parent starting second thread \\n \" ); err = pthread_create ( & tid2 , NULL , thr_fn2 , NULL ); if ( err != 0 ) { err_exit ( err , \"can\u2019t create thread 2\" ); } sleep ( 1 ); printfoo ( \"parent: \\n \" , fp ); exit ( 0 ); } /* * Fatal error unrelated to a system call. * Error code passed as explict parameter. * Print a message and terminate. */ void err_exit ( int error , const char * fmt , ...) { va_list ap ; va_start ( ap , fmt ); err_doit ( 1 , error , fmt , ap ); va_end ( ap ); exit ( 1 ); } /* * Print a message and return to caller. * Caller specifies \"errnoflag\". */ static void err_doit ( int errnoflag , int error , const char * fmt , va_list ap ) { char buf [ MAXLINE ]; vsnprintf ( buf , MAXLINE - 1 , fmt , ap ); if ( errnoflag ) snprintf ( buf + strlen ( buf ), MAXLINE - strlen ( buf ) - 1 , \": %s\" , strerror ( error )); strcat ( buf , \" \\n \" ); fflush ( stdout ); /* in case stdout and stderr are the same */ fputs ( buf , stderr ); fflush ( NULL ); /* flushes all stdio output streams */ } // gcc test.cpp -lpthread When we run this program on Linux, we get thread 1 : structure at 0x7f19939edf00 foo . a = 1 foo . b = 2 foo . c = 3 foo . d = 4 parent starting second thread thread 2 : ID is 139747827574528 parent : structure at 0x7f19939edf00 foo . a = -1818302720 foo . b = 32537 foo . c = 1 foo . d = 0 On Mac OS X, we get different results: $ . / a . out thread 1 : structure at 0x1000b6f00 foo . a = 1 foo . b = 2 foo . c = 3 foo . d = 4 parent starting second thread thread 2 : ID is 4295716864 parent : structure at 0x1000b6f00 Segmentation fault ( core dumped ) In this case, the memory is no longer valid when the parent tries to access the structure passed to it by the first thread that exited, and the parent is sent the SIGSEGV signal. As we can see, the contents of the structure (allocated on the stack of thread tid1 ) have changed by the time the main thread can access the structure. Note how the stack of the second thread ( tid2 ) has overwritten the first thread\u2019s stack. To solve this problem, we can either use a global structure or allocate the structure using malloc . NOTE: \u5173\u4e8e\u7b2c\u4e8c\u79cd\u65b9\u6cd5\u5373\u201callocate the structure using malloc \u201d\uff0c\u5728\u4e0b\u9762\u7684\u201c\u8865\u5145: POSIX : Detached vs Joinable threads | pthread_join() & pthread_detach() examples \u201d\u7684example\u4e2d\u6f14\u793a\u4e86\u5199\u6cd5\uff1b pthread_cancel One thread can request that another in the same process be canceled by calling the pthread_cancel function. In the default circumstances, pthread_cancel will cause the thread specified by tid to behave as if it had called pthread_exit with an argument of PTHREAD_CANCELED . However, a thread can elect to ignore or otherwise control how it is canceled. We will discuss this in detail in Section 12.7. Note that pthread_cancel doesn\u2019t wait for the thread to terminate; it merely makes the request. thread cleanup handlers A thread can arrange for functions to be called when it exits, similar to the way that the atexit function (Section 7.3) can be used by a process to arrange that functions are to be called when the process exits. The functions are known as thread cleanup handlers . More than one cleanup handler can be established for a thread. The handlers are recorded in a stack, which means that they are executed in the reverse order from that with which they were registered. void pthread_cleanup_push(void (*rtn)(void *), void *arg); void pthread_cleanup_pop(int execute); The pthread_cleanup_push function schedules the cleanup function, rtn , to be called with the single argument, arg , when the thread performs one of the following actions: Makes a call to pthread_exit Responds to a cancellation request Makes a call to pthread_cleanup_pop with a nonzero execute argument If the execute argument is set to zero, the cleanup function is not called. In either case, pthread_cleanup_pop removes the cleanup handler established by the last call to pthread_cleanup_push . A restriction with these functions is that, because they can be implemented as macros, they must be used in matched pairs within the same scope in a thread. The macro definition of pthread_cleanup_push can include a { character, in which case the matching } character is in the pthread_cleanup_pop definition. #include <stdio.h> // printf #include <stdlib.h> // exit #include <pthread.h> // pthread_create\u3001pthread_t #include <unistd.h> // getpid\u3001pid_t #include <errno.h> /* for definition of errno */ #include <stdarg.h> /* ISO C variable aruments */ #include <stddef.h> /* for offsetof */ #include <string.h> /* for string */ /*\u6253\u5370\u9519\u8bef\u65e5\u5fd7\u8f85\u52a9\u51fd\u6570*/ #define MAXLINE 4096 /* max line length */ void err_exit ( int , const char * , ...) __attribute__ (( noreturn )); static void err_doit ( int , int , const char * , va_list ); void cleanup ( void * arg ) { printf ( \"cleanup: %s \\n \" , ( char * ) arg ); } void * thr_fn1 ( void * arg ) { printf ( \"thread 1 start \\n \" ); pthread_cleanup_push ( cleanup , ( void * ) \"thread 1 first handler\" ); pthread_cleanup_push ( cleanup , ( void * ) \"thread 1 second handler\" ); printf ( \"thread 1 push complete \\n \" ); if ( arg ) return (( void * ) 1 ); pthread_cleanup_pop ( 0 ); pthread_cleanup_pop ( 0 ); return (( void * ) 1 ); } void * thr_fn2 ( void * arg ) { printf ( \"thread 2 start \\n \" ); pthread_cleanup_push ( cleanup , ( void * ) \"thread 2 first handler\" ); pthread_cleanup_push ( cleanup , ( void * ) \"thread 2 second handler\" ); printf ( \"thread 2 push complete \\n \" ); if ( arg ) pthread_exit (( void * ) 2 ); pthread_cleanup_pop ( 0 ); pthread_cleanup_pop ( 0 ); pthread_exit (( void * ) 2 ); } int main ( void ) { int err ; pthread_t tid1 , tid2 ; void * tret ; err = pthread_create ( & tid1 , NULL , thr_fn1 , ( void * ) 1 ); if ( err != 0 ) { err_exit ( err , \"can\u2019t create thread 1\" ); } err = pthread_create ( & tid2 , NULL , thr_fn2 , ( void * ) 1 ); if ( err != 0 ) { err_exit ( err , \"can\u2019t create thread 2\" ); } err = pthread_join ( tid1 , & tret ); if ( err != 0 ) { err_exit ( err , \"can\u2019t join with thread 1\" ); } printf ( \"thread 1 exit code %ld \\n \" , ( long ) tret ); err = pthread_join ( tid2 , & tret ); if ( err != 0 ) { err_exit ( err , \"can\u2019t join with thread 2\" ); } printf ( \"thread 2 exit code %ld \\n \" , ( long ) tret ); exit ( 0 ); } /* * Fatal error unrelated to a system call. * Error code passed as explict parameter. * Print a message and terminate. */ void err_exit ( int error , const char * fmt , ...) { va_list ap ; va_start ( ap , fmt ); err_doit ( 1 , error , fmt , ap ); va_end ( ap ); exit ( 1 ); } /* * Print a message and return to caller. * Caller specifies \"errnoflag\". */ static void err_doit ( int errnoflag , int error , const char * fmt , va_list ap ) { char buf [ MAXLINE ]; vsnprintf ( buf , MAXLINE - 1 , fmt , ap ); if ( errnoflag ) snprintf ( buf + strlen ( buf ), MAXLINE - strlen ( buf ) - 1 , \": %s\" , strerror ( error )); strcat ( buf , \" \\n \" ); fflush ( stdout ); /* in case stdout and stderr are the same */ fputs ( buf , stderr ); fflush ( NULL ); /* flushes all stdio output streams */ } // gcc test.c -lpthread Running the program in Figure 11.5 on Linux or Solaris gives us $ . / a . out thread 1 start thread 1 push complete thread 2 start thread 2 push complete cleanup : thread 2 second handler cleanup : thread 2 first handler thread 1 exit code 1 thread 2 exit code 2 From the output, we can see that both threads start properly and exit, but that only the second thread\u2019s cleanup handlers are called. Thus, if the thread terminates by returning from its start routine, its cleanup handlers are not called, although this behavior varies among implementations. Also note that the cleanup handlers are called in the reverse order from which they were installed. If we run the same program on FreeBSD or Mac OS X, we see that the program incurs a segmentation violation and drops core. This happens because on these systems, pthread_cleanup_push is implemented as a macro that stores some context on the stack. When thread 1 returns in between the call to pthread_cleanup_push and the call to pthread_cleanup_pop , the stack is overwritten and these platforms try to use this (now corrupted) context when they invoke the cleanup handlers . In the Single UNIX Specification, returning while in between a matched pair of calls to pthread_cleanup_push and pthread_cleanup_pop results in undefined behavior . The only portable way to return in between these two functions is to call pthread_exit . NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u6ca1\u6709\u7406\u89e3 pthread_detach By default, a thread\u2019s termination status is retained until we call pthread_join for that thread. A thread\u2019s underlying storage can be reclaimed immediately on termination if the thread has been detached . After a thread is detached , we can\u2019t use the pthread_join function to wait for its termination status, because calling pthread_join for a detached thread results in undefined behavior . We can detach a thread by calling pthread_detach . As we will see in the next chapter, we can create a thread that is already in the detached state by modifying the thread attributes we pass to pthread_create . \u8865\u5145:thispointer POSIX : Detached vs Joinable threads | pthread_join() & pthread_detach() examples With every thread some resources are associated like stack and thread local storage etc. When a thread exits ideally these resources should be reclaimed by process automatically. But that doesn\u2019t happens always. It depends on which mode thread is running. A Thread can run in two modes i.e. Joinable Mode Detached Mode Joinable Thread & pthread_join() By default a thread runs in joinable mode . Joinable thread will not release any resource even after the end of thread function, until some other thread calls pthread_join() with its ID. pthread_join() is a blocking call, it will block the calling thread until the other thread ends. Example #include <iostream> #include <stdlib.h> #include <stdio.h> #include <string.h> #include <pthread.h> #include <unistd.h> void * threadFunc ( void * arg ) { std :: cout << \"Thread Function :: Start\" << std :: endl ; // Sleep for 2 seconds sleep ( 2 ); std :: cout << \"Thread Function :: End\" << std :: endl ; // Return value from thread return new int ( 6 ); } int main () { // Thread id pthread_t threadId ; // Create a thread that will funtion threadFunc() int err = pthread_create ( & threadId , NULL , & threadFunc , NULL ); // Check if thread is created sucessfuly if ( err ) { std :: cout << \"Thread creation failed : \" << strerror ( err ); return err ; } else { std :: cout << \"Thread Created with ID : \" << threadId << std :: endl ; } // Do some stuff void * ptr = NULL ; std :: cout << \"Waiting for thread to exit\" << std :: endl ; // Wait for thread to exit err = pthread_join ( threadId , & ptr ); if ( err ) { std :: cout << \"Failed to join Thread : \" << strerror ( err ) << std :: endl ; return err ; } if ( ptr ) { std :: cout << \" value returned by thread : \" << * ( int * ) ptr << std :: endl ; } delete ( int * ) ptr ; return 0 ; } // g++ test.cpp -lpthread Output: Thread Created with ID : 140702080427776 Waiting for thread to exit Thread Function :: Start Thread Function :: End value returned by thread : 6 NOTE: \u4e0a\u8ff0\u4f8b\u5b50\u662f\u5178\u578b\u7684thread function Detached Thread & pthread_detach() A Detached thread automatically releases it allocated resources on exit. No other thread needs to join it. But by default all threads are joinable, so to make a thread detached we need to call pthread_detach() with thread id. Also, as detached thread automatically release the resources on exit, therefore there is no way to determine its return value of detached thread function. Example #include <iostream> #include <stdlib.h> #include <stdio.h> #include <string.h> #include <pthread.h> #include <unistd.h> void * threadFunc ( void * arg ) { std :: cout << \"Thread Function :: Start\" << std :: endl ; std :: cout << \"Thread Function :: End\" << std :: endl ; // Return value from thread return NULL ; } int main () { // Thread id pthread_t threadId ; // Create a thread that will funtion threadFunc() int err = pthread_create ( & threadId , NULL , & threadFunc , NULL ); // Check if thread is created sucessfuly if ( err ) { std :: cout << \"Thread creation failed : \" << strerror ( err ); return err ; } else { std :: cout << \"Thread Created with ID : \" << threadId << std :: endl ; } // Do some stuff err = pthread_detach ( threadId ); if ( err ) { std :: cout << \"Failed to detach Thread : \" << strerror ( err ) << std :: endl ; } // Sleep for 2 seconds because if main function exits, then other threads will // be also be killed. Sleep for 2 seconds, so that detached exits by then sleep ( 2 ); std :: cout << \"Main function ends \" << std :: endl ; return 0 ; } // g++ test.cpp -lpthread \u8865\u5145:stackoverflow Detached vs. Joinable POSIX threads Fork-join model \u5173\u4e8efork-join model\uff0c\u53c2\u89c1\u5de5\u7a0b parallel-computing \u7684 Model\\Fork\u2013join-model.md \u3002 \u5728 What does this thread join code mean? \u5bf9Java\u4e2d\u7684\u5199\u6cd5\u8fdb\u884c\u4e86\u8be6\u7ec6\u5206\u6790\uff1a Thread t1 = new Thread ( new EventThread ( \"e1\" )); t1 . start (); Thread t2 = new Thread ( new EventThread ( \"e2\" )); t2 . start (); while ( true ) { try { t1 . join (); t2 . join (); break ; } catch ( InterruptedException e ) { e . printStackTrace (); } } https://stackoverflow.com/a/15956265 It is important to understand that the t1 and t2 threads have been running in parallel but the main thread that started them needs to wait for them to finish before it can continue. That's a common pattern . Also, t1 and/or t2 could have finished before the main thread calls join() on them. If so then join() will not wait but will return immediately. The loop is there to ensure that both t1 and t2 finish. Ie. if t1 throws the InterruptedException , it will loop back and wait for t2 . An alternative is to wait for both threads in each their Try-Catch, so the loop can be avoided. Also, depending on EventThread , it can make sense to do it this way, as we're running 2 threads, not one. \u2013 Michael Bisbjerg Jun 11 '13 at 17:05","title":"Introduction"},{"location":"Multithread/Lifetime/Termination/#thread#termination","text":"","title":"Thread Termination"},{"location":"Multithread/Lifetime/Termination/#apue#115#thread#termination","text":"","title":"APUE 11.5 Thread Termination"},{"location":"Multithread/Lifetime/Termination/#pthread_exit","text":"#include <pthread.h> void pthread_exit ( void * rval_ptr ); The rval_ptr argument is a typeless pointer, similar to the single argument passed to the start routine. This pointer is available to other threads in the process by calling the pthread_join function.","title":"pthread_exit"},{"location":"Multithread/Lifetime/Termination/#pthread_join","text":"#include <pthread.h> int pthread_join ( pthread_t thread , void ** rval_ptr ); // Returns: 0 if OK, error number on failure NOTE:","title":"pthread_join"},{"location":"Multithread/Lifetime/Termination/#rval_ptr#is#typeless#pointer","text":"\u7ebf\u7a0b\u6267\u884c\u51fd\u6570\u7684\u539f\u578b\u5982\u4e0b: void * ( * start_rtn )( void * ) pthread_exit \u7684\u539f\u578b\u5982\u4e0b\uff1a void pthread_exit ( void * rval_ptr ); pthread_join \u7684\u539f\u578b\u5982\u4e0b\uff1a int pthread_join ( pthread_t thread , void ** rval_ptr ); \u53ef\u4ee5\u770b\u5230\uff0c rval_ptr \u7684\u7c7b\u578b\u662f void * \uff0c\u5373typeless pointer\uff0c\u8fd9\u5c31\u4f7f\u5f97\u6211\u4eec\u5728\u4f7f\u7528\u5b83\u7684\u65f6\u5019\uff0c\u9700\u8981\u8fdb\u884ccast\uff1b NOTE:","title":"rval_ptr is typeless pointer"},{"location":"Multithread/Lifetime/Termination/#double#pointer","text":"\u7528\u6237\u901a\u8fc7\u8c03\u7528 pthread_exit \u6765\u544a\u77e5OS \u672cthread\u7684\u8fd4\u56de\u503c\uff0c\u5373\u901a\u8fc7\u5c06\u8fd4\u56de\u503c\u7684\u5730\u5740\u4f20\u5165\u5230 pthread_exit \u51fd\u6570\uff0c\u6240\u4ee5 pthread_exit \u7684\u5165\u53c2\u7684\u7c7b\u578b\u662ftypeless pointer\uff1a void *retval \uff0c\u663e\u7136\u58f0\u660e\u4e3a void * \u662f\u4e3a\u4e86\u5b9e\u73b0generic\uff1b\u663e\u7136\u5728\u5b9e\u73b0\u5c42\uff0c\u80af\u5b9a\u4f1a\u6709\u4e00\u4e2aglobal variable\u6216\u8005thread local variable\u6765\u4fdd\u5b58\u5165\u53c2 retval \u7684\u503c\uff0c\u8fd9\u4e2avariable\u5e94\u8be5\u662fdouble pointer\u7c7b\u578b\u7684\uff1b \u8ba9 retval \u6307\u5411\u8fd4\u56de\u503c\uff1b pthread_join \u8981\u53bb\u53d6\u8fd9\u4e2a\u8fd4\u56de\u503c\uff0c\u5219\u9700\u8981\u4f7f\u7528\u4e00\u4e2apointer\u6307\u5411 \u8ba9\u4e00\u4e2a\u6307\u9488\u6307\u5411\uff0c\u5219\u9700\u8981\u4f20\u5165\u8fd9\u4e2a\u6307\u9488\u7684\u5730\u5740\uff0c\u8ba9\u540e\u5411\u8fd9\u4e2a\u5730\u5740\u4e2d\u5199\u5165\uff1b \u5173\u4e8edouble pointer\uff0c\u53c2\u89c1\u5de5\u7a0bprogramming-language\uff1b NOTE:","title":"Double pointer"},{"location":"Multithread/Lifetime/Termination/#promise-future","text":"\u4e0a\u8ff0 pthread_exit \u7684\u5165\u53c2 rval_ptr \u4e0e pthread_join \u7684\u5165\u53c2 rval_ptr \u540d\u79f0\u76f8\u540c\uff0c\u5b83\u6240\u8868\u8fbe\u7684\u542b\u4e49\u662f\uff1a\u901a\u8fc7\u8c03\u7528 pthread_join \u6765\u53d6\u5f97return value\uff1b pthread_exit \u548c pthread_join \u8ba9\u6211\u60f3\u8d77\u6765promise-future\u6a21\u578b\uff1a pthread_exit \u8fd4\u56defuture\uff0c pthread_join \u53d6\u5f97future\uff1b \u5173\u4e8ePromise-future\u6a21\u578b\uff0c\u53c2\u89c1\u5de5\u7a0bParallel-computing\u3002 Promise-future\u6a21\u578b\u548c\u540e\u9762\u4f1a\u4ecb\u7ecd\u7684\u201cFork-join model\u201d\u5bc6\u5207\u76f8\u5173\uff1b The calling thread will block until the specified thread calls pthread_exit , returns from its start routine, or is canceled. If the thread simply returned from its start routine, rval_ptr will contain the return code . If the thread was canceled, the memory location specified by rval_ptr is set to PTHREAD_CANCELED . By calling pthread_join , we automatically place the thread with which we\u2019re joining in the detached state (discussed shortly) so that its resources can be recovered. If the thread was already in the detached state , pthread_join can fail, returning EINVAL , although this behavior is implementation-specific. NOTE:","title":"Promise-future\u6a21\u578b"},{"location":"Multithread/Lifetime/Termination/#joinable#state#and#detached#state","text":"pthread_join \u4f1a\u5c06\u4e00\u4e2athread\u4ecejoinalbe state\u8f6c\u6362\u4e3adetached state\u3002 \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u6700\u540e\u4e00\u53e5\u544a\u8bc9\u6211\u4eec\uff0c pthread_join \u7684\u7528\u6cd5\u5e94\u8be5\u5982\u4e0b\uff1a if ( is_joinable ( thread_id )) { pthread_join ( thread_id ); } c++ thread library\u5c31\u662f\u4f7f\u7528\u7684\u8fd9\u79cd\u6a21\u5f0f\uff0c\u6211\u4eec\u5c06\u8fd9\u79cd\u6a21\u5f0f\u6210\u4e3a\uff1aFork-join model\uff0c\u5728\u4e0b\u9762\u7684\u201cFork-join model\u201d\u4e2d\u5bf9\u5b83\u8fdb\u884c\u4e86\u8be6\u7ec6\u8bf4\u660e\u3002 \u5982\u4e0b\u4e09\u79cd\u65b9\u5f0f\u53ef\u4ee5\u4f7f\u4e00\u4e2athread\u5904\u4e8edetached state\uff1a pthread_join pthread_detach create a thread that is already in the detached state by modifying the thread attributes we pass to pthread_create . \u5173\u4e8edetached state\uff0c\u5728\u672c\u7ae0\u6700\u540e\u4e00\u6bb5\u4e5f\u8fdb\u884c\u4e86\u8bf4\u660e\u3002 If we\u2019re not interested in a thread\u2019s return value , we can set rval_ptr to NULL. In this case, calling pthread_join allows us to wait for the specified thread, but does not retrieve the thread\u2019s termination status .","title":"Joinable state and detached state"},{"location":"Multithread/Lifetime/Termination/#example#113","text":"Figure 11.3 shows how to fetch the exit code from a thread that has terminated. #include <stdio.h> // printf #include <stdlib.h> // exit #include <pthread.h> // pthread_create\u3001pthread_t #include <unistd.h> // getpid\u3001pid_t #include <errno.h> /* for definition of errno */ #include <stdarg.h> /* ISO C variable aruments */ #include <stddef.h> /* for offsetof */ #include <string.h> /* for string */ /*\u6253\u5370\u9519\u8bef\u65e5\u5fd7\u8f85\u52a9\u51fd\u6570*/ #define MAXLINE 4096 /* max line length */ void err_exit ( int , const char * , ...) __attribute__ (( noreturn )); static void err_doit ( int , int , const char * , va_list ); void * thr_fn1 ( void * arg ) { printf ( \"thread 1 returning \\n \" ); return (( void * ) 1 ); // \u76f4\u63a5\u901a\u8fc7\u7ebf\u7a0b\u6267\u884c\u51fd\u6570\u8fd4\u56de } void * thr_fn2 ( void * arg ) { printf ( \"thread 2 exiting \\n \" ); pthread_exit (( void * ) 2 ); // \u76f4\u63a5\u901a\u8fc7\u7ebf\u7a0b\u6267\u884c\u51fd\u6570\u8fd4\u56de } int main ( void ) { int err ; pthread_t tid1 , tid2 ; void * tret ; // \u7ebf\u7a0b\u7684\u8fd4\u56de\u503c err = pthread_create ( & tid1 , NULL , thr_fn1 , NULL ); if ( err != 0 ) { err_exit ( err , \"can\u2019t create thread 1\" ); } err = pthread_create ( & tid2 , NULL , thr_fn2 , NULL ); if ( err != 0 ) { err_exit ( err , \"can\u2019t create thread 2\" ); } err = pthread_join ( tid1 , & tret ); if ( err != 0 ) { err_exit ( err , \"can\u2019t join with thread 1\" ); } printf ( \"thread 1 exit code %ld \\n \" , ( long ) tret ); err = pthread_join ( tid2 , & tret ); if ( err != 0 ) { err_exit ( err , \"can\u2019t join with thread 2\" ); } printf ( \"thread 2 exit code %ld \\n \" , ( long ) tret ); exit ( 0 ); } /* * Fatal error unrelated to a system call. * Error code passed as explict parameter. * Print a message and terminate. */ void err_exit ( int error , const char * fmt , ...) { va_list ap ; va_start ( ap , fmt ); err_doit ( 1 , error , fmt , ap ); va_end ( ap ); exit ( 1 ); } /* * Print a message and return to caller. * Caller specifies \"errnoflag\". */ static void err_doit ( int errnoflag , int error , const char * fmt , va_list ap ) { char buf [ MAXLINE ]; vsnprintf ( buf , MAXLINE - 1 , fmt , ap ); if ( errnoflag ) snprintf ( buf + strlen ( buf ), MAXLINE - strlen ( buf ) - 1 , \": %s\" , strerror ( error )); strcat ( buf , \" \\n \" ); fflush ( stdout ); /* in case stdout and stderr are the same */ fputs ( buf , stderr ); fflush ( NULL ); /* flushes all stdio output streams */ } // gcc test.cpp -lpthread NOTE: \u8fd0\u884c\u7ed3\u679c\u5982\u4e0b\uff1a thread 1 returning thread 1 exit code 1 thread 2 exiting thread 2 exit code 2 As we can see, when a thread exits by calling pthread_exit or by simply returning from the start routine, the exit status can be obtained by another thread by calling pthread_join . The typeless pointer passed to pthread_create and pthread_exit can be used to pass more than a single value. The pointer can be used to pass the address of a structure containing more complex information. Be careful that the memory used for the structure is still valid when the caller has completed. If the structure was allocated on the caller \u2019s stack, for example, the memory contents might have changed by the time the structure is used. If a thread allocates a structure on its stack and passes a pointer to this structure to pthread_exit , then the stack might be destroyed and its memory reused for something else by the time the caller of pthread_join tries to use it. NOTE: \u4e0a\u9762\u6240\u63cf\u8ff0\u7684\u95ee\u9898\u5c31\u662fdangling pointer\u95ee\u9898\uff0c\u5173\u4e8edangling pointer\uff0c\u53c2\u89c1 Programming\\Computer-errors\\Memory-access-error\\Dangling-and-wild-pointer","title":"Example 11.3"},{"location":"Multithread/Lifetime/Termination/#example#114#using#an#automatic#variable#allocated#on#the#stack#as#the#argument#to#pthread_exit","text":"The program in Figure 11.4 shows the problem with using an automatic variable (allocated on the stack) as the argument to pthread_exit . #include <stdio.h> // printf #include <stdlib.h> // exit #include <pthread.h> // pthread_create\u3001pthread_t #include <unistd.h> // getpid\u3001pid_t #include <errno.h> /* for definition of errno */ #include <stdarg.h> /* ISO C variable aruments */ #include <stddef.h> /* for offsetof */ #include <string.h> /* for string */ /*\u6253\u5370\u9519\u8bef\u65e5\u5fd7\u8f85\u52a9\u51fd\u6570*/ #define MAXLINE 4096 /* max line length */ void err_exit ( int , const char * , ...) __attribute__ (( noreturn )); static void err_doit ( int , int , const char * , va_list ); struct foo { int a , b , c , d ; }; void printfoo ( const char * s , const struct foo * fp ) { printf ( \"%s\" , s ); printf ( \" structure at 0x%lx \\n \" , ( unsigned long ) fp ); printf ( \" foo.a = %d \\n \" , fp -> a ); printf ( \" foo.b = %d \\n \" , fp -> b ); printf ( \" foo.c = %d \\n \" , fp -> c ); printf ( \" foo.d = %d \\n \" , fp -> d ); } void * thr_fn1 ( void * arg ) { struct foo foo = { 1 , 2 , 3 , 4 }; printfoo ( \"thread 1: \\n \" , & foo ); pthread_exit (( void * ) & foo ); } void * thr_fn2 ( void * arg ) { printf ( \"thread 2: ID is %lu \\n \" , ( unsigned long ) pthread_self ()); pthread_exit (( void * ) 0 ); } int main ( void ) { int err ; pthread_t tid1 , tid2 ; struct foo * fp ; err = pthread_create ( & tid1 , NULL , thr_fn1 , NULL ); if ( err != 0 ) { err_exit ( err , \"can\u2019t create thread 1\" ); } err = pthread_join ( tid1 , ( void ** ) & fp ); if ( err != 0 ) { err_exit ( err , \"can\u2019t join with thread 1\" ); } sleep ( 1 ); printf ( \"parent starting second thread \\n \" ); err = pthread_create ( & tid2 , NULL , thr_fn2 , NULL ); if ( err != 0 ) { err_exit ( err , \"can\u2019t create thread 2\" ); } sleep ( 1 ); printfoo ( \"parent: \\n \" , fp ); exit ( 0 ); } /* * Fatal error unrelated to a system call. * Error code passed as explict parameter. * Print a message and terminate. */ void err_exit ( int error , const char * fmt , ...) { va_list ap ; va_start ( ap , fmt ); err_doit ( 1 , error , fmt , ap ); va_end ( ap ); exit ( 1 ); } /* * Print a message and return to caller. * Caller specifies \"errnoflag\". */ static void err_doit ( int errnoflag , int error , const char * fmt , va_list ap ) { char buf [ MAXLINE ]; vsnprintf ( buf , MAXLINE - 1 , fmt , ap ); if ( errnoflag ) snprintf ( buf + strlen ( buf ), MAXLINE - strlen ( buf ) - 1 , \": %s\" , strerror ( error )); strcat ( buf , \" \\n \" ); fflush ( stdout ); /* in case stdout and stderr are the same */ fputs ( buf , stderr ); fflush ( NULL ); /* flushes all stdio output streams */ } // gcc test.cpp -lpthread When we run this program on Linux, we get thread 1 : structure at 0x7f19939edf00 foo . a = 1 foo . b = 2 foo . c = 3 foo . d = 4 parent starting second thread thread 2 : ID is 139747827574528 parent : structure at 0x7f19939edf00 foo . a = -1818302720 foo . b = 32537 foo . c = 1 foo . d = 0 On Mac OS X, we get different results: $ . / a . out thread 1 : structure at 0x1000b6f00 foo . a = 1 foo . b = 2 foo . c = 3 foo . d = 4 parent starting second thread thread 2 : ID is 4295716864 parent : structure at 0x1000b6f00 Segmentation fault ( core dumped ) In this case, the memory is no longer valid when the parent tries to access the structure passed to it by the first thread that exited, and the parent is sent the SIGSEGV signal. As we can see, the contents of the structure (allocated on the stack of thread tid1 ) have changed by the time the main thread can access the structure. Note how the stack of the second thread ( tid2 ) has overwritten the first thread\u2019s stack. To solve this problem, we can either use a global structure or allocate the structure using malloc . NOTE: \u5173\u4e8e\u7b2c\u4e8c\u79cd\u65b9\u6cd5\u5373\u201callocate the structure using malloc \u201d\uff0c\u5728\u4e0b\u9762\u7684\u201c\u8865\u5145: POSIX : Detached vs Joinable threads | pthread_join() & pthread_detach() examples \u201d\u7684example\u4e2d\u6f14\u793a\u4e86\u5199\u6cd5\uff1b","title":"Example 11.4: using an automatic variable (allocated on the stack) as the argument to pthread_exit"},{"location":"Multithread/Lifetime/Termination/#pthread_cancel","text":"One thread can request that another in the same process be canceled by calling the pthread_cancel function. In the default circumstances, pthread_cancel will cause the thread specified by tid to behave as if it had called pthread_exit with an argument of PTHREAD_CANCELED . However, a thread can elect to ignore or otherwise control how it is canceled. We will discuss this in detail in Section 12.7. Note that pthread_cancel doesn\u2019t wait for the thread to terminate; it merely makes the request.","title":"pthread_cancel"},{"location":"Multithread/Lifetime/Termination/#thread#cleanup#handlers","text":"A thread can arrange for functions to be called when it exits, similar to the way that the atexit function (Section 7.3) can be used by a process to arrange that functions are to be called when the process exits. The functions are known as thread cleanup handlers . More than one cleanup handler can be established for a thread. The handlers are recorded in a stack, which means that they are executed in the reverse order from that with which they were registered. void pthread_cleanup_push(void (*rtn)(void *), void *arg); void pthread_cleanup_pop(int execute); The pthread_cleanup_push function schedules the cleanup function, rtn , to be called with the single argument, arg , when the thread performs one of the following actions: Makes a call to pthread_exit Responds to a cancellation request Makes a call to pthread_cleanup_pop with a nonzero execute argument If the execute argument is set to zero, the cleanup function is not called. In either case, pthread_cleanup_pop removes the cleanup handler established by the last call to pthread_cleanup_push . A restriction with these functions is that, because they can be implemented as macros, they must be used in matched pairs within the same scope in a thread. The macro definition of pthread_cleanup_push can include a { character, in which case the matching } character is in the pthread_cleanup_pop definition. #include <stdio.h> // printf #include <stdlib.h> // exit #include <pthread.h> // pthread_create\u3001pthread_t #include <unistd.h> // getpid\u3001pid_t #include <errno.h> /* for definition of errno */ #include <stdarg.h> /* ISO C variable aruments */ #include <stddef.h> /* for offsetof */ #include <string.h> /* for string */ /*\u6253\u5370\u9519\u8bef\u65e5\u5fd7\u8f85\u52a9\u51fd\u6570*/ #define MAXLINE 4096 /* max line length */ void err_exit ( int , const char * , ...) __attribute__ (( noreturn )); static void err_doit ( int , int , const char * , va_list ); void cleanup ( void * arg ) { printf ( \"cleanup: %s \\n \" , ( char * ) arg ); } void * thr_fn1 ( void * arg ) { printf ( \"thread 1 start \\n \" ); pthread_cleanup_push ( cleanup , ( void * ) \"thread 1 first handler\" ); pthread_cleanup_push ( cleanup , ( void * ) \"thread 1 second handler\" ); printf ( \"thread 1 push complete \\n \" ); if ( arg ) return (( void * ) 1 ); pthread_cleanup_pop ( 0 ); pthread_cleanup_pop ( 0 ); return (( void * ) 1 ); } void * thr_fn2 ( void * arg ) { printf ( \"thread 2 start \\n \" ); pthread_cleanup_push ( cleanup , ( void * ) \"thread 2 first handler\" ); pthread_cleanup_push ( cleanup , ( void * ) \"thread 2 second handler\" ); printf ( \"thread 2 push complete \\n \" ); if ( arg ) pthread_exit (( void * ) 2 ); pthread_cleanup_pop ( 0 ); pthread_cleanup_pop ( 0 ); pthread_exit (( void * ) 2 ); } int main ( void ) { int err ; pthread_t tid1 , tid2 ; void * tret ; err = pthread_create ( & tid1 , NULL , thr_fn1 , ( void * ) 1 ); if ( err != 0 ) { err_exit ( err , \"can\u2019t create thread 1\" ); } err = pthread_create ( & tid2 , NULL , thr_fn2 , ( void * ) 1 ); if ( err != 0 ) { err_exit ( err , \"can\u2019t create thread 2\" ); } err = pthread_join ( tid1 , & tret ); if ( err != 0 ) { err_exit ( err , \"can\u2019t join with thread 1\" ); } printf ( \"thread 1 exit code %ld \\n \" , ( long ) tret ); err = pthread_join ( tid2 , & tret ); if ( err != 0 ) { err_exit ( err , \"can\u2019t join with thread 2\" ); } printf ( \"thread 2 exit code %ld \\n \" , ( long ) tret ); exit ( 0 ); } /* * Fatal error unrelated to a system call. * Error code passed as explict parameter. * Print a message and terminate. */ void err_exit ( int error , const char * fmt , ...) { va_list ap ; va_start ( ap , fmt ); err_doit ( 1 , error , fmt , ap ); va_end ( ap ); exit ( 1 ); } /* * Print a message and return to caller. * Caller specifies \"errnoflag\". */ static void err_doit ( int errnoflag , int error , const char * fmt , va_list ap ) { char buf [ MAXLINE ]; vsnprintf ( buf , MAXLINE - 1 , fmt , ap ); if ( errnoflag ) snprintf ( buf + strlen ( buf ), MAXLINE - strlen ( buf ) - 1 , \": %s\" , strerror ( error )); strcat ( buf , \" \\n \" ); fflush ( stdout ); /* in case stdout and stderr are the same */ fputs ( buf , stderr ); fflush ( NULL ); /* flushes all stdio output streams */ } // gcc test.c -lpthread Running the program in Figure 11.5 on Linux or Solaris gives us $ . / a . out thread 1 start thread 1 push complete thread 2 start thread 2 push complete cleanup : thread 2 second handler cleanup : thread 2 first handler thread 1 exit code 1 thread 2 exit code 2 From the output, we can see that both threads start properly and exit, but that only the second thread\u2019s cleanup handlers are called. Thus, if the thread terminates by returning from its start routine, its cleanup handlers are not called, although this behavior varies among implementations. Also note that the cleanup handlers are called in the reverse order from which they were installed. If we run the same program on FreeBSD or Mac OS X, we see that the program incurs a segmentation violation and drops core. This happens because on these systems, pthread_cleanup_push is implemented as a macro that stores some context on the stack. When thread 1 returns in between the call to pthread_cleanup_push and the call to pthread_cleanup_pop , the stack is overwritten and these platforms try to use this (now corrupted) context when they invoke the cleanup handlers . In the Single UNIX Specification, returning while in between a matched pair of calls to pthread_cleanup_push and pthread_cleanup_pop results in undefined behavior . The only portable way to return in between these two functions is to call pthread_exit . NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u6ca1\u6709\u7406\u89e3","title":"thread cleanup handlers"},{"location":"Multithread/Lifetime/Termination/#pthread_detach","text":"By default, a thread\u2019s termination status is retained until we call pthread_join for that thread. A thread\u2019s underlying storage can be reclaimed immediately on termination if the thread has been detached . After a thread is detached , we can\u2019t use the pthread_join function to wait for its termination status, because calling pthread_join for a detached thread results in undefined behavior . We can detach a thread by calling pthread_detach . As we will see in the next chapter, we can create a thread that is already in the detached state by modifying the thread attributes we pass to pthread_create .","title":"pthread_detach"},{"location":"Multithread/Lifetime/Termination/#thispointer#posix#detached#vs#joinable#threads#pthread_join#pthread_detach#examples","text":"With every thread some resources are associated like stack and thread local storage etc. When a thread exits ideally these resources should be reclaimed by process automatically. But that doesn\u2019t happens always. It depends on which mode thread is running. A Thread can run in two modes i.e. Joinable Mode Detached Mode","title":"\u8865\u5145:thispointer POSIX : Detached vs Joinable threads | pthread_join() &amp; pthread_detach() examples"},{"location":"Multithread/Lifetime/Termination/#joinable#thread#pthread_join","text":"By default a thread runs in joinable mode . Joinable thread will not release any resource even after the end of thread function, until some other thread calls pthread_join() with its ID. pthread_join() is a blocking call, it will block the calling thread until the other thread ends.","title":"Joinable Thread &amp; pthread_join()"},{"location":"Multithread/Lifetime/Termination/#example","text":"#include <iostream> #include <stdlib.h> #include <stdio.h> #include <string.h> #include <pthread.h> #include <unistd.h> void * threadFunc ( void * arg ) { std :: cout << \"Thread Function :: Start\" << std :: endl ; // Sleep for 2 seconds sleep ( 2 ); std :: cout << \"Thread Function :: End\" << std :: endl ; // Return value from thread return new int ( 6 ); } int main () { // Thread id pthread_t threadId ; // Create a thread that will funtion threadFunc() int err = pthread_create ( & threadId , NULL , & threadFunc , NULL ); // Check if thread is created sucessfuly if ( err ) { std :: cout << \"Thread creation failed : \" << strerror ( err ); return err ; } else { std :: cout << \"Thread Created with ID : \" << threadId << std :: endl ; } // Do some stuff void * ptr = NULL ; std :: cout << \"Waiting for thread to exit\" << std :: endl ; // Wait for thread to exit err = pthread_join ( threadId , & ptr ); if ( err ) { std :: cout << \"Failed to join Thread : \" << strerror ( err ) << std :: endl ; return err ; } if ( ptr ) { std :: cout << \" value returned by thread : \" << * ( int * ) ptr << std :: endl ; } delete ( int * ) ptr ; return 0 ; } // g++ test.cpp -lpthread Output: Thread Created with ID : 140702080427776 Waiting for thread to exit Thread Function :: Start Thread Function :: End value returned by thread : 6 NOTE: \u4e0a\u8ff0\u4f8b\u5b50\u662f\u5178\u578b\u7684thread function","title":"Example"},{"location":"Multithread/Lifetime/Termination/#detached#thread#pthread_detach","text":"A Detached thread automatically releases it allocated resources on exit. No other thread needs to join it. But by default all threads are joinable, so to make a thread detached we need to call pthread_detach() with thread id. Also, as detached thread automatically release the resources on exit, therefore there is no way to determine its return value of detached thread function.","title":"Detached Thread &amp; pthread_detach()"},{"location":"Multithread/Lifetime/Termination/#example_1","text":"#include <iostream> #include <stdlib.h> #include <stdio.h> #include <string.h> #include <pthread.h> #include <unistd.h> void * threadFunc ( void * arg ) { std :: cout << \"Thread Function :: Start\" << std :: endl ; std :: cout << \"Thread Function :: End\" << std :: endl ; // Return value from thread return NULL ; } int main () { // Thread id pthread_t threadId ; // Create a thread that will funtion threadFunc() int err = pthread_create ( & threadId , NULL , & threadFunc , NULL ); // Check if thread is created sucessfuly if ( err ) { std :: cout << \"Thread creation failed : \" << strerror ( err ); return err ; } else { std :: cout << \"Thread Created with ID : \" << threadId << std :: endl ; } // Do some stuff err = pthread_detach ( threadId ); if ( err ) { std :: cout << \"Failed to detach Thread : \" << strerror ( err ) << std :: endl ; } // Sleep for 2 seconds because if main function exits, then other threads will // be also be killed. Sleep for 2 seconds, so that detached exits by then sleep ( 2 ); std :: cout << \"Main function ends \" << std :: endl ; return 0 ; } // g++ test.cpp -lpthread","title":"Example"},{"location":"Multithread/Lifetime/Termination/#stackoverflow#detached#vs#joinable#posix#threads","text":"","title":"\u8865\u5145:stackoverflow Detached vs. Joinable POSIX threads"},{"location":"Multithread/Lifetime/Termination/#fork-join#model","text":"\u5173\u4e8efork-join model\uff0c\u53c2\u89c1\u5de5\u7a0b parallel-computing \u7684 Model\\Fork\u2013join-model.md \u3002 \u5728 What does this thread join code mean? \u5bf9Java\u4e2d\u7684\u5199\u6cd5\u8fdb\u884c\u4e86\u8be6\u7ec6\u5206\u6790\uff1a Thread t1 = new Thread ( new EventThread ( \"e1\" )); t1 . start (); Thread t2 = new Thread ( new EventThread ( \"e2\" )); t2 . start (); while ( true ) { try { t1 . join (); t2 . join (); break ; } catch ( InterruptedException e ) { e . printStackTrace (); } } https://stackoverflow.com/a/15956265 It is important to understand that the t1 and t2 threads have been running in parallel but the main thread that started them needs to wait for them to finish before it can continue. That's a common pattern . Also, t1 and/or t2 could have finished before the main thread calls join() on them. If so then join() will not wait but will return immediately. The loop is there to ensure that both t1 and t2 finish. Ie. if t1 throws the InterruptedException , it will loop back and wait for t2 . An alternative is to wait for both threads in each their Try-Catch, so the loop can be avoided. Also, depending on EventThread , it can make sense to do it this way, as we're running 2 threads, not one. \u2013 Michael Bisbjerg Jun 11 '13 at 17:05","title":"Fork-join model"},{"location":"Multithread/Thread-model/","text":"Thread model 1\u3001\u5982\u4f55\u8bbe\u8ba1thread model\uff1f \u5e76\u53d1\u7684\u5355\u4f4d\u3001\u7c92\u5ea6 \u6bd4\u5982\uff0c\u6839\u636e\u6570\u636e\u7ed3\u6784\u6765\u8bbe\u8ba1**\u5e76\u53d1\u5355\u4f4d**: \u884c\u7ea7\u9501\u3001\u8868\u7ea7\u9501\uff1b","title":"Introduction"},{"location":"Multithread/Thread-model/#thread#model","text":"1\u3001\u5982\u4f55\u8bbe\u8ba1thread model\uff1f \u5e76\u53d1\u7684\u5355\u4f4d\u3001\u7c92\u5ea6 \u6bd4\u5982\uff0c\u6839\u636e\u6570\u636e\u7ed3\u6784\u6765\u8bbe\u8ba1**\u5e76\u53d1\u5355\u4f4d**: \u884c\u7ea7\u9501\u3001\u8868\u7ea7\u9501\uff1b","title":"Thread model"},{"location":"Multithread/Thread-model/TODO-Locate-assign-%E7%BA%BF%E7%A8%8B%E5%AE%9A%E4%BD%8D/","text":"\u5b9a\u4f4dlocate\u3001assignment\u3001\u7ebf\u7a0b\u6a21\u578b\u3001\u670d\u52a1\u6a21\u578b \u65e0\u8bba\u662fmultithread\u3001distributed computing\uff0c\u90fd\u4f1a\u6d89\u53ca\u5b9a\u4f4dlocate: 1\u3001multithread: \u8fd9\u4e2arequest\u7531\u54ea\u4e2athread\u6765\u5904\u7406 \u8fd9\u53ef\u4ee5\u770b\u505a\u662f**\u7ebf\u7a0b\u6a21\u578b** 2\u3001distributed computing: \u8fd9\u4e2a\u8bf7\u6c42\u8981\u653e\u5230\u54ea\u4e2anode \u8fd9\u53ef\u4ee5\u770b\u505a\u662f**\u670d\u52a1\u6a21\u5f0f** \u4e0b\u9762\u662f\u5404\u79cdexample\u3002 Example 1\u3001jemalloc\u7684round-robin\u7ebf\u7a0b\u5b9a\u4f4d \u907f\u514d\u4e86\"cache sloshing-\u6643\u52a8\"\u3001\u63d0\u5347\u4e86cache locality\u3001Reduce lock contention\u63d0\u9ad8\u4e86concurrency\u3002 2\u3001\u4e4b\u524d\u5f00\u53d1\u8fc7\u7684\u4e00\u4e2a\u7cfb\u7edf\uff0c\u6839\u636e ***Index \u3001 LoginToken \u6765\u8fdb\u884c\u7ebf\u7a0b\u5b9a\u4f4d \u907f\u514d\u4e86\"cache sloshing-\u6643\u52a8\"\u3001\u63d0\u5347\u4e86cache locality\u3001\u3001Reduce lock contention\u63d0\u9ad8\u4e86concurrency\u3002 3\u3001Redis cluster\u7684\u505a\u6cd5 4\u3001consistent hash \u548c\u6b64\u4e5f\u6709\u4e00\u5b9a\u7684\u5173\u8054 5\u3001Redis\u7684\u7ebf\u7a0b\u6a21\u578b \u7ebf\u7a0b\u5b9a\u4f4d VS thread pool without \u7ebf\u7a0b\u5b9a\u4f4d \u4e00\u3001\u7ebf\u7a0b\u5b9a\u4f4d\u80fd\u591f\u8fdb\u884ccache optimization: 1\u3001\u907f\u514d\u4e86\"cache sloshing-\u6643\u52a8\" 2\u3001\u63d0\u5347\u4e86cache locality \u4e8c\u3001\u7ebf\u7a0b\u5b9a\u4f4d\u80fd\u591f\u5b9e\u73b0concurrency optimization: \u51cf\u5c11\u4e86lock contention \u8d1f\u8f7d\u5747\u8861 \u8d1f\u8f7d\u5747\u8861\u662f\u4e00\u4e2a\u8861\u91cf\u6307\u6807 \u7b97\u6cd5 consistent hash round-robin","title":"Introduction"},{"location":"Multithread/Thread-model/TODO-Locate-assign-%E7%BA%BF%E7%A8%8B%E5%AE%9A%E4%BD%8D/#locateassignment","text":"\u65e0\u8bba\u662fmultithread\u3001distributed computing\uff0c\u90fd\u4f1a\u6d89\u53ca\u5b9a\u4f4dlocate: 1\u3001multithread: \u8fd9\u4e2arequest\u7531\u54ea\u4e2athread\u6765\u5904\u7406 \u8fd9\u53ef\u4ee5\u770b\u505a\u662f**\u7ebf\u7a0b\u6a21\u578b** 2\u3001distributed computing: \u8fd9\u4e2a\u8bf7\u6c42\u8981\u653e\u5230\u54ea\u4e2anode \u8fd9\u53ef\u4ee5\u770b\u505a\u662f**\u670d\u52a1\u6a21\u5f0f** \u4e0b\u9762\u662f\u5404\u79cdexample\u3002","title":"\u5b9a\u4f4dlocate\u3001assignment\u3001\u7ebf\u7a0b\u6a21\u578b\u3001\u670d\u52a1\u6a21\u578b"},{"location":"Multithread/Thread-model/TODO-Locate-assign-%E7%BA%BF%E7%A8%8B%E5%AE%9A%E4%BD%8D/#example","text":"1\u3001jemalloc\u7684round-robin\u7ebf\u7a0b\u5b9a\u4f4d \u907f\u514d\u4e86\"cache sloshing-\u6643\u52a8\"\u3001\u63d0\u5347\u4e86cache locality\u3001Reduce lock contention\u63d0\u9ad8\u4e86concurrency\u3002 2\u3001\u4e4b\u524d\u5f00\u53d1\u8fc7\u7684\u4e00\u4e2a\u7cfb\u7edf\uff0c\u6839\u636e ***Index \u3001 LoginToken \u6765\u8fdb\u884c\u7ebf\u7a0b\u5b9a\u4f4d \u907f\u514d\u4e86\"cache sloshing-\u6643\u52a8\"\u3001\u63d0\u5347\u4e86cache locality\u3001\u3001Reduce lock contention\u63d0\u9ad8\u4e86concurrency\u3002 3\u3001Redis cluster\u7684\u505a\u6cd5 4\u3001consistent hash \u548c\u6b64\u4e5f\u6709\u4e00\u5b9a\u7684\u5173\u8054 5\u3001Redis\u7684\u7ebf\u7a0b\u6a21\u578b","title":"Example"},{"location":"Multithread/Thread-model/TODO-Locate-assign-%E7%BA%BF%E7%A8%8B%E5%AE%9A%E4%BD%8D/#vs#thread#pool#without","text":"\u4e00\u3001\u7ebf\u7a0b\u5b9a\u4f4d\u80fd\u591f\u8fdb\u884ccache optimization: 1\u3001\u907f\u514d\u4e86\"cache sloshing-\u6643\u52a8\" 2\u3001\u63d0\u5347\u4e86cache locality \u4e8c\u3001\u7ebf\u7a0b\u5b9a\u4f4d\u80fd\u591f\u5b9e\u73b0concurrency optimization: \u51cf\u5c11\u4e86lock contention","title":"\u7ebf\u7a0b\u5b9a\u4f4d VS thread pool without \u7ebf\u7a0b\u5b9a\u4f4d"},{"location":"Multithread/Thread-model/TODO-Locate-assign-%E7%BA%BF%E7%A8%8B%E5%AE%9A%E4%BD%8D/#_1","text":"\u8d1f\u8f7d\u5747\u8861\u662f\u4e00\u4e2a\u8861\u91cf\u6307\u6807","title":"\u8d1f\u8f7d\u5747\u8861"},{"location":"Multithread/Thread-model/TODO-Locate-assign-%E7%BA%BF%E7%A8%8B%E5%AE%9A%E4%BD%8D/#_2","text":"consistent hash round-robin","title":"\u7b97\u6cd5"},{"location":"Multithread/Thread-safety/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u8ba8\u8bbathread safety\uff0c\u91cd\u8981\u5185\u5bb9\u5982\u4e0b: 1\u3001\u8ba8\u8bbathread safety\u7684\u542b\u4e49\uff0c\u5728 \"wikipedia-Thread-safety\" \u7ae0\u8282 2\u3001\u8ba8\u8bba\u975e\u7ebf\u7a0b\u5b89\u5168\u7684\u4e00\u4e9b\u573a\u666f\uff0c\u5728 \"What-cause-unsafety\" \u7ae0\u8282 APUE 12.5\u8282\u5bf9\u7ebf\u7a0b\u5b89\u5168\u51fd\u6570\u548c\u5f02\u6b65\u4fe1\u53f7\u5b89\u5168\u51fd\u6570\u8fdb\u884c\u4e86\u975e\u5e38\u597d\u7684\u6bd4\u8f83\u3002\u663e\u7136\u4e24\u79cd\u5b89\u5168\u7684\u5b9a\u4e49\u90fd\u548c\u91cd\u5165\u6709\u5173\uff0c\u90fd\u662f\u6307\u5728\u53d1\u751f\u91cd\u5165\u7684\u65f6\u5019\uff0c\u53c2\u89c1\u5de5\u7a0bLinux-OS\u7684 Programming\\Book-APUE \u7ae0\u8282\u3002","title":"Introduction"},{"location":"Multithread/Thread-safety/#_1","text":"\u672c\u7ae0\u8ba8\u8bbathread safety\uff0c\u91cd\u8981\u5185\u5bb9\u5982\u4e0b: 1\u3001\u8ba8\u8bbathread safety\u7684\u542b\u4e49\uff0c\u5728 \"wikipedia-Thread-safety\" \u7ae0\u8282 2\u3001\u8ba8\u8bba\u975e\u7ebf\u7a0b\u5b89\u5168\u7684\u4e00\u4e9b\u573a\u666f\uff0c\u5728 \"What-cause-unsafety\" \u7ae0\u8282","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Multithread/Thread-safety/#apue","text":"12.5\u8282\u5bf9\u7ebf\u7a0b\u5b89\u5168\u51fd\u6570\u548c\u5f02\u6b65\u4fe1\u53f7\u5b89\u5168\u51fd\u6570\u8fdb\u884c\u4e86\u975e\u5e38\u597d\u7684\u6bd4\u8f83\u3002\u663e\u7136\u4e24\u79cd\u5b89\u5168\u7684\u5b9a\u4e49\u90fd\u548c\u91cd\u5165\u6709\u5173\uff0c\u90fd\u662f\u6307\u5728\u53d1\u751f\u91cd\u5165\u7684\u65f6\u5019\uff0c\u53c2\u89c1\u5de5\u7a0bLinux-OS\u7684 Programming\\Book-APUE \u7ae0\u8282\u3002","title":"APUE"},{"location":"Multithread/Thread-safety/What-cause-unsafety/","text":"\u5173\u4e8e\u672c\u7ae0 What cause unsafety\uff1f \u54ea\u4e9b\u539f\u56e0\u5bfc\u81f4shared data\u7ebf\u7a0b\u4e0d\u5b89\u5168\u5462\uff1f\u672c\u8282\u5bf9\u8fd9\u4e2atopic\u8fdb\u884c\u603b\u7ed3\uff0c\u77e5\u9053\u8fd9\u4e9b\u539f\u56e0\u662f\u5b9e\u73b0thread safety\u7684\u524d\u63d0\uff1b Race condition \u8fd9\u5728 \"Race\" \u7ae0\u8282\u4e2d\u8fdb\u884c\u4e86\u8ba8\u8bba\uff1b Out of order execution(\u4e3b\u8981\u662fmemory ordering) \u8fd9\u5728 \"Out-of-order-execution-and-memory-reordering\" \u7ae0\u8282\u8fdb\u884c\u4e86\u8ba8\u8bba\u3002 How to solve? \u9700\u8981\u5bf9shared data\u8fdb\u884cconcurrency control\uff1b\u663e\u7136\u5404\u79cdconcurrency control\u4e2d\uff0c\u90fd\u9700\u8981\u89e3\u51b3\u524d\u9762\u63cf\u8ff0\u7684cause\uff0c\u5178\u578b\u7684\u4f8b\u5b50\u5c31\u662fC++ atomic library\uff0c\u53c2\u89c1\u4e0b\u9762\"Implementation: C++ atomic library\"\u6bb5\u3002 Implementation: C++ atomic library \u53c2\u89c1\u5de5\u7a0bprogramming language\u7684 C++\\Guide\\Memory-model \u7ae0\u8282\u3002","title":"Introduction"},{"location":"Multithread/Thread-safety/What-cause-unsafety/#_1","text":"","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Multithread/Thread-safety/What-cause-unsafety/#what#cause#unsafety","text":"\u54ea\u4e9b\u539f\u56e0\u5bfc\u81f4shared data\u7ebf\u7a0b\u4e0d\u5b89\u5168\u5462\uff1f\u672c\u8282\u5bf9\u8fd9\u4e2atopic\u8fdb\u884c\u603b\u7ed3\uff0c\u77e5\u9053\u8fd9\u4e9b\u539f\u56e0\u662f\u5b9e\u73b0thread safety\u7684\u524d\u63d0\uff1b","title":"What cause unsafety\uff1f"},{"location":"Multithread/Thread-safety/What-cause-unsafety/#race#condition","text":"\u8fd9\u5728 \"Race\" \u7ae0\u8282\u4e2d\u8fdb\u884c\u4e86\u8ba8\u8bba\uff1b","title":"Race condition"},{"location":"Multithread/Thread-safety/What-cause-unsafety/#out#of#order#executionmemory#ordering","text":"\u8fd9\u5728 \"Out-of-order-execution-and-memory-reordering\" \u7ae0\u8282\u8fdb\u884c\u4e86\u8ba8\u8bba\u3002","title":"Out of order execution(\u4e3b\u8981\u662fmemory ordering)"},{"location":"Multithread/Thread-safety/What-cause-unsafety/#how#to#solve","text":"\u9700\u8981\u5bf9shared data\u8fdb\u884cconcurrency control\uff1b\u663e\u7136\u5404\u79cdconcurrency control\u4e2d\uff0c\u90fd\u9700\u8981\u89e3\u51b3\u524d\u9762\u63cf\u8ff0\u7684cause\uff0c\u5178\u578b\u7684\u4f8b\u5b50\u5c31\u662fC++ atomic library\uff0c\u53c2\u89c1\u4e0b\u9762\"Implementation: C++ atomic library\"\u6bb5\u3002","title":"How to solve?"},{"location":"Multithread/Thread-safety/What-cause-unsafety/#implementation#c#atomic#library","text":"\u53c2\u89c1\u5de5\u7a0bprogramming language\u7684 C++\\Guide\\Memory-model \u7ae0\u8282\u3002","title":"Implementation: C++ atomic library"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Out-of-order-execution%26%26memory-reorder/","text":"Out of order execution and memory reordering \u7531\u4e8eOut of order execution(\u4e3b\u8981\u662fmemory reordering)\u4e5f\u4f1a\u5bf9\u5bfc\u81f4shared data\u7ebf\u7a0b\u4e0d\u5b89\u5168\u3002 Examples \u53c2\u89c1\u5de5\u7a0bhardware\u7684 Memory-ordering \u7ae0\u8282\u3002 Order of write to shared data may be different among different threads \u8fd9\u662f\u5728\u9605\u8bfb\u4e00\u7bc7\u6587\u7ae0\u65f6\uff0c\u5176\u4e2d\u63d0\u51fa\u7684\u4e00\u4e2a\u89c2\u70b9\uff0c\u6211\u89c9\u5f97\u8fd9\u4e2a\u89c2\u70b9\u6bd4\u8f83\u597d\uff0c\u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\uff0c\u63cf\u8ff0\u4e86\u76f8\u5173\u7684\u4f8b\u5b50: 1\u3001\u53c2\u89c1 \"aristeia-C++and-the-Perils-of-Double-Checked-Locking\" \u7ae0\u8282 2\u3001queue.acm You Don\u2019t Know Jack about Shared Variables or Memory Models","title":"Introduction"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Out-of-order-execution%26%26memory-reorder/#out#of#order#execution#and#memory#reordering","text":"\u7531\u4e8eOut of order execution(\u4e3b\u8981\u662fmemory reordering)\u4e5f\u4f1a\u5bf9\u5bfc\u81f4shared data\u7ebf\u7a0b\u4e0d\u5b89\u5168\u3002","title":"Out of order execution and memory reordering"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Out-of-order-execution%26%26memory-reorder/#examples","text":"\u53c2\u89c1\u5de5\u7a0bhardware\u7684 Memory-ordering \u7ae0\u8282\u3002","title":"Examples"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Out-of-order-execution%26%26memory-reorder/#order#of#write#to#shared#data#may#be#different#among#different#threads","text":"\u8fd9\u662f\u5728\u9605\u8bfb\u4e00\u7bc7\u6587\u7ae0\u65f6\uff0c\u5176\u4e2d\u63d0\u51fa\u7684\u4e00\u4e2a\u89c2\u70b9\uff0c\u6211\u89c9\u5f97\u8fd9\u4e2a\u89c2\u70b9\u6bd4\u8f83\u597d\uff0c\u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\uff0c\u63cf\u8ff0\u4e86\u76f8\u5173\u7684\u4f8b\u5b50: 1\u3001\u53c2\u89c1 \"aristeia-C++and-the-Perils-of-Double-Checked-Locking\" \u7ae0\u8282 2\u3001queue.acm You Don\u2019t Know Jack about Shared Variables or Memory Models","title":"Order of write to shared data may be different among different threads"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Out-of-order-execution%26%26memory-reorder/acmqueue-Shared-Variables-or-Memory-Models/","text":"You Don\u2019t Know Jack about Shared Variables or Memory Models Data races are evil. Incrementing A Counter void incr () { x ++ ; } Using a mutex void incr () { mtx . lock (); x ++ ; mtx . unlock (); } In Java, this might look like void incr () { synchronized ( mtx ) { x ++ ; } } or perhaps just synchronized void incr () { x ++ ; } Assembly Those are the cases that are less surprising and easier to explain. The final count can also be too high. Consider a case in which the count is bigger than a machine word. To avoid dealing with binary numbers, assume we have a decimal machine in which each word holds three digits, and the counter x can hold six digits. The compiler translates x++ to something like tmp_hi = x_hi ; tmp_lo = x_lo ; ( tmp_hi , tmp_lo ) ++ ; x_hi = tmp_hi ; x_lo = tmp_lo ; Another Racy Example We\u2019ve only begun to see the problems caused by data races. Here\u2019s an example commonly tried in real code. One thread initializes a piece of data (say, x ) and sets a flag (call it done ) when it finishes. Any thread that later reads x first waits for the done flag, as in figure 1. What could possibly go wrong? This code may work reliably with a \u201cdumb\u201d(\u54d1\u5df4) compiler, but any \u201cclever\u201d optimizing compiler is likely to break it. When the compiler sees the loop, it is likely to observe that done is not modified in the loop (i.e., it is \u201cloop-invariant\u201d). Thus, it gets to assume that done does not change in the loop.","title":"Introduction"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Out-of-order-execution%26%26memory-reorder/acmqueue-Shared-Variables-or-Memory-Models/#you#dont#know#jack#about#shared#variables#or#memory#models","text":"","title":"You Don\u2019t Know Jack about Shared Variables or Memory Models"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Out-of-order-execution%26%26memory-reorder/acmqueue-Shared-Variables-or-Memory-Models/#data#races#are#evil","text":"","title":"Data races are evil."},{"location":"Multithread/Thread-safety/What-cause-unsafety/Out-of-order-execution%26%26memory-reorder/acmqueue-Shared-Variables-or-Memory-Models/#incrementing#a#counter","text":"void incr () { x ++ ; }","title":"Incrementing A Counter"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Out-of-order-execution%26%26memory-reorder/acmqueue-Shared-Variables-or-Memory-Models/#using#a#mutex","text":"void incr () { mtx . lock (); x ++ ; mtx . unlock (); } In Java, this might look like void incr () { synchronized ( mtx ) { x ++ ; } } or perhaps just synchronized void incr () { x ++ ; }","title":"Using a mutex"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Out-of-order-execution%26%26memory-reorder/acmqueue-Shared-Variables-or-Memory-Models/#assembly","text":"Those are the cases that are less surprising and easier to explain. The final count can also be too high. Consider a case in which the count is bigger than a machine word. To avoid dealing with binary numbers, assume we have a decimal machine in which each word holds three digits, and the counter x can hold six digits. The compiler translates x++ to something like tmp_hi = x_hi ; tmp_lo = x_lo ; ( tmp_hi , tmp_lo ) ++ ; x_hi = tmp_hi ; x_lo = tmp_lo ;","title":"Assembly"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Out-of-order-execution%26%26memory-reorder/acmqueue-Shared-Variables-or-Memory-Models/#another#racy#example","text":"We\u2019ve only begun to see the problems caused by data races. Here\u2019s an example commonly tried in real code. One thread initializes a piece of data (say, x ) and sets a flag (call it done ) when it finishes. Any thread that later reads x first waits for the done flag, as in figure 1. What could possibly go wrong? This code may work reliably with a \u201cdumb\u201d(\u54d1\u5df4) compiler, but any \u201cclever\u201d optimizing compiler is likely to break it. When the compiler sees the loop, it is likely to observe that done is not modified in the loop (i.e., it is \u201cloop-invariant\u201d). Thus, it gets to assume that done does not change in the loop.","title":"Another Racy Example"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Race/","text":"Race condition Race condition\u7684\u672c\u8d28 Many-to-one Wikipedia\u4e2d\u5173\u4e8e Race condition \u7684\u4ecb\u7ecd\u662f\u975e\u5e38\u597d\u7684\uff0c\u663e\u7136\u5b83\u7684\u6838\u5fc3\u5728\u4e8erace\uff0c\u5373\u7ade\u4e89\uff0crace\u7684\u672c\u8d28\u5728\u4e8emany-to-one\uff0c\"many share the one and system's substantive behavior is dependent on the sequence or timing of the many operate the one\"\uff1b NOTE: \u521a\u5f00\u59cb\u7684\u65f6\u5019\uff0c\u6211\u662f\u8fd9\u6837\u5b9a\u4e49\u7684\uff1amany share the one and operate the one at the same time\uff1b\u540e\u6765\u60f3\u5230 time of checker to time of use \uff0c\u663e\u7136TOCTTOU\u4e2d\u7684race\u5e76\u4e0d\u662foperate the one at the same time\uff0c\u6240\u4ee5operate the one at the same time\u7684\u8fd9\u4e2a\u9650\u5236\u662f\u592a\u8fc7\u4e8e\u5c40\u9650\u7684\uff1b\u540e\u6765\u53c2\u8003Wikipedia\u4e2d\u7684\u5185\u5bb9\u4fee\u6539\u4e3a\u4e0a\u8ff0\u5f62\u5f0f\uff1b\u9664\u4e86TOCTTOU\uff0c\u8fd8\u6709\u4e00\u79cd\u60c5\u51b5\u5c31\u662f\u5728APUE\u768410.6 Reentrant Functions\u4e2d\u63d0\u53ca\u7684\uff0c\u8fd9\u79cd\u60c5\u51b5\u4e2d\uff0cmany\u4e5f\u6ca1\u6709operate the one at the same time\uff0c\u4f46\u662f\u4f9d\u7136\u53d1\u751f\u4e86race\uff1b Many many\u53ef\u80fd\u662fmany threads\uff0cmany process\uff0c\u4e5f\u53ef\u80fd\u662f different ends of the same network \uff0c\u662f\u4e00\u5207\u53ef\u4ee5operate the one\u7684computing entity\uff1b The one the one\u662fmany \u53ef\u4ee5 operate\u7684target\uff0c\u5b83\u53ef\u80fd\u662f\u4e00\u4e2avariable\uff0cfile\uff0c\u4e5f\u53ef\u80fd\u662f\u4e00\u79cd\u62bd\u8c61\u7684 privilege \uff1b \u9700\u8981\u6ce8\u610f\u7684\u662f\uff1amany\u5bf9the one\u7684operate\u53ef\u80fdat the same time\uff0c\u4e5f\u53ef\u80fd\u524d\u540e\u53d1\u751f\uff1b \u4ece\u6307\u4ee4\u7ea7\u522b\u6765\u5206\u6790race condition 1\u3001\u5355\u6761instruction\u624d\u662fatomic\u7684 2\u3001\u4e00\u822cprogramming language statement\u90fd\u5bf9\u5e94\u591a\u6761instruction\uff0c\u5219\u4e0d\u662fatomic\u7684 3\u3001Preemptive-multitasking 4\u3001\u5728\u6267\u884c\u4e2d\u9014\u88abPreemped\u4e86\uff0c\u5219\u5bfc\u81f4\u4e86\u5404\u79cd\u95ee\u9898 \u51fa\u73b0race condition\u7684\u60c5\u51b5 \u901a\u8fc7\u4e0a\u8ff0\u603b\u7ed3\u53ef\u4ee5\u770b\u51fa\uff0c\u4f1a\u51fa\u73b0race condition\u7684\u60c5\u51b5\u662f\u975e\u5e38\u4e4b\u591a\u7684\uff1b race condition\u53ef\u80fd\u53d1\u751f\u4e8eapplication\u5185\u90e8\uff1a 1\u3001application\u5185\u90e8\u7684\u591a\u4e2acomputing entity operate the one in a uncontrollable sequence or time\uff1b\u5982\u5728application\u5185\u90e8\u4f7f\u7528\u4e86thread pool\uff0cprocess pool\uff1b 2\u3001\u5728APUE \u768410.6 Reentrant Functions\u7ae0\u8282\u4ecb\u7ecd\u7684\uff1b\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0capplication\u5e76\u6ca1\u6709\u4f7f\u7528thread pool\uff0cprocess pool\uff0c\u4f46\u662f\u4f9d\u7136\u5b58\u5728race\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c many\u6240\u6307\u4e3a\u5bf9async-signal unsafe function\uff08\u6216\u8005\u79f0\u4e4b\u4e3a**unreentrant** function\uff0c\u6ce8\u610f\u5b83\u4e0d\u662f\u53ea\u7684signal handler\uff09\u7684 re-enter \uff08\u91cd\u5165\uff09\uff0cthe one\u5219\u6307async-signal unsafe function\u4e2d\u4f7f\u7528\u5230\u7684data structure\uff1b NOTE: \u5728 drdobbs Use Lock Hierarchies to Avoid Deadlock \u4e2d\uff0c\u4e5f\u9610\u8ff0\u4e86\u76f8\u540c\u89c2\u70b9 3\u3001\u53ef\u80fd\u53d1\u751f\u4e8eapplication\u4e0eOS \u4e2d\u7684\u5176\u4ed6application\u4e4b\u95f4\uff0c\u5373OS\u4e2d\u7684\u591a\u4e2a\u4e0d\u540ccomputing entity operate the one in a uncontrollable sequence or time\uff0c\u8fd9\u79cd\u60c5\u51b5\u6700\u6700\u5178\u578b\u7684\u5c31\u662f time of checker to time of use \uff1b 4\u3001\u53ef\u80fd\u53d1\u751f\u4e8enetwork\u7684\u4e0d\u540cend\u4e4b\u95f4\uff0c\u8fd9\u79cd\u60c5\u51b5\u5728Wikipedia\u7684 Race condition \u7684 Networking \u7ae0\u8282\u4e2d\u6709\u4e00\u4e2a\u975e\u5e38\u597d\u7684\u4f8b\u5b50\uff1b Consequence 1\u3001read \u5230\u4e86 \u4e2d\u95f4\u72b6\u6001 2\u3001overwrite \u5178\u578b\u7684\u4f8b\u5b50\u5c31\u662f\uff1a i++ How to avoid it \u4e3a\u907f\u514drace condition\u53ef\u80fd\u9020\u6210\u7684\u4e0d\u826f\u540e\u679c\uff0ccomputer science\u4e2d\u6709\u5404\u79cd\u5404\u6837\u7684\u5e94\u5bf9\u65b9\u6cd5\uff1b programmer\u5e94\u8be5\u6839\u636eavoid race \u53d1\u751f\u7684\u8303\u56f4\u91c7\u53d6\u5408\u9002\u7684\u65b9\u6cd5 to avoid it\uff1b\u73b0\u5728\u770b\u6765\uff0cavoid race condition\u7684\u5404\u79cd\u65b9\u6cd5\u7684\u672c\u8d28\u662f\u907f\u514dmany\u5bf9the one\u7684\u4ee5uncontrollable sequence\u8fdb\u884coperate\uff0c\u800c\u662f\u8ba9many\u5bf9the one\u7684operate\u4ee5controllable sequence\uff1b \u4e92\u65a5 \u6bd4\u5982\u5728 Wikipedia Race condition \u7684 example \u4e2d\uff0c\u901a\u8fc7 mutually exclusive \u6765\u907f\u514d\u8fd9\u79cd\u4f1a\u9020\u6210\u9519\u8bef\u7684\u64cd\u4f5c\u65b9\u5f0f\uff1b Atomic \u591a\u4e2athread\u540c\u65f6\u5bf9\u539f\u5b50\u53d8\u91cf\u8fdb\u884c\u64cd\u4f5c\uff0c\u539f\u5b50\u53d8\u91cf\u80fd\u591f\u4fdd\u8bc1\u539f\u5b50\u6027\uff0c\u5373\u6ca1\u6709\u4e2d\u95f4\u72b6\u6001\uff0c\u4ece\u800c\u4fdd\u8bc1\u4e86\u7ebf\u7a0b\u5b89\u5168\uff1b \u4f7f\u7528\u539f\u5b50\u53d8\u91cf\u8fdb\u884clockless programming\u8fd8\u9700\u8981\u514b\u670d\u7684\u4e00\u4e2a\u95ee\u9898\u662f\uff1amemory reordering\uff0c\u8fd9\u5728\"memory ordering\"\u7ae0\u8282\u4e2d\u4f1a\u8fdb\u884c\u63cf\u8ff0\uff1b","title":"Introduction"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Race/#race#condition","text":"","title":"Race condition"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Race/#race#condition_1","text":"","title":"Race condition\u7684\u672c\u8d28"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Race/#many-to-one","text":"Wikipedia\u4e2d\u5173\u4e8e Race condition \u7684\u4ecb\u7ecd\u662f\u975e\u5e38\u597d\u7684\uff0c\u663e\u7136\u5b83\u7684\u6838\u5fc3\u5728\u4e8erace\uff0c\u5373\u7ade\u4e89\uff0crace\u7684\u672c\u8d28\u5728\u4e8emany-to-one\uff0c\"many share the one and system's substantive behavior is dependent on the sequence or timing of the many operate the one\"\uff1b NOTE: \u521a\u5f00\u59cb\u7684\u65f6\u5019\uff0c\u6211\u662f\u8fd9\u6837\u5b9a\u4e49\u7684\uff1amany share the one and operate the one at the same time\uff1b\u540e\u6765\u60f3\u5230 time of checker to time of use \uff0c\u663e\u7136TOCTTOU\u4e2d\u7684race\u5e76\u4e0d\u662foperate the one at the same time\uff0c\u6240\u4ee5operate the one at the same time\u7684\u8fd9\u4e2a\u9650\u5236\u662f\u592a\u8fc7\u4e8e\u5c40\u9650\u7684\uff1b\u540e\u6765\u53c2\u8003Wikipedia\u4e2d\u7684\u5185\u5bb9\u4fee\u6539\u4e3a\u4e0a\u8ff0\u5f62\u5f0f\uff1b\u9664\u4e86TOCTTOU\uff0c\u8fd8\u6709\u4e00\u79cd\u60c5\u51b5\u5c31\u662f\u5728APUE\u768410.6 Reentrant Functions\u4e2d\u63d0\u53ca\u7684\uff0c\u8fd9\u79cd\u60c5\u51b5\u4e2d\uff0cmany\u4e5f\u6ca1\u6709operate the one at the same time\uff0c\u4f46\u662f\u4f9d\u7136\u53d1\u751f\u4e86race\uff1b","title":"Many-to-one"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Race/#many","text":"many\u53ef\u80fd\u662fmany threads\uff0cmany process\uff0c\u4e5f\u53ef\u80fd\u662f different ends of the same network \uff0c\u662f\u4e00\u5207\u53ef\u4ee5operate the one\u7684computing entity\uff1b","title":"Many"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Race/#the#one","text":"the one\u662fmany \u53ef\u4ee5 operate\u7684target\uff0c\u5b83\u53ef\u80fd\u662f\u4e00\u4e2avariable\uff0cfile\uff0c\u4e5f\u53ef\u80fd\u662f\u4e00\u79cd\u62bd\u8c61\u7684 privilege \uff1b \u9700\u8981\u6ce8\u610f\u7684\u662f\uff1amany\u5bf9the one\u7684operate\u53ef\u80fdat the same time\uff0c\u4e5f\u53ef\u80fd\u524d\u540e\u53d1\u751f\uff1b","title":"The one"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Race/#race#condition_2","text":"1\u3001\u5355\u6761instruction\u624d\u662fatomic\u7684 2\u3001\u4e00\u822cprogramming language statement\u90fd\u5bf9\u5e94\u591a\u6761instruction\uff0c\u5219\u4e0d\u662fatomic\u7684 3\u3001Preemptive-multitasking 4\u3001\u5728\u6267\u884c\u4e2d\u9014\u88abPreemped\u4e86\uff0c\u5219\u5bfc\u81f4\u4e86\u5404\u79cd\u95ee\u9898","title":"\u4ece\u6307\u4ee4\u7ea7\u522b\u6765\u5206\u6790race condition"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Race/#race#condition_3","text":"\u901a\u8fc7\u4e0a\u8ff0\u603b\u7ed3\u53ef\u4ee5\u770b\u51fa\uff0c\u4f1a\u51fa\u73b0race condition\u7684\u60c5\u51b5\u662f\u975e\u5e38\u4e4b\u591a\u7684\uff1b race condition\u53ef\u80fd\u53d1\u751f\u4e8eapplication\u5185\u90e8\uff1a 1\u3001application\u5185\u90e8\u7684\u591a\u4e2acomputing entity operate the one in a uncontrollable sequence or time\uff1b\u5982\u5728application\u5185\u90e8\u4f7f\u7528\u4e86thread pool\uff0cprocess pool\uff1b 2\u3001\u5728APUE \u768410.6 Reentrant Functions\u7ae0\u8282\u4ecb\u7ecd\u7684\uff1b\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0capplication\u5e76\u6ca1\u6709\u4f7f\u7528thread pool\uff0cprocess pool\uff0c\u4f46\u662f\u4f9d\u7136\u5b58\u5728race\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c many\u6240\u6307\u4e3a\u5bf9async-signal unsafe function\uff08\u6216\u8005\u79f0\u4e4b\u4e3a**unreentrant** function\uff0c\u6ce8\u610f\u5b83\u4e0d\u662f\u53ea\u7684signal handler\uff09\u7684 re-enter \uff08\u91cd\u5165\uff09\uff0cthe one\u5219\u6307async-signal unsafe function\u4e2d\u4f7f\u7528\u5230\u7684data structure\uff1b NOTE: \u5728 drdobbs Use Lock Hierarchies to Avoid Deadlock \u4e2d\uff0c\u4e5f\u9610\u8ff0\u4e86\u76f8\u540c\u89c2\u70b9 3\u3001\u53ef\u80fd\u53d1\u751f\u4e8eapplication\u4e0eOS \u4e2d\u7684\u5176\u4ed6application\u4e4b\u95f4\uff0c\u5373OS\u4e2d\u7684\u591a\u4e2a\u4e0d\u540ccomputing entity operate the one in a uncontrollable sequence or time\uff0c\u8fd9\u79cd\u60c5\u51b5\u6700\u6700\u5178\u578b\u7684\u5c31\u662f time of checker to time of use \uff1b 4\u3001\u53ef\u80fd\u53d1\u751f\u4e8enetwork\u7684\u4e0d\u540cend\u4e4b\u95f4\uff0c\u8fd9\u79cd\u60c5\u51b5\u5728Wikipedia\u7684 Race condition \u7684 Networking \u7ae0\u8282\u4e2d\u6709\u4e00\u4e2a\u975e\u5e38\u597d\u7684\u4f8b\u5b50\uff1b","title":"\u51fa\u73b0race condition\u7684\u60c5\u51b5"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Race/#consequence","text":"1\u3001read \u5230\u4e86 \u4e2d\u95f4\u72b6\u6001 2\u3001overwrite \u5178\u578b\u7684\u4f8b\u5b50\u5c31\u662f\uff1a i++","title":"Consequence"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Race/#how#to#avoid#it","text":"\u4e3a\u907f\u514drace condition\u53ef\u80fd\u9020\u6210\u7684\u4e0d\u826f\u540e\u679c\uff0ccomputer science\u4e2d\u6709\u5404\u79cd\u5404\u6837\u7684\u5e94\u5bf9\u65b9\u6cd5\uff1b programmer\u5e94\u8be5\u6839\u636eavoid race \u53d1\u751f\u7684\u8303\u56f4\u91c7\u53d6\u5408\u9002\u7684\u65b9\u6cd5 to avoid it\uff1b\u73b0\u5728\u770b\u6765\uff0cavoid race condition\u7684\u5404\u79cd\u65b9\u6cd5\u7684\u672c\u8d28\u662f\u907f\u514dmany\u5bf9the one\u7684\u4ee5uncontrollable sequence\u8fdb\u884coperate\uff0c\u800c\u662f\u8ba9many\u5bf9the one\u7684operate\u4ee5controllable sequence\uff1b","title":"How to avoid it"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Race/#_1","text":"\u6bd4\u5982\u5728 Wikipedia Race condition \u7684 example \u4e2d\uff0c\u901a\u8fc7 mutually exclusive \u6765\u907f\u514d\u8fd9\u79cd\u4f1a\u9020\u6210\u9519\u8bef\u7684\u64cd\u4f5c\u65b9\u5f0f\uff1b","title":"\u4e92\u65a5"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Race/#atomic","text":"\u591a\u4e2athread\u540c\u65f6\u5bf9\u539f\u5b50\u53d8\u91cf\u8fdb\u884c\u64cd\u4f5c\uff0c\u539f\u5b50\u53d8\u91cf\u80fd\u591f\u4fdd\u8bc1\u539f\u5b50\u6027\uff0c\u5373\u6ca1\u6709\u4e2d\u95f4\u72b6\u6001\uff0c\u4ece\u800c\u4fdd\u8bc1\u4e86\u7ebf\u7a0b\u5b89\u5168\uff1b \u4f7f\u7528\u539f\u5b50\u53d8\u91cf\u8fdb\u884clockless programming\u8fd8\u9700\u8981\u514b\u670d\u7684\u4e00\u4e2a\u95ee\u9898\u662f\uff1amemory reordering\uff0c\u8fd9\u5728\"memory ordering\"\u7ae0\u8282\u4e2d\u4f1a\u8fdb\u884c\u63cf\u8ff0\uff1b","title":"Atomic"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Race/TODO-Increment-operator/","text":"++/ stackoverflow Can num++ be atomic for 'int num'? A","title":"Introduction"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Race/TODO-Increment-operator/#_1","text":"","title":"++/"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Race/TODO-Increment-operator/#stackoverflow#can#num#be#atomic#for#int#num","text":"A","title":"stackoverflow Can num++ be atomic for 'int num'?"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Race/stackoverflow-What-is-a-race-condition/","text":"stackoverflow What is a race condition? When writing multi-threaded applications, one of the most common problems experienced are race conditions . My questions to the community are: What is a race condition? How do you detect them? How do you handle them? Finally, how do you prevent them from occurring? COMMENTS : There is a great chapter in the Secure Programming for Linux HOWTO that describes what they are, and how to avoid them. \u2013 Craig H Aug 29 '08 at 15:59 I'd like to mention that - without specifying the language - most parts of this question cannot be answered properly, because in different languages, the definition, the consequences and the tools to prevent them might differ. \u2013 MikeMB Apr 21 '15 at 17:18 @MikeMB . Agreed, except when analyzing byte code execution, like it is done by Race Catcher (see this thread stackoverflow.com/a/29361427/1363844 ) we can address all those approximately 62 languages that compile to byte code (see en.wikipedia.org/wiki/List_of_JVM_languages ) \u2013 Ben Aug 12 '16 at 5:49 A A race condition occurs when two or more threads can access shared data and they try to change it at the same time. Because the thread scheduling algorithm can swap between threads at any time, you don't know the order in which the threads will attempt to access the shared data. Therefore, the result of the change in data is dependent on the thread scheduling algorithm, i.e. both threads are \"racing\" to access/change the data. Problems often occur when one thread does a \"check-then-act\" (e.g. \"check\" if the value is X, then \"act\" to do something that depends on the value being X) and another thread does something to the value in between the \"check\" and the \"act\". E.g: if ( x == 5 ) // The \"Check\" { y = x * 2 ; // The \"Act\" // If another thread changed x in between \"if (x == 5)\" and \"y = x * 2\" above, // y will not be equal to 10. } The point being, y could be 10, or it could be anything, depending on whether another thread changed x in between the check and act. You have no real way of knowing. In order to prevent race conditions from occurring, you would typically put a lock around the shared data to ensure only one thread can access the data at a time. This would mean something like this: // Obtain lock for x if ( x == 5 ) { y = x * 2 ; // Now, nothing can change x until the lock is released. // Therefore y = 10 } // release lock for x A A \"race condition\" exists when multithreaded (or otherwise parallel) code that would access a shared resource could do so in such a way as to cause unexpected results. Take this example: for ( int i = 0 ; i < 10000000 ; i ++ ) { x = x + 1 ; } If you had 5 threads executing this code at once, the value of x WOULD NOT end up being 50,000,000. It would in fact vary with each run. This is because, in order for each thread to increment the value of x, they have to do the following: (simplified, obviously) Retrieve the value of x Add 1 to this value Store this value to x Any thread can be at any step in this process at any time, and they can step on each other when a shared resource is involved. The state of x can be changed by another thread during the time between x is being read and when it is written back. Let's say a thread retrieves the value of x, but hasn't stored it yet. Another thread can also retrieve the same value of x (because no thread has changed it yet) and then they would both be storing the same value (x+1) back in x! Example: Thread 1: reads x, value is 7 Thread 1: add 1 to x, value is now 8 Thread 2: reads x, value is 7 Thread 1: stores 8 in x Thread 2: adds 1 to x, value is now 8 Thread 2: stores 8 in x Race conditions can be avoided by employing some sort of locking mechanism before the code that accesses the shared resource: for ( int i = 0 ; i < 10000000 ; i ++ ) { //lock x x = x + 1 ; //unlock x } Here, the answer comes out as 50,000,000 every time. For more on locking, search for: mutex, semaphore, critical section, shared resource. #include <pthread.h> #include <stdio.h> int i = 0 ; pthread_t ntid ; void * task ( void * arg ){ for ( int i = 0 ; i < 10000000 ; i ++ ) { x = x + 1 ; } print ( \"%d \\n \" , x ); } int main (){ int err ; for ( int i = 0 ; i < 5 ; i ++ ){ err = pthread_create ( & ntid , NULL , task , NULL ); if ( err != 0 ) print ( \"can\u2019t create thread\" ); exit ( err ); } exit ( 0 ); }","title":"Introduction"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Race/stackoverflow-What-is-a-race-condition/#stackoverflow#what#is#a#race#condition","text":"When writing multi-threaded applications, one of the most common problems experienced are race conditions . My questions to the community are: What is a race condition? How do you detect them? How do you handle them? Finally, how do you prevent them from occurring?","title":"stackoverflow What is a race condition?"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Race/stackoverflow-What-is-a-race-condition/#comments","text":"There is a great chapter in the Secure Programming for Linux HOWTO that describes what they are, and how to avoid them. \u2013 Craig H Aug 29 '08 at 15:59 I'd like to mention that - without specifying the language - most parts of this question cannot be answered properly, because in different languages, the definition, the consequences and the tools to prevent them might differ. \u2013 MikeMB Apr 21 '15 at 17:18 @MikeMB . Agreed, except when analyzing byte code execution, like it is done by Race Catcher (see this thread stackoverflow.com/a/29361427/1363844 ) we can address all those approximately 62 languages that compile to byte code (see en.wikipedia.org/wiki/List_of_JVM_languages ) \u2013 Ben Aug 12 '16 at 5:49","title":"COMMENTS :"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Race/stackoverflow-What-is-a-race-condition/#a","text":"A race condition occurs when two or more threads can access shared data and they try to change it at the same time. Because the thread scheduling algorithm can swap between threads at any time, you don't know the order in which the threads will attempt to access the shared data. Therefore, the result of the change in data is dependent on the thread scheduling algorithm, i.e. both threads are \"racing\" to access/change the data. Problems often occur when one thread does a \"check-then-act\" (e.g. \"check\" if the value is X, then \"act\" to do something that depends on the value being X) and another thread does something to the value in between the \"check\" and the \"act\". E.g: if ( x == 5 ) // The \"Check\" { y = x * 2 ; // The \"Act\" // If another thread changed x in between \"if (x == 5)\" and \"y = x * 2\" above, // y will not be equal to 10. } The point being, y could be 10, or it could be anything, depending on whether another thread changed x in between the check and act. You have no real way of knowing. In order to prevent race conditions from occurring, you would typically put a lock around the shared data to ensure only one thread can access the data at a time. This would mean something like this: // Obtain lock for x if ( x == 5 ) { y = x * 2 ; // Now, nothing can change x until the lock is released. // Therefore y = 10 } // release lock for x","title":"A"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Race/stackoverflow-What-is-a-race-condition/#a_1","text":"A \"race condition\" exists when multithreaded (or otherwise parallel) code that would access a shared resource could do so in such a way as to cause unexpected results. Take this example: for ( int i = 0 ; i < 10000000 ; i ++ ) { x = x + 1 ; } If you had 5 threads executing this code at once, the value of x WOULD NOT end up being 50,000,000. It would in fact vary with each run. This is because, in order for each thread to increment the value of x, they have to do the following: (simplified, obviously) Retrieve the value of x Add 1 to this value Store this value to x Any thread can be at any step in this process at any time, and they can step on each other when a shared resource is involved. The state of x can be changed by another thread during the time between x is being read and when it is written back. Let's say a thread retrieves the value of x, but hasn't stored it yet. Another thread can also retrieve the same value of x (because no thread has changed it yet) and then they would both be storing the same value (x+1) back in x! Example: Thread 1: reads x, value is 7 Thread 1: add 1 to x, value is now 8 Thread 2: reads x, value is 7 Thread 1: stores 8 in x Thread 2: adds 1 to x, value is now 8 Thread 2: stores 8 in x Race conditions can be avoided by employing some sort of locking mechanism before the code that accesses the shared resource: for ( int i = 0 ; i < 10000000 ; i ++ ) { //lock x x = x + 1 ; //unlock x } Here, the answer comes out as 50,000,000 every time. For more on locking, search for: mutex, semaphore, critical section, shared resource. #include <pthread.h> #include <stdio.h> int i = 0 ; pthread_t ntid ; void * task ( void * arg ){ for ( int i = 0 ; i < 10000000 ; i ++ ) { x = x + 1 ; } print ( \"%d \\n \" , x ); } int main (){ int err ; for ( int i = 0 ; i < 5 ; i ++ ){ err = pthread_create ( & ntid , NULL , task , NULL ); if ( err != 0 ) print ( \"can\u2019t create thread\" ); exit ( err ); } exit ( 0 ); }","title":"A"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Race/wikipedia-Race-condition/","text":"wikipedia Race condition A race condition or race hazard is the behavior of an electronics , software , or other system where the system's substantive(\u5b9e\u8d28\u7684) behavior is dependent on the sequence or timing of other uncontrollable events. It becomes a bug when one or more of the possible behaviors is undesirable. The term race condition was already in use by 1954, for example in David A. Huffman 's doctoral thesis(\u535a\u58eb\u8bba\u6587) \"The synthesis of sequential switching circuits\". [ 1] Race conditions can occur especially in logic circuits , multithreaded or distributed software programs. Software Race conditions arise in software when an application depends on the sequence or timing of processes or threads for it to operate properly. As with electronics, there are critical race conditions that result in invalid execution and bugs . Critical race conditions often happen when the processes or threads depend on some shared state . Operations upon shared states are critical sections that must be mutually exclusive . Failure to obey this rule opens up the possibility of corrupting the shared state. The memory model defined in the C11 and C++11 standards uses the term \" data race \" for a race condition caused by potentially concurrent operations on a shared memory location , of which at least one is a write. A C or C++ program containing a data race has undefined behavior .[ 3] [ 4] Race conditions have a reputation of being difficult to reproduce and debug, since the end result is nondeterministic and depends on the relative timing between interfering threads. Problems occurring in production systems can therefore disappear when running in debug mode, when additional logging is added, or when attaching a debugger, often referred to as a \" Heisenbug \". It is therefore better to avoid race conditions by careful software design rather than attempting to fix them afterwards. Example As a simple example, let us assume that two threads want to increment the value of a global integer variable by one. Ideally, the following sequence of operations would take place: Thread 1 Thread 2 Integer value 0 read value \u2190 0 increase value 0 write back \u2192 1 read value \u2190 1 increase value 1 write back \u2192 2 In the case shown above, the final value is 2, as expected. However, if the two threads run simultaneously without locking or synchronization, the outcome of the operation could be wrong. The alternative sequence of operations below demonstrates this scenario: Thread 1 Thread 2 Integer value 0 read value \u2190 0 read value \u2190 0 increase value 0 increase value 0 write back \u2192 1 write back \u2192 1 In this case, the final value is 1 instead of the expected result of 2. This occurs because here the increment operations are not mutually exclusive . Mutually exclusive operations are those that cannot be interrupted while accessing some resource such as a memory location. \u200b Computer security Many software race conditions have associated computer security implications. A race condition allows an attacker with access to a shared resource to cause other actors\uff08\u53c2\u4e0e\u8005\uff09 that utilize that resource to malfunction, resulting in effects including denial of service [ 5] and privilege escalation .[ 6] [ 7] NOTE : \u8bb8\u591a\u8f6f\u4ef6\u7ade\u4e89\u6761\u4ef6\u90fd\u4e0e\u8ba1\u7b97\u673a\u5b89\u5168\u6027\u6709\u5173\u3002 \u7ade\u4e89\u6761\u4ef6\u5141\u8bb8\u653b\u51fb\u8005\u8bbf\u95ee\u5171\u4eab\u8d44\u6e90\uff0c\u5bfc\u81f4\u5176\u4ed6\u4f7f\u7528\u8be5\u8d44\u6e90\u7684\u53c2\u4e0e\u8005\u51fa\u73b0\u6545\u969c\uff0c\u4ece\u800c\u5bfc\u81f4\u5305\u62ec\u62d2\u7edd\u670d\u52a1[5]\u548c\u6743\u9650\u63d0\u5347\u7b49\u5f71\u54cd\u3002 A specific kind of race condition involves checking for a predicate (e.g. for authentication ), then acting on the predicate, while the state can change between the time of check and the time of use . When this kind of bug exists in security-sensitive code, a security vulnerability called a time-of-check-to-time-of-use ( TOCTTOU ) bug is created. Race conditions are also intentionally used to create hardware random number generators and physically unclonable functions .[ 8] [ citation needed ] PUFs can be created by designing circuit topologies with identical paths to a node and relying on manufacturing variations to randomly determine which paths will complete first. By measuring each manufactured circuit's specific set of race condition outcomes, a profile can be collected for each circuit and kept secret in order to later verify a circuit's identity. NOTE : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u6d89\u53ca\u5230circuit\u7684\u77e5\u8bc6\uff0c\u975esoftware\u9886\u57df\u77e5\u8bc6\uff1b File systems Two or more programs may collide in their attempts to modify or access a file system, which can result in data corruption or privilege escalation.[ 6] File locking provides a commonly used solution. A more cumbersome remedy(\u8865\u6551\u63aa\u65bd) involves organizing the system in such a way that one unique process (running a daemon or the like) has exclusive access to the file, and all other processes that need to access the data in that file do so only via interprocess communication with that one process. This requires synchronization at the process level. A different form of race condition exists in file systems where unrelated programs may affect each other by suddenly using up(\u8017\u5c3d) available resources such as disk space, memory space, or processor cycles. Software not carefully designed to anticipate and handle this race situation may then become unpredictable. Such a risk may be overlooked(\u5ffd\u89c6) for a long time in a system that seems very reliable. But eventually enough data may accumulate or enough other software may be added to critically destabilize many parts of a system. An example of this occurred with the near loss of the Mars Rover \"Spirit\" not long after landing. A solution is for software to request and reserve all the resources it will need before beginning a task; if this request fails then the task is postponed\uff08\u63a8\u8fdf\uff09, avoiding the many points where failure could have occurred. Alternatively, each of those points can be equipped with error handling , or the success of the entire task can be verified afterwards, before continuing. A more common approach is to simply verify that enough system resources are available before starting a task; however, this may not be adequate because in complex systems the actions of other running programs can be unpredictable. NOTE : \u6587\u4ef6\u7cfb\u7edf\u4e2d\u5b58\u5728\u4e0d\u540c\u5f62\u5f0f\u7684\u7ade\u4e89\u6761\u4ef6\uff0c\u5176\u4e2d\u4e0d\u76f8\u5173\u7684\u7a0b\u5e8f\u53ef\u80fd\u901a\u8fc7\u7a81\u7136\u8017\u5c3d\u53ef\u7528\u8d44\u6e90\uff08\u4f8b\u5982\u78c1\u76d8\u7a7a\u95f4\uff0c\u5185\u5b58\u7a7a\u95f4\u6216\u5904\u7406\u5668\u5468\u671f\uff09\u800c\u76f8\u4e92\u5f71\u54cd\u3002\u672a\u7ecf\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u4ee5\u9884\u6d4b\u548c\u5904\u7406\u8fd9\u79cd\u7ade\u4e89\u60c5\u51b5\u7684\u8f6f\u4ef6\u53ef\u80fd\u4f1a\u53d8\u5f97\u65e0\u6cd5\u9884\u6d4b\u3002\u5728\u4e00\u4e2a\u770b\u8d77\u6765\u975e\u5e38\u53ef\u9760\u7684\u7cfb\u7edf\u4e2d\uff0c\u8fd9\u79cd\u98ce\u9669\u53ef\u80fd\u4f1a\u957f\u671f\u88ab\u5ffd\u89c6\u3002\u4f46\u6700\u7ec8\u53ef\u80fd\u4f1a\u7d2f\u79ef\u8db3\u591f\u7684\u6570\u636e\uff0c\u6216\u8005\u53ef\u80fd\u4f1a\u6dfb\u52a0\u8db3\u591f\u7684\u5176\u4ed6\u8f6f\u4ef6\u6765\u4e25\u91cd\u7834\u574f\u7cfb\u7edf\u7684\u8bb8\u591a\u90e8\u5206\u7684\u7a33\u5b9a\u6027\u3002\u8fd9\u79cd\u60c5\u51b5\u7684\u4e00\u4e2a\u4f8b\u5b50\u53d1\u751f\u5728\u7740\u9646\u540e\u4e0d\u4e45\uff0c\u706b\u661f\u63a2\u6d4b\u5668\u201c\u7cbe\u795e\u201d\u51e0\u4e4e\u5931\u53bb\u4e86\u3002\u89e3\u51b3\u65b9\u6848\u662f\u8f6f\u4ef6\u5728\u5f00\u59cb\u4efb\u52a1\u4e4b\u524d\u8bf7\u6c42\u5e76\u4fdd\u7559\u6240\u9700\u7684\u6240\u6709\u8d44\u6e90;\u5982\u679c\u6b64\u8bf7\u6c42\u5931\u8d25\uff0c\u5219\u8be5\u4efb\u52a1\u5c06\u88ab\u63a8\u8fdf\uff0c\u4ece\u800c\u907f\u514d\u53ef\u80fd\u53d1\u751f\u6545\u969c\u7684\u8bb8\u591a\u70b9\u3002\u6216\u8005\uff0c\u8fd9\u4e9b\u70b9\u4e2d\u7684\u6bcf\u4e00\u4e2a\u90fd\u53ef\u4ee5\u914d\u5907\u9519\u8bef\u5904\u7406\uff0c\u6216\u8005\u5728\u7ee7\u7eed\u4e4b\u524d\u53ef\u4ee5\u5728\u4e4b\u540e\u9a8c\u8bc1\u6574\u4e2a\u4efb\u52a1\u7684\u6210\u529f\u3002\u66f4\u5e38\u89c1\u7684\u65b9\u6cd5\u662f\u5728\u5f00\u59cb\u4efb\u52a1\u4e4b\u524d\u7b80\u5355\u5730\u9a8c\u8bc1\u6709\u8db3\u591f\u7684\u7cfb\u7edf\u8d44\u6e90\u53ef\u7528;\u4f46\u662f\uff0c\u8fd9\u53ef\u80fd\u4e0d\u591f\uff0c\u56e0\u4e3a\u5728\u590d\u6742\u7cfb\u7edf\u4e2d\uff0c\u5176\u4ed6\u6b63\u5728\u8fd0\u884c\u7684\u7a0b\u5e8f\u7684\u64cd\u4f5c\u53ef\u80fd\u662f\u4e0d\u53ef\u9884\u6d4b\u7684\u3002 Networking In networking, consider a distributed chat network like IRC , where a user who starts a channel automatically acquires channel-operator privileges . If two users on different servers, on different ends of the same network, try to start the same-named channel at the same time, each user's respective server will grant channel-operator privileges to each user, since neither server will yet have received the other server's signal that it has allocated that channel. (This problem has been largely solved by various IRC server implementations.) In this case of a race condition, the concept of the \" shared resource \" covers the state of the network (what channels exist, as well as what users started them and therefore have what privileges), which each server can freely change as long as it signals the other servers on the network about the changes so that they can update their conception of the state of the network. However, the latency across the network makes possible the kind of race condition described. In this case, heading off(\u963b\u6b62) race conditions by imposing a form of control over access to the shared resource\u2014say, appointing one server to control who holds what privileges\u2014would mean turning the distributed network into a centralized one (at least for that one part of the network operation). NOTE: \u8fd9\u6bb5\u5173\u4e8edistributed network\u548ccentralized one\u7684\u9610\u8ff0\u662f\u975e\u5e38\u597d\u7684\uff1b Race conditions can also exist when a computer program is written with non-blocking sockets , in which case the performance of the program can be dependent on the speed of the network link. NOTE : \u5728\u7f51\u7edc\u4e2d\uff0c\u8003\u8651\u50cfIRC\u8fd9\u6837\u7684\u5206\u5e03\u5f0f\u804a\u5929\u7f51\u7edc\uff0c\u5176\u4e2d\u542f\u52a8\u9891\u9053\u7684\u7528\u6237\u81ea\u52a8\u83b7\u53d6\u9891\u9053\u64cd\u4f5c\u5458\u6743\u9650\u3002\u5982\u679c\u4f4d\u4e8e\u540c\u4e00\u7f51\u7edc\u4e0d\u540c\u7aef\u7684\u4e0d\u540c\u670d\u52a1\u5668\u4e0a\u7684\u4e24\u4e2a\u7528\u6237\u5c1d\u8bd5\u540c\u65f6\u542f\u52a8\u540c\u540d\u901a\u9053\uff0c\u5219\u6bcf\u4e2a\u7528\u6237\u7684\u76f8\u5e94\u670d\u52a1\u5668\u5c06\u4e3a\u6bcf\u4e2a\u7528\u6237\u6388\u4e88\u901a\u9053\u64cd\u4f5c\u5458\u6743\u9650\uff0c\u56e0\u4e3a\u4e24\u4e2a\u670d\u52a1\u5668\u90fd\u6ca1\u6709\u6536\u5230\u5176\u4ed6\u670d\u52a1\u5668\u5df2\u5206\u914d\u8be5\u901a\u9053\u7684\u4fe1\u53f7\u3002 \uff08\u8fd9\u4e2a\u95ee\u9898\u5df2\u7ecf\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u901a\u8fc7\u5404\u79cdIRC\u670d\u52a1\u5668\u5b9e\u73b0\u6765\u89e3\u51b3\u3002\uff09 \u5728\u8fd9\u79cd\u7ade\u4e89\u6761\u4ef6\u7684\u60c5\u51b5\u4e0b\uff0c\u201c\u5171\u4eab\u8d44\u6e90\u201d\u7684\u6982\u5ff5\u6db5\u76d6\u4e86\u7f51\u7edc\u7684\u72b6\u6001\uff08\u5b58\u5728\u54ea\u4e9b\u4fe1\u9053\uff0c\u4ee5\u53ca\u7528\u6237\u542f\u52a8\u5b83\u4eec\u56e0\u6b64\u5177\u6709\u54ea\u4e9b\u7279\u6743\uff09\uff0c\u6bcf\u4e2a\u670d\u52a1\u5668\u53ef\u4ee5\u81ea\u7531\u66f4\u6539\uff0c\u53ea\u8981\u5b83\u5411\u7f51\u7edc\u4e0a\u7684\u5176\u4ed6\u670d\u52a1\u5668\u53d1\u51fa\u5173\u4e8e\u53d8\u5316\u7684\u4fe1\u53f7\uff0c\u4ee5\u4fbf\u4ed6\u4eec\u53ef\u4ee5\u66f4\u65b0\u4ed6\u4eec\u5bf9\u7f51\u7edc\u72b6\u6001\u7684\u6982\u5ff5\u3002\u4f46\u662f\uff0c\u7f51\u7edc\u4e0a\u7684\u5ef6\u8fdf\u4f7f\u5f97\u6240\u63cf\u8ff0\u7684\u7ade\u4e89\u6761\u4ef6\u6210\u4e3a\u53ef\u80fd\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u5bf9\u5171\u4eab\u8d44\u6e90\u7684\u8bbf\u95ee\u5b9e\u65bd\u4e00\u79cd\u63a7\u5236\u5f62\u5f0f\u6765\u63a7\u5236\u7ade\u4e89\u6761\u4ef6 - \u4f8b\u5982\uff0c\u6307\u5b9a\u4e00\u4e2a\u670d\u52a1\u5668\u6765\u63a7\u5236\u8c01\u62e5\u6709\u4ec0\u4e48\u7279\u6743 - \u610f\u5473\u7740\u5c06\u5206\u5e03\u5f0f\u7f51\u7edc\u8f6c\u53d8\u4e3a\u96c6\u4e2d\u5f0f\u7f51\u7edc\uff08\u81f3\u5c11\u5bf9\u4e8e\u90a3\u4e2a\u90e8\u5206\uff09\u7f51\u7edc\u8fd0\u8425\uff09\u3002 \u5f53\u8ba1\u7b97\u673a\u7a0b\u5e8f\u4f7f\u7528\u975e\u963b\u585e\u5957\u63a5\u5b57\u7f16\u5199\u65f6\uff0c\u7ade\u4e89\u6761\u4ef6\u4e5f\u53ef\u80fd\u5b58\u5728\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u7a0b\u5e8f\u7684\u6027\u80fd\u53ef\u80fd\u53d6\u51b3\u4e8e\u7f51\u7edc\u94fe\u63a5\u7684\u901f\u5ea6\u3002 Life-critical systems \u751f\u6b7b\u6538\u5173\u7684\u7cfb\u7edf Software flaws in life-critical systems can be disastrous. Race conditions were among the flaws in the Therac-25 radiation therapy machine, which led to the death of at least three patients and injuries to several more.[ 9] TRANSLATION : \u751f\u547d\u6538\u5173\u7cfb\u7edf\u4e2d\u7684\u8f6f\u4ef6\u7f3a\u9677\u53ef\u80fd\u662f\u707e\u96be\u6027\u7684\u3002 Another example is the Energy Management System provided by GE Energy and used by Ohio -based FirstEnergy Corp (among other power facilities). A race condition existed in the alarm subsystem; when three sagging power lines were tripped simultaneously, the condition prevented alerts from being raised to the monitoring technicians, delaying their awareness of the problem. This software flaw eventually led to the North American Blackout of 2003 .[ 10] GE Energy later developed a software patch to correct the previously undiscovered error. Tools Many software tools exist to help detect race conditions in software. They can be largely categorized into two groups: static analysis tools and dynamic analysis tools. Thread Safety Analysis is a static analysis tool for annotation-based intra-procedural static analysis, originally implemented as a branch of gcc, and now reimplemented in Clang , supporting PThreads.[ 13] [ non-primary source needed ] Dynamic analysis tools include: 1\u3001 Intel Inspector , a memory and thread checking and debugging tool to increase the reliability, security, and accuracy of C/C++ and Fortran applications; Intel Advisor , a sampling based, SIMD vectorization optimization and shared memory threading assistance tool for C, C++, C#, and Fortran software developers and architects; 2\u3001ThreadSanitizer, which uses binary ( Valgrind -based) or source, LLVM -based instrumentation, and supports PThreads);[ 14] [ non-primary source needed ] and Helgrind, a Valgrind tool for detecting synchronisation errors in C, C++ and Fortran programs that use the POSIX pthreads threading primitives.[ 15] [ non-primary source needed ] 3\u3001Data Race Detector[ 16] is designed to find data races in the Go Programming language.","title":"Introduction"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Race/wikipedia-Race-condition/#wikipedia#race#condition","text":"A race condition or race hazard is the behavior of an electronics , software , or other system where the system's substantive(\u5b9e\u8d28\u7684) behavior is dependent on the sequence or timing of other uncontrollable events. It becomes a bug when one or more of the possible behaviors is undesirable. The term race condition was already in use by 1954, for example in David A. Huffman 's doctoral thesis(\u535a\u58eb\u8bba\u6587) \"The synthesis of sequential switching circuits\". [ 1] Race conditions can occur especially in logic circuits , multithreaded or distributed software programs.","title":"wikipedia Race condition"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Race/wikipedia-Race-condition/#software","text":"Race conditions arise in software when an application depends on the sequence or timing of processes or threads for it to operate properly. As with electronics, there are critical race conditions that result in invalid execution and bugs . Critical race conditions often happen when the processes or threads depend on some shared state . Operations upon shared states are critical sections that must be mutually exclusive . Failure to obey this rule opens up the possibility of corrupting the shared state. The memory model defined in the C11 and C++11 standards uses the term \" data race \" for a race condition caused by potentially concurrent operations on a shared memory location , of which at least one is a write. A C or C++ program containing a data race has undefined behavior .[ 3] [ 4] Race conditions have a reputation of being difficult to reproduce and debug, since the end result is nondeterministic and depends on the relative timing between interfering threads. Problems occurring in production systems can therefore disappear when running in debug mode, when additional logging is added, or when attaching a debugger, often referred to as a \" Heisenbug \". It is therefore better to avoid race conditions by careful software design rather than attempting to fix them afterwards.","title":"Software"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Race/wikipedia-Race-condition/#example","text":"As a simple example, let us assume that two threads want to increment the value of a global integer variable by one. Ideally, the following sequence of operations would take place: Thread 1 Thread 2 Integer value 0 read value \u2190 0 increase value 0 write back \u2192 1 read value \u2190 1 increase value 1 write back \u2192 2 In the case shown above, the final value is 2, as expected. However, if the two threads run simultaneously without locking or synchronization, the outcome of the operation could be wrong. The alternative sequence of operations below demonstrates this scenario: Thread 1 Thread 2 Integer value 0 read value \u2190 0 read value \u2190 0 increase value 0 increase value 0 write back \u2192 1 write back \u2192 1 In this case, the final value is 1 instead of the expected result of 2. This occurs because here the increment operations are not mutually exclusive . Mutually exclusive operations are those that cannot be interrupted while accessing some resource such as a memory location. \u200b","title":"Example"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Race/wikipedia-Race-condition/#computer#security","text":"Many software race conditions have associated computer security implications. A race condition allows an attacker with access to a shared resource to cause other actors\uff08\u53c2\u4e0e\u8005\uff09 that utilize that resource to malfunction, resulting in effects including denial of service [ 5] and privilege escalation .[ 6] [ 7] NOTE : \u8bb8\u591a\u8f6f\u4ef6\u7ade\u4e89\u6761\u4ef6\u90fd\u4e0e\u8ba1\u7b97\u673a\u5b89\u5168\u6027\u6709\u5173\u3002 \u7ade\u4e89\u6761\u4ef6\u5141\u8bb8\u653b\u51fb\u8005\u8bbf\u95ee\u5171\u4eab\u8d44\u6e90\uff0c\u5bfc\u81f4\u5176\u4ed6\u4f7f\u7528\u8be5\u8d44\u6e90\u7684\u53c2\u4e0e\u8005\u51fa\u73b0\u6545\u969c\uff0c\u4ece\u800c\u5bfc\u81f4\u5305\u62ec\u62d2\u7edd\u670d\u52a1[5]\u548c\u6743\u9650\u63d0\u5347\u7b49\u5f71\u54cd\u3002 A specific kind of race condition involves checking for a predicate (e.g. for authentication ), then acting on the predicate, while the state can change between the time of check and the time of use . When this kind of bug exists in security-sensitive code, a security vulnerability called a time-of-check-to-time-of-use ( TOCTTOU ) bug is created. Race conditions are also intentionally used to create hardware random number generators and physically unclonable functions .[ 8] [ citation needed ] PUFs can be created by designing circuit topologies with identical paths to a node and relying on manufacturing variations to randomly determine which paths will complete first. By measuring each manufactured circuit's specific set of race condition outcomes, a profile can be collected for each circuit and kept secret in order to later verify a circuit's identity. NOTE : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u6d89\u53ca\u5230circuit\u7684\u77e5\u8bc6\uff0c\u975esoftware\u9886\u57df\u77e5\u8bc6\uff1b","title":"Computer security"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Race/wikipedia-Race-condition/#file#systems","text":"Two or more programs may collide in their attempts to modify or access a file system, which can result in data corruption or privilege escalation.[ 6] File locking provides a commonly used solution. A more cumbersome remedy(\u8865\u6551\u63aa\u65bd) involves organizing the system in such a way that one unique process (running a daemon or the like) has exclusive access to the file, and all other processes that need to access the data in that file do so only via interprocess communication with that one process. This requires synchronization at the process level. A different form of race condition exists in file systems where unrelated programs may affect each other by suddenly using up(\u8017\u5c3d) available resources such as disk space, memory space, or processor cycles. Software not carefully designed to anticipate and handle this race situation may then become unpredictable. Such a risk may be overlooked(\u5ffd\u89c6) for a long time in a system that seems very reliable. But eventually enough data may accumulate or enough other software may be added to critically destabilize many parts of a system. An example of this occurred with the near loss of the Mars Rover \"Spirit\" not long after landing. A solution is for software to request and reserve all the resources it will need before beginning a task; if this request fails then the task is postponed\uff08\u63a8\u8fdf\uff09, avoiding the many points where failure could have occurred. Alternatively, each of those points can be equipped with error handling , or the success of the entire task can be verified afterwards, before continuing. A more common approach is to simply verify that enough system resources are available before starting a task; however, this may not be adequate because in complex systems the actions of other running programs can be unpredictable. NOTE : \u6587\u4ef6\u7cfb\u7edf\u4e2d\u5b58\u5728\u4e0d\u540c\u5f62\u5f0f\u7684\u7ade\u4e89\u6761\u4ef6\uff0c\u5176\u4e2d\u4e0d\u76f8\u5173\u7684\u7a0b\u5e8f\u53ef\u80fd\u901a\u8fc7\u7a81\u7136\u8017\u5c3d\u53ef\u7528\u8d44\u6e90\uff08\u4f8b\u5982\u78c1\u76d8\u7a7a\u95f4\uff0c\u5185\u5b58\u7a7a\u95f4\u6216\u5904\u7406\u5668\u5468\u671f\uff09\u800c\u76f8\u4e92\u5f71\u54cd\u3002\u672a\u7ecf\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u4ee5\u9884\u6d4b\u548c\u5904\u7406\u8fd9\u79cd\u7ade\u4e89\u60c5\u51b5\u7684\u8f6f\u4ef6\u53ef\u80fd\u4f1a\u53d8\u5f97\u65e0\u6cd5\u9884\u6d4b\u3002\u5728\u4e00\u4e2a\u770b\u8d77\u6765\u975e\u5e38\u53ef\u9760\u7684\u7cfb\u7edf\u4e2d\uff0c\u8fd9\u79cd\u98ce\u9669\u53ef\u80fd\u4f1a\u957f\u671f\u88ab\u5ffd\u89c6\u3002\u4f46\u6700\u7ec8\u53ef\u80fd\u4f1a\u7d2f\u79ef\u8db3\u591f\u7684\u6570\u636e\uff0c\u6216\u8005\u53ef\u80fd\u4f1a\u6dfb\u52a0\u8db3\u591f\u7684\u5176\u4ed6\u8f6f\u4ef6\u6765\u4e25\u91cd\u7834\u574f\u7cfb\u7edf\u7684\u8bb8\u591a\u90e8\u5206\u7684\u7a33\u5b9a\u6027\u3002\u8fd9\u79cd\u60c5\u51b5\u7684\u4e00\u4e2a\u4f8b\u5b50\u53d1\u751f\u5728\u7740\u9646\u540e\u4e0d\u4e45\uff0c\u706b\u661f\u63a2\u6d4b\u5668\u201c\u7cbe\u795e\u201d\u51e0\u4e4e\u5931\u53bb\u4e86\u3002\u89e3\u51b3\u65b9\u6848\u662f\u8f6f\u4ef6\u5728\u5f00\u59cb\u4efb\u52a1\u4e4b\u524d\u8bf7\u6c42\u5e76\u4fdd\u7559\u6240\u9700\u7684\u6240\u6709\u8d44\u6e90;\u5982\u679c\u6b64\u8bf7\u6c42\u5931\u8d25\uff0c\u5219\u8be5\u4efb\u52a1\u5c06\u88ab\u63a8\u8fdf\uff0c\u4ece\u800c\u907f\u514d\u53ef\u80fd\u53d1\u751f\u6545\u969c\u7684\u8bb8\u591a\u70b9\u3002\u6216\u8005\uff0c\u8fd9\u4e9b\u70b9\u4e2d\u7684\u6bcf\u4e00\u4e2a\u90fd\u53ef\u4ee5\u914d\u5907\u9519\u8bef\u5904\u7406\uff0c\u6216\u8005\u5728\u7ee7\u7eed\u4e4b\u524d\u53ef\u4ee5\u5728\u4e4b\u540e\u9a8c\u8bc1\u6574\u4e2a\u4efb\u52a1\u7684\u6210\u529f\u3002\u66f4\u5e38\u89c1\u7684\u65b9\u6cd5\u662f\u5728\u5f00\u59cb\u4efb\u52a1\u4e4b\u524d\u7b80\u5355\u5730\u9a8c\u8bc1\u6709\u8db3\u591f\u7684\u7cfb\u7edf\u8d44\u6e90\u53ef\u7528;\u4f46\u662f\uff0c\u8fd9\u53ef\u80fd\u4e0d\u591f\uff0c\u56e0\u4e3a\u5728\u590d\u6742\u7cfb\u7edf\u4e2d\uff0c\u5176\u4ed6\u6b63\u5728\u8fd0\u884c\u7684\u7a0b\u5e8f\u7684\u64cd\u4f5c\u53ef\u80fd\u662f\u4e0d\u53ef\u9884\u6d4b\u7684\u3002","title":"File systems"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Race/wikipedia-Race-condition/#networking","text":"In networking, consider a distributed chat network like IRC , where a user who starts a channel automatically acquires channel-operator privileges . If two users on different servers, on different ends of the same network, try to start the same-named channel at the same time, each user's respective server will grant channel-operator privileges to each user, since neither server will yet have received the other server's signal that it has allocated that channel. (This problem has been largely solved by various IRC server implementations.) In this case of a race condition, the concept of the \" shared resource \" covers the state of the network (what channels exist, as well as what users started them and therefore have what privileges), which each server can freely change as long as it signals the other servers on the network about the changes so that they can update their conception of the state of the network. However, the latency across the network makes possible the kind of race condition described. In this case, heading off(\u963b\u6b62) race conditions by imposing a form of control over access to the shared resource\u2014say, appointing one server to control who holds what privileges\u2014would mean turning the distributed network into a centralized one (at least for that one part of the network operation). NOTE: \u8fd9\u6bb5\u5173\u4e8edistributed network\u548ccentralized one\u7684\u9610\u8ff0\u662f\u975e\u5e38\u597d\u7684\uff1b Race conditions can also exist when a computer program is written with non-blocking sockets , in which case the performance of the program can be dependent on the speed of the network link. NOTE : \u5728\u7f51\u7edc\u4e2d\uff0c\u8003\u8651\u50cfIRC\u8fd9\u6837\u7684\u5206\u5e03\u5f0f\u804a\u5929\u7f51\u7edc\uff0c\u5176\u4e2d\u542f\u52a8\u9891\u9053\u7684\u7528\u6237\u81ea\u52a8\u83b7\u53d6\u9891\u9053\u64cd\u4f5c\u5458\u6743\u9650\u3002\u5982\u679c\u4f4d\u4e8e\u540c\u4e00\u7f51\u7edc\u4e0d\u540c\u7aef\u7684\u4e0d\u540c\u670d\u52a1\u5668\u4e0a\u7684\u4e24\u4e2a\u7528\u6237\u5c1d\u8bd5\u540c\u65f6\u542f\u52a8\u540c\u540d\u901a\u9053\uff0c\u5219\u6bcf\u4e2a\u7528\u6237\u7684\u76f8\u5e94\u670d\u52a1\u5668\u5c06\u4e3a\u6bcf\u4e2a\u7528\u6237\u6388\u4e88\u901a\u9053\u64cd\u4f5c\u5458\u6743\u9650\uff0c\u56e0\u4e3a\u4e24\u4e2a\u670d\u52a1\u5668\u90fd\u6ca1\u6709\u6536\u5230\u5176\u4ed6\u670d\u52a1\u5668\u5df2\u5206\u914d\u8be5\u901a\u9053\u7684\u4fe1\u53f7\u3002 \uff08\u8fd9\u4e2a\u95ee\u9898\u5df2\u7ecf\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u901a\u8fc7\u5404\u79cdIRC\u670d\u52a1\u5668\u5b9e\u73b0\u6765\u89e3\u51b3\u3002\uff09 \u5728\u8fd9\u79cd\u7ade\u4e89\u6761\u4ef6\u7684\u60c5\u51b5\u4e0b\uff0c\u201c\u5171\u4eab\u8d44\u6e90\u201d\u7684\u6982\u5ff5\u6db5\u76d6\u4e86\u7f51\u7edc\u7684\u72b6\u6001\uff08\u5b58\u5728\u54ea\u4e9b\u4fe1\u9053\uff0c\u4ee5\u53ca\u7528\u6237\u542f\u52a8\u5b83\u4eec\u56e0\u6b64\u5177\u6709\u54ea\u4e9b\u7279\u6743\uff09\uff0c\u6bcf\u4e2a\u670d\u52a1\u5668\u53ef\u4ee5\u81ea\u7531\u66f4\u6539\uff0c\u53ea\u8981\u5b83\u5411\u7f51\u7edc\u4e0a\u7684\u5176\u4ed6\u670d\u52a1\u5668\u53d1\u51fa\u5173\u4e8e\u53d8\u5316\u7684\u4fe1\u53f7\uff0c\u4ee5\u4fbf\u4ed6\u4eec\u53ef\u4ee5\u66f4\u65b0\u4ed6\u4eec\u5bf9\u7f51\u7edc\u72b6\u6001\u7684\u6982\u5ff5\u3002\u4f46\u662f\uff0c\u7f51\u7edc\u4e0a\u7684\u5ef6\u8fdf\u4f7f\u5f97\u6240\u63cf\u8ff0\u7684\u7ade\u4e89\u6761\u4ef6\u6210\u4e3a\u53ef\u80fd\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u5bf9\u5171\u4eab\u8d44\u6e90\u7684\u8bbf\u95ee\u5b9e\u65bd\u4e00\u79cd\u63a7\u5236\u5f62\u5f0f\u6765\u63a7\u5236\u7ade\u4e89\u6761\u4ef6 - \u4f8b\u5982\uff0c\u6307\u5b9a\u4e00\u4e2a\u670d\u52a1\u5668\u6765\u63a7\u5236\u8c01\u62e5\u6709\u4ec0\u4e48\u7279\u6743 - \u610f\u5473\u7740\u5c06\u5206\u5e03\u5f0f\u7f51\u7edc\u8f6c\u53d8\u4e3a\u96c6\u4e2d\u5f0f\u7f51\u7edc\uff08\u81f3\u5c11\u5bf9\u4e8e\u90a3\u4e2a\u90e8\u5206\uff09\u7f51\u7edc\u8fd0\u8425\uff09\u3002 \u5f53\u8ba1\u7b97\u673a\u7a0b\u5e8f\u4f7f\u7528\u975e\u963b\u585e\u5957\u63a5\u5b57\u7f16\u5199\u65f6\uff0c\u7ade\u4e89\u6761\u4ef6\u4e5f\u53ef\u80fd\u5b58\u5728\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u7a0b\u5e8f\u7684\u6027\u80fd\u53ef\u80fd\u53d6\u51b3\u4e8e\u7f51\u7edc\u94fe\u63a5\u7684\u901f\u5ea6\u3002","title":"Networking"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Race/wikipedia-Race-condition/#life-critical#systems","text":"Software flaws in life-critical systems can be disastrous. Race conditions were among the flaws in the Therac-25 radiation therapy machine, which led to the death of at least three patients and injuries to several more.[ 9] TRANSLATION : \u751f\u547d\u6538\u5173\u7cfb\u7edf\u4e2d\u7684\u8f6f\u4ef6\u7f3a\u9677\u53ef\u80fd\u662f\u707e\u96be\u6027\u7684\u3002 Another example is the Energy Management System provided by GE Energy and used by Ohio -based FirstEnergy Corp (among other power facilities). A race condition existed in the alarm subsystem; when three sagging power lines were tripped simultaneously, the condition prevented alerts from being raised to the monitoring technicians, delaying their awareness of the problem. This software flaw eventually led to the North American Blackout of 2003 .[ 10] GE Energy later developed a software patch to correct the previously undiscovered error.","title":"Life-critical systems \u751f\u6b7b\u6538\u5173\u7684\u7cfb\u7edf"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Race/wikipedia-Race-condition/#tools","text":"Many software tools exist to help detect race conditions in software. They can be largely categorized into two groups: static analysis tools and dynamic analysis tools. Thread Safety Analysis is a static analysis tool for annotation-based intra-procedural static analysis, originally implemented as a branch of gcc, and now reimplemented in Clang , supporting PThreads.[ 13] [ non-primary source needed ] Dynamic analysis tools include: 1\u3001 Intel Inspector , a memory and thread checking and debugging tool to increase the reliability, security, and accuracy of C/C++ and Fortran applications; Intel Advisor , a sampling based, SIMD vectorization optimization and shared memory threading assistance tool for C, C++, C#, and Fortran software developers and architects; 2\u3001ThreadSanitizer, which uses binary ( Valgrind -based) or source, LLVM -based instrumentation, and supports PThreads);[ 14] [ non-primary source needed ] and Helgrind, a Valgrind tool for detecting synchronisation errors in C, C++ and Fortran programs that use the POSIX pthreads threading primitives.[ 15] [ non-primary source needed ] 3\u3001Data Race Detector[ 16] is designed to find data races in the Go Programming language.","title":"Tools"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Time-of-check-to-time-of-use/","text":"","title":"Introduction"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Time-of-check-to-time-of-use/wikipedia-Time-of-check-to-time-of-use/","text":"wikipedia Time of check to time of use In software development , time of check to time of use ( TOCTOU , TOCTTOU or TOC/TOU) is a class of software bugs caused by changes in a system between the checking of a condition (such as a security credential) and the use of the results of that check. This is one example of a race condition . SUMMARY : \u7531\u53d1\u751f\u5728checking of condition\u548cuse the result of the check\u4e4b\u95f4\u7684change\u800c\u5bfc\u81f4\u7684 A simple example is as follows: Consider a Web application that allows a user to edit pages, and also allows administrators to lock pages to prevent editing. A user requests to edit a page, getting a form which can be used to alter its content. Before the user submits the form, an administrator locks the page, which should prevent editing. However, since editing has already begun, when the user submits the form, those edits (which have already been made) are accepted. When the user began editing, the appropriate authorization was checked , and the user was indeed allowed to edit. However, the authorization was used later, at a time when edits should no longer have been allowed. \u7ffb\u8bd1 \uff1a \u4e00\u4e2a\u7b80\u5355\u7684\u793a\u4f8b\u5982\u4e0b\uff1a\u8003\u8651\u5141\u8bb8\u7528\u6237\u7f16\u8f91\u9875\u9762\u7684Web\u5e94\u7528\u7a0b\u5e8f\uff0c\u8fd8\u5141\u8bb8\u7ba1\u7406\u5458\u9501\u5b9a\u9875\u9762\u4ee5\u9632\u6b62\u7f16\u8f91\u3002 \u7528\u6237\u8bf7\u6c42\u7f16\u8f91\u9875\u9762\uff0c\u83b7\u53d6\u53ef\u7528\u4e8e\u66f4\u6539\u5176\u5185\u5bb9\u7684\u8868\u5355\u3002 \u5728\u7528\u6237\u63d0\u4ea4\u8868\u5355\u4e4b\u524d\uff0c\u7ba1\u7406\u5458\u4f1a\u9501\u5b9a\u9875\u9762\uff0c\u8fd9\u4f1a\u963b\u6b62\u7f16\u8f91\u3002 \u4f46\u662f\uff0c\u7531\u4e8e\u7f16\u8f91\u5df2\u7ecf\u5f00\u59cb\uff0c\u5f53\u7528\u6237\u63d0\u4ea4\u8868\u5355\u65f6\uff0c\u5c06\u63a5\u53d7\u8fd9\u4e9b\u7f16\u8f91\uff08\u5df2\u7ecf\u8fdb\u884c\u4e86\u7f16\u8f91\uff09\u3002 \u5f53\u7528\u6237\u5f00\u59cb\u7f16\u8f91\u65f6\uff0c\u68c0\u67e5\u4e86\u76f8\u5e94\u7684\u6388\u6743\uff0c\u786e\u5b9e\u5141\u8bb8\u7528\u6237\u7f16\u8f91\u3002 \u4f46\u662f\uff0c\u5728\u4e0d\u518d\u5141\u8bb8\u7f16\u8f91\u7684\u60c5\u51b5\u4e0b\uff0c\u7a0d\u540e\u4f1a\u4f7f\u7528\u6388\u6743\u3002 TOCTOU race conditions are common in Unix between operations on the file system ,[ 1] but can occur in other contexts, including local sockets and improper use of database transactions . In the early 1990s, the mail utility of BSD 4.3 UNIX had an exploitable race condition for temporary files because it used the mktemp() C library function .[ 2] Early versions of OpenSSH had an exploitable race condition for Unix domain sockets .[ 3] Examples In Unix , the following C code, when used in a setuid program, has a TOCTOU bug: if ( access ( \"file\" , W_OK ) != 0 ) { exit ( 1 ); } fd = open ( \"file\" , O_WRONLY ); write ( fd , buffer , sizeof ( buffer )); Here, access is intended to check whether the real user who executed the setuid program would normally be allowed to write the file (i.e., access checks the real userid rather than effective userid ). This race condition is vulnerable to an attack: Victim Attacker if (access(\"file\", W_OK) != 0) { exit(1); } fd = open(\"file\", O_WRONLY); // Actually writing over /etc/passwd write(fd, buffer, sizeof(buffer)); // // // After the access check symlink(\"/etc/passwd\", \"file\"); // Before the open, \"file\" points to the password database // // In this example, an attacker can exploit the race condition between the access and open to trick the setuid victim into overwriting an entry in the system password database. TOCTOU races can be used for privilege escalation , to get administrative access to a machine. Although this sequence of events requires precise timing, it is possible for an attacker to arrange such conditions without too much difficulty. The implication is that applications cannot assume the state managed by the operating system (in this case the file system namespace) will not change between system calls. Reliably timing TOCTOU Exploiting a TOCTOU race condition requires precise timing to ensure that the attacker's operations interleave properly with the victim's. In the example above, the attacker must execute the symlink system call precisely between the access and open . For the most general attack, the attacker must be scheduled for execution after each operation by the victim, also known as \"single-stepping\" the victim. In the case of BSD 4.3 mail utility and mktemp(),[ 4] the attacker can simply keep launching mail utility in one process, and keep guessing the temporary file names and keep making symlinks in another process. The attack can usually succeed in less than one minute. Techniques for single-stepping a victim program include file system mazes[ 5] and algorithmic complexity attacks.[ 6] In both cases, the attacker manipulates the OS state to control scheduling of the victim. File system mazes force the victim to read a directory entry that is not in the OS cache, and the OS puts the victim to sleep while it is reading the directory from disk. Algorithmic complexity attacks force the victim to spend its entire scheduling quantum inside a single system call traversing the kernel's hash table of cached file names. The attacker creates a very large number of files with names that hash to the same value as the file the victim will look up. Preventing TOCTOU Despite conceptual simplicity, TOCTOU race conditions are difficult to avoid and eliminate. One general technique is to use exception handling instead of checking, under the philosophy of EAFP \"It is easier to ask for forgiveness than permission\" rather than LBYL \"look before you leap\" \u2013 in this case there is no check, and failure of assumptions to hold are detected at use time, by an exception.[ 7] In the context of file system TOCTOU race conditions, the fundamental challenge is ensuring that the file system cannot be changed between two system calls. In 2004, an impossibility result was published, showing that there was no portable, deterministic technique for avoiding TOCTOU race conditions.[ 8] Since this impossibility result, libraries for tracking file descriptors and ensuring correctness have been proposed by researchers.[ 9] An alternative solution proposed in the research community is for UNIX systems to adopt transactions in the file system or the OS kernel. Transactions provide a concurrency control abstraction for the OS, and can be used to prevent TOCTOU races. While no production UNIX kernel has yet adopted transactions, proof-of-concept research prototypes have been developed for Linux, including the Valor file system[ 10] and the TxOS kernel.[ 11] Microsoft Windows has added transactions to its NTFS file system,[ 12] but Microsoft discourages their use, and has indicated that they may be removed in a future version of Windows.[ 13] File locking is a common technique for preventing race conditions for a single file, but it does not extend to the file system namespace and other metadata, nor does locking work well with networked filesystems, and cannot prevent TOCTOU race conditions. For setuid binaries a possible solution is to use the seteuid() system call to change the effective user and then perform the open() . Differences in setuid() between operating systems can be problematic.[ 14] Preventing TOCTOU Despite conceptual simplicity, TOCTOU race conditions are difficult to avoid and eliminate. One general technique is to use exception handling instead of checking, under the philosophy of EAFP \"It is easier to ask for forgiveness than permission\" rather than LBYL \"look before you leap\" \u2013 in this case there is no check, and failure of assumptions to hold are detected at use time, by an exception.[ 7] In the context of file system TOCTOU race conditions, the fundamental challenge is ensuring that the file system cannot be changed between two system calls. In 2004, an impossibility result was published, showing that there was no portable, deterministic technique for avoiding TOCTOU race conditions.[ 8] Since this impossibility result, libraries for tracking file descriptors and ensuring correctness have been proposed by researchers.[ 9] An alternative solution proposed in the research community is for UNIX systems to adopt transactions in the file system or the OS kernel. Transactions provide a concurrency control abstraction for the OS, and can be used to prevent TOCTOU races. While no production UNIX kernel has yet adopted transactions, proof-of-concept research prototypes have been developed for Linux, including the Valor file system[ 10] and the TxOS kernel.[ 11] Microsoft Windows has added transactions to its NTFS file system,[ 12] but Microsoft discourages their use, and has indicated that they may be removed in a future version of Windows.[ 13] File locking is a common technique for preventing race conditions for a single file, but it does not extend to the file system namespace and other metadata, nor does locking work well with networked filesystems, and cannot prevent TOCTOU race conditions. For setuid binaries a possible solution is to use the seteuid() system call to change the effective user and then perform the open() . Differences in setuid() between operating systems can be problematic.[ 14] See also Linearizability","title":"Introduction"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Time-of-check-to-time-of-use/wikipedia-Time-of-check-to-time-of-use/#wikipedia#time#of#check#to#time#of#use","text":"In software development , time of check to time of use ( TOCTOU , TOCTTOU or TOC/TOU) is a class of software bugs caused by changes in a system between the checking of a condition (such as a security credential) and the use of the results of that check. This is one example of a race condition . SUMMARY : \u7531\u53d1\u751f\u5728checking of condition\u548cuse the result of the check\u4e4b\u95f4\u7684change\u800c\u5bfc\u81f4\u7684 A simple example is as follows: Consider a Web application that allows a user to edit pages, and also allows administrators to lock pages to prevent editing. A user requests to edit a page, getting a form which can be used to alter its content. Before the user submits the form, an administrator locks the page, which should prevent editing. However, since editing has already begun, when the user submits the form, those edits (which have already been made) are accepted. When the user began editing, the appropriate authorization was checked , and the user was indeed allowed to edit. However, the authorization was used later, at a time when edits should no longer have been allowed. \u7ffb\u8bd1 \uff1a \u4e00\u4e2a\u7b80\u5355\u7684\u793a\u4f8b\u5982\u4e0b\uff1a\u8003\u8651\u5141\u8bb8\u7528\u6237\u7f16\u8f91\u9875\u9762\u7684Web\u5e94\u7528\u7a0b\u5e8f\uff0c\u8fd8\u5141\u8bb8\u7ba1\u7406\u5458\u9501\u5b9a\u9875\u9762\u4ee5\u9632\u6b62\u7f16\u8f91\u3002 \u7528\u6237\u8bf7\u6c42\u7f16\u8f91\u9875\u9762\uff0c\u83b7\u53d6\u53ef\u7528\u4e8e\u66f4\u6539\u5176\u5185\u5bb9\u7684\u8868\u5355\u3002 \u5728\u7528\u6237\u63d0\u4ea4\u8868\u5355\u4e4b\u524d\uff0c\u7ba1\u7406\u5458\u4f1a\u9501\u5b9a\u9875\u9762\uff0c\u8fd9\u4f1a\u963b\u6b62\u7f16\u8f91\u3002 \u4f46\u662f\uff0c\u7531\u4e8e\u7f16\u8f91\u5df2\u7ecf\u5f00\u59cb\uff0c\u5f53\u7528\u6237\u63d0\u4ea4\u8868\u5355\u65f6\uff0c\u5c06\u63a5\u53d7\u8fd9\u4e9b\u7f16\u8f91\uff08\u5df2\u7ecf\u8fdb\u884c\u4e86\u7f16\u8f91\uff09\u3002 \u5f53\u7528\u6237\u5f00\u59cb\u7f16\u8f91\u65f6\uff0c\u68c0\u67e5\u4e86\u76f8\u5e94\u7684\u6388\u6743\uff0c\u786e\u5b9e\u5141\u8bb8\u7528\u6237\u7f16\u8f91\u3002 \u4f46\u662f\uff0c\u5728\u4e0d\u518d\u5141\u8bb8\u7f16\u8f91\u7684\u60c5\u51b5\u4e0b\uff0c\u7a0d\u540e\u4f1a\u4f7f\u7528\u6388\u6743\u3002 TOCTOU race conditions are common in Unix between operations on the file system ,[ 1] but can occur in other contexts, including local sockets and improper use of database transactions . In the early 1990s, the mail utility of BSD 4.3 UNIX had an exploitable race condition for temporary files because it used the mktemp() C library function .[ 2] Early versions of OpenSSH had an exploitable race condition for Unix domain sockets .[ 3]","title":"wikipedia Time of check to time of use"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Time-of-check-to-time-of-use/wikipedia-Time-of-check-to-time-of-use/#examples","text":"In Unix , the following C code, when used in a setuid program, has a TOCTOU bug: if ( access ( \"file\" , W_OK ) != 0 ) { exit ( 1 ); } fd = open ( \"file\" , O_WRONLY ); write ( fd , buffer , sizeof ( buffer )); Here, access is intended to check whether the real user who executed the setuid program would normally be allowed to write the file (i.e., access checks the real userid rather than effective userid ). This race condition is vulnerable to an attack: Victim Attacker if (access(\"file\", W_OK) != 0) { exit(1); } fd = open(\"file\", O_WRONLY); // Actually writing over /etc/passwd write(fd, buffer, sizeof(buffer)); // // // After the access check symlink(\"/etc/passwd\", \"file\"); // Before the open, \"file\" points to the password database // // In this example, an attacker can exploit the race condition between the access and open to trick the setuid victim into overwriting an entry in the system password database. TOCTOU races can be used for privilege escalation , to get administrative access to a machine. Although this sequence of events requires precise timing, it is possible for an attacker to arrange such conditions without too much difficulty. The implication is that applications cannot assume the state managed by the operating system (in this case the file system namespace) will not change between system calls.","title":"Examples"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Time-of-check-to-time-of-use/wikipedia-Time-of-check-to-time-of-use/#reliably#timing#toctou","text":"Exploiting a TOCTOU race condition requires precise timing to ensure that the attacker's operations interleave properly with the victim's. In the example above, the attacker must execute the symlink system call precisely between the access and open . For the most general attack, the attacker must be scheduled for execution after each operation by the victim, also known as \"single-stepping\" the victim. In the case of BSD 4.3 mail utility and mktemp(),[ 4] the attacker can simply keep launching mail utility in one process, and keep guessing the temporary file names and keep making symlinks in another process. The attack can usually succeed in less than one minute. Techniques for single-stepping a victim program include file system mazes[ 5] and algorithmic complexity attacks.[ 6] In both cases, the attacker manipulates the OS state to control scheduling of the victim. File system mazes force the victim to read a directory entry that is not in the OS cache, and the OS puts the victim to sleep while it is reading the directory from disk. Algorithmic complexity attacks force the victim to spend its entire scheduling quantum inside a single system call traversing the kernel's hash table of cached file names. The attacker creates a very large number of files with names that hash to the same value as the file the victim will look up.","title":"Reliably timing TOCTOU"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Time-of-check-to-time-of-use/wikipedia-Time-of-check-to-time-of-use/#preventing#toctou","text":"Despite conceptual simplicity, TOCTOU race conditions are difficult to avoid and eliminate. One general technique is to use exception handling instead of checking, under the philosophy of EAFP \"It is easier to ask for forgiveness than permission\" rather than LBYL \"look before you leap\" \u2013 in this case there is no check, and failure of assumptions to hold are detected at use time, by an exception.[ 7] In the context of file system TOCTOU race conditions, the fundamental challenge is ensuring that the file system cannot be changed between two system calls. In 2004, an impossibility result was published, showing that there was no portable, deterministic technique for avoiding TOCTOU race conditions.[ 8] Since this impossibility result, libraries for tracking file descriptors and ensuring correctness have been proposed by researchers.[ 9] An alternative solution proposed in the research community is for UNIX systems to adopt transactions in the file system or the OS kernel. Transactions provide a concurrency control abstraction for the OS, and can be used to prevent TOCTOU races. While no production UNIX kernel has yet adopted transactions, proof-of-concept research prototypes have been developed for Linux, including the Valor file system[ 10] and the TxOS kernel.[ 11] Microsoft Windows has added transactions to its NTFS file system,[ 12] but Microsoft discourages their use, and has indicated that they may be removed in a future version of Windows.[ 13] File locking is a common technique for preventing race conditions for a single file, but it does not extend to the file system namespace and other metadata, nor does locking work well with networked filesystems, and cannot prevent TOCTOU race conditions. For setuid binaries a possible solution is to use the seteuid() system call to change the effective user and then perform the open() . Differences in setuid() between operating systems can be problematic.[ 14]","title":"Preventing TOCTOU"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Time-of-check-to-time-of-use/wikipedia-Time-of-check-to-time-of-use/#preventing#toctou_1","text":"Despite conceptual simplicity, TOCTOU race conditions are difficult to avoid and eliminate. One general technique is to use exception handling instead of checking, under the philosophy of EAFP \"It is easier to ask for forgiveness than permission\" rather than LBYL \"look before you leap\" \u2013 in this case there is no check, and failure of assumptions to hold are detected at use time, by an exception.[ 7] In the context of file system TOCTOU race conditions, the fundamental challenge is ensuring that the file system cannot be changed between two system calls. In 2004, an impossibility result was published, showing that there was no portable, deterministic technique for avoiding TOCTOU race conditions.[ 8] Since this impossibility result, libraries for tracking file descriptors and ensuring correctness have been proposed by researchers.[ 9] An alternative solution proposed in the research community is for UNIX systems to adopt transactions in the file system or the OS kernel. Transactions provide a concurrency control abstraction for the OS, and can be used to prevent TOCTOU races. While no production UNIX kernel has yet adopted transactions, proof-of-concept research prototypes have been developed for Linux, including the Valor file system[ 10] and the TxOS kernel.[ 11] Microsoft Windows has added transactions to its NTFS file system,[ 12] but Microsoft discourages their use, and has indicated that they may be removed in a future version of Windows.[ 13] File locking is a common technique for preventing race conditions for a single file, but it does not extend to the file system namespace and other metadata, nor does locking work well with networked filesystems, and cannot prevent TOCTOU race conditions. For setuid binaries a possible solution is to use the seteuid() system call to change the effective user and then perform the open() . Differences in setuid() between operating systems can be problematic.[ 14]","title":"Preventing TOCTOU"},{"location":"Multithread/Thread-safety/What-cause-unsafety/Time-of-check-to-time-of-use/wikipedia-Time-of-check-to-time-of-use/#see#also","text":"Linearizability","title":"See also"},{"location":"Multithread/Thread-safety/wikipedia-Thread-safety/","text":"wikipedia Thread safety Thread safety is a computer programming concept applicable to multi-threaded code. Thread-safe code only manipulates shared data structures in a manner that ensures that all threads behave properly and fulfill their design specifications without unintended interaction. There are various strategies for making thread-safe data structures.[ 1] [ 2] A program may execute code in several threads simultaneously in a shared address space where each of those threads has access to virtually all of the memory of every other thread. Thread safety is a property that allows code to run in multithreaded environments by re-establishing some of the correspondences between the actual flow of control and the text of the program, by means of synchronization . Levels of thread safety Software libraries can provide certain thread-safety guarantees. For example, concurrent reads might be guaranteed to be thread-safe, but concurrent writes might not be. Whether a program using such a library is thread-safe depends on whether it uses the library in a manner consistent with those guarantees. Different vendors use slightly different terminology for thread-safety:[ 3] [ 4] [ 5] [ 6] Thread safe : Implementation is guaranteed to be free of race conditions when accessed by multiple threads simultaneously. Conditionally safe : Different threads can access different objects simultaneously, and access to shared data is protected from race conditions. Not thread safe : Data structures should not be accessed simultaneously by different threads. Thread safety guarantees usually also include design steps to prevent or limit the risk of different forms of deadlocks , as well as optimizations to maximize concurrent performance. However, deadlock-free guarantees cannot always be given, since deadlocks can be caused by callbacks and violation of architectural layering independent of the library itself. Implementation approaches Below we discuss two approaches for avoiding race conditions to achieve thread safety. The first class of approaches focuses on avoiding shared state , and includes: 1\u3001 Re-entrancy Writing code in such a way that it can be partially executed by a thread, reexecuted by the same thread or simultaneously executed by another thread and still correctly complete the original execution. This requires the saving of state information in variables local to each execution, usually on a stack, instead of in static or global variables or other non-local state. All non-local state must be accessed through atomic operations and the data-structures must also be reentrant. 2\u3001 Thread-local storage Variables are localized so that each thread has its own private copy. These variables retain their values across subroutine and other code boundaries, and are thread-safe since they are local to each thread, even though the code which accesses them might be executed simultaneously by another thread. 3\u3001 Immutable objects The state of an object cannot be changed after construction. This implies both that only read-only data is shared and that inherent(\u56fa\u6709\u7684) thread safety is attained. Mutable (non-const) operations can then be implemented in such a way that they create new objects instead of modifying existing ones. This approach is characteristic of functional programming and is also used by the string implementations in Java, C# and Python.[ 7] The second class of approaches are synchronization-related , and are used in situations where shared state cannot be avoided: 4\u3001 Mutual exclusion Access to shared data is serialized using mechanisms that ensure only one thread reads or writes to the shared data at any time. Incorporation of mutual exclusion needs to be well thought out, since improper usage can lead to side-effects like deadlocks , livelocks and resource starvation . 5\u3001 Atomic operations Shared data is accessed by using atomic operations which cannot be interrupted by other threads. This usually requires using special machine language instructions, which might be available in a runtime library . Since the operations are atomic, the shared data is always kept in a valid state, no matter how other threads access it. Atomic operations form the basis of many thread locking mechanisms, and are used to implement mutual exclusion primitives. Examples In the following piece of Java code, the method is thread-safe: class Counter { private int i = 0 ; public synchronized void inc () { i ++ ; } } In the C programming language , each thread has its own stack. However, a static variable is not kept on the stack; all threads share simultaneous access to it. If multiple threads overlap while running the same function, it is possible that a static variable might be changed by one thread while another is midway through checking it. This difficult-to-diagnose logic error , which may compile and run properly most of the time, is called a race condition . One common way to avoid this is to use another shared variable as a \"lock\" or \"mutex\" (from **mut**ual **ex**clusion). In the following piece of C code, the function is thread-safe, but not reentrant: # include <pthread.h> int increment_counter () { static int counter = 0 ; static pthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER ; // only allow one thread to increment at a time pthread_mutex_lock ( & mutex ); ++ counter ; // store value before any other threads increment it further int result = counter ; pthread_mutex_unlock ( & mutex ); return result ; } SUMMARY:the above code is not reentrant,because it use static variable.Refer to this article for an answer\u3002 In the above, increment_counter can be called by different threads without any problem since a mutex is used to synchronize all access to the shared counter variable. But if the function is used in a reentrant interrupt handler and a second interrupt arises inside the function, the second routine will hang forever. As interrupt servicing can disable other interrupts, the whole system could suffer. The same function can be implemented to be both thread-safe and reentrant using the lock-free atomics in C++11 : # include <atomic> int increment_counter () { static std :: atomic < int > counter ( 0 ); // increment is guaranteed to be done atomically int result = ++ counter ; return result ; } See also Concurrency control Exception safety Priority inversion ThreadSafe","title":"Introduction"},{"location":"Multithread/Thread-safety/wikipedia-Thread-safety/#wikipedia#thread#safety","text":"Thread safety is a computer programming concept applicable to multi-threaded code. Thread-safe code only manipulates shared data structures in a manner that ensures that all threads behave properly and fulfill their design specifications without unintended interaction. There are various strategies for making thread-safe data structures.[ 1] [ 2] A program may execute code in several threads simultaneously in a shared address space where each of those threads has access to virtually all of the memory of every other thread. Thread safety is a property that allows code to run in multithreaded environments by re-establishing some of the correspondences between the actual flow of control and the text of the program, by means of synchronization .","title":"wikipedia Thread safety"},{"location":"Multithread/Thread-safety/wikipedia-Thread-safety/#levels#of#thread#safety","text":"Software libraries can provide certain thread-safety guarantees. For example, concurrent reads might be guaranteed to be thread-safe, but concurrent writes might not be. Whether a program using such a library is thread-safe depends on whether it uses the library in a manner consistent with those guarantees. Different vendors use slightly different terminology for thread-safety:[ 3] [ 4] [ 5] [ 6] Thread safe : Implementation is guaranteed to be free of race conditions when accessed by multiple threads simultaneously. Conditionally safe : Different threads can access different objects simultaneously, and access to shared data is protected from race conditions. Not thread safe : Data structures should not be accessed simultaneously by different threads. Thread safety guarantees usually also include design steps to prevent or limit the risk of different forms of deadlocks , as well as optimizations to maximize concurrent performance. However, deadlock-free guarantees cannot always be given, since deadlocks can be caused by callbacks and violation of architectural layering independent of the library itself.","title":"Levels of thread safety"},{"location":"Multithread/Thread-safety/wikipedia-Thread-safety/#implementation#approaches","text":"Below we discuss two approaches for avoiding race conditions to achieve thread safety. The first class of approaches focuses on avoiding shared state , and includes: 1\u3001 Re-entrancy Writing code in such a way that it can be partially executed by a thread, reexecuted by the same thread or simultaneously executed by another thread and still correctly complete the original execution. This requires the saving of state information in variables local to each execution, usually on a stack, instead of in static or global variables or other non-local state. All non-local state must be accessed through atomic operations and the data-structures must also be reentrant. 2\u3001 Thread-local storage Variables are localized so that each thread has its own private copy. These variables retain their values across subroutine and other code boundaries, and are thread-safe since they are local to each thread, even though the code which accesses them might be executed simultaneously by another thread. 3\u3001 Immutable objects The state of an object cannot be changed after construction. This implies both that only read-only data is shared and that inherent(\u56fa\u6709\u7684) thread safety is attained. Mutable (non-const) operations can then be implemented in such a way that they create new objects instead of modifying existing ones. This approach is characteristic of functional programming and is also used by the string implementations in Java, C# and Python.[ 7] The second class of approaches are synchronization-related , and are used in situations where shared state cannot be avoided: 4\u3001 Mutual exclusion Access to shared data is serialized using mechanisms that ensure only one thread reads or writes to the shared data at any time. Incorporation of mutual exclusion needs to be well thought out, since improper usage can lead to side-effects like deadlocks , livelocks and resource starvation . 5\u3001 Atomic operations Shared data is accessed by using atomic operations which cannot be interrupted by other threads. This usually requires using special machine language instructions, which might be available in a runtime library . Since the operations are atomic, the shared data is always kept in a valid state, no matter how other threads access it. Atomic operations form the basis of many thread locking mechanisms, and are used to implement mutual exclusion primitives.","title":"Implementation approaches"},{"location":"Multithread/Thread-safety/wikipedia-Thread-safety/#examples","text":"In the following piece of Java code, the method is thread-safe: class Counter { private int i = 0 ; public synchronized void inc () { i ++ ; } } In the C programming language , each thread has its own stack. However, a static variable is not kept on the stack; all threads share simultaneous access to it. If multiple threads overlap while running the same function, it is possible that a static variable might be changed by one thread while another is midway through checking it. This difficult-to-diagnose logic error , which may compile and run properly most of the time, is called a race condition . One common way to avoid this is to use another shared variable as a \"lock\" or \"mutex\" (from **mut**ual **ex**clusion). In the following piece of C code, the function is thread-safe, but not reentrant: # include <pthread.h> int increment_counter () { static int counter = 0 ; static pthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER ; // only allow one thread to increment at a time pthread_mutex_lock ( & mutex ); ++ counter ; // store value before any other threads increment it further int result = counter ; pthread_mutex_unlock ( & mutex ); return result ; } SUMMARY:the above code is not reentrant,because it use static variable.Refer to this article for an answer\u3002 In the above, increment_counter can be called by different threads without any problem since a mutex is used to synchronize all access to the shared counter variable. But if the function is used in a reentrant interrupt handler and a second interrupt arises inside the function, the second routine will hang forever. As interrupt servicing can disable other interrupts, the whole system could suffer. The same function can be implemented to be both thread-safe and reentrant using the lock-free atomics in C++11 : # include <atomic> int increment_counter () { static std :: atomic < int > counter ( 0 ); // increment is guaranteed to be done atomically int result = ++ counter ; return result ; }","title":"Examples"},{"location":"Multithread/Thread-safety/wikipedia-Thread-safety/#see#also","text":"Concurrency control Exception safety Priority inversion ThreadSafe","title":"See also"},{"location":"Multithread/Topics/Execute-once/","text":"Execute once \u5728\u9605\u8bfbcppreference Storage class specifiers#Static local variables \uff0c\u5176\u4e2d\u8c08\u53ca\u4e86\u8fd9\u6837\u7684\u95ee\u9898\uff1a If multiple threads attempt to initialize the same static local variable concurrently, the initialization occurs exactly once (similar behavior can be obtained for arbitrary functions with std::call_once ). Note: usual implementations of this feature use variants of the double-checked locking pattern, which reduces runtime overhead for already-initialized local statics to a single non-atomic boolean comparison. \u8fd9\u662f\u5728\u8fdb\u884cmultiple thread programming\u4e2d\uff0c\u7ecf\u5e38\u4f1a\u9047\u5230\u7684\u4e00\u7c7b\u95ee\u9898\uff0c\u6709\u5fc5\u8981\u603b\u7ed3\u4e00\u4e0b\u3002 Implementation 1\u3001cppreference std::call_once 2\u3001 pthread_once 3\u3001cppreference function-local statics \u5176\u5b9e\u5c31\u662fstatic local object","title":"Introduction"},{"location":"Multithread/Topics/Execute-once/#execute#once","text":"\u5728\u9605\u8bfbcppreference Storage class specifiers#Static local variables \uff0c\u5176\u4e2d\u8c08\u53ca\u4e86\u8fd9\u6837\u7684\u95ee\u9898\uff1a If multiple threads attempt to initialize the same static local variable concurrently, the initialization occurs exactly once (similar behavior can be obtained for arbitrary functions with std::call_once ). Note: usual implementations of this feature use variants of the double-checked locking pattern, which reduces runtime overhead for already-initialized local statics to a single non-atomic boolean comparison. \u8fd9\u662f\u5728\u8fdb\u884cmultiple thread programming\u4e2d\uff0c\u7ecf\u5e38\u4f1a\u9047\u5230\u7684\u4e00\u7c7b\u95ee\u9898\uff0c\u6709\u5fc5\u8981\u603b\u7ed3\u4e00\u4e0b\u3002","title":"Execute once"},{"location":"Multithread/Topics/Execute-once/#implementation","text":"1\u3001cppreference std::call_once 2\u3001 pthread_once 3\u3001cppreference function-local statics \u5176\u5b9e\u5c31\u662fstatic local object","title":"Implementation"},{"location":"Multithread/Topics/False-death/","text":"\u7ebf\u7a0b\u5047\u6b7b \u5206\u6790\u4e00\u4e2a\u5e38\u89c1\u7684java\u591a\u7ebf\u7a0b\u901a\u4fe1\u95ee\u9898\uff08\u5047\u6b7b\u73b0\u8c61\uff09 NOTE: \u8fd9\u7bc7\u6587\u7ae0\u5206\u6790\u7684\u8f83\u597d \u4e8c\u3001\u5047\u6b7b\u72b6\u6001\u5206\u6790 \u5176\u5b9e\u51fa\u73b0\u8fd9\u4e2a\u73b0\u8c61\u7684\u539f\u56e0\u5f88\u7b80\u5355\uff0c\u90a3\u5c31\u662f\u548c\u6211\u4eec\u7684wait/notify\u673a\u5236\u6709\u5173\uff0c\u6211\u4eec\u51e0\u53e5\u8bdd\u6765\u603b\u7ed3\u4e00\u4e0b\uff1a \u201c\u5047\u6b7b\u201d\u7684\u73b0\u8c61\u5c31\u662f\u5168\u90e8\u7ebf\u7a0b\u90fd\u8fdb\u5165\u4e86WAITING\u72b6\u6001\uff08\u6b7b\u9501\uff09\uff0c\u5219\u7a0b\u5e8f\u5c31\u4e0d\u518d\u6267\u884c\u4efb\u4f55\u4e1a\u52a1\u529f\u80fd\u4e86\uff0c\u6574\u4e2a\u9879\u76ee\u5448\u505c\u6b62\u72b6\u6001\u3002\u4e0a\u9762\u7684\u6848\u4f8b\u4e2d\u51fa\u73b0\u5047\u6b7b\u7684\u73b0\u8c61\u662f\u7531\u4e8e\u4ec5\u4ec5\u5524\u9192\u4e86\u540c\u7c7b\uff08\u751f\u4ea7\u8005\u5524\u9192\u4e86\u751f\u4ea7\u8005\uff0c\u6d88\u8d39\u8005\u5524\u9192\u4e86\u6d88\u8d39\u8005\uff09\u7684\u73b0\u8c61\u5927\u91cf\u51fa\u73b0\u5bfc\u81f4\u7684\u3002 NOTE: \u5176\u5b9e\u5c31\u662f**\u6b7b\u9501** \u4e0b\u9762\u6211\u4eec\u753b\u4e00\u5f20\u56fe\u6765\u5206\u6790\u4e00\u4e0b\uff1a \u4e5f\u5c31\u662f\u8bf4notify\u901a\u77e5\u7684\u662f\u662f\u540c\u7c7b\u3002\u9020\u6210\u4e86\u8fd9\u79cd\u5835\u585e\u73b0\u8c61\u3002\u8fd9\u662f\u5176\u6839\u672c\u539f\u56e0\uff0c\u800c\u4e14\u8fd9\u5f20\u56fe\u662f\u6211\u4eec\u81ea\u5df1\u753b\u7684\u3002\u4e0b\u9762\u6211\u4eec\u5c31\u76f4\u63a5\u4f7f\u7528jstack\u5de5\u5177\u6765\u5206\u6790\u4e00\u4e0b\u7ebf\u7a0b\u7684\u72b6\u6001\u3002\u8fd9\u4e24\u4e2a\u5de5\u5177\u662fjdk\u81ea\u5e26\u7684\uff0c\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u3002 \u73b0\u5728\u6211\u4eec\u77e5\u9053\u4e86\uff0c\u76ee\u524d\u6240\u6709\u7684\u7ebf\u7a0b\u90fd\u662f\u5904\u4e8e\u7b49\u5f85\u7684\u72b6\u6001\uff0c\u8fd9\u4e5f\u5c31\u662f\u5047\u6b7b\u73b0\u8c61\u7684\u9a8c\u8bc1\u3002 \u5047\u6b7b\u73b0\u8c61\u7684\u539f\u56e0\u6211\u4eec\u77e5\u9053\u4e86\uff0c\u90a3\u4e48\u6211\u4eec\u5982\u4f55\u6539\u6b63\u5462\uff1f \u4e09\u3001\u5047\u6b7b\u72b6\u6001\u4fee\u590d \u5047\u6b7b\u73b0\u8c61\u7684\u6539\u6b63\u5176\u5b9e\u5f88\u7b80\u5355\uff0c\u7f51\u4e0a\u7684\u65b9\u5f0f\u4e5f\u5f88\u591a\uff0c\u6bd4\u5982\u8bf4\u901a\u8fc7BlockingQueue\u6216\u8005\u662fnotifyAll\u65b9\u6cd5\u3002notifyAll\u65b9\u6cd5\u8d85\u7ea7\u7b80\u5355\uff0c\u5c31\u662f\u628a\u4e0a\u9762produce\u548cconsume\u65b9\u6cd5\u4e2d\u7684notify\u6539\u6210notifyAll\u65b9\u6cd5\u5373\u53ef\u3002\u76ee\u7684\u5c31\u662f\u901a\u77e5\u5230\u6240\u6709\u7684\u5176\u4ed6\u7ebf\u7a0b\uff0c\u751f\u4ea7\u7ebf\u7a0b\u8be5\u751f\u4ea7\u7684\u751f\u4ea7\uff0c\u6d88\u8d39\u7ebf\u7a0b\u8be5\u6d88\u8d39\u7684\u6d88\u8d39\u3002 \u5bf9\u4e8ejava\u591a\u7ebf\u7a0b\u7684\u7cfb\u5217\u7684\u6587\u7ae0\uff0c\u8fd9\u7b97\u662f\u57fa\u7840\u5165\u95e8\u3002\u8fd8\u6709\u66f4\u591a\u6587\u7ae0\u6211\u4e5f\u7ecf\u6301\u7eed\u53d1\u5e03\u3002\u611f\u8c22\u652f\u6301\u3002OK\uff0c\u4eca\u5929\u7684\u6587\u7ae0\u5148\u5230\u8fd9\u3002 \u9ad8\u5e76\u53d1\u7f16\u7a0b\uff1a9\u3001wait\u3001notify\u3001notifyAll\uff08\u4e8c \u7a0b\u5e8f\u5047\u6b7b\u7684\u539f\u56e0\u5206\u6790\uff09 NOTE: \u751f\u4ea7\u8005\u7ebf\u7a0b\u548c\u6d88\u8d39\u8005\u7ebf\u7a0b\u540c\u65f6\u8fdb\u5165\u7b49\u5f85\u72b6\u6001","title":"Introduction"},{"location":"Multithread/Topics/False-death/#_1","text":"","title":"\u7ebf\u7a0b\u5047\u6b7b"},{"location":"Multithread/Topics/False-death/#java","text":"NOTE: \u8fd9\u7bc7\u6587\u7ae0\u5206\u6790\u7684\u8f83\u597d","title":"\u5206\u6790\u4e00\u4e2a\u5e38\u89c1\u7684java\u591a\u7ebf\u7a0b\u901a\u4fe1\u95ee\u9898\uff08\u5047\u6b7b\u73b0\u8c61\uff09"},{"location":"Multithread/Topics/False-death/#_2","text":"\u5176\u5b9e\u51fa\u73b0\u8fd9\u4e2a\u73b0\u8c61\u7684\u539f\u56e0\u5f88\u7b80\u5355\uff0c\u90a3\u5c31\u662f\u548c\u6211\u4eec\u7684wait/notify\u673a\u5236\u6709\u5173\uff0c\u6211\u4eec\u51e0\u53e5\u8bdd\u6765\u603b\u7ed3\u4e00\u4e0b\uff1a \u201c\u5047\u6b7b\u201d\u7684\u73b0\u8c61\u5c31\u662f\u5168\u90e8\u7ebf\u7a0b\u90fd\u8fdb\u5165\u4e86WAITING\u72b6\u6001\uff08\u6b7b\u9501\uff09\uff0c\u5219\u7a0b\u5e8f\u5c31\u4e0d\u518d\u6267\u884c\u4efb\u4f55\u4e1a\u52a1\u529f\u80fd\u4e86\uff0c\u6574\u4e2a\u9879\u76ee\u5448\u505c\u6b62\u72b6\u6001\u3002\u4e0a\u9762\u7684\u6848\u4f8b\u4e2d\u51fa\u73b0\u5047\u6b7b\u7684\u73b0\u8c61\u662f\u7531\u4e8e\u4ec5\u4ec5\u5524\u9192\u4e86\u540c\u7c7b\uff08\u751f\u4ea7\u8005\u5524\u9192\u4e86\u751f\u4ea7\u8005\uff0c\u6d88\u8d39\u8005\u5524\u9192\u4e86\u6d88\u8d39\u8005\uff09\u7684\u73b0\u8c61\u5927\u91cf\u51fa\u73b0\u5bfc\u81f4\u7684\u3002 NOTE: \u5176\u5b9e\u5c31\u662f**\u6b7b\u9501** \u4e0b\u9762\u6211\u4eec\u753b\u4e00\u5f20\u56fe\u6765\u5206\u6790\u4e00\u4e0b\uff1a \u4e5f\u5c31\u662f\u8bf4notify\u901a\u77e5\u7684\u662f\u662f\u540c\u7c7b\u3002\u9020\u6210\u4e86\u8fd9\u79cd\u5835\u585e\u73b0\u8c61\u3002\u8fd9\u662f\u5176\u6839\u672c\u539f\u56e0\uff0c\u800c\u4e14\u8fd9\u5f20\u56fe\u662f\u6211\u4eec\u81ea\u5df1\u753b\u7684\u3002\u4e0b\u9762\u6211\u4eec\u5c31\u76f4\u63a5\u4f7f\u7528jstack\u5de5\u5177\u6765\u5206\u6790\u4e00\u4e0b\u7ebf\u7a0b\u7684\u72b6\u6001\u3002\u8fd9\u4e24\u4e2a\u5de5\u5177\u662fjdk\u81ea\u5e26\u7684\uff0c\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u3002 \u73b0\u5728\u6211\u4eec\u77e5\u9053\u4e86\uff0c\u76ee\u524d\u6240\u6709\u7684\u7ebf\u7a0b\u90fd\u662f\u5904\u4e8e\u7b49\u5f85\u7684\u72b6\u6001\uff0c\u8fd9\u4e5f\u5c31\u662f\u5047\u6b7b\u73b0\u8c61\u7684\u9a8c\u8bc1\u3002 \u5047\u6b7b\u73b0\u8c61\u7684\u539f\u56e0\u6211\u4eec\u77e5\u9053\u4e86\uff0c\u90a3\u4e48\u6211\u4eec\u5982\u4f55\u6539\u6b63\u5462\uff1f","title":"\u4e8c\u3001\u5047\u6b7b\u72b6\u6001\u5206\u6790"},{"location":"Multithread/Topics/False-death/#_3","text":"\u5047\u6b7b\u73b0\u8c61\u7684\u6539\u6b63\u5176\u5b9e\u5f88\u7b80\u5355\uff0c\u7f51\u4e0a\u7684\u65b9\u5f0f\u4e5f\u5f88\u591a\uff0c\u6bd4\u5982\u8bf4\u901a\u8fc7BlockingQueue\u6216\u8005\u662fnotifyAll\u65b9\u6cd5\u3002notifyAll\u65b9\u6cd5\u8d85\u7ea7\u7b80\u5355\uff0c\u5c31\u662f\u628a\u4e0a\u9762produce\u548cconsume\u65b9\u6cd5\u4e2d\u7684notify\u6539\u6210notifyAll\u65b9\u6cd5\u5373\u53ef\u3002\u76ee\u7684\u5c31\u662f\u901a\u77e5\u5230\u6240\u6709\u7684\u5176\u4ed6\u7ebf\u7a0b\uff0c\u751f\u4ea7\u7ebf\u7a0b\u8be5\u751f\u4ea7\u7684\u751f\u4ea7\uff0c\u6d88\u8d39\u7ebf\u7a0b\u8be5\u6d88\u8d39\u7684\u6d88\u8d39\u3002 \u5bf9\u4e8ejava\u591a\u7ebf\u7a0b\u7684\u7cfb\u5217\u7684\u6587\u7ae0\uff0c\u8fd9\u7b97\u662f\u57fa\u7840\u5165\u95e8\u3002\u8fd8\u6709\u66f4\u591a\u6587\u7ae0\u6211\u4e5f\u7ecf\u6301\u7eed\u53d1\u5e03\u3002\u611f\u8c22\u652f\u6301\u3002OK\uff0c\u4eca\u5929\u7684\u6587\u7ae0\u5148\u5230\u8fd9\u3002","title":"\u4e09\u3001\u5047\u6b7b\u72b6\u6001\u4fee\u590d"},{"location":"Multithread/Topics/False-death/#9waitnotifynotifyall","text":"NOTE: \u751f\u4ea7\u8005\u7ebf\u7a0b\u548c\u6d88\u8d39\u8005\u7ebf\u7a0b\u540c\u65f6\u8fdb\u5165\u7b49\u5f85\u72b6\u6001","title":"\u9ad8\u5e76\u53d1\u7f16\u7a0b\uff1a9\u3001wait\u3001notify\u3001notifyAll\uff08\u4e8c \u7a0b\u5e8f\u5047\u6b7b\u7684\u539f\u56e0\u5206\u6790\uff09"},{"location":"Multithread/Topics/Sleep-forever/","text":"\u8ba9\u4e00\u4e2a\u7ebf\u7a0b\u7b49\u5f85\u53e6\u5916\u4e00\u4e2a\u7ebf\u7a0b\u5b8c\u6210\uff1b","title":"Introduction"},{"location":"Multithread/Topics/Stop-a-blocked-thread/","text":"how to stop a blocked thread \u4eca\u5929\u5728\u4f7f\u7528 redis-plus-plus \u7684 pub/sub \u529f\u80fd\u7684\u65f6\u5019\uff0c\u5176\u6587\u6863\u4e2d\u6709\u8fd9\u6837\u7684\u63cf\u8ff0\uff1a Subscriber::consume waits for message from the underlying connection. If the ConnectionOptions::socket_timeout is reached, and there's no message sent to this connection, Subscriber::consume throws a TimeoutError exception. If ConnectionOptions::socket_timeout is 0ms , Subscriber::consume blocks until it receives a message. \u5728\u6211\u4eec\u7684application\u4e2d\uff0c\u6211\u4f7f\u7528\u4e00\u4e2athread\u6765\u6267\u884c Subscriber::consume \uff0c\u8fd9\u610f\u5473\u4e2d\u5728\u6ca1\u6709message\u7684\u65f6\u5019\u6211\u7684thread\u5c06block\uff1b\u90a3\u5bf9\u4e8e\u4e00\u4e2ablocked\u7684thread\uff0c\u662f\u5426\u4f1a\u5b58\u5728\u548csleeping thread\u4e00\u6837\u7684stop\u7684\u95ee\u9898\u5462\uff1f stackoverflow Terminate thread c++11 blocked on read I've got the following code: class Foo { private : std :: thread thread ; void run (); std :: atomic_flag running ; std :: thread :: native_handle_type native ; public : Foo ( const std :: string & filename ); virtual ~ Foo (); virtual void doOnChange (); void start (); void quit (); }; #include \"Foo.h\" #include <functional> #include <iostream> Foo :: Foo ( const std :: string & filename ) : thread (), running ( ATOMIC_FLAG_INIT ) { file = filename ; native = 0 ; } Foo ::~ Foo () { quit (); } void Foo :: start () { running . test_and_set (); try { thread = std :: thread ( & Foo :: run , this ); } catch (...) { running . clear (); throw ; } native = thread . native_handle (); } void Foo :: quit () { running . clear (); pthread_cancel ( native ); pthread_join ( native , nullptr ); //c++11-style not working here /*if (thread.joinable()) { thread.join(); thread.detach(); }*/ } void Foo :: run () { while ( running . test_and_set ()) { numRead = read ( fd , buf , BUF_LEN ); ..... bla bla bla ....... } } I'm trying to quit from this thread in my program cleanup code. Using pthread works but I'm wondering if I can do something better with c++11 only (no native handle). It seems to me there's no good way to handle all cases using c++11 code. As you can see here the thread is blocked on a read system call. So even if I clear the flag the thread will be still blocked and join call will block forever. So what I really need is an interrupt (in this case pthread_cancel ). But if I call pthread_cancel I can't call anymore the c++11 join() method because it fails, I can only call pthread_join() . So it seems the standard has a really big limitation, am I miss anything? NOTE: read \u662f\u963b\u585eIO\uff0c\u53ef\u80fd\u6c38\u8fdc\u5730\u5c06thread\u963b\u585e Edit: After discussion below I changed the Foo class implementation replacing std::atomic_flag with std::atomic and using signal handler . I used the signal handler because in my opinion is better to have a general base class, using the self-pipe trick is too hard in a base class, the logic should be delegated to the child. Final implementation: #include <thread> #include <atomic> class Foo { private : std :: thread thread ; void mainFoo (); std :: atomic < bool > running ; std :: string name ; std :: thread :: native_handle_type native ; static void signalHandler ( int signal ); void run (); public : Thread ( const std :: string & name ); virtual ~ Thread (); void start (); void quit (); void interrupt (); void join (); void detach (); const std :: string & getName () const ; bool isRunning () const ; }; Cpp file: #include <functional> #include <fcntl.h> #include <limits.h> #include <stdio.h> #include <stdlib.h> #include <unistd.h> #include <sys/stat.h> #include <sys/types.h> #include <sys/inotify.h> #include <Foo.h> #include <csignal> #include <iostream> Foo :: Foo ( const std :: string & name ) : name ( name ) { running = false ; native = 0 ; this -> name . resize ( 16 , '\\0' ); } Foo ::~ Foo () { } void Foo :: start () { running = true ; try { thread = std :: thread ( & Foo :: mainFoo , this ); } catch (...) { running = false ; throw ; } native = thread . native_handle (); pthread_setname_np ( native , name . c_str ()); } void Foo :: quit () { if ( running ) { running = false ; pthread_kill ( native , SIGINT ); //\u5411\u81ea\u5df1\u53d1\u9001\u4fe1\u53f7 if ( thread . joinable ()) { thread . join (); } } } void Foo :: mainFoo () { //enforce POSIX semantics siginterrupt ( SIGINT , true ); std :: signal ( SIGINT , signalHandler ); run (); running = false ; } void Foo :: join () { if ( thread . joinable ()) thread . join (); } void Foo :: signalHandler ( int signal ) { } void Foo :: interrupt () { pthread_kill ( native , SIGINT ); } void Foo :: detach () { if ( thread . joinable ()) thread . detach (); } const std :: string & Foo :: getName () const { return name ; } bool Foo :: isRunning () const { return running ; } void Foo :: run () { while ( isRunning ()) { num = read (.....); //if read is interrupted loop again, this time //isRunning() will return false } } COMMENTS : SUMMARY : \u6839\u636e\u4e0b\u9762\u7684\u8ba8\u8bba\uff0c\u4f5c\u8005\u7684\u8fd9\u4e2a\u95ee\u9898\u53ef\u4ee5\u603b\u7ed3\u4e3a\uff1a non-cooperatively kill a single thread in a multi-thread program \u548c non-cooperative thread cancellation Forcibly \"killing\" a thread is never a good idea, as the thread will not be able to release any possible resources it might have allocated. If you need to be able to ask a thread to exit before it's end, then consider using e.g. non-blocking I/O or similar. \u2013 Some programmer dude Aug 8 '18 at 8:40 There are some platforms (MS Windows) where terminating a thread will leave your application in an unstable state . This is documented by MS. Simple example: thread is holding the (internal) C++ heap lock when it gets terminated - now you have no heap. \u2013 Richard Critten Aug 8 '18 at 8:41 @Some programmer dude It's actually possible to call a cleanup handler using pthread, not in my example but possible \u2013 greywolf82 Aug 8 '18 at 8:41 It's still not enough. Lets take this hypothetical case: You allocate objects in a loop in the thread with new . The allocation have succeeded, but the assignment to the pointer haven't been done yet. Then the thread is killed, and you leak the object (not to mention the possible UB (undefined behavior) if the pointer is not initialized). Unless you can synchronize the thread and the killing of it there will always be chances of leaks and UB. And if you can synchronize the killing of the thread, why forcibly kill it in the first place instead of letting it clean up itself? \u2013 Some programmer dude Aug 8 '18 at 8:51 SUMMARY : \u8fd9\u6bb5\u4e2d\u6240\u5217\u4e3e\u7684\u4f8b\u5b50\u975e\u5e38\u597d\uff0c\u5b83\u5426\u5b9a\u4e86\u4f7f\u7528cleanup handler\u548ckill thread So the reply to my question is: using c++11 (maybe even without it) don't call any blocking system call in the thread. It seems a big limitation to me. \u2013 greywolf82 Aug 8 '18 at 8:54 May be helpful: stackoverflow.com/a/12207835/5376789 \u2013 xskxzr Aug 8 '18 at 8:58 @xskxzr I know there is no portable way in C++11 to non-cooperatively kill a single thread in a multi-thread program, but I'm wondering how to handle a case where the thread is blocked in IO and the response seems to be: never use blocking io in a thread . \u2013 greywolf82 Aug 8 '18 at 9:01 Don't block . Look into \"readsome\", \"in_avail\", and related functions. \u2013 Jive Dadson Aug 8 '18 at 9:03 SUMMARY : readsome in_avail @Someprogrammerdude Just to add info here: if you use a deferred cancellation point , the thread will be canceled when calling the next cancellation point , not immediately. So in your example and with this configuration no leak can happen because the thread won't be stopped during allocation but only to the next cancellation point. \u2013 greywolf82 Aug 8 '18 at 9:06 SUMMARY : pthread_cancel\u4e5f\u662f\u4e00\u79cd\u89e3\u6cd5 Yup, non-blocking read . Its the only clean solution imo. \u2013 Galik Aug 8 '18 at 9:21 Just a side note: The use of ATOMIC_FLAG_INIT in a (default) member initializer seems to be unspecified since C++14: stackoverflow.com/questions/24437396/\u2026 . My suggestion: Use std::atomic<bool> . \u2013 Julius Aug 8 '18 at 9:37 @greywolf82 to add to the reasons to avoid non-cooperative thread cancellation in C++: on GNU/Linux pthread_cancel is implemented with a magic unstoppable exception, so if any function in the thread's call stack is noexcept then pthread_cancel will terminate the whole process, and you can't prevent it. \u2013 Jonathan Wakely Aug 8 '18 at 10:47 If on Linux or POSIX learn about poll(2) (you'll call it before the blocking read) \u2013 Basile Starynkevitch Aug 9 '18 at 10:26 That's right, don't use any indefinitely(\u4e0d\u786e\u5b9a\u7684\uff0c\u65e0\u9650\u7684) blocking system calls, with threads or without. \u2013 n.m. Aug 9 '18 at 10:33 system call with max blocking time \u4eca\u5929\u5728\u9605\u8bfb\u300aredis\u8bbe\u8ba1\u4e0e\u5b9e\u73b0\u300b\u8fd9\u672c\u4e66\u7684\u7b2c12.3\u8282\u300a\u4e8b\u4ef6\u7684\u8c03\u5ea6\u4e0e\u6267\u884c\u300b\u7684\u65f6\u5019\uff0c\u4f5c\u8005\u6240\u7ed9\u51fa\u7684\u4f2a\u4ee3\u7801\u8ba9\u6211\u7a81\u7136\u610f\u8bc6\u5230max blocking time\u53c2\u6570\u7684\u91cd\u8981\u4ef7\u503c\uff0c\u5728youdao \u300a redis\u8bbe\u8ba1\u4e0e\u5b9e\u73b0-\u7b2c12\u7ae0-\u4e8b\u4ef6.md \u300b\u4e2d\u5bf9\u8fd9\u4e2a\u8fdb\u884c\u4e86\u603b\u7ed3\uff0c\u6211\u89c9\u5f97\u6709\u5fc5\u8981\u5c06\u5176copy\u8fc7\u6765\uff1a def aeProcessEvents: # \u83b7\u5f97\u5230\u8fbe\u65f6\u95f4\u79bb\u5f53\u524d\u65f6\u95f4\u6700\u63a5\u8fd1\u7684\u65f6\u95f4\u4e8b\u4ef6 time_event = aeSearchNearestTimer() # \u8ba1\u7b97\u6700\u63a5\u8fd1\u7684\u65f6\u95f4\u4e8b\u4ef6\u8ddd\u79bb\u5230\u8fbe\u8fd8\u6709\u591a\u5c11\u6beb\u79d2 remained_ms = time_event.when - unix_ts_now() # \u5982\u679c\u65f6\u95f4\u5df2\u7ecf\u5230\u8fbe\uff0c\u90a3\u4e48remained_ms\u5219\u53ef\u80fd\u4e3a\u8d1f\u6570\uff0c\u5c06\u5b83\u8bbe\u7f6e\u4e3a0 if remained_ms < 0: remained_ms = 0 # \u6839\u636eremained_ms\u7684\u503c\uff0c\u521b\u5efatimeval\u7ed3\u6784 timeval = create_timeval_with_ms(remained_ms) # \u963b\u585e\u5e76\u7b49\u5f85\u6587\u4ef6\u4e8b\u4ef6\u4ea7\u751f\uff0c\u6700\u5927\u963b\u585e\u65f6\u95f4\u7531\u4f20\u5165\u7684timeval\u7ed3\u6784\u51b3\u5b9a # \u5982\u679cremained_ms\u7684\u503c\u4e3a0\uff0c\u90a3\u4e48aeApiPoll\u8c03\u7528\u4e4b\u540e\u9a6c\u4e0a\u8fd4\u56de\uff0c\u4e0d\u963b\u585e aeApiPoll(timeval) # \u5904\u7406\u6240\u6709\u5df2\u7ecf\u4ea7\u751f\u7684\u6587\u4ef6\u4e8b\u4ef6 processFileEvents() # \u5904\u7406\u6240\u6709\u5df2\u7ecf\u5230\u8fbe\u7684\u65f6\u95f4\u4e8b\u4ef6 processTimeEvents() Unix\u6240\u63d0\u4f9b\u7684\u5f88\u591asystem call\uff0c\u5982\u679c\u6d89\u53ca\u5230\u963b\u585e\u6267\u884c\u7684\u7ebf\u7a0b\uff0c\u5f88\u591a\u90fd\u4f1a\u63d0\u4f9b\u4e00\u4e2a timeout \u6765\u4f5c\u4e3a\u53c2\u6570\uff0c\u8868\u793a\u6700\u5927\u963b\u585e\u65f6\u95f4\uff1b\u73b0\u5728\u60f3\u6765\u8fd9\u4e2a timeout \u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u5982\u679c\u4e0d\u63d0\u4f9b\u8fd9\u4e2a timeout \u53c2\u6570\uff0c\u8fd9\u4e9bsystem call\u5c31\u5b58\u5728\u5c06\u6267\u884c\u5b83\u7684thread\u6c38\u8fdc\u5730\u963b\u585e\u4e0b\u53bb\u7684\u53ef\u80fd\u6027\uff0c\u4e00\u65e6\u51fa\u73b0\u4e86\u8fd9\u79cd\u60c5\u51b5\uff0c\u5219\u662f\u6bd4\u8f83\u4e0d\u597d\u641e\u7684\uff0c\u8fd9\u4e2a\u95ee\u9898\u5728youdao\u7684\u300a Unix-thread-blocked.md \u300b\u8fdb\u884c\u4e86\u603b\u7ed3\uff1b\u73b0\u5728\u60f3\u6765\uff0c\u5f53\u6211\u4e0d\u77e5\u9053\u8fd9\u4e2a\u53c2\u6570\u7684\u7528\u9014\u7684\u65f6\u5019\uff0c\u5f80\u5f80\u5bb9\u6613\u5ffd\u89c6\u8fd9\u4e2a\u53c2\u6570\uff0c\u5176\u5b9e\u8fd9\u4e9b\u53c2\u6570\u7684\u4ef7\u503c\u662f\u975e\u5e38\u5927\u7684\uff1b \u8fd8\u6709\uff0c\u63d0\u4f9b\u4e00\u4e2a timeout \u53c2\u6570\uff0c\u5176\u5b9e\u80fd\u591f\u8fbe\u5230\u4e00\u7bad\u53cc\u96d5\u7684\u6548\u679c\uff0c\u7b2c\u4e00\u96d5\u5df2\u7ecf\u5728\u4e0a\u4e00\u6bb5\u4e2d\u63cf\u8ff0\u4e86\uff0c\u5373\u9632\u6b62\u6c38\u8fdc\u5730\u963b\u585ethread\uff0c\u7b2c\u4e8c\u96d5\u5219\u662f\u63d0\u4f9b\u4e86\u5b9a\u65f6\u7684\u529f\u80fd\uff0c\u5c24\u5176\u5230\u6211\u4eec\u7684\u7a0b\u5e8f\u4e2d\u6d89\u53ca\u5230timer\u7684\u65f6\u5019\uff0c\u8fd9\u4e2a\u529f\u80fd\u7684\u4ef7\u503c\u662f\u6bd4\u8f83\u9ad8\u7684\uff1b\u8fd9\u8ba9\u6211\u60f3\u8d77\u4e86\u4f7f\u7528c++\u7684thread library\u7684 wait_until \u6765\u907f\u514d sleep \u5e26\u6765\u7684\u53ef\u80fd\u5bfc\u81f4\u65e0\u6cd5\u5c06thead\u968f\u65f6\u5524\u9192\u7684\u95ee\u9898\uff1b system call with max blocking time http://www.cs.um.edu.mt/~ssrg/AThesis.pdf self-pipe Interrupted System Calls \u4f7f\u7528signal\u4e86\u6765interrupte System Calls\uff0c\u5982\u679cprocess\u6b63blocked \u5728\u4e00\u4e2alow system call\uff0c\u8fd9\u79cd\u65b9\u6cd5\u662f\u6bd4\u8f83\u597d\u7684\uff1b\u4e0a\u9762\u6240\u91c7\u7528\u7684\u5c31\u662f\u8fd9\u79cd\u65b9\u6cd5\uff1b celery\u505c\u6b62\u7684\u65f6\u5019\u4e5f\u662f\u91c7\u7528\u7684\u8fd9\u79cd\u65b9\u6cd5\uff0ccelery\u6240\u53d1\u9001\u7684\u662fterm\uff1b","title":"Introduction"},{"location":"Multithread/Topics/Stop-a-blocked-thread/#how#to#stop#a#blocked#thread","text":"\u4eca\u5929\u5728\u4f7f\u7528 redis-plus-plus \u7684 pub/sub \u529f\u80fd\u7684\u65f6\u5019\uff0c\u5176\u6587\u6863\u4e2d\u6709\u8fd9\u6837\u7684\u63cf\u8ff0\uff1a Subscriber::consume waits for message from the underlying connection. If the ConnectionOptions::socket_timeout is reached, and there's no message sent to this connection, Subscriber::consume throws a TimeoutError exception. If ConnectionOptions::socket_timeout is 0ms , Subscriber::consume blocks until it receives a message. \u5728\u6211\u4eec\u7684application\u4e2d\uff0c\u6211\u4f7f\u7528\u4e00\u4e2athread\u6765\u6267\u884c Subscriber::consume \uff0c\u8fd9\u610f\u5473\u4e2d\u5728\u6ca1\u6709message\u7684\u65f6\u5019\u6211\u7684thread\u5c06block\uff1b\u90a3\u5bf9\u4e8e\u4e00\u4e2ablocked\u7684thread\uff0c\u662f\u5426\u4f1a\u5b58\u5728\u548csleeping thread\u4e00\u6837\u7684stop\u7684\u95ee\u9898\u5462\uff1f","title":"how to stop a blocked thread"},{"location":"Multithread/Topics/Stop-a-blocked-thread/#stackoverflow#terminate#thread#c11#blocked#on#read","text":"I've got the following code: class Foo { private : std :: thread thread ; void run (); std :: atomic_flag running ; std :: thread :: native_handle_type native ; public : Foo ( const std :: string & filename ); virtual ~ Foo (); virtual void doOnChange (); void start (); void quit (); }; #include \"Foo.h\" #include <functional> #include <iostream> Foo :: Foo ( const std :: string & filename ) : thread (), running ( ATOMIC_FLAG_INIT ) { file = filename ; native = 0 ; } Foo ::~ Foo () { quit (); } void Foo :: start () { running . test_and_set (); try { thread = std :: thread ( & Foo :: run , this ); } catch (...) { running . clear (); throw ; } native = thread . native_handle (); } void Foo :: quit () { running . clear (); pthread_cancel ( native ); pthread_join ( native , nullptr ); //c++11-style not working here /*if (thread.joinable()) { thread.join(); thread.detach(); }*/ } void Foo :: run () { while ( running . test_and_set ()) { numRead = read ( fd , buf , BUF_LEN ); ..... bla bla bla ....... } } I'm trying to quit from this thread in my program cleanup code. Using pthread works but I'm wondering if I can do something better with c++11 only (no native handle). It seems to me there's no good way to handle all cases using c++11 code. As you can see here the thread is blocked on a read system call. So even if I clear the flag the thread will be still blocked and join call will block forever. So what I really need is an interrupt (in this case pthread_cancel ). But if I call pthread_cancel I can't call anymore the c++11 join() method because it fails, I can only call pthread_join() . So it seems the standard has a really big limitation, am I miss anything? NOTE: read \u662f\u963b\u585eIO\uff0c\u53ef\u80fd\u6c38\u8fdc\u5730\u5c06thread\u963b\u585e Edit: After discussion below I changed the Foo class implementation replacing std::atomic_flag with std::atomic and using signal handler . I used the signal handler because in my opinion is better to have a general base class, using the self-pipe trick is too hard in a base class, the logic should be delegated to the child. Final implementation: #include <thread> #include <atomic> class Foo { private : std :: thread thread ; void mainFoo (); std :: atomic < bool > running ; std :: string name ; std :: thread :: native_handle_type native ; static void signalHandler ( int signal ); void run (); public : Thread ( const std :: string & name ); virtual ~ Thread (); void start (); void quit (); void interrupt (); void join (); void detach (); const std :: string & getName () const ; bool isRunning () const ; }; Cpp file: #include <functional> #include <fcntl.h> #include <limits.h> #include <stdio.h> #include <stdlib.h> #include <unistd.h> #include <sys/stat.h> #include <sys/types.h> #include <sys/inotify.h> #include <Foo.h> #include <csignal> #include <iostream> Foo :: Foo ( const std :: string & name ) : name ( name ) { running = false ; native = 0 ; this -> name . resize ( 16 , '\\0' ); } Foo ::~ Foo () { } void Foo :: start () { running = true ; try { thread = std :: thread ( & Foo :: mainFoo , this ); } catch (...) { running = false ; throw ; } native = thread . native_handle (); pthread_setname_np ( native , name . c_str ()); } void Foo :: quit () { if ( running ) { running = false ; pthread_kill ( native , SIGINT ); //\u5411\u81ea\u5df1\u53d1\u9001\u4fe1\u53f7 if ( thread . joinable ()) { thread . join (); } } } void Foo :: mainFoo () { //enforce POSIX semantics siginterrupt ( SIGINT , true ); std :: signal ( SIGINT , signalHandler ); run (); running = false ; } void Foo :: join () { if ( thread . joinable ()) thread . join (); } void Foo :: signalHandler ( int signal ) { } void Foo :: interrupt () { pthread_kill ( native , SIGINT ); } void Foo :: detach () { if ( thread . joinable ()) thread . detach (); } const std :: string & Foo :: getName () const { return name ; } bool Foo :: isRunning () const { return running ; } void Foo :: run () { while ( isRunning ()) { num = read (.....); //if read is interrupted loop again, this time //isRunning() will return false } } COMMENTS : SUMMARY : \u6839\u636e\u4e0b\u9762\u7684\u8ba8\u8bba\uff0c\u4f5c\u8005\u7684\u8fd9\u4e2a\u95ee\u9898\u53ef\u4ee5\u603b\u7ed3\u4e3a\uff1a non-cooperatively kill a single thread in a multi-thread program \u548c non-cooperative thread cancellation Forcibly \"killing\" a thread is never a good idea, as the thread will not be able to release any possible resources it might have allocated. If you need to be able to ask a thread to exit before it's end, then consider using e.g. non-blocking I/O or similar. \u2013 Some programmer dude Aug 8 '18 at 8:40 There are some platforms (MS Windows) where terminating a thread will leave your application in an unstable state . This is documented by MS. Simple example: thread is holding the (internal) C++ heap lock when it gets terminated - now you have no heap. \u2013 Richard Critten Aug 8 '18 at 8:41 @Some programmer dude It's actually possible to call a cleanup handler using pthread, not in my example but possible \u2013 greywolf82 Aug 8 '18 at 8:41 It's still not enough. Lets take this hypothetical case: You allocate objects in a loop in the thread with new . The allocation have succeeded, but the assignment to the pointer haven't been done yet. Then the thread is killed, and you leak the object (not to mention the possible UB (undefined behavior) if the pointer is not initialized). Unless you can synchronize the thread and the killing of it there will always be chances of leaks and UB. And if you can synchronize the killing of the thread, why forcibly kill it in the first place instead of letting it clean up itself? \u2013 Some programmer dude Aug 8 '18 at 8:51 SUMMARY : \u8fd9\u6bb5\u4e2d\u6240\u5217\u4e3e\u7684\u4f8b\u5b50\u975e\u5e38\u597d\uff0c\u5b83\u5426\u5b9a\u4e86\u4f7f\u7528cleanup handler\u548ckill thread So the reply to my question is: using c++11 (maybe even without it) don't call any blocking system call in the thread. It seems a big limitation to me. \u2013 greywolf82 Aug 8 '18 at 8:54 May be helpful: stackoverflow.com/a/12207835/5376789 \u2013 xskxzr Aug 8 '18 at 8:58 @xskxzr I know there is no portable way in C++11 to non-cooperatively kill a single thread in a multi-thread program, but I'm wondering how to handle a case where the thread is blocked in IO and the response seems to be: never use blocking io in a thread . \u2013 greywolf82 Aug 8 '18 at 9:01 Don't block . Look into \"readsome\", \"in_avail\", and related functions. \u2013 Jive Dadson Aug 8 '18 at 9:03 SUMMARY : readsome in_avail @Someprogrammerdude Just to add info here: if you use a deferred cancellation point , the thread will be canceled when calling the next cancellation point , not immediately. So in your example and with this configuration no leak can happen because the thread won't be stopped during allocation but only to the next cancellation point. \u2013 greywolf82 Aug 8 '18 at 9:06 SUMMARY : pthread_cancel\u4e5f\u662f\u4e00\u79cd\u89e3\u6cd5 Yup, non-blocking read . Its the only clean solution imo. \u2013 Galik Aug 8 '18 at 9:21 Just a side note: The use of ATOMIC_FLAG_INIT in a (default) member initializer seems to be unspecified since C++14: stackoverflow.com/questions/24437396/\u2026 . My suggestion: Use std::atomic<bool> . \u2013 Julius Aug 8 '18 at 9:37 @greywolf82 to add to the reasons to avoid non-cooperative thread cancellation in C++: on GNU/Linux pthread_cancel is implemented with a magic unstoppable exception, so if any function in the thread's call stack is noexcept then pthread_cancel will terminate the whole process, and you can't prevent it. \u2013 Jonathan Wakely Aug 8 '18 at 10:47 If on Linux or POSIX learn about poll(2) (you'll call it before the blocking read) \u2013 Basile Starynkevitch Aug 9 '18 at 10:26 That's right, don't use any indefinitely(\u4e0d\u786e\u5b9a\u7684\uff0c\u65e0\u9650\u7684) blocking system calls, with threads or without. \u2013 n.m. Aug 9 '18 at 10:33","title":"stackoverflow Terminate thread c++11 blocked on read"},{"location":"Multithread/Topics/Stop-a-blocked-thread/#system#call#with#max#blocking#time","text":"\u4eca\u5929\u5728\u9605\u8bfb\u300aredis\u8bbe\u8ba1\u4e0e\u5b9e\u73b0\u300b\u8fd9\u672c\u4e66\u7684\u7b2c12.3\u8282\u300a\u4e8b\u4ef6\u7684\u8c03\u5ea6\u4e0e\u6267\u884c\u300b\u7684\u65f6\u5019\uff0c\u4f5c\u8005\u6240\u7ed9\u51fa\u7684\u4f2a\u4ee3\u7801\u8ba9\u6211\u7a81\u7136\u610f\u8bc6\u5230max blocking time\u53c2\u6570\u7684\u91cd\u8981\u4ef7\u503c\uff0c\u5728youdao \u300a redis\u8bbe\u8ba1\u4e0e\u5b9e\u73b0-\u7b2c12\u7ae0-\u4e8b\u4ef6.md \u300b\u4e2d\u5bf9\u8fd9\u4e2a\u8fdb\u884c\u4e86\u603b\u7ed3\uff0c\u6211\u89c9\u5f97\u6709\u5fc5\u8981\u5c06\u5176copy\u8fc7\u6765\uff1a def aeProcessEvents: # \u83b7\u5f97\u5230\u8fbe\u65f6\u95f4\u79bb\u5f53\u524d\u65f6\u95f4\u6700\u63a5\u8fd1\u7684\u65f6\u95f4\u4e8b\u4ef6 time_event = aeSearchNearestTimer() # \u8ba1\u7b97\u6700\u63a5\u8fd1\u7684\u65f6\u95f4\u4e8b\u4ef6\u8ddd\u79bb\u5230\u8fbe\u8fd8\u6709\u591a\u5c11\u6beb\u79d2 remained_ms = time_event.when - unix_ts_now() # \u5982\u679c\u65f6\u95f4\u5df2\u7ecf\u5230\u8fbe\uff0c\u90a3\u4e48remained_ms\u5219\u53ef\u80fd\u4e3a\u8d1f\u6570\uff0c\u5c06\u5b83\u8bbe\u7f6e\u4e3a0 if remained_ms < 0: remained_ms = 0 # \u6839\u636eremained_ms\u7684\u503c\uff0c\u521b\u5efatimeval\u7ed3\u6784 timeval = create_timeval_with_ms(remained_ms) # \u963b\u585e\u5e76\u7b49\u5f85\u6587\u4ef6\u4e8b\u4ef6\u4ea7\u751f\uff0c\u6700\u5927\u963b\u585e\u65f6\u95f4\u7531\u4f20\u5165\u7684timeval\u7ed3\u6784\u51b3\u5b9a # \u5982\u679cremained_ms\u7684\u503c\u4e3a0\uff0c\u90a3\u4e48aeApiPoll\u8c03\u7528\u4e4b\u540e\u9a6c\u4e0a\u8fd4\u56de\uff0c\u4e0d\u963b\u585e aeApiPoll(timeval) # \u5904\u7406\u6240\u6709\u5df2\u7ecf\u4ea7\u751f\u7684\u6587\u4ef6\u4e8b\u4ef6 processFileEvents() # \u5904\u7406\u6240\u6709\u5df2\u7ecf\u5230\u8fbe\u7684\u65f6\u95f4\u4e8b\u4ef6 processTimeEvents() Unix\u6240\u63d0\u4f9b\u7684\u5f88\u591asystem call\uff0c\u5982\u679c\u6d89\u53ca\u5230\u963b\u585e\u6267\u884c\u7684\u7ebf\u7a0b\uff0c\u5f88\u591a\u90fd\u4f1a\u63d0\u4f9b\u4e00\u4e2a timeout \u6765\u4f5c\u4e3a\u53c2\u6570\uff0c\u8868\u793a\u6700\u5927\u963b\u585e\u65f6\u95f4\uff1b\u73b0\u5728\u60f3\u6765\u8fd9\u4e2a timeout \u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u5982\u679c\u4e0d\u63d0\u4f9b\u8fd9\u4e2a timeout \u53c2\u6570\uff0c\u8fd9\u4e9bsystem call\u5c31\u5b58\u5728\u5c06\u6267\u884c\u5b83\u7684thread\u6c38\u8fdc\u5730\u963b\u585e\u4e0b\u53bb\u7684\u53ef\u80fd\u6027\uff0c\u4e00\u65e6\u51fa\u73b0\u4e86\u8fd9\u79cd\u60c5\u51b5\uff0c\u5219\u662f\u6bd4\u8f83\u4e0d\u597d\u641e\u7684\uff0c\u8fd9\u4e2a\u95ee\u9898\u5728youdao\u7684\u300a Unix-thread-blocked.md \u300b\u8fdb\u884c\u4e86\u603b\u7ed3\uff1b\u73b0\u5728\u60f3\u6765\uff0c\u5f53\u6211\u4e0d\u77e5\u9053\u8fd9\u4e2a\u53c2\u6570\u7684\u7528\u9014\u7684\u65f6\u5019\uff0c\u5f80\u5f80\u5bb9\u6613\u5ffd\u89c6\u8fd9\u4e2a\u53c2\u6570\uff0c\u5176\u5b9e\u8fd9\u4e9b\u53c2\u6570\u7684\u4ef7\u503c\u662f\u975e\u5e38\u5927\u7684\uff1b \u8fd8\u6709\uff0c\u63d0\u4f9b\u4e00\u4e2a timeout \u53c2\u6570\uff0c\u5176\u5b9e\u80fd\u591f\u8fbe\u5230\u4e00\u7bad\u53cc\u96d5\u7684\u6548\u679c\uff0c\u7b2c\u4e00\u96d5\u5df2\u7ecf\u5728\u4e0a\u4e00\u6bb5\u4e2d\u63cf\u8ff0\u4e86\uff0c\u5373\u9632\u6b62\u6c38\u8fdc\u5730\u963b\u585ethread\uff0c\u7b2c\u4e8c\u96d5\u5219\u662f\u63d0\u4f9b\u4e86\u5b9a\u65f6\u7684\u529f\u80fd\uff0c\u5c24\u5176\u5230\u6211\u4eec\u7684\u7a0b\u5e8f\u4e2d\u6d89\u53ca\u5230timer\u7684\u65f6\u5019\uff0c\u8fd9\u4e2a\u529f\u80fd\u7684\u4ef7\u503c\u662f\u6bd4\u8f83\u9ad8\u7684\uff1b\u8fd9\u8ba9\u6211\u60f3\u8d77\u4e86\u4f7f\u7528c++\u7684thread library\u7684 wait_until \u6765\u907f\u514d sleep \u5e26\u6765\u7684\u53ef\u80fd\u5bfc\u81f4\u65e0\u6cd5\u5c06thead\u968f\u65f6\u5524\u9192\u7684\u95ee\u9898\uff1b","title":"system call with max blocking time"},{"location":"Multithread/Topics/Stop-a-blocked-thread/#system#call#with#max#blocking#time_1","text":"http://www.cs.um.edu.mt/~ssrg/AThesis.pdf","title":"system call with max blocking time"},{"location":"Multithread/Topics/Stop-a-blocked-thread/#self-pipe","text":"","title":"self-pipe"},{"location":"Multithread/Topics/Stop-a-blocked-thread/#interrupted#system#calls","text":"\u4f7f\u7528signal\u4e86\u6765interrupte System Calls\uff0c\u5982\u679cprocess\u6b63blocked \u5728\u4e00\u4e2alow system call\uff0c\u8fd9\u79cd\u65b9\u6cd5\u662f\u6bd4\u8f83\u597d\u7684\uff1b\u4e0a\u9762\u6240\u91c7\u7528\u7684\u5c31\u662f\u8fd9\u79cd\u65b9\u6cd5\uff1b celery\u505c\u6b62\u7684\u65f6\u5019\u4e5f\u662f\u91c7\u7528\u7684\u8fd9\u79cd\u65b9\u6cd5\uff0ccelery\u6240\u53d1\u9001\u7684\u662fterm\uff1b","title":"Interrupted System Calls"},{"location":"Multithread/Topics/Stop-a-sleeping-thread/","text":"\u5982\u4f55\u4e2d\u65ad\u4e00\u4e2a\u6b63\u5728sleeping\u7684thread Summary \u603b\u7684\u6765\u8bf4\uff0c\u76ee\u524d\u77e5\u9053\u7684\u662f\u4e24\u79cd\u65b9\u6848\uff0c\u8fd9\u4e24\u79cd\u65b9\u6848\u5728 stackoverflow Stopping long-sleep threads \u4e2d\u90fd\u63d0\u51fa\u6765\u4e86\u3002 stackoverflow how to terminate a sleeping thread in pthread? I have thread which sleeps for a long time, then wakes up to do something, then sleep again, like this: while ( some_condition ) { // do something sleep ( 1000 ); } How could I make this thread exit gracefully and QUICKLY? I tried to use pthread_cancel() , but sleeping threads could not be canceled. I also tried changing the condition of the while loop, but it will still take long to exit. And I don't want to use pthread_kill() , since it may kill the thread when it's working. So, are there any good ideas? COMMENTS well, i think i make a mistake, sleep() is a cancel point difined by posix.1 \u2013 qiuxiafei Jan 24 '11 at 3:40 Just a note - pthread_kill does not kill a thread. It sends a signal to a thread . If the action of that signal is to terminate the process (e.g. the default action of SIGTERM , or the unblockable action of SIGKILL ) then it will terminate the whole process , not the target thread. Really the only time pthread_kill is useful is if you've installed an interrupting signal handler , and you want to interrupt a syscall that's blocked in a particular thread (which would be a potential solution to your question). \u2013 R.. May 18 '11 at 23:11 Why did you say that the thread could not exit quickly ? I tried this and the thread canceled immediately upon thread_cancel . And I was able to pthread_join without delay. \u2013 user1502776 Jun 7 at 8:15 NOTE: \u5173\u4e8e pthread_cancel \u53c2\u89c1 pthread_cancel \uff0c\u5176\u4e2d\u63d0\u53ca sleep \u662f\u4e00\u4e2acancel point A As an alternative to sleep , you could use pthread_cond_timedwait with a 1000 ms timeout. Then when you want to exit, signal the condition variable. This is similar to how you might do this in C#/Java using wait and notify. COMMENTS Correct. This is the simple and non-racy way to do it. \u2013 caf Jan 24 '11 at 4:04 This is correct, but cancellation is easier and perhaps slightly more efficient. Of course if you call pthread_cancel on the thread, you need to make sure that it can't get cancelled at a point it shouldn't. Calling pthread_setcancelstate to disable cancellation when the thread starts, and only enabling it just before the sleep , would be a safe approach. \u2013 R.. May 18 '11 at 23:10 IMO, pthread_cancel() may be preferable since sleep() is already a cancellation point. And be sure to call pthread_cleanup_push() and pthread_cleanup_pop() to set up cleanup handlers if you have allocated resources. \u2013 zeekvfu Oct 22 '13 at 4:45 What is the difference between using pthread_cond_timedwait with 1000ms timeout and using a flag variable with 100ms sleep in regard to race condition ? \u2013 user1502776 Jun 5 at 5:05 stackoverflow How can I kill a thread? without using stop(); I'm wanting a reasonably reliable threaded timer, so I've written a timer object that fires a std::function on a thread. I would like to give this timer the ability to stop before it gets to the next tick; something you can't do with ::sleep (at least I don't think you can). So what I've done is put a condition variable on a mutex. If the condition times out, I fire the event. If the condition is signalled the thread is exited\uff08\u4e00\u4e2acondition variable\u517c\u5177\u4e24\u79cd\u529f\u80fd\uff09. So the Stop method needs to be able to get the thread to stop and/or interrupt its wait, which I think is what it's doing right now. There are problems with this however. Sometimes the thread isn't joinable() and sometimes the condition is signalled after its timeout but before it's put into its wait state. How can I improve this and make it robust? The following is a full repo. The wait is 10 seconds here but the program should terminate immediately as the Foo is created and then immediately destroyed. It does sometimes but mostly it does not. #include <atomic> #include <thread> #include <future> #include <sstream> #include <chrono> #include <iostream> class Timer { public : Timer () {} ~ Timer () { Stop (); } void Start ( std :: chrono :: milliseconds const & interval , std :: function < void ( void ) > const & callback ) { Stop (); thread = std :: thread ([ = ]() { for (;;) { auto locked = std :: unique_lock < std :: mutex > ( mutex ); auto result = terminate . wait_for ( locked , interval ); if ( result == std :: cv_status :: timeout ) { callback (); } else { return ; } } }); } void Stop () { terminate . notify_one (); if ( thread . joinable ()) { thread . join (); } } private : std :: thread thread ; std :: mutex mutex ; std :: condition_variable terminate ; }; class Foo { public : Foo () { timer = std :: make_unique < Timer > (); timer -> Start ( std :: chrono :: milliseconds ( 10000 ), std :: bind ( & Foo :: Callback , this )); } ~ Foo () { } void Callback () { static int count = 0 ; std :: ostringstream o ; std :: cout << count ++ << std :: endl ; } std :: unique_ptr < Timer > timer ; }; int main ( void ) { { Foo foo ; } return 0 ; } Somewhere you need a variable protected by the mutex that stores whether or not the thread is supposed to stop. Condition variables are stateless -- it is your responsibility to maintain the state of the thing you're waiting for (called the \"predicate\"). Basically, you've missed the point of condition variables . Your Stop function notifies the thread. But it hasn't changed any condition that the thread is waiting for! Notice that the mutex doesn't protect anything. This is just totally wrong. \u2013 David Schwartz Sep 21 '15 at 10:44 Why return on a spurious wakeup? (Same problem. You have no way to know whether you should wakeup or not because you have no predicate to check.) \u2013 David Schwartz Sep 21 '15 at 11:03 NOTE: \u4e0a\u8ff0\u5e76\u4e0d\u6d89\u53ca**condition**\u7684\u4fee\u6539\uff1b\u4f7f\u7528condition variable\u7684\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u76ee\u7684\u662f\u4ec5\u4ec5\u5f53condition\u6ee1\u8db3\u7684\u65f6\u5019\u624d\u8fd0\u884cthread\u7ee7\u7eed\u6267\u884c\uff0c\u800c\u5f53 Spurious wakeup \u7684\u65f6\u5019\uff0c\u5219\u4e0d\u80fd\u591f\u5141\u8bb8\u5b83\u5141\u8bb8\uff0c\u6240\u4ee5\u624d\u9700\u8981\u4e00\u4e2acondition\uff1b\u800c\u4e0a\u8ff0\u4ee3\u7801\u5219\u663e\u7136\u6ca1\u6709\u8fd9\u6837\u505a\uff0c\u5982\u4e0b\u9762\u7684\u8bc4\u8bba\u6240\u8ff0\uff0c\u5b83\u5b58\u5728\u7740\u8fd9\u6837\u7684\u4e00\u4e2a\u98ce\u9669\uff1a return on a spurious wakeup What other kinds of wakeup are there? There's timeout and user signalled the wait. Can the OS wake it up for other reasons? \u2013 Robinson Sep 21 '15 at 11:04 You still don't understand condition variables. They are stateless. You are imagining that you will only be woken up in some particular set of states, but they don't have states. It's your responsibility to track the state. There is no \"signalled\" state, so you have no way to tell if there was a signal. \u2013 David Schwartz Sep 21 '15 at 11:06 If you think there is some particular state in which you will be woken up, answer me this -- in your code, what variable holds this state? In my code, it's stop . What is it in yours? Tracking state is your responsibility. The call to notify_one doesn't change the state to a signaled state. \u2013 David Schwartz Sep 21 '15 at 11:07 You need some way to know whether the thing the thread is waiting for has happened or not. That is the state. Condition variables are stateless. Maintaining that state is your responsibility. The condition variable cannot assure that a wakeup has occurred without tracking the state itself (how else would it do that?) and it, by design, doesn't do that. \u2013 David Schwartz Sep 21 '15 at 11:20 A See my comment. You forgot to implement the state of the thing the thread is waiting for, leaving the mutex nothing to protect and the thread nothing to wait for. Condition variables are stateless -- your code must track the state of the thing whose change you're notifying the thread about. Here's the code fixed. Notice that the mutex protects stop , and stop is the thing the thread is waiting for. class Timer { public : Timer () {} ~ Timer () { Stop (); } void Start ( std :: chrono :: milliseconds const & interval , std :: function < void ( void ) > const & callback ) { Stop (); { auto locked = std :: unique_lock < std :: mutex > ( mutex ); stop = false ; } thread = std :: thread ([ = ]() { auto locked = std :: unique_lock < std :: mutex > ( mutex ); while ( ! stop ) // We hold the mutex that protects stop { auto result = terminate . wait_for ( locked , interval ); if ( result == std :: cv_status :: timeout ) { callback (); } } }); } void Stop () { { // Set the predicate auto locked = std :: unique_lock < std :: mutex > ( mutex ); stop = true ; } // Tell the thread the predicate has changed terminate . notify_one (); if ( thread . joinable ()) { thread . join (); } } private : bool stop ; // This is the thing the thread is waiting for std :: thread thread ; std :: mutex mutex ; std :: condition_variable terminate ; }; stackoverflow Stopping long-sleep threads Let's suppose I have a thread which should perform some task periodically but this period is 6 times each hour 12 times each hour (every 5 minutes), I've often seen code which controls the thread loop with a is_running flag which is checked every loop, like this: std :: atomic < bool > is_running ; void start () { is_running . store ( true ); std :: thread { thread_function }. detach (); } void stop () { is_running . store ( false ); } void thread_function () { using namespace std :: literals ; while ( is_running . load ()) { // do some task... std :: this_thread :: sleep_for ( 5 min ); } } But if the stop() function is called, let's say, 1 millisecond after start() the thread would be alive for 299999 additional milliseconds until it awakes, checks the flag, and die. Is my understanding correct? How to avoid keeping alive (but sleeping) a thread which should have been ended? My best approach until now is the following: void thread_function () { using namespace std :: literals ; while ( is_running . load ()) { // do some task... for ( unsigned int b = 0u , e = 1500u ; is_running . load () && ( b != e ); ++ b ) { // 1500 * 200 = 300000ms = 5min std :: this_thread :: sleep_for ( 200 ms ); } } } Is there a less-dirty and more straightforward way to achieve this? Comments en.cppreference.com/w/cpp/thread/condition_variable , see first sentence there. Instead of sleeping for a fixed amount of time, you enter a signallable wait state for that amount of time so other threads can still interrupt you \u2013 stijn Apr 21 '15 at 14:40 Another option is boost::basic_waitable_timer . \u2013 tenfour Apr 21 '15 at 14:46 A Use a condition variable. You wait on the condition variable or 5 minutes passing. Remember to check for spurious wakeups. cppreference I cannot find a good stack overflow post on how to use a condition variable in a minute or two of google searching. The tricky part is realizing that the wait can wake up with neither 5 minutes passing, nor a signal being sent. The cleanest way to handle this is to use the wait methods with a lambda that double-checks that the wakeup was a \"good\" one. NOTE: double check\u6765\u907f\u514d**spurious wakeup** here is some sample code over at cppreference that uses wait_until with a lambda. ( wait_for with a lambda is equivalent to wait_until with a lambda). I modified it slightly. Here is an version: #include <iostream> #include <thread> #include <condition_variable> #include <mutex> #include <future> #include <chrono> #include <iostream> #include <vector> using namespace std :: literals :: chrono_literals ; struct timer_killer { // returns false if killed: template < class R , class P > bool wait_for ( std :: chrono :: duration < R , P > const & time ) { std :: unique_lock < std :: mutex > lock ( m ); return ! cv . wait_for ( lock , time , [ & ] { return terminate ;}); } void kill () { std :: unique_lock < std :: mutex > lock ( m ); terminate = true ; cv . notify_all (); } private : std :: condition_variable cv ; std :: mutex m ; bool terminate = false ; }; timer_killer bob ; int main () { std :: vector < std :: future < void > > tasks ; tasks . push_back ( std :: async ( std :: launch :: async , [] { while ( bob . wait_for ( 500 ms )) { std :: cout << \"thread 1 says hi \\n \" ; } std :: cout << \"thread 1 dead \\n \" ; })); bob . wait_for ( 250 ms ); tasks . push_back ( std :: async ( std :: launch :: async , [] { while ( bob . wait_for ( 500 ms )) { std :: cout << \"thread 2 says hi \\n \" ; } std :: cout << \"thread 2 dead \\n \" ; })); bob . wait_for ( 1000 ms ); std :: cout << \"killing threads \\n \" ; bob . kill (); for ( auto && f : tasks ) f . wait (); std :: cout << \"done \\n \" ; // your code goes here return 0 ; } // g++ -std=c++17 -O2 -Wall -pedantic -pthread main.cpp && ./a.out NOTE: \u4e0a\u8ff0\u7a0b\u5e8f\u662f\u975e\u5e38\u503c\u5f97\u5b66\u4e60\u7684\uff0c\u4e3a\u4e86\u4f7f\u7a0b\u5e8f\u7684\u6548\u679c\u66f4\u52a0\u663e\u8457\uff0c\u53ef\u4ee5\u4f7f\u7528\u4e0b\u9762\u7684\u7248\u672c: #include <iostream> #include <thread> #include <condition_variable> #include <mutex> #include <future> #include <chrono> #include <iostream> #include <vector> using namespace std :: literals :: chrono_literals ; struct timer_killer { // returns false if killed: template < class R , class P > bool wait_for ( std :: chrono :: duration < R , P > const & time ) { std :: unique_lock < std :: mutex > lock ( m ); return ! cv . wait_for ( lock , time , [ & ] { return terminate ;}); } void kill () { std :: unique_lock < std :: mutex > lock ( m ); terminate = true ; cv . notify_all (); } private : std :: condition_variable cv ; std :: mutex m ; bool terminate = false ; }; timer_killer bob ; int main () { std :: vector < std :: future < void > > tasks ; tasks . push_back ( std :: async ( std :: launch :: async , [] { while ( bob . wait_for ( 50 ms )) { std :: cout << \"thread 1 says hi \\n \" ; } std :: cout << \"thread 1 dead \\n \" ; })); bob . wait_for ( 250 ms ); tasks . push_back ( std :: async ( std :: launch :: async , [] { while ( bob . wait_for ( 50 ms )) { std :: cout << \"thread 2 says hi \\n \" ; } std :: cout << \"thread 2 dead \\n \" ; })); bob . wait_for ( 1000 ms ); std :: cout << \"killing threads \\n \" ; bob . kill (); for ( auto && f : tasks ) f . wait (); std :: cout << \"done \\n \" ; // your code goes here return 0 ; } // g++ -std=c++17 -O2 -Wall -pedantic -pthread main.cpp && ./a.out \u5728 bob.kill(); \u6267\u884c\u4e4b\u524d\uff0c asynchronous task \u4f1a\u91cd\u590d\u6267\u884c\u591a\u6b21\uff1b \u4e0b\u9762\u662fC++11\u7248\u672c\u7684 #include <iostream> #include <thread> #include <condition_variable> #include <mutex> #include <future> #include <chrono> #include <iostream> #include <vector> struct timer_killer { // returns false if killed: template < class R , class P > bool wait_for ( std :: chrono :: duration < R , P > const & time ) const { std :: unique_lock < std :: mutex > lock ( m ); return ! cv . wait_for ( lock , time , [ & ] { return terminate ;}); } void kill () { std :: unique_lock < std :: mutex > lock ( m ); terminate = true ; // should be modified inside mutex lock cv . notify_all (); // it is safe, and *sometimes* optimal, to do this outside the lock } // I like to explicitly delete/default special member functions: timer_killer () = default ; timer_killer ( timer_killer && ) = delete ; timer_killer ( timer_killer const & ) = delete ; timer_killer & operator = ( timer_killer && ) = delete ; timer_killer & operator = ( timer_killer const & ) = delete ; private : mutable std :: condition_variable cv ; mutable std :: mutex m ; bool terminate = false ; }; timer_killer bob ; int main () { std :: vector < std :: future < void > > tasks ; tasks . push_back ( std :: async ( std :: launch :: async , [] { while ( bob . wait_for ( std :: chrono :: milliseconds ( 50 ))) { std :: cout << \"thread 1 says hi \\n \" ; } std :: cout << \"thread 1 dead \\n \" ; })); bob . wait_for ( std :: chrono :: milliseconds ( 250 )); tasks . push_back ( std :: async ( std :: launch :: async , [] { while ( bob . wait_for ( std :: chrono :: milliseconds ( 50 ))) { std :: cout << \"thread 2 says hi \\n \" ; } std :: cout << \"thread 2 dead \\n \" ; })); bob . wait_for ( std :: chrono :: milliseconds ( 1000 )); std :: cout << \"killing threads \\n \" ; bob . kill (); for ( auto && f : tasks ) f . wait (); std :: cout << \"done \\n \" ; // your code goes here return 0 ; } // g++ --std=c++11 test.cpp -lpthread live example (coliru). You create a timer_killer in a shared spot. Client threads can wait_for( time ) . If it returns false, it means you where killed before your wait was complete. The controlling thread just calls kill() and everyone doing a wait_for gets a false return. Note that there is some contention (locking of the mutex), so this isn't suitable for infinite threads (but few things are). Consider using a scheduler if you need to have an unbounded number of tasks that are run with arbitrary delays instead of a full thread per delayed repeating task -- each real thread is upwards of a megabyte of system address space used (just for the stack). NOTE: \u9700\u8981\u7406\u89e3\u4e0a\u8ff0 contention \u7684\u542b\u4e49: std::async(std::launch::async) \u8868\u793a\u6bcf\u4e2aasynchronous task\u90fd\u6709\u4e00\u4e2a\u5bf9\u5e94\u7684thread\uff0c\u56e0\u6b64\uff0c\u5728\u4e0a\u8ff0\u7a0b\u5e8f\u4e2d\uff0c\u5c06\u7531\u591a\u4e2a\u7ebf\u7a0b\u7ade\u4e89 timer_killer::m A There are two traditional ways you could do this. You could use a timed wait on a condition variable , and have the other thread signal your periodic thread to wake up and die when it's time. Alternately you could poll on a pipe with your sleep as a timeout instead of of sleeping. Then you just write a byte to the pipe and the thread wakes up and can exit. NOTE: \u5728Linux APUE\u4e0a\u6709\u63d0\u51fa\u8fc7\u7c7b\u4f3c\u7684\u65b9\u6cd5\uff0c\u8fd9\u79cd\u65b9\u6cd5\u7b80\u8bb0\u4e3a: poll-on-a-pipe-with-timeout superuser Can't kill a sleeping process stackexchange Can a signal in one thread, interrupt sleep period of another thread of the same process in linux stackoverflow How to wake a std::thread while it is sleeping TODO how to interrupt a sleeping thread https://www.codeproject.com/Questions/619339/How-to-interpt-the-Sleeping-thread-in-cplusplus","title":"Introduction"},{"location":"Multithread/Topics/Stop-a-sleeping-thread/#sleepingthread","text":"","title":"\u5982\u4f55\u4e2d\u65ad\u4e00\u4e2a\u6b63\u5728sleeping\u7684thread"},{"location":"Multithread/Topics/Stop-a-sleeping-thread/#summary","text":"\u603b\u7684\u6765\u8bf4\uff0c\u76ee\u524d\u77e5\u9053\u7684\u662f\u4e24\u79cd\u65b9\u6848\uff0c\u8fd9\u4e24\u79cd\u65b9\u6848\u5728 stackoverflow Stopping long-sleep threads \u4e2d\u90fd\u63d0\u51fa\u6765\u4e86\u3002","title":"Summary"},{"location":"Multithread/Topics/Stop-a-sleeping-thread/#stackoverflow#how#to#terminate#a#sleeping#thread#in#pthread","text":"I have thread which sleeps for a long time, then wakes up to do something, then sleep again, like this: while ( some_condition ) { // do something sleep ( 1000 ); } How could I make this thread exit gracefully and QUICKLY? I tried to use pthread_cancel() , but sleeping threads could not be canceled. I also tried changing the condition of the while loop, but it will still take long to exit. And I don't want to use pthread_kill() , since it may kill the thread when it's working. So, are there any good ideas? COMMENTS well, i think i make a mistake, sleep() is a cancel point difined by posix.1 \u2013 qiuxiafei Jan 24 '11 at 3:40 Just a note - pthread_kill does not kill a thread. It sends a signal to a thread . If the action of that signal is to terminate the process (e.g. the default action of SIGTERM , or the unblockable action of SIGKILL ) then it will terminate the whole process , not the target thread. Really the only time pthread_kill is useful is if you've installed an interrupting signal handler , and you want to interrupt a syscall that's blocked in a particular thread (which would be a potential solution to your question). \u2013 R.. May 18 '11 at 23:11 Why did you say that the thread could not exit quickly ? I tried this and the thread canceled immediately upon thread_cancel . And I was able to pthread_join without delay. \u2013 user1502776 Jun 7 at 8:15 NOTE: \u5173\u4e8e pthread_cancel \u53c2\u89c1 pthread_cancel \uff0c\u5176\u4e2d\u63d0\u53ca sleep \u662f\u4e00\u4e2acancel point","title":"stackoverflow how to terminate a sleeping thread in pthread?"},{"location":"Multithread/Topics/Stop-a-sleeping-thread/#a","text":"As an alternative to sleep , you could use pthread_cond_timedwait with a 1000 ms timeout. Then when you want to exit, signal the condition variable. This is similar to how you might do this in C#/Java using wait and notify. COMMENTS Correct. This is the simple and non-racy way to do it. \u2013 caf Jan 24 '11 at 4:04 This is correct, but cancellation is easier and perhaps slightly more efficient. Of course if you call pthread_cancel on the thread, you need to make sure that it can't get cancelled at a point it shouldn't. Calling pthread_setcancelstate to disable cancellation when the thread starts, and only enabling it just before the sleep , would be a safe approach. \u2013 R.. May 18 '11 at 23:10 IMO, pthread_cancel() may be preferable since sleep() is already a cancellation point. And be sure to call pthread_cleanup_push() and pthread_cleanup_pop() to set up cleanup handlers if you have allocated resources. \u2013 zeekvfu Oct 22 '13 at 4:45 What is the difference between using pthread_cond_timedwait with 1000ms timeout and using a flag variable with 100ms sleep in regard to race condition ? \u2013 user1502776 Jun 5 at 5:05","title":"A"},{"location":"Multithread/Topics/Stop-a-sleeping-thread/#stackoverflow#how#can#i#kill#a#thread#without#using#stop","text":"I'm wanting a reasonably reliable threaded timer, so I've written a timer object that fires a std::function on a thread. I would like to give this timer the ability to stop before it gets to the next tick; something you can't do with ::sleep (at least I don't think you can). So what I've done is put a condition variable on a mutex. If the condition times out, I fire the event. If the condition is signalled the thread is exited\uff08\u4e00\u4e2acondition variable\u517c\u5177\u4e24\u79cd\u529f\u80fd\uff09. So the Stop method needs to be able to get the thread to stop and/or interrupt its wait, which I think is what it's doing right now. There are problems with this however. Sometimes the thread isn't joinable() and sometimes the condition is signalled after its timeout but before it's put into its wait state. How can I improve this and make it robust? The following is a full repo. The wait is 10 seconds here but the program should terminate immediately as the Foo is created and then immediately destroyed. It does sometimes but mostly it does not. #include <atomic> #include <thread> #include <future> #include <sstream> #include <chrono> #include <iostream> class Timer { public : Timer () {} ~ Timer () { Stop (); } void Start ( std :: chrono :: milliseconds const & interval , std :: function < void ( void ) > const & callback ) { Stop (); thread = std :: thread ([ = ]() { for (;;) { auto locked = std :: unique_lock < std :: mutex > ( mutex ); auto result = terminate . wait_for ( locked , interval ); if ( result == std :: cv_status :: timeout ) { callback (); } else { return ; } } }); } void Stop () { terminate . notify_one (); if ( thread . joinable ()) { thread . join (); } } private : std :: thread thread ; std :: mutex mutex ; std :: condition_variable terminate ; }; class Foo { public : Foo () { timer = std :: make_unique < Timer > (); timer -> Start ( std :: chrono :: milliseconds ( 10000 ), std :: bind ( & Foo :: Callback , this )); } ~ Foo () { } void Callback () { static int count = 0 ; std :: ostringstream o ; std :: cout << count ++ << std :: endl ; } std :: unique_ptr < Timer > timer ; }; int main ( void ) { { Foo foo ; } return 0 ; } Somewhere you need a variable protected by the mutex that stores whether or not the thread is supposed to stop. Condition variables are stateless -- it is your responsibility to maintain the state of the thing you're waiting for (called the \"predicate\"). Basically, you've missed the point of condition variables . Your Stop function notifies the thread. But it hasn't changed any condition that the thread is waiting for! Notice that the mutex doesn't protect anything. This is just totally wrong. \u2013 David Schwartz Sep 21 '15 at 10:44 Why return on a spurious wakeup? (Same problem. You have no way to know whether you should wakeup or not because you have no predicate to check.) \u2013 David Schwartz Sep 21 '15 at 11:03 NOTE: \u4e0a\u8ff0\u5e76\u4e0d\u6d89\u53ca**condition**\u7684\u4fee\u6539\uff1b\u4f7f\u7528condition variable\u7684\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u76ee\u7684\u662f\u4ec5\u4ec5\u5f53condition\u6ee1\u8db3\u7684\u65f6\u5019\u624d\u8fd0\u884cthread\u7ee7\u7eed\u6267\u884c\uff0c\u800c\u5f53 Spurious wakeup \u7684\u65f6\u5019\uff0c\u5219\u4e0d\u80fd\u591f\u5141\u8bb8\u5b83\u5141\u8bb8\uff0c\u6240\u4ee5\u624d\u9700\u8981\u4e00\u4e2acondition\uff1b\u800c\u4e0a\u8ff0\u4ee3\u7801\u5219\u663e\u7136\u6ca1\u6709\u8fd9\u6837\u505a\uff0c\u5982\u4e0b\u9762\u7684\u8bc4\u8bba\u6240\u8ff0\uff0c\u5b83\u5b58\u5728\u7740\u8fd9\u6837\u7684\u4e00\u4e2a\u98ce\u9669\uff1a return on a spurious wakeup What other kinds of wakeup are there? There's timeout and user signalled the wait. Can the OS wake it up for other reasons? \u2013 Robinson Sep 21 '15 at 11:04 You still don't understand condition variables. They are stateless. You are imagining that you will only be woken up in some particular set of states, but they don't have states. It's your responsibility to track the state. There is no \"signalled\" state, so you have no way to tell if there was a signal. \u2013 David Schwartz Sep 21 '15 at 11:06 If you think there is some particular state in which you will be woken up, answer me this -- in your code, what variable holds this state? In my code, it's stop . What is it in yours? Tracking state is your responsibility. The call to notify_one doesn't change the state to a signaled state. \u2013 David Schwartz Sep 21 '15 at 11:07 You need some way to know whether the thing the thread is waiting for has happened or not. That is the state. Condition variables are stateless. Maintaining that state is your responsibility. The condition variable cannot assure that a wakeup has occurred without tracking the state itself (how else would it do that?) and it, by design, doesn't do that. \u2013 David Schwartz Sep 21 '15 at 11:20","title":"stackoverflow How can I kill a thread? without using stop();"},{"location":"Multithread/Topics/Stop-a-sleeping-thread/#a_1","text":"See my comment. You forgot to implement the state of the thing the thread is waiting for, leaving the mutex nothing to protect and the thread nothing to wait for. Condition variables are stateless -- your code must track the state of the thing whose change you're notifying the thread about. Here's the code fixed. Notice that the mutex protects stop , and stop is the thing the thread is waiting for. class Timer { public : Timer () {} ~ Timer () { Stop (); } void Start ( std :: chrono :: milliseconds const & interval , std :: function < void ( void ) > const & callback ) { Stop (); { auto locked = std :: unique_lock < std :: mutex > ( mutex ); stop = false ; } thread = std :: thread ([ = ]() { auto locked = std :: unique_lock < std :: mutex > ( mutex ); while ( ! stop ) // We hold the mutex that protects stop { auto result = terminate . wait_for ( locked , interval ); if ( result == std :: cv_status :: timeout ) { callback (); } } }); } void Stop () { { // Set the predicate auto locked = std :: unique_lock < std :: mutex > ( mutex ); stop = true ; } // Tell the thread the predicate has changed terminate . notify_one (); if ( thread . joinable ()) { thread . join (); } } private : bool stop ; // This is the thing the thread is waiting for std :: thread thread ; std :: mutex mutex ; std :: condition_variable terminate ; };","title":"A"},{"location":"Multithread/Topics/Stop-a-sleeping-thread/#stackoverflow#stopping#long-sleep#threads","text":"Let's suppose I have a thread which should perform some task periodically but this period is 6 times each hour 12 times each hour (every 5 minutes), I've often seen code which controls the thread loop with a is_running flag which is checked every loop, like this: std :: atomic < bool > is_running ; void start () { is_running . store ( true ); std :: thread { thread_function }. detach (); } void stop () { is_running . store ( false ); } void thread_function () { using namespace std :: literals ; while ( is_running . load ()) { // do some task... std :: this_thread :: sleep_for ( 5 min ); } } But if the stop() function is called, let's say, 1 millisecond after start() the thread would be alive for 299999 additional milliseconds until it awakes, checks the flag, and die. Is my understanding correct? How to avoid keeping alive (but sleeping) a thread which should have been ended? My best approach until now is the following: void thread_function () { using namespace std :: literals ; while ( is_running . load ()) { // do some task... for ( unsigned int b = 0u , e = 1500u ; is_running . load () && ( b != e ); ++ b ) { // 1500 * 200 = 300000ms = 5min std :: this_thread :: sleep_for ( 200 ms ); } } } Is there a less-dirty and more straightforward way to achieve this?","title":"stackoverflow Stopping long-sleep threads"},{"location":"Multithread/Topics/Stop-a-sleeping-thread/#comments","text":"en.cppreference.com/w/cpp/thread/condition_variable , see first sentence there. Instead of sleeping for a fixed amount of time, you enter a signallable wait state for that amount of time so other threads can still interrupt you \u2013 stijn Apr 21 '15 at 14:40 Another option is boost::basic_waitable_timer . \u2013 tenfour Apr 21 '15 at 14:46","title":"Comments"},{"location":"Multithread/Topics/Stop-a-sleeping-thread/#a_2","text":"Use a condition variable. You wait on the condition variable or 5 minutes passing. Remember to check for spurious wakeups. cppreference I cannot find a good stack overflow post on how to use a condition variable in a minute or two of google searching. The tricky part is realizing that the wait can wake up with neither 5 minutes passing, nor a signal being sent. The cleanest way to handle this is to use the wait methods with a lambda that double-checks that the wakeup was a \"good\" one. NOTE: double check\u6765\u907f\u514d**spurious wakeup** here is some sample code over at cppreference that uses wait_until with a lambda. ( wait_for with a lambda is equivalent to wait_until with a lambda). I modified it slightly. Here is an version: #include <iostream> #include <thread> #include <condition_variable> #include <mutex> #include <future> #include <chrono> #include <iostream> #include <vector> using namespace std :: literals :: chrono_literals ; struct timer_killer { // returns false if killed: template < class R , class P > bool wait_for ( std :: chrono :: duration < R , P > const & time ) { std :: unique_lock < std :: mutex > lock ( m ); return ! cv . wait_for ( lock , time , [ & ] { return terminate ;}); } void kill () { std :: unique_lock < std :: mutex > lock ( m ); terminate = true ; cv . notify_all (); } private : std :: condition_variable cv ; std :: mutex m ; bool terminate = false ; }; timer_killer bob ; int main () { std :: vector < std :: future < void > > tasks ; tasks . push_back ( std :: async ( std :: launch :: async , [] { while ( bob . wait_for ( 500 ms )) { std :: cout << \"thread 1 says hi \\n \" ; } std :: cout << \"thread 1 dead \\n \" ; })); bob . wait_for ( 250 ms ); tasks . push_back ( std :: async ( std :: launch :: async , [] { while ( bob . wait_for ( 500 ms )) { std :: cout << \"thread 2 says hi \\n \" ; } std :: cout << \"thread 2 dead \\n \" ; })); bob . wait_for ( 1000 ms ); std :: cout << \"killing threads \\n \" ; bob . kill (); for ( auto && f : tasks ) f . wait (); std :: cout << \"done \\n \" ; // your code goes here return 0 ; } // g++ -std=c++17 -O2 -Wall -pedantic -pthread main.cpp && ./a.out NOTE: \u4e0a\u8ff0\u7a0b\u5e8f\u662f\u975e\u5e38\u503c\u5f97\u5b66\u4e60\u7684\uff0c\u4e3a\u4e86\u4f7f\u7a0b\u5e8f\u7684\u6548\u679c\u66f4\u52a0\u663e\u8457\uff0c\u53ef\u4ee5\u4f7f\u7528\u4e0b\u9762\u7684\u7248\u672c: #include <iostream> #include <thread> #include <condition_variable> #include <mutex> #include <future> #include <chrono> #include <iostream> #include <vector> using namespace std :: literals :: chrono_literals ; struct timer_killer { // returns false if killed: template < class R , class P > bool wait_for ( std :: chrono :: duration < R , P > const & time ) { std :: unique_lock < std :: mutex > lock ( m ); return ! cv . wait_for ( lock , time , [ & ] { return terminate ;}); } void kill () { std :: unique_lock < std :: mutex > lock ( m ); terminate = true ; cv . notify_all (); } private : std :: condition_variable cv ; std :: mutex m ; bool terminate = false ; }; timer_killer bob ; int main () { std :: vector < std :: future < void > > tasks ; tasks . push_back ( std :: async ( std :: launch :: async , [] { while ( bob . wait_for ( 50 ms )) { std :: cout << \"thread 1 says hi \\n \" ; } std :: cout << \"thread 1 dead \\n \" ; })); bob . wait_for ( 250 ms ); tasks . push_back ( std :: async ( std :: launch :: async , [] { while ( bob . wait_for ( 50 ms )) { std :: cout << \"thread 2 says hi \\n \" ; } std :: cout << \"thread 2 dead \\n \" ; })); bob . wait_for ( 1000 ms ); std :: cout << \"killing threads \\n \" ; bob . kill (); for ( auto && f : tasks ) f . wait (); std :: cout << \"done \\n \" ; // your code goes here return 0 ; } // g++ -std=c++17 -O2 -Wall -pedantic -pthread main.cpp && ./a.out \u5728 bob.kill(); \u6267\u884c\u4e4b\u524d\uff0c asynchronous task \u4f1a\u91cd\u590d\u6267\u884c\u591a\u6b21\uff1b \u4e0b\u9762\u662fC++11\u7248\u672c\u7684 #include <iostream> #include <thread> #include <condition_variable> #include <mutex> #include <future> #include <chrono> #include <iostream> #include <vector> struct timer_killer { // returns false if killed: template < class R , class P > bool wait_for ( std :: chrono :: duration < R , P > const & time ) const { std :: unique_lock < std :: mutex > lock ( m ); return ! cv . wait_for ( lock , time , [ & ] { return terminate ;}); } void kill () { std :: unique_lock < std :: mutex > lock ( m ); terminate = true ; // should be modified inside mutex lock cv . notify_all (); // it is safe, and *sometimes* optimal, to do this outside the lock } // I like to explicitly delete/default special member functions: timer_killer () = default ; timer_killer ( timer_killer && ) = delete ; timer_killer ( timer_killer const & ) = delete ; timer_killer & operator = ( timer_killer && ) = delete ; timer_killer & operator = ( timer_killer const & ) = delete ; private : mutable std :: condition_variable cv ; mutable std :: mutex m ; bool terminate = false ; }; timer_killer bob ; int main () { std :: vector < std :: future < void > > tasks ; tasks . push_back ( std :: async ( std :: launch :: async , [] { while ( bob . wait_for ( std :: chrono :: milliseconds ( 50 ))) { std :: cout << \"thread 1 says hi \\n \" ; } std :: cout << \"thread 1 dead \\n \" ; })); bob . wait_for ( std :: chrono :: milliseconds ( 250 )); tasks . push_back ( std :: async ( std :: launch :: async , [] { while ( bob . wait_for ( std :: chrono :: milliseconds ( 50 ))) { std :: cout << \"thread 2 says hi \\n \" ; } std :: cout << \"thread 2 dead \\n \" ; })); bob . wait_for ( std :: chrono :: milliseconds ( 1000 )); std :: cout << \"killing threads \\n \" ; bob . kill (); for ( auto && f : tasks ) f . wait (); std :: cout << \"done \\n \" ; // your code goes here return 0 ; } // g++ --std=c++11 test.cpp -lpthread live example (coliru). You create a timer_killer in a shared spot. Client threads can wait_for( time ) . If it returns false, it means you where killed before your wait was complete. The controlling thread just calls kill() and everyone doing a wait_for gets a false return. Note that there is some contention (locking of the mutex), so this isn't suitable for infinite threads (but few things are). Consider using a scheduler if you need to have an unbounded number of tasks that are run with arbitrary delays instead of a full thread per delayed repeating task -- each real thread is upwards of a megabyte of system address space used (just for the stack). NOTE: \u9700\u8981\u7406\u89e3\u4e0a\u8ff0 contention \u7684\u542b\u4e49: std::async(std::launch::async) \u8868\u793a\u6bcf\u4e2aasynchronous task\u90fd\u6709\u4e00\u4e2a\u5bf9\u5e94\u7684thread\uff0c\u56e0\u6b64\uff0c\u5728\u4e0a\u8ff0\u7a0b\u5e8f\u4e2d\uff0c\u5c06\u7531\u591a\u4e2a\u7ebf\u7a0b\u7ade\u4e89 timer_killer::m","title":"A"},{"location":"Multithread/Topics/Stop-a-sleeping-thread/#a_3","text":"There are two traditional ways you could do this. You could use a timed wait on a condition variable , and have the other thread signal your periodic thread to wake up and die when it's time. Alternately you could poll on a pipe with your sleep as a timeout instead of of sleeping. Then you just write a byte to the pipe and the thread wakes up and can exit. NOTE: \u5728Linux APUE\u4e0a\u6709\u63d0\u51fa\u8fc7\u7c7b\u4f3c\u7684\u65b9\u6cd5\uff0c\u8fd9\u79cd\u65b9\u6cd5\u7b80\u8bb0\u4e3a: poll-on-a-pipe-with-timeout","title":"A"},{"location":"Multithread/Topics/Stop-a-sleeping-thread/#superuser#cant#kill#a#sleeping#process","text":"","title":"superuser Can't kill a sleeping process"},{"location":"Multithread/Topics/Stop-a-sleeping-thread/#stackexchange#can#a#signal#in#one#thread#interrupt#sleep#period#of#another#thread#of#the#same#process#in#linux","text":"","title":"stackexchange Can a signal in one thread, interrupt sleep period of another thread of the same process in linux"},{"location":"Multithread/Topics/Stop-a-sleeping-thread/#stackoverflow#how#to#wake#a#stdthread#while#it#is#sleeping","text":"","title":"stackoverflow How to wake a std::thread while it is sleeping"},{"location":"Multithread/Topics/Stop-a-sleeping-thread/#todo","text":"","title":"TODO"},{"location":"Multithread/Topics/Stop-a-sleeping-thread/#how#to#interrupt#a#sleeping#thread","text":"https://www.codeproject.com/Questions/619339/How-to-interpt-the-Sleeping-thread-in-cplusplus","title":"how to interrupt a sleeping thread"},{"location":"Multithread/Topics/Thread-pool/","text":"Thread pool wikipedia Thread pool zephyrproject Memory Pools Implementation progschj/Thread*Pool* vit-vit/CTPL inkooboo/thread- pool -cpp mtrebi/thread- pool progschj ThreadPool TODO stackoverflow Ensuring task execution order in threadpool","title":"Introduction"},{"location":"Multithread/Topics/Thread-pool/#thread#pool","text":"","title":"Thread pool"},{"location":"Multithread/Topics/Thread-pool/#wikipedia#thread#pool","text":"","title":"wikipedia Thread pool"},{"location":"Multithread/Topics/Thread-pool/#zephyrproject#memory#pools","text":"","title":"zephyrproject Memory Pools"},{"location":"Multithread/Topics/Thread-pool/#implementation","text":"","title":"Implementation"},{"location":"Multithread/Topics/Thread-pool/#progschjthreadpool","text":"","title":"progschj/Thread*Pool*"},{"location":"Multithread/Topics/Thread-pool/#vit-vitctpl","text":"","title":"vit-vit/CTPL"},{"location":"Multithread/Topics/Thread-pool/#inkooboothread-pool-cpp","text":"","title":"inkooboo/thread-pool-cpp"},{"location":"Multithread/Topics/Thread-pool/#mtrebithread-pool","text":"","title":"mtrebi/thread-pool"},{"location":"Multithread/Topics/Thread-pool/#progschj#threadpool","text":"","title":"progschj ThreadPool"},{"location":"Multithread/Topics/Thread-pool/#todo","text":"","title":"TODO"},{"location":"Multithread/Topics/Thread-pool/#stackoverflow#ensuring#task#execution#order#in#threadpool","text":"","title":"stackoverflow Ensuring task execution order in threadpool"},{"location":"Multithread/Topics/Thread-pool/library-progschj-ThreadPool/","text":"progschj / ThreadPool Read code 1\u3001\u4ee3\u7801\u5e76\u4e0d\u957f\uff0c\u56e0\u6b64\u53ef\u4ee5\u5f88\u5feb\u5b8c\u5168\u9605\u8bfb\u3002 2\u3001\u5178\u578b\u7684\"asynchronous method invocation-thread pool-message queue mailbox\" #ifndef THREAD_POOL_H #define THREAD_POOL_H #include <vector> #include <queue> #include <memory> #include <thread> #include <mutex> #include <condition_variable> #include <future> #include <functional> #include <stdexcept> class ThreadPool { public : ThreadPool ( size_t ); /** * @brief * * @tparam F * @tparam Args * @param f * @param args * @return */ template < class F , class ... Args > auto enqueue ( F && f , Args && ... args ) -> std :: future < typename std :: result_of < F ( Args ...) >:: type > ; ~ ThreadPool (); private : // need to keep track of threads so we can join them std :: vector < std :: thread > workers ; // the task queue std :: queue < std :: function < void () > > tasks ; // synchronization std :: mutex queue_mutex ; std :: condition_variable condition ; /** * \u662f\u5426\u505c\u6b62thread pool\uff0c\u5728destructor\u5c06\u5176\u8bbe\u7f6e\u4e3atrue */ bool stop ; }; // the constructor just launches some amount of workers inline ThreadPool :: ThreadPool ( size_t threads ) : stop ( false ) { for ( size_t i = 0 ; i < threads ; ++ i ) workers . emplace_back ( [ this ] { for (;;) { std :: function < void () > task ; { std :: unique_lock < std :: mutex > lock ( this -> queue_mutex ); /** * \u7b49\u5f85\u961f\u5217\u975e\u7a7a\u6216\u8005\u7528\u6237\u505c\u6b62\u4e86thread pool */ this -> condition . wait ( lock , [ this ]{ return this -> stop || ! this -> tasks . empty (); }); /** * \u7528\u6237\u901a\u77e5\u4e86thread pool */ if ( this -> stop && this -> tasks . empty ()) return ; task = std :: move ( this -> tasks . front ()); this -> tasks . pop (); } task (); } } ); } // add new work item to the pool template < class F , class ... Args > auto ThreadPool :: enqueue ( F && f , Args && ... args ) -> std :: future < typename std :: result_of < F ( Args ...) >:: type > { using return_type = typename std :: result_of < F ( Args ...) >:: type ; auto task = std :: make_shared < std :: packaged_task < return_type () > > ( std :: bind ( std :: forward < F > ( f ), std :: forward < Args > ( args )...) ); std :: future < return_type > res = task -> get_future (); { std :: unique_lock < std :: mutex > lock ( queue_mutex ); // don't allow enqueueing after stopping the pool if ( stop ) throw std :: runtime_error ( \"enqueue on stopped ThreadPool\" ); /** * \"[task](){ (*task)(); }\"\u662flambda expression */ tasks . emplace ([ task ](){ ( * task )(); }); } condition . notify_one (); return res ; } // the destructor joins all threads inline ThreadPool ::~ ThreadPool () { { std :: unique_lock < std :: mutex > lock ( queue_mutex ); stop = true ; } condition . notify_all (); for ( std :: thread & worker : workers ) worker . join (); } #endif \u4f18\u5316 \u5728 ThreadPool::enqueue(F&& f, Args&&... args) \u4e2d\uff0c\u4f7f\u7528 task = std::make_shared< std::packaged_task<return_type()> \u5373new \u6765\u6784\u9020task\u5bf9\u8c61\uff0c\u663e\u7136\u5b83\u4f1a\u9891\u7e41\u5730\u8fdb\u884cnew\u3001delete\uff1b\u8fd9\u9020\u6210\u5982\u4e0b\u95ee\u9898: 1\u3001\u5185\u5b58\u788e\u7247 2\u3001\u6027\u80fd \u90a3\u5982\u4f55\u6765\u8fdb\u884c\u4f18\u5316\u5462\uff1f 1\u3001automatic variable + move","title":"Introduction"},{"location":"Multithread/Topics/Thread-pool/library-progschj-ThreadPool/#progschjthreadpool","text":"","title":"progschj/ThreadPool"},{"location":"Multithread/Topics/Thread-pool/library-progschj-ThreadPool/#read#code","text":"1\u3001\u4ee3\u7801\u5e76\u4e0d\u957f\uff0c\u56e0\u6b64\u53ef\u4ee5\u5f88\u5feb\u5b8c\u5168\u9605\u8bfb\u3002 2\u3001\u5178\u578b\u7684\"asynchronous method invocation-thread pool-message queue mailbox\" #ifndef THREAD_POOL_H #define THREAD_POOL_H #include <vector> #include <queue> #include <memory> #include <thread> #include <mutex> #include <condition_variable> #include <future> #include <functional> #include <stdexcept> class ThreadPool { public : ThreadPool ( size_t ); /** * @brief * * @tparam F * @tparam Args * @param f * @param args * @return */ template < class F , class ... Args > auto enqueue ( F && f , Args && ... args ) -> std :: future < typename std :: result_of < F ( Args ...) >:: type > ; ~ ThreadPool (); private : // need to keep track of threads so we can join them std :: vector < std :: thread > workers ; // the task queue std :: queue < std :: function < void () > > tasks ; // synchronization std :: mutex queue_mutex ; std :: condition_variable condition ; /** * \u662f\u5426\u505c\u6b62thread pool\uff0c\u5728destructor\u5c06\u5176\u8bbe\u7f6e\u4e3atrue */ bool stop ; }; // the constructor just launches some amount of workers inline ThreadPool :: ThreadPool ( size_t threads ) : stop ( false ) { for ( size_t i = 0 ; i < threads ; ++ i ) workers . emplace_back ( [ this ] { for (;;) { std :: function < void () > task ; { std :: unique_lock < std :: mutex > lock ( this -> queue_mutex ); /** * \u7b49\u5f85\u961f\u5217\u975e\u7a7a\u6216\u8005\u7528\u6237\u505c\u6b62\u4e86thread pool */ this -> condition . wait ( lock , [ this ]{ return this -> stop || ! this -> tasks . empty (); }); /** * \u7528\u6237\u901a\u77e5\u4e86thread pool */ if ( this -> stop && this -> tasks . empty ()) return ; task = std :: move ( this -> tasks . front ()); this -> tasks . pop (); } task (); } } ); } // add new work item to the pool template < class F , class ... Args > auto ThreadPool :: enqueue ( F && f , Args && ... args ) -> std :: future < typename std :: result_of < F ( Args ...) >:: type > { using return_type = typename std :: result_of < F ( Args ...) >:: type ; auto task = std :: make_shared < std :: packaged_task < return_type () > > ( std :: bind ( std :: forward < F > ( f ), std :: forward < Args > ( args )...) ); std :: future < return_type > res = task -> get_future (); { std :: unique_lock < std :: mutex > lock ( queue_mutex ); // don't allow enqueueing after stopping the pool if ( stop ) throw std :: runtime_error ( \"enqueue on stopped ThreadPool\" ); /** * \"[task](){ (*task)(); }\"\u662flambda expression */ tasks . emplace ([ task ](){ ( * task )(); }); } condition . notify_one (); return res ; } // the destructor joins all threads inline ThreadPool ::~ ThreadPool () { { std :: unique_lock < std :: mutex > lock ( queue_mutex ); stop = true ; } condition . notify_all (); for ( std :: thread & worker : workers ) worker . join (); } #endif","title":"Read code"},{"location":"Multithread/Topics/Thread-pool/library-progschj-ThreadPool/#_1","text":"\u5728 ThreadPool::enqueue(F&& f, Args&&... args) \u4e2d\uff0c\u4f7f\u7528 task = std::make_shared< std::packaged_task<return_type()> \u5373new \u6765\u6784\u9020task\u5bf9\u8c61\uff0c\u663e\u7136\u5b83\u4f1a\u9891\u7e41\u5730\u8fdb\u884cnew\u3001delete\uff1b\u8fd9\u9020\u6210\u5982\u4e0b\u95ee\u9898: 1\u3001\u5185\u5b58\u788e\u7247 2\u3001\u6027\u80fd \u90a3\u5982\u4f55\u6765\u8fdb\u884c\u4f18\u5316\u5462\uff1f 1\u3001automatic variable + move","title":"\u4f18\u5316"},{"location":"Multithread/Topics/Wait-VS-sleep/","text":"wait() VS sleep() stackoverflow Difference between wait() and sleep()","title":"Introduction"},{"location":"Multithread/Topics/Wait-VS-sleep/#wait#vs#sleep","text":"","title":"wait() VS sleep()"},{"location":"Multithread/Topics/Wait-VS-sleep/#stackoverflow#difference#between#wait#and#sleep","text":"","title":"stackoverflow Difference between wait() and sleep()"},{"location":"Programming-model/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u63cf\u8ff0\u5728parallel-computing\u4e2d\u4e00\u4e9b\u975e\u5e38\u5e38\u89c1\u7684programming model\uff0c\u719f\u6089\u8fd9\u4e9bprogramming model\uff0c\u80fd\u591f\u8ba9\u6211\u4eec\u4ee5\u6700\u4f73\u5b9e\u8df5\u6765\u7f16\u5199\u7a0b\u5e8f\u3002","title":"Introduction"},{"location":"Programming-model/#_1","text":"\u672c\u7ae0\u63cf\u8ff0\u5728parallel-computing\u4e2d\u4e00\u4e9b\u975e\u5e38\u5e38\u89c1\u7684programming model\uff0c\u719f\u6089\u8fd9\u4e9bprogramming model\uff0c\u80fd\u591f\u8ba9\u6211\u4eec\u4ee5\u6700\u4f73\u5b9e\u8df5\u6765\u7f16\u5199\u7a0b\u5e8f\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Programming-model/API/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u63cf\u8ff0\u4e00\u4e9b\u7528\u4e8eparallel computing\u7684API\u3002 TODO: OpenMP and TBB","title":"Introduction"},{"location":"Programming-model/API/#_1","text":"\u672c\u7ae0\u63cf\u8ff0\u4e00\u4e9b\u7528\u4e8eparallel computing\u7684API\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Programming-model/API/#todo#openmp#and#tbb","text":"","title":"TODO: OpenMP and TBB"},{"location":"Programming-model/API/Cilk/","text":"Cilk \u5728 \u9605\u8bfb ustc.edu False Sharing \u65f6\uff0c\u5176\u4e2d\u7ed9\u51fa\u7684 example\u4f7f\u7528 cilk_spawn \u3001 cilk_sync \uff0c\u4ece\u800c\u4f7f\u6211\u53d1\u73b0\u4e86Cilk\u3002 wikipedia Cilk Cilk , Cilk++ and Cilk Plus are general-purpose programming languages designed for multithreaded parallel computing . They are based on the C and C++ programming languages, which they extend with constructs to express parallel loops and the fork\u2013join idiom . mit.edu Programming in Cilk The Cilk programming language provides a simple extension to the C and C++ languages that allow programmers to expose logically parallel tasks. Cilk extends C and C++ with three keywords: cilk_spawn , cilk_sync , and cilk_for . This page describes the Cilk language extension.","title":"Introduction"},{"location":"Programming-model/API/Cilk/#cilk","text":"\u5728 \u9605\u8bfb ustc.edu False Sharing \u65f6\uff0c\u5176\u4e2d\u7ed9\u51fa\u7684 example\u4f7f\u7528 cilk_spawn \u3001 cilk_sync \uff0c\u4ece\u800c\u4f7f\u6211\u53d1\u73b0\u4e86Cilk\u3002","title":"Cilk"},{"location":"Programming-model/API/Cilk/#wikipedia#cilk","text":"Cilk , Cilk++ and Cilk Plus are general-purpose programming languages designed for multithreaded parallel computing . They are based on the C and C++ programming languages, which they extend with constructs to express parallel loops and the fork\u2013join idiom .","title":"wikipedia Cilk"},{"location":"Programming-model/API/Cilk/#mitedu#programming#in#cilk","text":"The Cilk programming language provides a simple extension to the C and C++ languages that allow programmers to expose logically parallel tasks. Cilk extends C and C++ with three keywords: cilk_spawn , cilk_sync , and cilk_for . This page describes the Cilk language extension.","title":"mit.edu Programming in Cilk"},{"location":"Programming-model/API/OpenMP/","text":"OpenMP wikipedia OpenMP The application programming interface (API) OpenMP ( Open Multi-Processing ) supports multi-platform shared-memory multiprocessing programming in C , C++ , and Fortran ,[ 3] on many platforms, instruction-set architectures and operating systems , including Solaris , AIX , HP-UX , Linux , macOS , and Windows . It consists of a set of compiler directives , library routines , and environment variables that influence run-time behavior.[ 2] [ 4] [ 5] \u5b98\u7f51: OpenMP","title":"Introduction"},{"location":"Programming-model/API/OpenMP/#openmp","text":"","title":"OpenMP"},{"location":"Programming-model/API/OpenMP/#wikipedia#openmp","text":"The application programming interface (API) OpenMP ( Open Multi-Processing ) supports multi-platform shared-memory multiprocessing programming in C , C++ , and Fortran ,[ 3] on many platforms, instruction-set architectures and operating systems , including Solaris , AIX , HP-UX , Linux , macOS , and Windows . It consists of a set of compiler directives , library routines , and environment variables that influence run-time behavior.[ 2] [ 4] [ 5]","title":"wikipedia OpenMP"},{"location":"Programming-model/API/OpenMP/#openmp_1","text":"","title":"\u5b98\u7f51: OpenMP"},{"location":"Programming-model/API/Threading-Building-Blocks/","text":"Threading Building Blocks wikipedia Threading Building Blocks Threading Building Blocks ( TBB ) is a C++ template library developed by Intel for parallel programming on multi-core processors . Using TBB, a computation is broken down into tasks that can run in parallel. The library manages and schedules threads to execute these tasks. intel Intel\u00ae Threading Building Blocks","title":"Introduction"},{"location":"Programming-model/API/Threading-Building-Blocks/#threading#building#blocks","text":"","title":"Threading Building Blocks"},{"location":"Programming-model/API/Threading-Building-Blocks/#wikipedia#threading#building#blocks","text":"Threading Building Blocks ( TBB ) is a C++ template library developed by Intel for parallel programming on multi-core processors . Using TBB, a computation is broken down into tasks that can run in parallel. The library manages and schedules threads to execute these tasks.","title":"wikipedia Threading Building Blocks"},{"location":"Programming-model/API/Threading-Building-Blocks/#intel#intel#threading#building#blocks","text":"","title":"intel Intel\u00ae Threading Building Blocks"},{"location":"Programming-model/Active-and-passive/","text":"\u5173\u4e8e\u672c\u7ae0 polling\u662factive\uff0c\u5373\u4e3b\u52a8\uff1b notify\u662fpassive\uff0c\u5373\u88ab\u52a8\uff1b Active polling example redis IO thread Passive notify example spdlog Example \u5982\u4f55\u5b9e\u73b0\u7cbe\u51c6\u7684\u65f6\u95f4\u63a7\u5236\uff1f \u7ebf\u7a0b\u8c03\u5ea6\u65f6\u95f4\u4e5f\u5305\u62ec\u5728\u5185 polling\uff1a \u4e0d\u8ba9thread\u63a5\u5165sleeping\u72b6\u6001\uff0c\u800c\u662f\u4e0d\u65ad\u5730\u68c0\u67e5\u65f6\u95f4 non-polling\uff1asleep https://www.softwaretestinghelp.com/cpp-sleep/ https://stackoverflow.com/questions/12823598/effect-of-usleep0-in-c-on-linux \u57fa\u51c6\u7684\u65f6\u95f4\u63a7\u5236\uff0c\u8ba9\u6211\u60f3\u8d77\u4e86Nginx\u7684\u5b9e\u73b0\uff1b bit mask\u9700\u8981\u8fdb\u884c\u603b\u7ed3 spinlock VS mutex \u53c2\u89c1 modernescpp The Atomic Flag \u3002","title":"Introduction"},{"location":"Programming-model/Active-and-passive/#_1","text":"polling\u662factive\uff0c\u5373\u4e3b\u52a8\uff1b notify\u662fpassive\uff0c\u5373\u88ab\u52a8\uff1b","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Programming-model/Active-and-passive/#active#polling#example","text":"redis IO thread","title":"Active polling example"},{"location":"Programming-model/Active-and-passive/#passive#notify#example","text":"spdlog","title":"Passive notify example"},{"location":"Programming-model/Active-and-passive/#example","text":"","title":"Example"},{"location":"Programming-model/Active-and-passive/#_2","text":"\u7ebf\u7a0b\u8c03\u5ea6\u65f6\u95f4\u4e5f\u5305\u62ec\u5728\u5185 polling\uff1a \u4e0d\u8ba9thread\u63a5\u5165sleeping\u72b6\u6001\uff0c\u800c\u662f\u4e0d\u65ad\u5730\u68c0\u67e5\u65f6\u95f4 non-polling\uff1asleep https://www.softwaretestinghelp.com/cpp-sleep/ https://stackoverflow.com/questions/12823598/effect-of-usleep0-in-c-on-linux \u57fa\u51c6\u7684\u65f6\u95f4\u63a7\u5236\uff0c\u8ba9\u6211\u60f3\u8d77\u4e86Nginx\u7684\u5b9e\u73b0\uff1b bit mask\u9700\u8981\u8fdb\u884c\u603b\u7ed3","title":"\u5982\u4f55\u5b9e\u73b0\u7cbe\u51c6\u7684\u65f6\u95f4\u63a7\u5236\uff1f"},{"location":"Programming-model/Active-and-passive/#spinlock#vs#mutex","text":"\u53c2\u89c1 modernescpp The Atomic Flag \u3002","title":"spinlock VS mutex"},{"location":"Programming-model/Active-and-passive/Polling-and-notify/","text":"","title":"Polling-and-notify"},{"location":"Programming-model/Active-and-passive/Busy-waiting/","text":"Busy waiting wikipedia Busy waiting In computer science and software engineering , busy-waiting , busy-looping or spinning is a technique in which a process repeatedly checks to see if a condition is true, such as whether keyboard input or a lock is available. Spinning can also be used to generate an arbitrary time delay, a technique that was necessary on systems that lacked a method of waiting a specific length of time. Processor speeds vary greatly from computer to computer, especially as some processors are designed to dynamically adjust speed based on current workload[ 1] . TODO https://practice.geeksforgeeks.org/problems/what-is-busy-wait https://dev.to/rinsama77/process-synchronization-with-busy-waiting-4gho https://stackoverflow.com/questions/1107593/what-are-trade-offs-for-busy-wait-vs-sleep https://www.auto.tuwien.ac.at/~blieb/papers/busywait.pdf","title":"Introduction"},{"location":"Programming-model/Active-and-passive/Busy-waiting/#busy#waiting","text":"","title":"Busy waiting"},{"location":"Programming-model/Active-and-passive/Busy-waiting/#wikipedia#busy#waiting","text":"In computer science and software engineering , busy-waiting , busy-looping or spinning is a technique in which a process repeatedly checks to see if a condition is true, such as whether keyboard input or a lock is available. Spinning can also be used to generate an arbitrary time delay, a technique that was necessary on systems that lacked a method of waiting a specific length of time. Processor speeds vary greatly from computer to computer, especially as some processors are designed to dynamically adjust speed based on current workload[ 1] .","title":"wikipedia Busy waiting"},{"location":"Programming-model/Active-and-passive/Busy-waiting/#todo","text":"https://practice.geeksforgeeks.org/problems/what-is-busy-wait https://dev.to/rinsama77/process-synchronization-with-busy-waiting-4gho https://stackoverflow.com/questions/1107593/what-are-trade-offs-for-busy-wait-vs-sleep https://www.auto.tuwien.ac.at/~blieb/papers/busywait.pdf","title":"TODO"},{"location":"Programming-model/Active-and-passive/Polling/","text":"Polling \u8f6e\u8be2\u3002 wikipedia Polling (computer science) Polling , or polled operation, in computer science , refers to actively sampling the status of an external device by a client program as a synchronous(\u540c\u6b65\u7684) activity. Polling is most often used in terms of input/output (I/O), and is also referred to as polled I/O or software-driven I/O . NOTE: \u5173\u952e\u5b57: sample\uff1bsynchronous\uff1b Description Polling is the process where the computer or controlling device waits for an external device to check for its readiness or state, often with low-level hardware. For example, when a printer is connected via a parallel port , the computer waits until the printer has received the next character. These processes can be as minute as only reading one bit . \u03a4his is sometimes used synonymously\uff08\u540c\u4e49\uff09 with busy-wait polling. In this situation, when an I/O operation is required, the computer does nothing other than check the status of the I/O device until it is ready, at which point the device is accessed. In other words, the computer waits until the device is ready. Polling also refers to the situation where a device is repeatedly checked for readiness, and if it is not, the computer returns to a different task. Although not as wasteful of CPU cycles as busy waiting, this is generally not as efficient as the alternative to polling, interrupt -driven I/O. NOTE:\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u63cf\u8ff0\u4e86\u4e24\u79cd**polling**\uff0c\u7b2c\u4e00\u79cd\u4e3a busy-wait polling\uff0c\u7b2c\u4e8c\u79cd\u7684\u505a\u6cd5\u4e0e busy-wait polling\u4e0d\u540c\uff0c\u5b83\u4e0d\u4f1a\u4e00\u76f4loop\uff1b\u6700\u540e\u4e00\u6bb5\u8bdd\u5219\u5c06polling\u4e0e interrupt -driven I/O\u8fdb\u884c\u4e86\u5bf9\u6bd4\uff0c\u4f5c\u8005\u76f4\u63a5\u6307\u51fa\u8fd9\u4e24\u79cdpolling\u5728\u6027\u80fd\u4e0a\u90fd\u4e0d\u53ca interrupt -driven I/O\u3002 Algorithm Polling can be described in the following steps (add a pic): Host actions: The host repeatedly reads the busy bit of the controller until it becomes clear. When clear, the host writes in the command register and writes a byte into the data-out register. The host sets the command-ready bit (set to 1). Controller actions: When the controller senses command-ready bit is set, it sets busy bit. The controller reads the command register and since write bit is set, it performs necessary I/O operations on the device. If the read bit is set to one instead of write bit, data from device is loaded into data-in register, which is further read by the host. The controller clears the command-ready bit once everything is over, it clears error bit to show successful operation and reset busy bit (0). Poll message A poll message is a control-acknowledgment message(\u63a7\u5236\u786e\u8ba4\u6d88\u606f). In a multidrop line arrangement (a central computer and different terminals in which the terminals share a single communication line to and from the computer), the system uses a master/slave polling arrangement whereby the central computer sends message (called polling message ) to a specific terminal on the outgoing line. All terminals listen to the outgoing line, but only the terminal that is polled replies by sending any information that it has ready for transmission on the incoming line.[ 1] In star networks , which, in its simplest form, consists of one central switch , hub , or computer that acts as a conduit to transmit messages, polling is not required to avoid chaos on the lines, but it is often used to allow the master to acquire input in an orderly fashion. These poll messages differ from those of the multidrop lines case because there are no site addresses needed, and each terminal only receives those polls that are directed to it.[ 1]","title":"Introduction"},{"location":"Programming-model/Active-and-passive/Polling/#polling","text":"\u8f6e\u8be2\u3002","title":"Polling"},{"location":"Programming-model/Active-and-passive/Polling/#wikipedia#polling#computer#science","text":"Polling , or polled operation, in computer science , refers to actively sampling the status of an external device by a client program as a synchronous(\u540c\u6b65\u7684) activity. Polling is most often used in terms of input/output (I/O), and is also referred to as polled I/O or software-driven I/O . NOTE: \u5173\u952e\u5b57: sample\uff1bsynchronous\uff1b","title":"wikipedia Polling (computer science)"},{"location":"Programming-model/Active-and-passive/Polling/#description","text":"Polling is the process where the computer or controlling device waits for an external device to check for its readiness or state, often with low-level hardware. For example, when a printer is connected via a parallel port , the computer waits until the printer has received the next character. These processes can be as minute as only reading one bit . \u03a4his is sometimes used synonymously\uff08\u540c\u4e49\uff09 with busy-wait polling. In this situation, when an I/O operation is required, the computer does nothing other than check the status of the I/O device until it is ready, at which point the device is accessed. In other words, the computer waits until the device is ready. Polling also refers to the situation where a device is repeatedly checked for readiness, and if it is not, the computer returns to a different task. Although not as wasteful of CPU cycles as busy waiting, this is generally not as efficient as the alternative to polling, interrupt -driven I/O. NOTE:\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u63cf\u8ff0\u4e86\u4e24\u79cd**polling**\uff0c\u7b2c\u4e00\u79cd\u4e3a busy-wait polling\uff0c\u7b2c\u4e8c\u79cd\u7684\u505a\u6cd5\u4e0e busy-wait polling\u4e0d\u540c\uff0c\u5b83\u4e0d\u4f1a\u4e00\u76f4loop\uff1b\u6700\u540e\u4e00\u6bb5\u8bdd\u5219\u5c06polling\u4e0e interrupt -driven I/O\u8fdb\u884c\u4e86\u5bf9\u6bd4\uff0c\u4f5c\u8005\u76f4\u63a5\u6307\u51fa\u8fd9\u4e24\u79cdpolling\u5728\u6027\u80fd\u4e0a\u90fd\u4e0d\u53ca interrupt -driven I/O\u3002","title":"Description"},{"location":"Programming-model/Active-and-passive/Polling/#algorithm","text":"Polling can be described in the following steps (add a pic): Host actions: The host repeatedly reads the busy bit of the controller until it becomes clear. When clear, the host writes in the command register and writes a byte into the data-out register. The host sets the command-ready bit (set to 1). Controller actions: When the controller senses command-ready bit is set, it sets busy bit. The controller reads the command register and since write bit is set, it performs necessary I/O operations on the device. If the read bit is set to one instead of write bit, data from device is loaded into data-in register, which is further read by the host. The controller clears the command-ready bit once everything is over, it clears error bit to show successful operation and reset busy bit (0).","title":"Algorithm"},{"location":"Programming-model/Active-and-passive/Polling/#poll#message","text":"A poll message is a control-acknowledgment message(\u63a7\u5236\u786e\u8ba4\u6d88\u606f). In a multidrop line arrangement (a central computer and different terminals in which the terminals share a single communication line to and from the computer), the system uses a master/slave polling arrangement whereby the central computer sends message (called polling message ) to a specific terminal on the outgoing line. All terminals listen to the outgoing line, but only the terminal that is polled replies by sending any information that it has ready for transmission on the incoming line.[ 1] In star networks , which, in its simplest form, consists of one central switch , hub , or computer that acts as a conduit to transmit messages, polling is not required to avoid chaos on the lines, but it is often used to allow the master to acquire input in an orderly fashion. These poll messages differ from those of the multidrop lines case because there are no site addresses needed, and each terminal only receives those polls that are directed to it.[ 1]","title":"Poll message"},{"location":"Programming-model/Blocking-and-non-blocking/","text":"Blocking and non-blocking Block\u7684\u542b\u4e49 \"block\"\u7684\u539f\u610f\u662f: \"\u963b\u585e\"\uff0c\u662f\u6307\u5f53\u524d\u7684\u6d41\u7a0b\u88ab\u6682\u505c\u4e86\uff0c\u5b83\u88ab\u6682\u505c\u7684\u539f\u56e0\u6709: 1) \u7b49\u5f85\u8017\u65f6\u7684\u64cd\u4f5c\u5b8c\u6210 \u6bd4\u5982\u6267\u884c\u4e86\u4e00\u4e2a\u975e\u5e38\u8017\u65f6\u7684\u64cd\u4f5c\u3002 2) \u7b49\u5f85\u6761\u4ef6\u6ee1\u8db3 \u6bd4\u5982\u7b49\u5f85lock\u3002 Blocking wikipedia Blocking (computing) In computing , a process is an instance of a computer program that is being executed. A process always exists in exactly one process state . A process that is blocked is one that is waiting for some event , such as a resource becoming available or the completion of an I/O operation.[ 1] NOTE: \u5728I/O system call\u4e2d\uff0c\u5c31\u6709non blocking I/O In a multitasking computer system, individual tasks , or threads of execution , must share the resources of the system. Shared resources include: the CPU, network and network interfaces, memory and disk. When one task is using a resource, it is generally not possible, or desirable, for another task to access it. The techniques of mutual exclusion are used to prevent this concurrent use. When the other task is blocked, it is unable to execute until the first task has finished using the shared resource. Programming languages and scheduling algorithms are designed to minimize the over-all(\u5168\u9762\u7684\uff0c\u6574\u4f53\u7684) effect blocking. A process that blocks may prevent local work-tasks from progressing. In this case \"blocking\" often is seen as not wanted.[ 2] However, such work-tasks may instead have been assigned to independent processes, where halting one has no or little effect on the others, since scheduling will continue. An example is \"blocking on a channel \" where passively(\u53d8\u52a8\u5730) waiting for the other part (no polling or spin loop) is part of the semantics of channels.[ 3] Correctly engineered any of these may be used to implement reactive systems. Deadlock means that processes pathologically wait for each other in a circle. As such it is not directly associated with blocking. Once the event occurs for which the process is waiting (\"is blocked on\"), the process is advanced from blocked state to an imminent one, such as runnable . See also Non-blocking algorithm Non-blocking synchronization Concurrent computing Race condition Deadlock Scheduling (computing) Data dependency Blocking and timeout \u5728\u5f88\u591a\u5e94\u7528\u4e2d\uff0c\u4e00\u76f4\u5c06application block\u662f\u975e\u5e38\u5371\u9669\u7684\uff0c\u56e0\u4e3a\u5b83\u53ef\u80fd\u5bfc\u81f4\u7cfb\u7edf\u4e0d\u53ef\u7528\uff1b\u6240\u4ee5\u4e3ablock\u8bbe\u7f6e\u4e00\u4e2atimeout\u662f\u975e\u5e38\u91cd\u8981\u7684\uff1b NOTE: 1\u3001system call with timeout Process\u7684blocked\u72b6\u6001 OS\u4e2d\uff0cprocess\u6709blocked\u72b6\u6001\uff0c\u663e\u7136\u662f\u7b26\u5408\u524d\u9762\u7684\u63cf\u8ff0\u7684\u3002 Non-blocking \u663e\u7136Non-blocking\u662f\u548cBlocking\u76f8\u5bf9\u7684\u3002 Wikipedia Non-blocking algorithm NOTE: \u8fd9\u7bc7\u6587\u7ae0\u91cd\u8981\u8bb2\u8ff0\u7684\u662fnon-blocking concurrency control\uff0c\u6240\u4ee5\u5c06\u5b83\u653e\u5230\u4e86 Concurrent-computing\\Concurrency-control\\Non-blocking \u7ae0\u8282\u3002 Wikipedia Non-blocking I/O \u8865\u5145\u5185\u5bb9 1) Node.js About Node.js Node.js-Overview of Blocking vs Non-Blocking 2) Unix-The TTY demystified 3) redis-An introduction to Redis data types and abstractions \u5176\u4e2dBlocking operations on lists\u7ae0\u8282\u7684\u89c2\u70b9\u975e\u5e38\u5177\u6709\u53c2\u8003\u610f\u4e49 wikipedia-Asynchronous-IO wikipedia-Process-state","title":"Introduction"},{"location":"Programming-model/Blocking-and-non-blocking/#blocking#and#non-blocking","text":"","title":"Blocking and non-blocking"},{"location":"Programming-model/Blocking-and-non-blocking/#block","text":"\"block\"\u7684\u539f\u610f\u662f: \"\u963b\u585e\"\uff0c\u662f\u6307\u5f53\u524d\u7684\u6d41\u7a0b\u88ab\u6682\u505c\u4e86\uff0c\u5b83\u88ab\u6682\u505c\u7684\u539f\u56e0\u6709: 1) \u7b49\u5f85\u8017\u65f6\u7684\u64cd\u4f5c\u5b8c\u6210 \u6bd4\u5982\u6267\u884c\u4e86\u4e00\u4e2a\u975e\u5e38\u8017\u65f6\u7684\u64cd\u4f5c\u3002 2) \u7b49\u5f85\u6761\u4ef6\u6ee1\u8db3 \u6bd4\u5982\u7b49\u5f85lock\u3002","title":"Block\u7684\u542b\u4e49"},{"location":"Programming-model/Blocking-and-non-blocking/#blocking","text":"","title":"Blocking"},{"location":"Programming-model/Blocking-and-non-blocking/#wikipedia#blocking#computing","text":"In computing , a process is an instance of a computer program that is being executed. A process always exists in exactly one process state . A process that is blocked is one that is waiting for some event , such as a resource becoming available or the completion of an I/O operation.[ 1] NOTE: \u5728I/O system call\u4e2d\uff0c\u5c31\u6709non blocking I/O In a multitasking computer system, individual tasks , or threads of execution , must share the resources of the system. Shared resources include: the CPU, network and network interfaces, memory and disk. When one task is using a resource, it is generally not possible, or desirable, for another task to access it. The techniques of mutual exclusion are used to prevent this concurrent use. When the other task is blocked, it is unable to execute until the first task has finished using the shared resource. Programming languages and scheduling algorithms are designed to minimize the over-all(\u5168\u9762\u7684\uff0c\u6574\u4f53\u7684) effect blocking. A process that blocks may prevent local work-tasks from progressing. In this case \"blocking\" often is seen as not wanted.[ 2] However, such work-tasks may instead have been assigned to independent processes, where halting one has no or little effect on the others, since scheduling will continue. An example is \"blocking on a channel \" where passively(\u53d8\u52a8\u5730) waiting for the other part (no polling or spin loop) is part of the semantics of channels.[ 3] Correctly engineered any of these may be used to implement reactive systems. Deadlock means that processes pathologically wait for each other in a circle. As such it is not directly associated with blocking. Once the event occurs for which the process is waiting (\"is blocked on\"), the process is advanced from blocked state to an imminent one, such as runnable .","title":"wikipedia Blocking (computing)"},{"location":"Programming-model/Blocking-and-non-blocking/#see#also","text":"Non-blocking algorithm Non-blocking synchronization Concurrent computing Race condition Deadlock Scheduling (computing) Data dependency","title":"See also"},{"location":"Programming-model/Blocking-and-non-blocking/#blocking#and#timeout","text":"\u5728\u5f88\u591a\u5e94\u7528\u4e2d\uff0c\u4e00\u76f4\u5c06application block\u662f\u975e\u5e38\u5371\u9669\u7684\uff0c\u56e0\u4e3a\u5b83\u53ef\u80fd\u5bfc\u81f4\u7cfb\u7edf\u4e0d\u53ef\u7528\uff1b\u6240\u4ee5\u4e3ablock\u8bbe\u7f6e\u4e00\u4e2atimeout\u662f\u975e\u5e38\u91cd\u8981\u7684\uff1b NOTE: 1\u3001system call with timeout","title":"Blocking and timeout"},{"location":"Programming-model/Blocking-and-non-blocking/#processblocked","text":"OS\u4e2d\uff0cprocess\u6709blocked\u72b6\u6001\uff0c\u663e\u7136\u662f\u7b26\u5408\u524d\u9762\u7684\u63cf\u8ff0\u7684\u3002","title":"Process\u7684blocked\u72b6\u6001"},{"location":"Programming-model/Blocking-and-non-blocking/#non-blocking","text":"\u663e\u7136Non-blocking\u662f\u548cBlocking\u76f8\u5bf9\u7684\u3002","title":"Non-blocking"},{"location":"Programming-model/Blocking-and-non-blocking/#wikipedia#non-blocking#algorithm","text":"NOTE: \u8fd9\u7bc7\u6587\u7ae0\u91cd\u8981\u8bb2\u8ff0\u7684\u662fnon-blocking concurrency control\uff0c\u6240\u4ee5\u5c06\u5b83\u653e\u5230\u4e86 Concurrent-computing\\Concurrency-control\\Non-blocking \u7ae0\u8282\u3002","title":"Wikipedia Non-blocking algorithm"},{"location":"Programming-model/Blocking-and-non-blocking/#wikipedia#non-blocking#io","text":"","title":"Wikipedia Non-blocking I/O"},{"location":"Programming-model/Blocking-and-non-blocking/#_1","text":"1) Node.js About Node.js Node.js-Overview of Blocking vs Non-Blocking 2) Unix-The TTY demystified 3) redis-An introduction to Redis data types and abstractions \u5176\u4e2dBlocking operations on lists\u7ae0\u8282\u7684\u89c2\u70b9\u975e\u5e38\u5177\u6709\u53c2\u8003\u610f\u4e49 wikipedia-Asynchronous-IO wikipedia-Process-state","title":"\u8865\u5145\u5185\u5bb9"},{"location":"Programming-model/Blocking-and-synchronous/","text":"\u5173\u4e8e\u672c\u7ae0 Blocking\u3001non-blocking\u548csynchronous\u3001asynchronous\u662f\u7ecf\u5e38\u51fa\u73b0\u7684\u6982\u5ff5\uff0c\u672c\u7ae0\u5bf9\u5b83\u4eec\u8fdb\u884c\u533a\u5206\u3001\u603b\u7ed3\u3002\u5728\u4f7f\u7528\u3001\u7406\u89e3\u7740\u4e24\u4e2a\u4e24\u7ec4\u8bcd\u8bed\u7684\u65f6\u5019\uff0c\u9700\u8981\u4ece\u5b83\u7684\u672c\u8d28\u542b\u4e49\u51fa\u53d1\uff0c\u800c\u4e0d\u5e94\u8be5\u4ece\u5b83\u7684\u5177\u4f53\u6848\u4f8b\u6765\u7406\u89e3\uff0c\u4e00\u4e2a\u5178\u578b\u7684\u4f8b\u5b50\u5c31\u662f\u5728\u5de5\u7a0bLinux-OS\u7684 Programming\\IO\\IO-model \u7ae0\u8282\u4e2d\uff0c\u4ece\u4e0d\u540c\u7684\u89d2\u5ea6\u6765\u8bf4\u660e\uff0c\u53ef\u4ee5\u5bf9IO model\u8fdb\u884c\u4e0d\u540c\u7684\u5206\u7c7b\uff0c\u8fd9\u4e9b\u5206\u7c7b\u90fd\u662f\u6b63\u786e\u7684\uff0c\u5b83\u4eec\u4f7f\u7528\u8fd9\u4e24\u7ec4\u8bcd\u8bed\u4e5f\u662f\u6b63\u786e\u7684\u3002 IO: blocking\u3001non-blocking\u548csynchronous\u3001asynchronous\u7ecf\u5e38\u4fee\u9970IO\uff0c\u5173\u4e8e\u8fd9\u4e2a\u4e3b\u9898\uff0c\u53c2\u89c1\u5de5\u7a0bLinux-OS\u7684 Programming\\IO\\IO-model \u7ae0\u8282\u3002","title":"Introduction"},{"location":"Programming-model/Blocking-and-synchronous/#_1","text":"Blocking\u3001non-blocking\u548csynchronous\u3001asynchronous\u662f\u7ecf\u5e38\u51fa\u73b0\u7684\u6982\u5ff5\uff0c\u672c\u7ae0\u5bf9\u5b83\u4eec\u8fdb\u884c\u533a\u5206\u3001\u603b\u7ed3\u3002\u5728\u4f7f\u7528\u3001\u7406\u89e3\u7740\u4e24\u4e2a\u4e24\u7ec4\u8bcd\u8bed\u7684\u65f6\u5019\uff0c\u9700\u8981\u4ece\u5b83\u7684\u672c\u8d28\u542b\u4e49\u51fa\u53d1\uff0c\u800c\u4e0d\u5e94\u8be5\u4ece\u5b83\u7684\u5177\u4f53\u6848\u4f8b\u6765\u7406\u89e3\uff0c\u4e00\u4e2a\u5178\u578b\u7684\u4f8b\u5b50\u5c31\u662f\u5728\u5de5\u7a0bLinux-OS\u7684 Programming\\IO\\IO-model \u7ae0\u8282\u4e2d\uff0c\u4ece\u4e0d\u540c\u7684\u89d2\u5ea6\u6765\u8bf4\u660e\uff0c\u53ef\u4ee5\u5bf9IO model\u8fdb\u884c\u4e0d\u540c\u7684\u5206\u7c7b\uff0c\u8fd9\u4e9b\u5206\u7c7b\u90fd\u662f\u6b63\u786e\u7684\uff0c\u5b83\u4eec\u4f7f\u7528\u8fd9\u4e24\u7ec4\u8bcd\u8bed\u4e5f\u662f\u6b63\u786e\u7684\u3002 IO: blocking\u3001non-blocking\u548csynchronous\u3001asynchronous\u7ecf\u5e38\u4fee\u9970IO\uff0c\u5173\u4e8e\u8fd9\u4e2a\u4e3b\u9898\uff0c\u53c2\u89c1\u5de5\u7a0bLinux-OS\u7684 Programming\\IO\\IO-model \u7ae0\u8282\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Programming-model/Sync-and-async/","text":"Async and sync \u4eca\u5929\u5728\u9605\u8bfbjQuery\u7684ajax\u7684\u4ee3\u7801\u7684\u65f6\u5019\uff0c\u770b\u5230\u4e86\u5176\u4e2d\u6709\u975e\u5e38\u591a\u7684**\u56de\u8c03\u51fd\u6570**\u7b49\uff0c\u663e\u7136\u8fd9\u662f**\u5f02\u6b65\u7f16\u7a0b**\u7684\u601d\u60f3\uff0c\u4e8e\u662f\u6211\u60f3\uff0c\u6709\u5fc5\u8981\u603b\u7ed3Asynchronous programming\u7684\u76f8\u5173\u77e5\u8bc6\u4e86\u3002 Async \"async\"\u5373\"\u5f02\u6b65\"\u3002asynchronous\u7684\u601d\u60f3\u662f: \u5bf9\u4e8e\u8017\u65f6\u64cd\u4f5c\uff0c\u4e0d\u7b49\u5f85\u5b83\u6267\u884c\u5b8c\u6210\uff0c\u8ba9\u5b83\u5728**main loop/flow \u4e4b\u5916\u72ec\u7acb\u8fd0\u884c\u800c\u4e0d\u963b\u585e**main loop \uff0c\u8fd9\u6837main loop\u5c31\u53ef\u4ee5\u5728\u5b83\u6267\u884c\u7684\u8fd9\u6bb5\u65f6\u95f4\u5185\u53bb\u5904\u7406\u5176\u4ed6\u7684\u4e8b\u60c5\uff0c\u5f85\u8017\u65f6\u64cd\u4f5c\u5b8c\u6210\u7684\u65f6\u5019(\u662f\u4e00\u4e2a**event**)\uff0c\u5b83\u518d**\u901a\u77e5**(notify)main loop\u5b83\u7684\u5b8c\u6210\uff0c\u7136\u540e**main loop**\u518d\u6765\u5904\u7406\u5b83\u7684\u7ed3\u679c\u3002 \u5f53\u8fdb\u884c**asynchronous programming**\u7684\u65f6\u5019\uff0c\u5f80\u5f80\u9700\u8981\u8868\u8fbe\"\u5f53 \u67d0\u4e2a**event** \u53d1\u751f\u65f6\uff0c\u6267\u884c \u67d0\u4e2a callback \"\uff0c\u8fd9\u4e2a**callback**\u5f80\u5f80\u662f\u7528\u6237\u6ce8\u518c\u7684\u81ea\u5b9a\u4e49\u51fd\u6570\uff0c\u8fd9\u79cd\u662f\u53ef\u4ee5\u4f7f\u7528fluent API\u6765\u8fdb\u884c\u63cf\u8ff0\u7684\uff0c\u5b83\u5176\u5b9e\u975e\u5e38\u7c7b\u4f3c\u4e8ebuilder pattern\uff0c\u5373\u7531\u7528\u6237\u6765\u8bbe\u7f6e**callback**\uff0c\u6700\u6700\u5178\u578b\u7684\u5c31\u662fjQuery\uff0c\u73b0\u4ee3\u5f88\u591aprogramming language\u90fd\u662f\u652f\u6301\u8fd9\u79cdparadigm\u7684\u3002 Wikipedia Asynchrony (computer programming) https://en.wikipedia.org/wiki/Asynchrony_(computer_programming)asynchronous Asynchrony , in computer programming , refers to the occurrence of events independently of the main program flow and ways to deal with such events. These may be \"outside\" events such as the arrival of signals , or actions instigated(\u53d1\u8d77\u7684) by a program that take place concurrently with program execution, without the program blocking to wait for results.[ 1] Asynchronous input/output is an example of the latter cause(\u539f\u56e0) of asynchrony , and lets programs issue commands to storage or network devices that service these requests while the processor continues executing the program. Doing so provides a degree of parallelism .[ 1] NOTE: Asynchrony**\u7684\u672c\u8d28\u542b\u4e49: \u67d0\u4e2a**event**\u7684\u51fa\u73b0\u662f\u72ec\u7acb\u4e8e **main program flow \u7684\uff0c\u8fd9\u4e2aevent\u53ef\u80fd\u662finside\u7684\uff0c\u4e5f\u53ef\u80fd\u662foutside\u7684\uff0c\u4e0b\u9762\u662f\u4f8b\u5b50: 1) outside: signal\u662f asynchronous event\uff0c\u5e76\u4e14\u5b83\u662f\u5178\u578b\u7684 outside event\uff0c\u5b83\u7684\u51fa\u73b0\u662f\u72ec\u7acb\u4e8emain program flow\u7684\u3002 2) inside: \" Asynchronous input/output is an example of the latter cause(\u539f\u56e0) of asynchrony \"\u7684\u610f\u601d\u662f: asynchronous IO\u5f15\u53d1**asynchrony**\u7684\u53e6\u5916\u4e00\u4e2a\u539f\u56e0\uff0c\u56e0\u4e3aIO\u5b8c\u6210\u7684event\u662f \u72ec\u7acb\u4e8e main program flow\u7684\u3002 \u4ececontrol theory\u7684\u89d2\u5ea6\u6765\u770b\uff0casynchronous event\u662f\u4e0d\u662f\u7531\u5f53\u524dprogram\u63a7\u5236\u7684\uff0c\u5b83\u662f\u7531\u5916\u90e8\u7cfb\u7edf\u63a7\u5236\u7684\uff0c\u7531\u5916\u90e8\u7cfb\u7edf\u8fdb\u884c\u901a\u77e5\u3002 A common way for dealing with asynchrony in a programming interface is to provide subroutines (methods, functions) that return to their caller an object , sometimes called a future or promise , that represents the ongoing events . Such an object will then typically come with a synchronizing operation that blocks until the operation is completed. Some programming languages, such as Cilk , have special syntax for expressing an asynchronous procedure call.[ 2] NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u5bf9future/promise\u7684\u4ecb\u7ecd\u662f\u975e\u5e38\u597d\u7684\uff0c\u4eceasynchrony\u7684\u89d2\u5ea6\u975e\u5e38\u5bb9\u6613\u7406\u89e3\u5f15\u5165\"future or promise\"\u7684\u76ee\u7684\u3002 Examples of asynchrony 1) Ajax \" Ajax \", short for \"asynchronous JavaScript and XML \")[ 3] [ 4] [ 5] is a set of web development techniques utilizing many web technologies used on the client-side to create asynchronous I/O Web applications . 2) Asynchronous method dispatch Asynchronous method dispatch (AMD), a data communication method used when there is a need for the server side to handle a large number of long lasting client requests.[ 6] 3) Asynchronous method invocation https://en.wikipedia.org/wiki/Asynchronous_method_invocation 4) Interrupt \u4eca\u5929\u5728\u9605\u8bfb Wikipedia\u7684 context switch \u7684\u65f6\u5019\uff0c\u5176\u4e2d\u63d0\u53ca\u4e86interrupt\uff0c\u663e\u7136\uff0cinterrupt\u662f\u4e00\u79cd\u5178\u578b\u7684asynchronous\u65b9\u5f0f\u3002 5) Unix signal \u53c2\u89c1\u5de5\u7a0bLinux-OS\u7684 Programming\\IO\\Blocking-and-non-blocking-VS-sync-and-async \u7ae0\u8282\u3002 6) Asynchronous IO \u53c2\u89c1\u5de5\u7a0bLinux-OS\u7684 Programming\\IO\\IO-model \u7ae0\u8282\u3002 Async to sync \u672c\u6bb5\u7684\u610f\u601d\u662f: \u5c06asynchronous API\u8f6c\u6362\u4e3asynchronous API\uff0c\u56e0\u4e3a\u5728\u4e00\u4e9b\u60c5\u51b5\u4e0b\uff0c\u57fa\u4e8easynchronous + callback\u7684\u6a21\u5f0f\u662f\u4e0d\u65b9\u4fbf\u4f7f\u7528\u7684\uff0c Examples \u4e0b\u9762\u662f\u6211\u9047\u5230\u7684\u4e00\u4e9b\u5c06asynchronous operation\u8f6c\u6362\u4e3asynchronize operation\u7684\u4f8b\u5b50: 1\u3001amust API\uff0c\u4f7f\u7528promise-future\u3001condition variable 2\u3001 tgockel / zookeeper-cpp Synchronous API The C library offers both a synchronous and an asynchronous API. This library offers only an asynchronous version. If you prefer a synchronous API, call get() on the returned future to block until you receive the response. 3\u3001 yyzybb537 / libgo Provide golang's General powerful protocol, write code based on coroutine, can write simple code in a synchronous manner, while achieving asynchronous performance. Blocking with timeout \u663e\u7136\uff0c\u5728\u6ca1\u6709\u6536\u5230\u54cd\u5e94\u4e4b\u524d\uff0c\u9700\u8981\u5c06caller\u963b\u585e\uff0c\u56e0\u6b64\u9700\u8981\u8003\u8651\u7684\u4e00\u4e2a\u95ee\u9898\u662f: blocking with timeout\uff0c\u56e0\u4e3a\u6709\u53ef\u80fd\u4e00\u76f4\u65e0\u6cd5\u6536\u5230\u54cd\u5e94\uff0c\u56e0\u6b64\u9700\u8981\u8bbe\u7f6e\u4e00\u4e2a\u8d85\u65f6\u65f6\u95f4\u3002 Sync to async 1\u3001\u5178\u578b\u7684\u5b9e\u73b0\u65b9\u5f0f\u662fasynchronous method invocation + message queue(mailbox) + thread pool \u4f8b\u5b50\u5305\u62ec: a\u3001libuv filesystem operation\uff0c\u53c2\u89c1 http://docs.libuv.org/en/v1.x/design.html","title":"Introduction"},{"location":"Programming-model/Sync-and-async/#async#and#sync","text":"\u4eca\u5929\u5728\u9605\u8bfbjQuery\u7684ajax\u7684\u4ee3\u7801\u7684\u65f6\u5019\uff0c\u770b\u5230\u4e86\u5176\u4e2d\u6709\u975e\u5e38\u591a\u7684**\u56de\u8c03\u51fd\u6570**\u7b49\uff0c\u663e\u7136\u8fd9\u662f**\u5f02\u6b65\u7f16\u7a0b**\u7684\u601d\u60f3\uff0c\u4e8e\u662f\u6211\u60f3\uff0c\u6709\u5fc5\u8981\u603b\u7ed3Asynchronous programming\u7684\u76f8\u5173\u77e5\u8bc6\u4e86\u3002","title":"Async and sync"},{"location":"Programming-model/Sync-and-async/#async","text":"\"async\"\u5373\"\u5f02\u6b65\"\u3002asynchronous\u7684\u601d\u60f3\u662f: \u5bf9\u4e8e\u8017\u65f6\u64cd\u4f5c\uff0c\u4e0d\u7b49\u5f85\u5b83\u6267\u884c\u5b8c\u6210\uff0c\u8ba9\u5b83\u5728**main loop/flow \u4e4b\u5916\u72ec\u7acb\u8fd0\u884c\u800c\u4e0d\u963b\u585e**main loop \uff0c\u8fd9\u6837main loop\u5c31\u53ef\u4ee5\u5728\u5b83\u6267\u884c\u7684\u8fd9\u6bb5\u65f6\u95f4\u5185\u53bb\u5904\u7406\u5176\u4ed6\u7684\u4e8b\u60c5\uff0c\u5f85\u8017\u65f6\u64cd\u4f5c\u5b8c\u6210\u7684\u65f6\u5019(\u662f\u4e00\u4e2a**event**)\uff0c\u5b83\u518d**\u901a\u77e5**(notify)main loop\u5b83\u7684\u5b8c\u6210\uff0c\u7136\u540e**main loop**\u518d\u6765\u5904\u7406\u5b83\u7684\u7ed3\u679c\u3002 \u5f53\u8fdb\u884c**asynchronous programming**\u7684\u65f6\u5019\uff0c\u5f80\u5f80\u9700\u8981\u8868\u8fbe\"\u5f53 \u67d0\u4e2a**event** \u53d1\u751f\u65f6\uff0c\u6267\u884c \u67d0\u4e2a callback \"\uff0c\u8fd9\u4e2a**callback**\u5f80\u5f80\u662f\u7528\u6237\u6ce8\u518c\u7684\u81ea\u5b9a\u4e49\u51fd\u6570\uff0c\u8fd9\u79cd\u662f\u53ef\u4ee5\u4f7f\u7528fluent API\u6765\u8fdb\u884c\u63cf\u8ff0\u7684\uff0c\u5b83\u5176\u5b9e\u975e\u5e38\u7c7b\u4f3c\u4e8ebuilder pattern\uff0c\u5373\u7531\u7528\u6237\u6765\u8bbe\u7f6e**callback**\uff0c\u6700\u6700\u5178\u578b\u7684\u5c31\u662fjQuery\uff0c\u73b0\u4ee3\u5f88\u591aprogramming language\u90fd\u662f\u652f\u6301\u8fd9\u79cdparadigm\u7684\u3002","title":"Async"},{"location":"Programming-model/Sync-and-async/#wikipedia#asynchrony#computer#programming","text":"https://en.wikipedia.org/wiki/Asynchrony_(computer_programming)asynchronous Asynchrony , in computer programming , refers to the occurrence of events independently of the main program flow and ways to deal with such events. These may be \"outside\" events such as the arrival of signals , or actions instigated(\u53d1\u8d77\u7684) by a program that take place concurrently with program execution, without the program blocking to wait for results.[ 1] Asynchronous input/output is an example of the latter cause(\u539f\u56e0) of asynchrony , and lets programs issue commands to storage or network devices that service these requests while the processor continues executing the program. Doing so provides a degree of parallelism .[ 1] NOTE: Asynchrony**\u7684\u672c\u8d28\u542b\u4e49: \u67d0\u4e2a**event**\u7684\u51fa\u73b0\u662f\u72ec\u7acb\u4e8e **main program flow \u7684\uff0c\u8fd9\u4e2aevent\u53ef\u80fd\u662finside\u7684\uff0c\u4e5f\u53ef\u80fd\u662foutside\u7684\uff0c\u4e0b\u9762\u662f\u4f8b\u5b50: 1) outside: signal\u662f asynchronous event\uff0c\u5e76\u4e14\u5b83\u662f\u5178\u578b\u7684 outside event\uff0c\u5b83\u7684\u51fa\u73b0\u662f\u72ec\u7acb\u4e8emain program flow\u7684\u3002 2) inside: \" Asynchronous input/output is an example of the latter cause(\u539f\u56e0) of asynchrony \"\u7684\u610f\u601d\u662f: asynchronous IO\u5f15\u53d1**asynchrony**\u7684\u53e6\u5916\u4e00\u4e2a\u539f\u56e0\uff0c\u56e0\u4e3aIO\u5b8c\u6210\u7684event\u662f \u72ec\u7acb\u4e8e main program flow\u7684\u3002 \u4ececontrol theory\u7684\u89d2\u5ea6\u6765\u770b\uff0casynchronous event\u662f\u4e0d\u662f\u7531\u5f53\u524dprogram\u63a7\u5236\u7684\uff0c\u5b83\u662f\u7531\u5916\u90e8\u7cfb\u7edf\u63a7\u5236\u7684\uff0c\u7531\u5916\u90e8\u7cfb\u7edf\u8fdb\u884c\u901a\u77e5\u3002 A common way for dealing with asynchrony in a programming interface is to provide subroutines (methods, functions) that return to their caller an object , sometimes called a future or promise , that represents the ongoing events . Such an object will then typically come with a synchronizing operation that blocks until the operation is completed. Some programming languages, such as Cilk , have special syntax for expressing an asynchronous procedure call.[ 2] NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u5bf9future/promise\u7684\u4ecb\u7ecd\u662f\u975e\u5e38\u597d\u7684\uff0c\u4eceasynchrony\u7684\u89d2\u5ea6\u975e\u5e38\u5bb9\u6613\u7406\u89e3\u5f15\u5165\"future or promise\"\u7684\u76ee\u7684\u3002","title":"Wikipedia Asynchrony (computer programming)"},{"location":"Programming-model/Sync-and-async/#examples#of#asynchrony","text":"1) Ajax \" Ajax \", short for \"asynchronous JavaScript and XML \")[ 3] [ 4] [ 5] is a set of web development techniques utilizing many web technologies used on the client-side to create asynchronous I/O Web applications . 2) Asynchronous method dispatch Asynchronous method dispatch (AMD), a data communication method used when there is a need for the server side to handle a large number of long lasting client requests.[ 6] 3) Asynchronous method invocation https://en.wikipedia.org/wiki/Asynchronous_method_invocation 4) Interrupt \u4eca\u5929\u5728\u9605\u8bfb Wikipedia\u7684 context switch \u7684\u65f6\u5019\uff0c\u5176\u4e2d\u63d0\u53ca\u4e86interrupt\uff0c\u663e\u7136\uff0cinterrupt\u662f\u4e00\u79cd\u5178\u578b\u7684asynchronous\u65b9\u5f0f\u3002 5) Unix signal \u53c2\u89c1\u5de5\u7a0bLinux-OS\u7684 Programming\\IO\\Blocking-and-non-blocking-VS-sync-and-async \u7ae0\u8282\u3002 6) Asynchronous IO \u53c2\u89c1\u5de5\u7a0bLinux-OS\u7684 Programming\\IO\\IO-model \u7ae0\u8282\u3002","title":"Examples of asynchrony"},{"location":"Programming-model/Sync-and-async/#async#to#sync","text":"\u672c\u6bb5\u7684\u610f\u601d\u662f: \u5c06asynchronous API\u8f6c\u6362\u4e3asynchronous API\uff0c\u56e0\u4e3a\u5728\u4e00\u4e9b\u60c5\u51b5\u4e0b\uff0c\u57fa\u4e8easynchronous + callback\u7684\u6a21\u5f0f\u662f\u4e0d\u65b9\u4fbf\u4f7f\u7528\u7684\uff0c","title":"Async to sync"},{"location":"Programming-model/Sync-and-async/#examples","text":"\u4e0b\u9762\u662f\u6211\u9047\u5230\u7684\u4e00\u4e9b\u5c06asynchronous operation\u8f6c\u6362\u4e3asynchronize operation\u7684\u4f8b\u5b50: 1\u3001amust API\uff0c\u4f7f\u7528promise-future\u3001condition variable 2\u3001 tgockel / zookeeper-cpp","title":"Examples"},{"location":"Programming-model/Sync-and-async/#synchronous#api","text":"The C library offers both a synchronous and an asynchronous API. This library offers only an asynchronous version. If you prefer a synchronous API, call get() on the returned future to block until you receive the response. 3\u3001 yyzybb537 / libgo Provide golang's General powerful protocol, write code based on coroutine, can write simple code in a synchronous manner, while achieving asynchronous performance.","title":"Synchronous API"},{"location":"Programming-model/Sync-and-async/#blocking#with#timeout","text":"\u663e\u7136\uff0c\u5728\u6ca1\u6709\u6536\u5230\u54cd\u5e94\u4e4b\u524d\uff0c\u9700\u8981\u5c06caller\u963b\u585e\uff0c\u56e0\u6b64\u9700\u8981\u8003\u8651\u7684\u4e00\u4e2a\u95ee\u9898\u662f: blocking with timeout\uff0c\u56e0\u4e3a\u6709\u53ef\u80fd\u4e00\u76f4\u65e0\u6cd5\u6536\u5230\u54cd\u5e94\uff0c\u56e0\u6b64\u9700\u8981\u8bbe\u7f6e\u4e00\u4e2a\u8d85\u65f6\u65f6\u95f4\u3002","title":"Blocking with timeout"},{"location":"Programming-model/Sync-and-async/#sync#to#async","text":"1\u3001\u5178\u578b\u7684\u5b9e\u73b0\u65b9\u5f0f\u662fasynchronous method invocation + message queue(mailbox) + thread pool \u4f8b\u5b50\u5305\u62ec: a\u3001libuv filesystem operation\uff0c\u53c2\u89c1 http://docs.libuv.org/en/v1.x/design.html","title":"Sync to async"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/","text":"Asynchronous programming Programming model Fluent API Asynchronous programming and multithreading https://stackoverflow.com/questions/34680985/what-is-the-difference-between-asynchronous-programming-and-multithreading https://codewala.net/2015/07/29/concurrency-vs-multi-threading-vs-asynchronous-programming-explained/asynchronous Asynchronous programming patterns asynchronous programming paradigm asynchronous programming javascript https://eloquentjavascript.net/11_async.html Asynchronous programming python https://realpython.com/async-io-python/ https://dbader.org/blog/understanding-asynchronous-programming-in-python https://stackoverflow.com/questions/3221314/asynchronous-programming-in-python https://xph.us/2009/12/10/asynchronous-programming-in-python.html https://stackoverflow.com/questions/2625493/asynchronous-vs-non-blocking https://stackoverflow.com/questions/7931537/whats-the-difference-between-asynchronous-non-blocking-event-base-architectu/9489547#9489547 Now that Python has coroutines (e.g. async/await), does multi-threading still have a use? ( self.Python ) \u8fd9\u7bc7\u6587\u7ae0\u63d0\u53ca\u4e86IO bound\u548cCPU bound\uff0c\u8fd9\u5bf9\u4e8e\u9009\u53d6concurrency\u65b9\u5f0f\u975e\u5e38\u91cd\u8981\uff1b","title":"Introduction"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/#asynchronous#programming","text":"","title":"Asynchronous programming"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/#programming#model","text":"","title":"Programming model"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/#fluent#api","text":"","title":"Fluent API"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/#asynchronous#programming#and#multithreading","text":"https://stackoverflow.com/questions/34680985/what-is-the-difference-between-asynchronous-programming-and-multithreading https://codewala.net/2015/07/29/concurrency-vs-multi-threading-vs-asynchronous-programming-explained/asynchronous","title":"Asynchronous programming and multithreading"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/#asynchronous#programming#patterns","text":"asynchronous programming paradigm asynchronous programming javascript https://eloquentjavascript.net/11_async.html","title":"Asynchronous programming patterns"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/#asynchronous#programming#python","text":"https://realpython.com/async-io-python/ https://dbader.org/blog/understanding-asynchronous-programming-in-python https://stackoverflow.com/questions/3221314/asynchronous-programming-in-python https://xph.us/2009/12/10/asynchronous-programming-in-python.html https://stackoverflow.com/questions/2625493/asynchronous-vs-non-blocking https://stackoverflow.com/questions/7931537/whats-the-difference-between-asynchronous-non-blocking-event-base-architectu/9489547#9489547 Now that Python has coroutines (e.g. async/await), does multi-threading still have a use? ( self.Python ) \u8fd9\u7bc7\u6587\u7ae0\u63d0\u53ca\u4e86IO bound\u548cCPU bound\uff0c\u8fd9\u5bf9\u4e8e\u9009\u53d6concurrency\u65b9\u5f0f\u975e\u5e38\u91cd\u8981\uff1b","title":"Asynchronous programming python"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Coroutine-async-await/","text":"Async/await 1\u3001\"async\"\u7684\u542b\u4e49\u662f\"asynchronous\"\uff0c\"await\"\u7684\u542b\u4e49\u662f\"\u7b49\u5f85\"\u3002 2\u3001\u611f\u89c9\u5b83\u4e5f\u662f\u57fa\u4e8e\"promise-future communication channel\"\u7684 wikipedia Async/await In computer programming , the async/await pattern is a syntactic feature of many programming languages that allows an asynchronous , non-blocking function to be structured in a way similar to an ordinary synchronous function . NOTE: async/await\u5176\u5b9e\u662fsyntax sugar\u3002 asynchronous function\u7684\u7ed3\u679cwill be binded to a promise\uff0c\u53c2\u89c1\u4e0b\u9762\u7684\"Example C#\"\u7ae0\u8282 It is semantically related to the concept of a coroutine (\u643a\u7a0b) and is often implemented using similar techniques, and is primarily intended to provide opportunities for the program to execute other code while waiting for a long-running, asynchronous task to complete, usually represented by promises or similar data structures. NOTE: \u63d0\u9ad8\u5e76\u53d1\u6027 The feature is found in C# 5.0 , Python 3.5, Hack , Dart , Kotlin 1.1, Rust 1.39,[ 1] Nim 0.9.4[ 2] and JavaScript ES2017 , with some experimental work in extensions, beta versions, and particular implementations of Scala [ 3] and C++ . NOTE: \u73b0\u4ee3\u4e3b\u6d41\u8bed\u8a00\u90fd\u5f00\u59cb\u8003\u8651\u652f\u6301\u5b83\u4e86 Example C The C# function below, which downloads a resource from a URI and returns the resource's length, uses this async/await pattern: public async Task < int > FindPageSize ( Uri uri ) { byte [] data = await new WebClient (). DownloadDataTaskAsync ( uri ); return data . Length ; } 1) First, the async keyword indicates to C# that the method is asynchronous , meaning that it may use an arbitrary number of await expressions and will bind the result to a promise . NOTE: \u8fd9\u6bb5\u8bdd\u7684\u610f\u601d\u662f the async keyword \u8868\u9762: \u8fd9\u4e2amethod\u662fasynchronous\u7684\uff0c\u610f\u5473\u7740 \u8fd9\u4e2amethod \u53ef\u4ee5\u4f7f\u7528 await expression\u3002 2) The return type, Task<T> , is C#'s analogue(\u7c7b\u4f3c\u7269) to the concept of a promise, and here is indicated to have a result value of type int . 3) The first expression to execute when this method is called will be new WebClient().DownloadDataTaskAsync(uri) , which is another asynchronous method returning a Task<byte[]> . Because this method is asynchronous , it will not download the entire batch of data before returning. Instead, it will begin the download process using a non-blocking mechanism (such as a background thread ), and immediately return an unresolved, unrejected Task<byte[]> to this function. NOTE: \u7acb\u5373\u8fd4\u56de\u800c\u4e0d\u963b\u585e 4) With the await keyword attached to the Task , this function will immediately proceed to return a Task<int> to its caller, who may then continue on with other processing as needed. NOTE: int Length = await FindPageSize(Uri) \uff0c\u4ece\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u6211\u4eec\u53ef\u4ee5\u63a8\u6d4b: await \u7684\u4f5c\u7528\u5bf9\u8c61\u662fpromise\u5373 Task \uff0c\u5b83\u4ecepromise\u53d6\u5f97\u7ed3\u679c\u7ed9\u7528\u6237\u3002 5) Once DownloadDataTaskAsync() finishes its download, it will resolve the Task it returned with the downloaded data. This will trigger a callback and cause FindPageSize() to continue execution by assigning that value to data . NOTE: \u65e2\u7136\u662fasynchronous\u7684\uff0c\u90a3\u4e48\u5b83\u5fc5\u987b\u8981\u63d0\u4f9b\u4e00\u79cd\u673a\u5236\u6765\u4fdd\u8bc1\u5f53asynchronous operation\u5b8c\u6210\u65f6\u8fdb\u884c\u901a\u77e5\uff0c\u901a\u8fc7\u4e0a\u8ff0\u63cf\u8ff0\u6765\u770b\uff0c\u5b83\u4e5f\u662f\u901a\u8fc7callback\u6765\u5b9e\u73b0\u7684\u3002\u901a\u8fc7\u4e0a\u8ff0\u63cf\u8ff0\u8fd8\u53ef\u4ee5\u770b\u51fa\uff0c C# \u7684\u5b9e\u73b0\u5176\u5b9e\u4fdd\u8bc1\u4e86\u4e00\u5b9a\u7684\u987a\u5e8f\u6027\u7684\u3002 6) Finally, the method returns data.Length , a simple integer indicating the length of the array. The compiler re-interprets this as resolving the Task it returned earlier, triggering a callback in the method's caller to do something with that length value. A function using async / await can use as many await expressions as it wants, and each will be handled in the same way (though(\u867d\u7136) a promise will only be returned to the caller for the first await, while every other await will utilize(\u5229\u7528) internal callbacks). A function can also hold a promise object directly and do other processing first (including starting other asynchronous tasks ), delaying awaiting the promise until its result is needed. Functions with promises also have promise aggregation methods that allow you to await multiple promises at once or in some special pattern (such as C#'s Task.WhenAll() , which returns a valueless Task that resolves when all of the tasks in the arguments have resolved). Many promise types also have additional features beyond what the async / await pattern normally uses, such as being able to set up more than one result callback or inspect the progress(\u8fdb\u5c55) of an especially long-running task. In C++ In C++, await (named co_await in C++) has been officially merged into C++20 draft, so it is on course to be formally accepted as a part of official C++20;[ 11] also MSVC and Clang compilers are already supporting at least some form of co_await ( GCC still has no support for it). #include <future> #include <iostream> using namespace std ; future < int > add ( int a , int b ) { int c = a + b ; co_return c ; } future < void > test () { int ret = co_await add ( 1 , 2 ); cout << \"return \" << ret << endl ; } int main () { auto fut = test (); fut . wait (); return 0 ; } TODO raywenderlich 5Async/Await lankydan Async/await in coroutines","title":"Introduction"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Coroutine-async-await/#asyncawait","text":"1\u3001\"async\"\u7684\u542b\u4e49\u662f\"asynchronous\"\uff0c\"await\"\u7684\u542b\u4e49\u662f\"\u7b49\u5f85\"\u3002 2\u3001\u611f\u89c9\u5b83\u4e5f\u662f\u57fa\u4e8e\"promise-future communication channel\"\u7684","title":"Async/await"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Coroutine-async-await/#wikipedia#asyncawait","text":"In computer programming , the async/await pattern is a syntactic feature of many programming languages that allows an asynchronous , non-blocking function to be structured in a way similar to an ordinary synchronous function . NOTE: async/await\u5176\u5b9e\u662fsyntax sugar\u3002 asynchronous function\u7684\u7ed3\u679cwill be binded to a promise\uff0c\u53c2\u89c1\u4e0b\u9762\u7684\"Example C#\"\u7ae0\u8282 It is semantically related to the concept of a coroutine (\u643a\u7a0b) and is often implemented using similar techniques, and is primarily intended to provide opportunities for the program to execute other code while waiting for a long-running, asynchronous task to complete, usually represented by promises or similar data structures. NOTE: \u63d0\u9ad8\u5e76\u53d1\u6027 The feature is found in C# 5.0 , Python 3.5, Hack , Dart , Kotlin 1.1, Rust 1.39,[ 1] Nim 0.9.4[ 2] and JavaScript ES2017 , with some experimental work in extensions, beta versions, and particular implementations of Scala [ 3] and C++ . NOTE: \u73b0\u4ee3\u4e3b\u6d41\u8bed\u8a00\u90fd\u5f00\u59cb\u8003\u8651\u652f\u6301\u5b83\u4e86","title":"wikipedia Async/await"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Coroutine-async-await/#example#c","text":"The C# function below, which downloads a resource from a URI and returns the resource's length, uses this async/await pattern: public async Task < int > FindPageSize ( Uri uri ) { byte [] data = await new WebClient (). DownloadDataTaskAsync ( uri ); return data . Length ; } 1) First, the async keyword indicates to C# that the method is asynchronous , meaning that it may use an arbitrary number of await expressions and will bind the result to a promise . NOTE: \u8fd9\u6bb5\u8bdd\u7684\u610f\u601d\u662f the async keyword \u8868\u9762: \u8fd9\u4e2amethod\u662fasynchronous\u7684\uff0c\u610f\u5473\u7740 \u8fd9\u4e2amethod \u53ef\u4ee5\u4f7f\u7528 await expression\u3002 2) The return type, Task<T> , is C#'s analogue(\u7c7b\u4f3c\u7269) to the concept of a promise, and here is indicated to have a result value of type int . 3) The first expression to execute when this method is called will be new WebClient().DownloadDataTaskAsync(uri) , which is another asynchronous method returning a Task<byte[]> . Because this method is asynchronous , it will not download the entire batch of data before returning. Instead, it will begin the download process using a non-blocking mechanism (such as a background thread ), and immediately return an unresolved, unrejected Task<byte[]> to this function. NOTE: \u7acb\u5373\u8fd4\u56de\u800c\u4e0d\u963b\u585e 4) With the await keyword attached to the Task , this function will immediately proceed to return a Task<int> to its caller, who may then continue on with other processing as needed. NOTE: int Length = await FindPageSize(Uri) \uff0c\u4ece\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u6211\u4eec\u53ef\u4ee5\u63a8\u6d4b: await \u7684\u4f5c\u7528\u5bf9\u8c61\u662fpromise\u5373 Task \uff0c\u5b83\u4ecepromise\u53d6\u5f97\u7ed3\u679c\u7ed9\u7528\u6237\u3002 5) Once DownloadDataTaskAsync() finishes its download, it will resolve the Task it returned with the downloaded data. This will trigger a callback and cause FindPageSize() to continue execution by assigning that value to data . NOTE: \u65e2\u7136\u662fasynchronous\u7684\uff0c\u90a3\u4e48\u5b83\u5fc5\u987b\u8981\u63d0\u4f9b\u4e00\u79cd\u673a\u5236\u6765\u4fdd\u8bc1\u5f53asynchronous operation\u5b8c\u6210\u65f6\u8fdb\u884c\u901a\u77e5\uff0c\u901a\u8fc7\u4e0a\u8ff0\u63cf\u8ff0\u6765\u770b\uff0c\u5b83\u4e5f\u662f\u901a\u8fc7callback\u6765\u5b9e\u73b0\u7684\u3002\u901a\u8fc7\u4e0a\u8ff0\u63cf\u8ff0\u8fd8\u53ef\u4ee5\u770b\u51fa\uff0c C# \u7684\u5b9e\u73b0\u5176\u5b9e\u4fdd\u8bc1\u4e86\u4e00\u5b9a\u7684\u987a\u5e8f\u6027\u7684\u3002 6) Finally, the method returns data.Length , a simple integer indicating the length of the array. The compiler re-interprets this as resolving the Task it returned earlier, triggering a callback in the method's caller to do something with that length value. A function using async / await can use as many await expressions as it wants, and each will be handled in the same way (though(\u867d\u7136) a promise will only be returned to the caller for the first await, while every other await will utilize(\u5229\u7528) internal callbacks). A function can also hold a promise object directly and do other processing first (including starting other asynchronous tasks ), delaying awaiting the promise until its result is needed. Functions with promises also have promise aggregation methods that allow you to await multiple promises at once or in some special pattern (such as C#'s Task.WhenAll() , which returns a valueless Task that resolves when all of the tasks in the arguments have resolved). Many promise types also have additional features beyond what the async / await pattern normally uses, such as being able to set up more than one result callback or inspect the progress(\u8fdb\u5c55) of an especially long-running task.","title":"Example C"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Coroutine-async-await/#in#c","text":"In C++, await (named co_await in C++) has been officially merged into C++20 draft, so it is on course to be formally accepted as a part of official C++20;[ 11] also MSVC and Clang compilers are already supporting at least some form of co_await ( GCC still has no support for it). #include <future> #include <iostream> using namespace std ; future < int > add ( int a , int b ) { int c = a + b ; co_return c ; } future < void > test () { int ret = co_await add ( 1 , 2 ); cout << \"return \" << ret << endl ; } int main () { auto fut = test (); fut . wait (); return 0 ; }","title":"In C++"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Coroutine-async-await/#todo","text":"raywenderlich 5Async/Await lankydan Async/await in coroutines","title":"TODO"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Coroutine-async-await/Coroutine/","text":"Coroutine \u8d8a\u6765\u8d8a\u591a\u7684language\u652f\u6301Coroutine\u3002 Coroutine\u662f\u73b0\u4ee3programming language\u5b9e\u73b0\"cooperative multitasking\"\u7684\u4e3b\u8981\u65b9\u5f0f\uff0c\u652f\u6301coroutine\u7684programming language\u57fa\u4e8e\u4e86programmer\u6765\u8fdb\u884ccooperative schedule task\u7684\u80fd\u529b\uff0c\u8ba9programmer\u80fd\u591fcontrol schedule of task\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0cprogrammer\u662f\u65e0\u6cd5\u76f4\u63a5\u63a7\u5236thread\u3001process\u7684schedule \u3002 wikipedia Coroutine Coroutines are computer program components that generalize subroutines for non-preemptive multitasking , by allowing execution to be suspended and resumed. Coroutines are well-suited for implementing familiar program components such as cooperative tasks , exceptions , event loops , iterators , infinite lists and pipes . According to Donald Knuth , Melvin Conway coined the term coroutine in 1958 when he applied it to construction of an assembly program . The first published explanation of the coroutine appeared later, in 1963. Comparison with subroutines Subroutines are special cases of coroutines . When subroutines are invoked, execution begins at the start, and once a subroutine exits, it is finished; an instance of a subroutine only returns once, and does not hold state between invocations. By contrast, coroutines can exit by calling other coroutines, which may later return to the point where they were invoked in the original coroutine; from the coroutine's point of view, it is not exiting but calling another coroutine. Thus, a coroutine instance holds state, and varies between invocations; there can be multiple instances of a given coroutine at once. The difference between calling another coroutine by means of \"yielding\" to it and simply calling another routine (which then, also, would return to the original point), is that the relationship between two coroutines which yield to each other is not that of caller-callee, but instead symmetric. NOTE: \u6700\u540e\u4e00\u6bb5\u8bdd\u975e\u5e38\u91cd\u8981\uff0c\u5b83\u7684\u610f\u601d\u662f\uff1a\u901a\u8fc7 \"yielding\" \u6765\u8c03\u7528\u53e6\u5916\u4e00\u4e2acoroutine\u548c\u7b80\u5355\u8c03\u7528\u53e6\u5916\u4e00\u4e2aroutine\u4e4b\u95f4\u7684\u5dee\u5f02\u662f\u4e24\u8005\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u901a\u8fc7yield\u7684coroutine\u4e4b\u95f4\u7684\u5173\u7cfb\u4e0d\u662fcaller-callee\uff0c\u800c\u662f\u5bf9\u79f0\u7684\uff1b\u7406\u89e3\u8fd9\u6bb5\u8bdd\u7684\u5173\u952e\u662f\u5bf9 \"yielding\" \u7406\u89e3\uff0cyield\u5373\u8ba9\u6e21\uff0c\u505c\u6b62\u5f53\u524dcoroutine\u5e76\u8ba9\u53e6\u5916\u4e00\u4e2acoroutine\u5f00\u59cb\u6267\u884c\u3002 Any subroutine can be translated to a coroutine which does not call yield . Here is a simple example of how coroutines can be useful. Suppose you have a consumer-producer relationship where one routine creates items and adds them to a queue and another removes items from the queue and uses them. For reasons of efficiency, you want to add and remove several items at once. The code might look like this: var q := new queue coroutine produce loop while q is not full create some new items add the items to q yield to consume coroutine consume loop while q is not empty remove some items from q use the items yield to produce The queue is then completely filled or emptied before yielding control to the other coroutine using the yield command. The further coroutines calls are starting right after the yield , in the outer coroutine loop. Although this example is often used as an introduction to multithreading , two threads are not needed for this: the yield statement can be implemented by a jump directly from one routine into the other. Comparison with threads Coroutines are very similar to threads . However, coroutines are cooperatively multitasked, whereas threads are typically preemptively multitasked . This means that coroutines provide concurrency but not parallelism . The advantages of coroutines over threads are that they may be used in a hard-realtime context ( switching between coroutines need not involve any system calls or any blocking calls whatsoever), there is no need for synchronisation primitives such as mutexes , semaphores, etc. in order to guard critical sections , and there is no need for support from the operating system. It is possible to implement coroutines using preemptively-scheduled threads, in a way that will be transparent to the calling code, but some of the advantages (particularly the suitability for hard-realtime operation and relative cheapness of switching between them) will be lost. Comparison with generators Generators , also known as semicoroutines ,[ 5] are a subset of coroutines. Specifically, while both can yield multiple times, suspending their execution and allowing re-entry at multiple entry points, they differ in coroutines ' ability to control where execution continues immediately after they yield, while generators cannot, instead transferring control back to the generator's caller.[ 6] That is, since generators are primarily used to simplify the writing of iterators , the yield statement in a generator does not specify a coroutine to jump to, but rather passes a value back to a parent routine. However, it is still possible to implement coroutines on top of a generator facility, with the aid of a top-level dispatcher routine (a trampoline , essentially) that passes control explicitly to child generators identified by tokens passed back from the generators: var q := new queue generator produce loop while q is not full create some new items add the items to q yield consume generator consume loop while q is not empty remove some items from q use the items yield produce subroutine dispatcher var d := new dictionary(generator \u2192 iterator) d[produce] := start produce d[consume] := start consume var current := produce loop current := next d[current] A number of implementations of coroutines for languages with generator support but no native coroutines (e.g. Python[ 7] before 2.5) use this or a similar model. Comparison with mutual recursion Further information: Mutual recursion Using coroutines for state machines or concurrency is similar to using mutual recursion with tail calls , as in both cases the control changes to a different one of a set of routines. However, coroutines are more flexible and generally more efficient. Since coroutines yield rather than return, and then resume execution rather than restarting from the beginning, they are able to hold state, both variables (as in a closure) and execution point, and yields are not limited to being in tail position; mutually recursive subroutines must either use shared variables or pass state as parameters. Further, each mutually recursive call of a subroutine requires a new stack frame (unless tail call elimination is implemented), while passing control between coroutines uses the existing contexts and can be implemented simply by a jump. Common uses Coroutines are useful to implement the following: State machines within a single subroutine, where the state is determined by the current entry/exit point of the procedure; this can result in more readable code compared to use of goto , and may also be implemented via mutual recursion with tail calls . Actor model of concurrency, for instance in video games . Each actor has its own procedures (this again logically separates the code), but they voluntarily give up control to central scheduler, which executes them sequentially (this is a form of cooperative multitasking ). Generators , and these are useful for streams \u2013 particularly input/output \u2013 and for generic traversal of data structures. Communicating sequential processes where each sub-process is a coroutine. Channel inputs/outputs and blocking operations yield coroutines and a scheduler unblocks them on completion events. Alternatively, each sub-process may be the parent of the one following it in the data pipeline (or preceding it, in which case the pattern can be expressed as nested generators). Reverse communication, commonly used in mathematical software, wherein a procedure such as a solver, integral evaluator, ... needs the using process to make a computation, such as evaluating an equation or integrand. Implementations As of 2003, many of the most popular programming languages, including C and its derivatives, do not have direct support for coroutines within the language or their standard libraries. (This is, in large part, due to the limitations of stack-based subroutine implementation.) An exception is the C++ library Boost.Context , part of boost libraries , which supports context swapping on ARM, MIPS, PowerPC, SPARC and x86 on POSIX, Mac OS X and Windows. Coroutines can be built upon Boost.Context. In situations where a coroutine would be the natural implementation of a mechanism, but is not available, the typical response is to use a closure \u2013 a subroutine with state variables ( static variables , often boolean flags) to maintain an internal state between calls, and to transfer control to the correct point. Conditionals within the code result in the execution of different code paths on successive calls, based on the values of the state variables. Another typical response is to implement an explicit state machine in the form of a large and complex switch statement or via a goto statement, particularly a computed goto . Such implementations are considered difficult to understand and maintain, and a motivation for coroutine support. Threads , and to a lesser extent fibers , are an alternative to coroutines in mainstream programming environments today. Threads provide facilities for managing the realtime cooperative interaction of simultaneously executing pieces of code. Threads are widely available in environments that support C (and are supported natively in many other modern languages), are familiar to many programmers, and are usually well-implemented, well-documented and well-supported. However, as they solve a large and difficult problem they include many powerful and complex facilities and have a correspondingly difficult learning curve. As such, when a coroutine is all that is needed, using a thread can be overkill. One important difference between threads and coroutines is that threads are typically preemptively scheduled while coroutines are not. Because threads can be rescheduled at any instant and can execute concurrently, programs using threads must be careful about locking . In contrast, because coroutines can only be rescheduled at specific points in the program and do not execute concurrently, programs using coroutines can often avoid locking entirely. (This property is also cited as a benefit of event-driven or asynchronous programming.) Since fibers are cooperatively scheduled, they provide an ideal base for implementing coroutines above.[ 20] However, system support for fibers is often lacking compared to that for threads. stackoverflow What is a coroutine?","title":"Introduction"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Coroutine-async-await/Coroutine/#coroutine","text":"\u8d8a\u6765\u8d8a\u591a\u7684language\u652f\u6301Coroutine\u3002 Coroutine\u662f\u73b0\u4ee3programming language\u5b9e\u73b0\"cooperative multitasking\"\u7684\u4e3b\u8981\u65b9\u5f0f\uff0c\u652f\u6301coroutine\u7684programming language\u57fa\u4e8e\u4e86programmer\u6765\u8fdb\u884ccooperative schedule task\u7684\u80fd\u529b\uff0c\u8ba9programmer\u80fd\u591fcontrol schedule of task\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0cprogrammer\u662f\u65e0\u6cd5\u76f4\u63a5\u63a7\u5236thread\u3001process\u7684schedule \u3002","title":"Coroutine"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Coroutine-async-await/Coroutine/#wikipedia#coroutine","text":"Coroutines are computer program components that generalize subroutines for non-preemptive multitasking , by allowing execution to be suspended and resumed. Coroutines are well-suited for implementing familiar program components such as cooperative tasks , exceptions , event loops , iterators , infinite lists and pipes . According to Donald Knuth , Melvin Conway coined the term coroutine in 1958 when he applied it to construction of an assembly program . The first published explanation of the coroutine appeared later, in 1963.","title":"wikipedia Coroutine"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Coroutine-async-await/Coroutine/#comparison#with#subroutines","text":"Subroutines are special cases of coroutines . When subroutines are invoked, execution begins at the start, and once a subroutine exits, it is finished; an instance of a subroutine only returns once, and does not hold state between invocations. By contrast, coroutines can exit by calling other coroutines, which may later return to the point where they were invoked in the original coroutine; from the coroutine's point of view, it is not exiting but calling another coroutine. Thus, a coroutine instance holds state, and varies between invocations; there can be multiple instances of a given coroutine at once. The difference between calling another coroutine by means of \"yielding\" to it and simply calling another routine (which then, also, would return to the original point), is that the relationship between two coroutines which yield to each other is not that of caller-callee, but instead symmetric. NOTE: \u6700\u540e\u4e00\u6bb5\u8bdd\u975e\u5e38\u91cd\u8981\uff0c\u5b83\u7684\u610f\u601d\u662f\uff1a\u901a\u8fc7 \"yielding\" \u6765\u8c03\u7528\u53e6\u5916\u4e00\u4e2acoroutine\u548c\u7b80\u5355\u8c03\u7528\u53e6\u5916\u4e00\u4e2aroutine\u4e4b\u95f4\u7684\u5dee\u5f02\u662f\u4e24\u8005\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u901a\u8fc7yield\u7684coroutine\u4e4b\u95f4\u7684\u5173\u7cfb\u4e0d\u662fcaller-callee\uff0c\u800c\u662f\u5bf9\u79f0\u7684\uff1b\u7406\u89e3\u8fd9\u6bb5\u8bdd\u7684\u5173\u952e\u662f\u5bf9 \"yielding\" \u7406\u89e3\uff0cyield\u5373\u8ba9\u6e21\uff0c\u505c\u6b62\u5f53\u524dcoroutine\u5e76\u8ba9\u53e6\u5916\u4e00\u4e2acoroutine\u5f00\u59cb\u6267\u884c\u3002 Any subroutine can be translated to a coroutine which does not call yield . Here is a simple example of how coroutines can be useful. Suppose you have a consumer-producer relationship where one routine creates items and adds them to a queue and another removes items from the queue and uses them. For reasons of efficiency, you want to add and remove several items at once. The code might look like this: var q := new queue coroutine produce loop while q is not full create some new items add the items to q yield to consume coroutine consume loop while q is not empty remove some items from q use the items yield to produce The queue is then completely filled or emptied before yielding control to the other coroutine using the yield command. The further coroutines calls are starting right after the yield , in the outer coroutine loop. Although this example is often used as an introduction to multithreading , two threads are not needed for this: the yield statement can be implemented by a jump directly from one routine into the other.","title":"Comparison with subroutines"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Coroutine-async-await/Coroutine/#comparison#with#threads","text":"Coroutines are very similar to threads . However, coroutines are cooperatively multitasked, whereas threads are typically preemptively multitasked . This means that coroutines provide concurrency but not parallelism . The advantages of coroutines over threads are that they may be used in a hard-realtime context ( switching between coroutines need not involve any system calls or any blocking calls whatsoever), there is no need for synchronisation primitives such as mutexes , semaphores, etc. in order to guard critical sections , and there is no need for support from the operating system. It is possible to implement coroutines using preemptively-scheduled threads, in a way that will be transparent to the calling code, but some of the advantages (particularly the suitability for hard-realtime operation and relative cheapness of switching between them) will be lost.","title":"Comparison with threads"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Coroutine-async-await/Coroutine/#comparison#with#generators","text":"Generators , also known as semicoroutines ,[ 5] are a subset of coroutines. Specifically, while both can yield multiple times, suspending their execution and allowing re-entry at multiple entry points, they differ in coroutines ' ability to control where execution continues immediately after they yield, while generators cannot, instead transferring control back to the generator's caller.[ 6] That is, since generators are primarily used to simplify the writing of iterators , the yield statement in a generator does not specify a coroutine to jump to, but rather passes a value back to a parent routine. However, it is still possible to implement coroutines on top of a generator facility, with the aid of a top-level dispatcher routine (a trampoline , essentially) that passes control explicitly to child generators identified by tokens passed back from the generators: var q := new queue generator produce loop while q is not full create some new items add the items to q yield consume generator consume loop while q is not empty remove some items from q use the items yield produce subroutine dispatcher var d := new dictionary(generator \u2192 iterator) d[produce] := start produce d[consume] := start consume var current := produce loop current := next d[current] A number of implementations of coroutines for languages with generator support but no native coroutines (e.g. Python[ 7] before 2.5) use this or a similar model.","title":"Comparison with generators"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Coroutine-async-await/Coroutine/#comparison#with#mutual#recursion","text":"Further information: Mutual recursion Using coroutines for state machines or concurrency is similar to using mutual recursion with tail calls , as in both cases the control changes to a different one of a set of routines. However, coroutines are more flexible and generally more efficient. Since coroutines yield rather than return, and then resume execution rather than restarting from the beginning, they are able to hold state, both variables (as in a closure) and execution point, and yields are not limited to being in tail position; mutually recursive subroutines must either use shared variables or pass state as parameters. Further, each mutually recursive call of a subroutine requires a new stack frame (unless tail call elimination is implemented), while passing control between coroutines uses the existing contexts and can be implemented simply by a jump.","title":"Comparison with mutual recursion"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Coroutine-async-await/Coroutine/#common#uses","text":"Coroutines are useful to implement the following: State machines within a single subroutine, where the state is determined by the current entry/exit point of the procedure; this can result in more readable code compared to use of goto , and may also be implemented via mutual recursion with tail calls . Actor model of concurrency, for instance in video games . Each actor has its own procedures (this again logically separates the code), but they voluntarily give up control to central scheduler, which executes them sequentially (this is a form of cooperative multitasking ). Generators , and these are useful for streams \u2013 particularly input/output \u2013 and for generic traversal of data structures. Communicating sequential processes where each sub-process is a coroutine. Channel inputs/outputs and blocking operations yield coroutines and a scheduler unblocks them on completion events. Alternatively, each sub-process may be the parent of the one following it in the data pipeline (or preceding it, in which case the pattern can be expressed as nested generators). Reverse communication, commonly used in mathematical software, wherein a procedure such as a solver, integral evaluator, ... needs the using process to make a computation, such as evaluating an equation or integrand.","title":"Common uses"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Coroutine-async-await/Coroutine/#implementations","text":"As of 2003, many of the most popular programming languages, including C and its derivatives, do not have direct support for coroutines within the language or their standard libraries. (This is, in large part, due to the limitations of stack-based subroutine implementation.) An exception is the C++ library Boost.Context , part of boost libraries , which supports context swapping on ARM, MIPS, PowerPC, SPARC and x86 on POSIX, Mac OS X and Windows. Coroutines can be built upon Boost.Context. In situations where a coroutine would be the natural implementation of a mechanism, but is not available, the typical response is to use a closure \u2013 a subroutine with state variables ( static variables , often boolean flags) to maintain an internal state between calls, and to transfer control to the correct point. Conditionals within the code result in the execution of different code paths on successive calls, based on the values of the state variables. Another typical response is to implement an explicit state machine in the form of a large and complex switch statement or via a goto statement, particularly a computed goto . Such implementations are considered difficult to understand and maintain, and a motivation for coroutine support. Threads , and to a lesser extent fibers , are an alternative to coroutines in mainstream programming environments today. Threads provide facilities for managing the realtime cooperative interaction of simultaneously executing pieces of code. Threads are widely available in environments that support C (and are supported natively in many other modern languages), are familiar to many programmers, and are usually well-implemented, well-documented and well-supported. However, as they solve a large and difficult problem they include many powerful and complex facilities and have a correspondingly difficult learning curve. As such, when a coroutine is all that is needed, using a thread can be overkill. One important difference between threads and coroutines is that threads are typically preemptively scheduled while coroutines are not. Because threads can be rescheduled at any instant and can execute concurrently, programs using threads must be careful about locking . In contrast, because coroutines can only be rescheduled at specific points in the program and do not execute concurrently, programs using coroutines can often avoid locking entirely. (This property is also cited as a benefit of event-driven or asynchronous programming.) Since fibers are cooperatively scheduled, they provide an ideal base for implementing coroutines above.[ 20] However, system support for fibers is often lacking compared to that for threads.","title":"Implementations"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Coroutine-async-await/Coroutine/#stackoverflow#what#is#a#coroutine","text":"","title":"stackoverflow What is a coroutine?"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Promise-future/","text":"Futures and promises dist-prog-book Futures and Promises NOTE: \u8fd9\u7bc7\u6587\u7ae0\u6536\u5f55\u5728 Distributed-computing\\Book-Programming-Models-for-Distributed-Computing\\chapter-2 \u7ae0\u8282\u3002 wikipedia Futures and promises In computer science , future , promise , delay , and deferred refer to constructs used for synchronizing program execution in some concurrent programming languages . They describe an object that acts as a proxy for a result that is initially unknown, usually because the computation of its value is yet incomplete. NOTE: \u4e0a\u9762\u6240\u63d0\u53ca\u7684proxy\u6b63\u662f Proxy pattern \u7684\u7528\u6cd5--Virtual Proxy The term promise was proposed in 1976 by Daniel P. Friedman and David Wise,[ 1] and Peter Hibbard called it eventual .[ 2] A somewhat similar concept future was introduced in 1977 in a paper by Henry Baker and Carl Hewitt .[ 3] The terms future , promise , delay , and deferred are often used interchangeably\uff08\u4e92\u6362\uff09, although some differences in usage between future and promise are treated below. Specifically, when usage is distinguished, a future is a read-only placeholder view of a variable, while a promise is a writable, single assignment container which sets the value of the future\uff08promise\u7528\u4e8e\u8bbe\u7f6efuture\u7684\u503c\uff09.[ 4] Notably, a future may be defined without specifying which specific promise will set its value, and different possible promises may set the value of a given future , though this can be done only once for a given future . In other cases a future and a promise are created together and associated with each other: the future is the value, the promise is the function that sets the value \u2013 essentially the return value ( future ) of an asynchronous function ( promise ). Setting the value of a future is also called resolving , fulfilling , or binding it. NOTE: \u8fd9\u6bb5\u8bdd\u5bf9future\u548cpromise\u7684\u89e3\u91ca\u975e\u5e38\u5230\u4f4d\uff0c\u5176\u5b9e\u5b83\u6240\u9610\u8ff0\u7684\u5c31\u662f\"Promise-future communication channel\"\uff0c\u53c2\u89c1\u540e\u9762\u7684\"Promise-future communication channel\"\u3002 Applications Futures and promises originated in functional programming and related paradigms (such as logic programming ) to decouple a value (a future) from how it was computed (a promise), allowing the computation to be done more flexibly, notably by parallelizing it. Later, it found use in distributed computing , in reducing the latency from communication round trips. Later still, it gained more use by allowing writing asynchronous programs in direct style , rather than in continuation-passing style . NOTE: \u5206\u79bb Implicit vs. explicit Use of futures may be implicit (any use of the future automatically obtains its value, as if it were an ordinary reference ) or explicit (the user must call a function to obtain the value, such as the get method of java.util.concurrent.Future or java.util.concurrent.CompletableFuture in Java ). Obtaining the value of an explicit future can be called stinging or forcing . Explicit futures can be implemented as a library, whereas implicit futures are usually implemented as part of the language. NOTE: C++ std::future \u3001Java java.util.concurrent.Future or java.util.concurrent.CompletableFuture \u5c5e\u4e8eexplicit The original Baker and Hewitt paper described implicit futures , which are naturally supported in the actor model of computation and pure object-oriented programming languages like Smalltalk . The Friedman and Wise paper described only explicit futures (\u5f17\u91cc\u5fb7\u66fc\u548c\u6000\u65af\u8bba\u6587\u4ec5\u63cf\u8ff0\u4e86**explicit futures**), probably reflecting the difficulty of efficiently implementing implicit futures on stock hardware. The difficulty is that stock hardware does not deal with futures for primitive data types like integers. For example, an add instruction does not know how to deal with 3 + future factorial(100000) . In pure actor or object languages this problem can be solved by sending future factorial(100000) the message +[3] , which asks the future to add 3 to itself and return the result. Note that the message passing approach works regardless of when factorial(100000) finishes computation and that no stinging/forcing is needed. Promise pipelining NOTE: \u6b64\u5904\u7684pipeline\u7684\u542b\u4e49\u662f\u5c06\u591a\u4e2apromise\u8fde\u63a5\u8d77\u6765\u3002 The use of futures can dramatically reduce latency in distributed systems . For instance, futures enable promise pipelining ,[ 5] [ 6] as implemented in the languages E and Joule , which was also called call-stream [ 7] in the language Argus . \u603b\u7ed3\uff1a\u4e0a\u9762\u8fd9\u6bb5\u4e2d\u7684stream\u7684\u542b\u4e49\u662f\u6d41\uff0c\u5176\u5b9e\u5c31\u662f\u4e00\u4e2a\u4e32\u3002 Consider an expression involving conventional remote procedure calls , such as: t3 := ( x.a() ).c( y.b() ) which could be expanded to t1 := x.a(); t2 := y.b(); t3 := t1.c(t2); Each statement needs a message to be sent and a reply received before the next statement can proceed. Suppose, for example, that x , y , t1 , and t2 are all located on the same remote machine. In this case, two complete network round-trips to that machine must take place before the third statement can begin to execute. The third statement will then cause yet another round-trip to the same remote machine. Using futures, the above expression could be written t3 := (x <- a()) <- c(y <- b()) which could be expanded to t1 := x <- a(); t2 := y <- b(); t3 := t1 <- c(t2); The syntax used here is that of the language E, where x <- a() means to send the message a() asynchronously to x . All three variables are immediately assigned futures for their results, and execution proceeds to subsequent statements. Later attempts to resolve the value of t3 may cause a delay \uff08\u601d\u8003\uff1a\u6b64\u5904\u7684delay\u5982\u4f55\u6765\u7406\u89e3\uff09; however, pipelining can reduce the number of round-trips needed(pipelining\u80fd\u591f\u51cf\u5c11round-trip). If, as in the prior example, x , y , t1 , and t2 are all located on the same remote machine, a pipelined implementation can compute t3 with one round-trip instead of three. Because all three messages are destined for objects which are on the same remote machine, only one request need be sent and only one response need be received containing the result. Note also that the send t1 <- c(t2) would not block even if t1 and t2 were on different machines to each other, or to x or y . NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u5bf9\u6bd4\u4e86\u662f\u5426\u4f7f\u7528future\u6240\u5e26\u6765\u7684\u4f18\u52bf\u3002 Promise pipelining should be distinguished from parallel asynchronous message passing . In a system supporting parallel message passing but not pipelining, the message sends x <- a() and y <- b() in the above example could proceed in parallel, but the send of t1 <- c(t2) would have to wait until both t1 and t2 had been received, even when x , y , t1 , and t2 are on the same remote machine. The relative latency advantage of pipelining becomes even greater in more complicated situations involving many messages. Promise pipelining also should not be confused with pipelined message processing in actor systems, where it is possible for an actor to specify and begin executing a behaviour for the next message before having completed processing of the current message. Read-only views In some programming languages such as Oz , E , and AmbientTalk , it is possible to obtain a read-only view of a future, which allows reading its value when resolved, but does not permit resolving it: NOTE: \u53ea\u6709\u5f53promise\u5df2\u7ecf\u5b8c\u6210resolve\u7684\u60c5\u51b5\u4e0b\uff0c\u624d\u5141\u8bb8read\u5b83\u7684value\uff0c\u5426\u5219\u4e0d\u5141\u8bb8\u53bbresolve\u5b83\u3002 In C++11 a std::future provides a read-only view. The value is set directly by using a std::promise , or set to the result of a function call using std::packaged_task or std::async . Blocking vs non-blocking semantics As an example of the first possibility, in C++11 , a thread that needs the value of a future can block until it is available by calling the wait() or get() member functions. You can also specify a timeout on the wait using the wait_for() or wait_until() member functions to avoid indefinite blocking. If the future arose from a call to std::async then a blocking wait (without a timeout) may cause synchronous invocation of the function to compute the result on the waiting thread. List of implementations Coroutines Futures can be implemented in coroutines [ 27] or generators ,[ 95] resulting in the same evaluation strategy (e.g., cooperative multitasking or lazy evaluation). NOTE: C++ coroutine\u662f\u57fa\u4e8epromise\u3001future\u6982\u5ff5\u7684 Channels Main article: Channel (programming) Futures can easily be implemented in channels : a future is a one-element channel, and a promise is a process that sends to the channel, fulfilling the future.[ 96] [ 97] This allows futures to be implemented in concurrent programming languages with support for channels, such as CSP and Go . The resulting futures are explicit, as they must be accessed by reading from the channel, rather than only evaluation. NOTE: \u8fd9\u5c31\u662f\"Promise-future communication channel\"\u3002 Promise-future communication channel \"promise-future communication channel\"\u662f\u5728cppreference std::promise \u4e2d\u63d0\u51fa\u7684\uff0c\u6211\u89c9\u5f97\u5b83\u975e\u5e38\u597d\u7684\u63cf\u8ff0\u4e86promise-future: The promise is the \"push\" end of the promise-future communication channel: the operation that stores a value in the shared state synchronizes-with (as defined in std::memory_order ) the successful return from any function that is waiting on the shared state (such as std::future::get ). \u5728wikipedia Futures and promises \u7684\u7b2c\u4e8c\u6bb5\u4e2d\u6240\u603b\u7ed3\u7684: future: read-only placeholder/view of shared data promise: set the value of the future \u5728wikipedia Futures and promises \u7684\"Channels\"\u4e2d\u4e5f\u603b\u7ed3\u4e86\u76f8\u540c\u7684\u5185\u5bb9\u3002 Futures and promises originated in functional programming \u5173\u4e8efunctional programming\uff0c\u53c2\u89c1\u5de5\u7a0bprogramming-language\u7684 Theory\\Programming-paradigm\\Functional-programming \u7ae0\u8282\u3002 Application of future and promise 1) C++ TODO stackoverflow What's the difference between a Future and a Promise? softwareengineering What is the difference between a Future and a Promise?","title":"Introduction"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Promise-future/#futures#and#promises","text":"","title":"Futures and promises"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Promise-future/#dist-prog-book#futures#and#promises","text":"NOTE: \u8fd9\u7bc7\u6587\u7ae0\u6536\u5f55\u5728 Distributed-computing\\Book-Programming-Models-for-Distributed-Computing\\chapter-2 \u7ae0\u8282\u3002","title":"dist-prog-book Futures and Promises"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Promise-future/#wikipedia#futures#and#promises","text":"In computer science , future , promise , delay , and deferred refer to constructs used for synchronizing program execution in some concurrent programming languages . They describe an object that acts as a proxy for a result that is initially unknown, usually because the computation of its value is yet incomplete. NOTE: \u4e0a\u9762\u6240\u63d0\u53ca\u7684proxy\u6b63\u662f Proxy pattern \u7684\u7528\u6cd5--Virtual Proxy The term promise was proposed in 1976 by Daniel P. Friedman and David Wise,[ 1] and Peter Hibbard called it eventual .[ 2] A somewhat similar concept future was introduced in 1977 in a paper by Henry Baker and Carl Hewitt .[ 3] The terms future , promise , delay , and deferred are often used interchangeably\uff08\u4e92\u6362\uff09, although some differences in usage between future and promise are treated below. Specifically, when usage is distinguished, a future is a read-only placeholder view of a variable, while a promise is a writable, single assignment container which sets the value of the future\uff08promise\u7528\u4e8e\u8bbe\u7f6efuture\u7684\u503c\uff09.[ 4] Notably, a future may be defined without specifying which specific promise will set its value, and different possible promises may set the value of a given future , though this can be done only once for a given future . In other cases a future and a promise are created together and associated with each other: the future is the value, the promise is the function that sets the value \u2013 essentially the return value ( future ) of an asynchronous function ( promise ). Setting the value of a future is also called resolving , fulfilling , or binding it. NOTE: \u8fd9\u6bb5\u8bdd\u5bf9future\u548cpromise\u7684\u89e3\u91ca\u975e\u5e38\u5230\u4f4d\uff0c\u5176\u5b9e\u5b83\u6240\u9610\u8ff0\u7684\u5c31\u662f\"Promise-future communication channel\"\uff0c\u53c2\u89c1\u540e\u9762\u7684\"Promise-future communication channel\"\u3002","title":"wikipedia Futures and promises"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Promise-future/#applications","text":"Futures and promises originated in functional programming and related paradigms (such as logic programming ) to decouple a value (a future) from how it was computed (a promise), allowing the computation to be done more flexibly, notably by parallelizing it. Later, it found use in distributed computing , in reducing the latency from communication round trips. Later still, it gained more use by allowing writing asynchronous programs in direct style , rather than in continuation-passing style . NOTE: \u5206\u79bb","title":"Applications"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Promise-future/#implicit#vs#explicit","text":"Use of futures may be implicit (any use of the future automatically obtains its value, as if it were an ordinary reference ) or explicit (the user must call a function to obtain the value, such as the get method of java.util.concurrent.Future or java.util.concurrent.CompletableFuture in Java ). Obtaining the value of an explicit future can be called stinging or forcing . Explicit futures can be implemented as a library, whereas implicit futures are usually implemented as part of the language. NOTE: C++ std::future \u3001Java java.util.concurrent.Future or java.util.concurrent.CompletableFuture \u5c5e\u4e8eexplicit The original Baker and Hewitt paper described implicit futures , which are naturally supported in the actor model of computation and pure object-oriented programming languages like Smalltalk . The Friedman and Wise paper described only explicit futures (\u5f17\u91cc\u5fb7\u66fc\u548c\u6000\u65af\u8bba\u6587\u4ec5\u63cf\u8ff0\u4e86**explicit futures**), probably reflecting the difficulty of efficiently implementing implicit futures on stock hardware. The difficulty is that stock hardware does not deal with futures for primitive data types like integers. For example, an add instruction does not know how to deal with 3 + future factorial(100000) . In pure actor or object languages this problem can be solved by sending future factorial(100000) the message +[3] , which asks the future to add 3 to itself and return the result. Note that the message passing approach works regardless of when factorial(100000) finishes computation and that no stinging/forcing is needed.","title":"Implicit vs. explicit"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Promise-future/#promise#pipelining","text":"NOTE: \u6b64\u5904\u7684pipeline\u7684\u542b\u4e49\u662f\u5c06\u591a\u4e2apromise\u8fde\u63a5\u8d77\u6765\u3002 The use of futures can dramatically reduce latency in distributed systems . For instance, futures enable promise pipelining ,[ 5] [ 6] as implemented in the languages E and Joule , which was also called call-stream [ 7] in the language Argus . \u603b\u7ed3\uff1a\u4e0a\u9762\u8fd9\u6bb5\u4e2d\u7684stream\u7684\u542b\u4e49\u662f\u6d41\uff0c\u5176\u5b9e\u5c31\u662f\u4e00\u4e2a\u4e32\u3002 Consider an expression involving conventional remote procedure calls , such as: t3 := ( x.a() ).c( y.b() ) which could be expanded to t1 := x.a(); t2 := y.b(); t3 := t1.c(t2); Each statement needs a message to be sent and a reply received before the next statement can proceed. Suppose, for example, that x , y , t1 , and t2 are all located on the same remote machine. In this case, two complete network round-trips to that machine must take place before the third statement can begin to execute. The third statement will then cause yet another round-trip to the same remote machine. Using futures, the above expression could be written t3 := (x <- a()) <- c(y <- b()) which could be expanded to t1 := x <- a(); t2 := y <- b(); t3 := t1 <- c(t2); The syntax used here is that of the language E, where x <- a() means to send the message a() asynchronously to x . All three variables are immediately assigned futures for their results, and execution proceeds to subsequent statements. Later attempts to resolve the value of t3 may cause a delay \uff08\u601d\u8003\uff1a\u6b64\u5904\u7684delay\u5982\u4f55\u6765\u7406\u89e3\uff09; however, pipelining can reduce the number of round-trips needed(pipelining\u80fd\u591f\u51cf\u5c11round-trip). If, as in the prior example, x , y , t1 , and t2 are all located on the same remote machine, a pipelined implementation can compute t3 with one round-trip instead of three. Because all three messages are destined for objects which are on the same remote machine, only one request need be sent and only one response need be received containing the result. Note also that the send t1 <- c(t2) would not block even if t1 and t2 were on different machines to each other, or to x or y . NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u5bf9\u6bd4\u4e86\u662f\u5426\u4f7f\u7528future\u6240\u5e26\u6765\u7684\u4f18\u52bf\u3002 Promise pipelining should be distinguished from parallel asynchronous message passing . In a system supporting parallel message passing but not pipelining, the message sends x <- a() and y <- b() in the above example could proceed in parallel, but the send of t1 <- c(t2) would have to wait until both t1 and t2 had been received, even when x , y , t1 , and t2 are on the same remote machine. The relative latency advantage of pipelining becomes even greater in more complicated situations involving many messages. Promise pipelining also should not be confused with pipelined message processing in actor systems, where it is possible for an actor to specify and begin executing a behaviour for the next message before having completed processing of the current message.","title":"Promise pipelining"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Promise-future/#read-only#views","text":"In some programming languages such as Oz , E , and AmbientTalk , it is possible to obtain a read-only view of a future, which allows reading its value when resolved, but does not permit resolving it: NOTE: \u53ea\u6709\u5f53promise\u5df2\u7ecf\u5b8c\u6210resolve\u7684\u60c5\u51b5\u4e0b\uff0c\u624d\u5141\u8bb8read\u5b83\u7684value\uff0c\u5426\u5219\u4e0d\u5141\u8bb8\u53bbresolve\u5b83\u3002 In C++11 a std::future provides a read-only view. The value is set directly by using a std::promise , or set to the result of a function call using std::packaged_task or std::async .","title":"Read-only views"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Promise-future/#blocking#vs#non-blocking#semantics","text":"As an example of the first possibility, in C++11 , a thread that needs the value of a future can block until it is available by calling the wait() or get() member functions. You can also specify a timeout on the wait using the wait_for() or wait_until() member functions to avoid indefinite blocking. If the future arose from a call to std::async then a blocking wait (without a timeout) may cause synchronous invocation of the function to compute the result on the waiting thread.","title":"Blocking vs non-blocking semantics"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Promise-future/#list#of#implementations","text":"","title":"List of implementations"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Promise-future/#coroutines","text":"Futures can be implemented in coroutines [ 27] or generators ,[ 95] resulting in the same evaluation strategy (e.g., cooperative multitasking or lazy evaluation). NOTE: C++ coroutine\u662f\u57fa\u4e8epromise\u3001future\u6982\u5ff5\u7684","title":"Coroutines"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Promise-future/#channels","text":"Main article: Channel (programming) Futures can easily be implemented in channels : a future is a one-element channel, and a promise is a process that sends to the channel, fulfilling the future.[ 96] [ 97] This allows futures to be implemented in concurrent programming languages with support for channels, such as CSP and Go . The resulting futures are explicit, as they must be accessed by reading from the channel, rather than only evaluation. NOTE: \u8fd9\u5c31\u662f\"Promise-future communication channel\"\u3002","title":"Channels"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Promise-future/#promise-future#communication#channel","text":"\"promise-future communication channel\"\u662f\u5728cppreference std::promise \u4e2d\u63d0\u51fa\u7684\uff0c\u6211\u89c9\u5f97\u5b83\u975e\u5e38\u597d\u7684\u63cf\u8ff0\u4e86promise-future: The promise is the \"push\" end of the promise-future communication channel: the operation that stores a value in the shared state synchronizes-with (as defined in std::memory_order ) the successful return from any function that is waiting on the shared state (such as std::future::get ). \u5728wikipedia Futures and promises \u7684\u7b2c\u4e8c\u6bb5\u4e2d\u6240\u603b\u7ed3\u7684: future: read-only placeholder/view of shared data promise: set the value of the future \u5728wikipedia Futures and promises \u7684\"Channels\"\u4e2d\u4e5f\u603b\u7ed3\u4e86\u76f8\u540c\u7684\u5185\u5bb9\u3002","title":"Promise-future communication channel"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Promise-future/#futures#and#promises#originated#in#functional#programming","text":"\u5173\u4e8efunctional programming\uff0c\u53c2\u89c1\u5de5\u7a0bprogramming-language\u7684 Theory\\Programming-paradigm\\Functional-programming \u7ae0\u8282\u3002","title":"Futures and promises originated in functional programming"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Promise-future/#application#of#future#and#promise","text":"1) C++","title":"Application of future and promise"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Promise-future/#todo","text":"stackoverflow What's the difference between a Future and a Promise? softwareengineering What is the difference between a Future and a Promise?","title":"TODO"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Promise-future/Future-promise-VS-observer-pattern-VS-callback/","text":"Future promise\u3001observe pattern\u3001callback \u4e09\u79cd\u90fd\u80fd\u591f\u7528\u4e8enotification\u3001\u901a\u77e5\u3002 1\u3001future promise \u901a\u8fc7\u5224\u65adfuture\u7684\u72b6\u6001\u6765\u8fdb\u884c\u5224\u65ad\uff0c\u5b83\u662f\u8f83\u9ad8\u7684\u5c01\u88c5\u3001\u62bd\u8c61 2\u3001observe pattern\u3001callback \u901a\u8fc7\u6267\u884c\u5bf9\u5e94\u7684callback\u6765\u8fdb\u884c\u901a\u77e5 observer pattern\u5141\u8bb8\u5f00\u653e\u5f0f\u7684\u6ce8\u518c\u3001\u6ce8\u9500 TODO stackoverflow Java executors: how to be notified, without blocking, when a task completes? jayconrod Futures are better than callbacks hackernoon From Callback to Future -> Functor -> Monad callback vs promise vs observer pattern https://itnext.io/javascript-promises-vs-rxjs-observables-de5309583ca2 https://scotch.io/courses/10-need-to-know-javascript-concepts/callbacks-promises-and-async","title":"Future-promise-VS-observer-pattern-VS-callback"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Promise-future/Future-promise-VS-observer-pattern-VS-callback/#future#promiseobserve#patterncallback","text":"\u4e09\u79cd\u90fd\u80fd\u591f\u7528\u4e8enotification\u3001\u901a\u77e5\u3002 1\u3001future promise \u901a\u8fc7\u5224\u65adfuture\u7684\u72b6\u6001\u6765\u8fdb\u884c\u5224\u65ad\uff0c\u5b83\u662f\u8f83\u9ad8\u7684\u5c01\u88c5\u3001\u62bd\u8c61 2\u3001observe pattern\u3001callback \u901a\u8fc7\u6267\u884c\u5bf9\u5e94\u7684callback\u6765\u8fdb\u884c\u901a\u77e5 observer pattern\u5141\u8bb8\u5f00\u653e\u5f0f\u7684\u6ce8\u518c\u3001\u6ce8\u9500","title":"Future promise\u3001observe pattern\u3001callback"},{"location":"Programming-model/Sync-and-async/Asynchronous-programming/Promise-future/Future-promise-VS-observer-pattern-VS-callback/#todo","text":"stackoverflow Java executors: how to be notified, without blocking, when a task completes? jayconrod Futures are better than callbacks hackernoon From Callback to Future -> Functor -> Monad callback vs promise vs observer pattern https://itnext.io/javascript-promises-vs-rxjs-observables-de5309583ca2 https://scotch.io/courses/10-need-to-know-javascript-concepts/callbacks-promises-and-async","title":"TODO"},{"location":"Programming-model/TODO-MapReduce/","text":"MapReduce MapReduce is a programming model and an associated implementation for processing and generating big data sets with a parallel , distributed algorithm on a cluster .[ 1] [ 2] [ 3] A MapReduce program is composed of a map procedure (or method), which performs filtering and sorting (such as sorting students by first name into queues, one queue for each name), and a reduce method, which performs a summary operation (such as counting the number of students in each queue, yielding name frequencies). The \"MapReduce System\" (also called \"infrastructure\" or \"framework\") orchestrates the processing by marshalling the distributed servers, running the various tasks in parallel, managing all communications and data transfers between the various parts of the system, and providing for redundancy and fault tolerance . The model is a specialization of the split-apply-combine strategy for data analysis.[ 4] It is inspired by the map and reduce functions commonly used in functional programming ,[ 5] although their purpose in the MapReduce framework is not the same as in their original forms.[ 6] The key contributions of the MapReduce framework are not the actual map and reduce functions (which, for example, resemble the 1995 Message Passing Interface standard's[ 7] reduce [ 8] and scatter [ 9] operations), but the scalability and fault-tolerance achieved for a variety of applications by optimizing the execution engine[ citation needed ]. As such, a single-threaded implementation of MapReduce is usually not faster than a traditional (non-MapReduce) implementation; any gains are usually only seen with multi-threaded implementations on multi-processor hardware.[ 10] The use of this model is beneficial only when the optimized distributed shuffle operation (which reduces network communication cost) and fault tolerance features of the MapReduce framework come into play. Optimizing the communication cost is essential to a good MapReduce algorithm.[ 11] MapReduce libraries have been written in many programming languages, with different levels of optimization. A popular open-source implementation that has support for distributed shuffles is part of Apache Hadoop . The name MapReduce originally referred to the proprietary Google technology, but has since been genericized . By 2014, Google was no longer using MapReduce as their primary big data processing model,[ 12] and development on Apache Mahout had moved on to more capable and less disk-oriented mechanisms that incorporated full map and reduce capabilities.[ 13]","title":"Introduction"},{"location":"Programming-model/TODO-MapReduce/#mapreduce","text":"MapReduce is a programming model and an associated implementation for processing and generating big data sets with a parallel , distributed algorithm on a cluster .[ 1] [ 2] [ 3] A MapReduce program is composed of a map procedure (or method), which performs filtering and sorting (such as sorting students by first name into queues, one queue for each name), and a reduce method, which performs a summary operation (such as counting the number of students in each queue, yielding name frequencies). The \"MapReduce System\" (also called \"infrastructure\" or \"framework\") orchestrates the processing by marshalling the distributed servers, running the various tasks in parallel, managing all communications and data transfers between the various parts of the system, and providing for redundancy and fault tolerance . The model is a specialization of the split-apply-combine strategy for data analysis.[ 4] It is inspired by the map and reduce functions commonly used in functional programming ,[ 5] although their purpose in the MapReduce framework is not the same as in their original forms.[ 6] The key contributions of the MapReduce framework are not the actual map and reduce functions (which, for example, resemble the 1995 Message Passing Interface standard's[ 7] reduce [ 8] and scatter [ 9] operations), but the scalability and fault-tolerance achieved for a variety of applications by optimizing the execution engine[ citation needed ]. As such, a single-threaded implementation of MapReduce is usually not faster than a traditional (non-MapReduce) implementation; any gains are usually only seen with multi-threaded implementations on multi-processor hardware.[ 10] The use of this model is beneficial only when the optimized distributed shuffle operation (which reduces network communication cost) and fault tolerance features of the MapReduce framework come into play. Optimizing the communication cost is essential to a good MapReduce algorithm.[ 11] MapReduce libraries have been written in many programming languages, with different levels of optimization. A popular open-source implementation that has support for distributed shuffles is part of Apache Hadoop . The name MapReduce originally referred to the proprietary Google technology, but has since been genericized . By 2014, Google was no longer using MapReduce as their primary big data processing model,[ 12] and development on Apache Mahout had moved on to more capable and less disk-oriented mechanisms that incorporated full map and reduce capabilities.[ 13]","title":"MapReduce"},{"location":"Programming-model/TODO-MapReduce/tag-divide%20and%20conquer-VS-MapReduce/","text":"https://cs.stackexchange.com/questions/3019/what-is-the-novelty-in-mapreduce","title":"tag divide and conquer VS MapReduce"},{"location":"RPC/","text":"RPC wikipedia Remote procedure call developer.51cto \u82b1\u4e86\u4e00\u4e2a\u661f\u671f\uff0c\u6211\u7ec8\u4e8e\u628aRPC\u6846\u67b6\u6574\u660e\u767d\u4e86\uff01 NOTE: \u5199\u7684\u4e0d\u9519 RPC(Remote Procedure Call)\uff1a\u8fdc\u7a0b\u8fc7\u7a0b\u8c03\u7528\uff0c\u5b83\u662f\u4e00\u79cd\u901a\u8fc7\u7f51\u7edc\u4ece\u8fdc\u7a0b\u8ba1\u7b97\u673a\u7a0b\u5e8f\u4e0a\u8bf7\u6c42\u670d\u52a1\uff0c\u800c\u4e0d\u9700\u8981\u4e86\u89e3\u5e95\u5c42\u7f51\u7edc\u6280\u672f\u7684\u601d\u60f3\u3002","title":"Introduction"},{"location":"RPC/#rpc","text":"","title":"RPC"},{"location":"RPC/#wikipedia#remote#procedure#call","text":"","title":"wikipedia Remote procedure call"},{"location":"RPC/#developer51cto#rpc","text":"NOTE: \u5199\u7684\u4e0d\u9519 RPC(Remote Procedure Call)\uff1a\u8fdc\u7a0b\u8fc7\u7a0b\u8c03\u7528\uff0c\u5b83\u662f\u4e00\u79cd\u901a\u8fc7\u7f51\u7edc\u4ece\u8fdc\u7a0b\u8ba1\u7b97\u673a\u7a0b\u5e8f\u4e0a\u8bf7\u6c42\u670d\u52a1\uff0c\u800c\u4e0d\u9700\u8981\u4e86\u89e3\u5e95\u5c42\u7f51\u7edc\u6280\u672f\u7684\u601d\u60f3\u3002","title":"developer.51cto \u82b1\u4e86\u4e00\u4e2a\u661f\u671f\uff0c\u6211\u7ec8\u4e8e\u628aRPC\u6846\u67b6\u6574\u660e\u767d\u4e86\uff01"},{"location":"RPC/gRPC/","text":"","title":"Introduction"},{"location":"Scalability/","text":"parallel computing \u7684 scalability 1\u3001concurrency\u7684scalability 2\u3001distributed computing\u7684scalability","title":"Introduction"},{"location":"Scalability/#parallel#computing#scalability","text":"1\u3001concurrency\u7684scalability 2\u3001distributed computing\u7684scalability","title":"parallel computing \u7684 scalability"},{"location":"TODO/20201103-distributed-counter/","text":"distributed counter medium Poor man\u2019s distributed counter architect.dennyzhang Design: A Distributed Counter redis distributed counter: https://github.com/ningyu1/distributed-counter https://redis.io/commands/incr","title":"distributed counter"},{"location":"TODO/20201103-distributed-counter/#distributed#counter","text":"medium Poor man\u2019s distributed counter architect.dennyzhang Design: A Distributed Counter redis distributed counter: https://github.com/ningyu1/distributed-counter https://redis.io/commands/incr","title":"distributed counter"},{"location":"TODO/20201108-Tradeoff/","text":"Tradeoff \u5728computer science\u4e2d\uff0c\u5b58\u5728\u7740\u975e\u5e38\u591a\u7684tradeoff: 1) CAP CAP\u7406\u8bba\u5c31\u662f\u5178\u578b\u7684tradeoff: \u5f53partition\u53d1\u751f\u7684\u65f6\u5019\uff0c\u6211\u4eec\u9700\u8981\u5728consistency\u548cavailability\u4e2d\u8fdb\u884ctradeoff\u3002 2) TensorFlow whitepaper Some TensorFlow uses allow some flexibility in terms of the consistency of parameter updates, and we can easily express and take advantage of these relaxed synchronization requirements in some of our larger deployments. 3) \u6027\u80fd\u4e0e\u53ef\u9760\u6027\u4e4b\u95f4\u7684tradeoff \u540c\u6b65\u5199\u5165\u5230\u6587\u4ef6: \u727a\u7272\u6027\u80fd\u6765\u83b7\u53d6\u53ef\u9760\u6027 \u5f02\u6b65\u5199\u5165\u5230\u6587\u4ef6: \u727a\u7272\u53ef\u9760\u6027\u6765\u83b7\u53d6\u6027\u80fd 4) consistency\u548cperformance\u4e4b\u95f4\u7684tradeoff \u5728e-reading.club Distributed operating systems#6.3. CONSISTENCY MODELS \u4e2d\u63cf\u8ff0\u4e86\u8fd9\u6837\u7684\u573a\u666f: In some DSM (and multiprocessor) systems, the solution is to accept less than perfect consistency as the price for better performance . Precisely what consistency means and how it can be relaxed without making programming unbearable is a major issue among DSM researchers.","title":"Tradeoff"},{"location":"TODO/20201108-Tradeoff/#tradeoff","text":"\u5728computer science\u4e2d\uff0c\u5b58\u5728\u7740\u975e\u5e38\u591a\u7684tradeoff: 1) CAP CAP\u7406\u8bba\u5c31\u662f\u5178\u578b\u7684tradeoff: \u5f53partition\u53d1\u751f\u7684\u65f6\u5019\uff0c\u6211\u4eec\u9700\u8981\u5728consistency\u548cavailability\u4e2d\u8fdb\u884ctradeoff\u3002 2) TensorFlow whitepaper Some TensorFlow uses allow some flexibility in terms of the consistency of parameter updates, and we can easily express and take advantage of these relaxed synchronization requirements in some of our larger deployments. 3) \u6027\u80fd\u4e0e\u53ef\u9760\u6027\u4e4b\u95f4\u7684tradeoff \u540c\u6b65\u5199\u5165\u5230\u6587\u4ef6: \u727a\u7272\u6027\u80fd\u6765\u83b7\u53d6\u53ef\u9760\u6027 \u5f02\u6b65\u5199\u5165\u5230\u6587\u4ef6: \u727a\u7272\u53ef\u9760\u6027\u6765\u83b7\u53d6\u6027\u80fd 4) consistency\u548cperformance\u4e4b\u95f4\u7684tradeoff \u5728e-reading.club Distributed operating systems#6.3. CONSISTENCY MODELS \u4e2d\u63cf\u8ff0\u4e86\u8fd9\u6837\u7684\u573a\u666f: In some DSM (and multiprocessor) systems, the solution is to accept less than perfect consistency as the price for better performance . Precisely what consistency means and how it can be relaxed without making programming unbearable is a major issue among DSM researchers.","title":"Tradeoff"},{"location":"TODO/20201108-contract/","text":"Contract/theory model Consistency model \u53c2\u89c1 infogalactic Consistency model : Strict consistency Sequential Consistency Causal consistency Processor consistency Chomsky hierarchy \u53c2\u89c1 wikipedia Chomsky hierarchy Database normalization \u53c2\u89c1 infogalactic Database normalization C++ Memory Model In the first approach, the C++ memory model defines a contract. This contract is established between the programmer and the system. The system consists of the compiler, which compiles the program into assembler instructions, the processor, which performs the assembler instructions and the different caches, which stores the state of the program. The contract requires from the programmer to obey certain rules and gives the system the full power to optimise the program as far as no rules are broken. The result is - in the good case - a well-defined program, that is maximal optimised. Precisely spoken, there is not only a single contract, but a fine-grained set of contracts. Or to say it differently. The weaker the rules are the programmer has to follow, the more potential is there for the system to generate a highly optimised executable.","title":"Contract/theory model"},{"location":"TODO/20201108-contract/#contracttheory#model","text":"","title":"Contract/theory model"},{"location":"TODO/20201108-contract/#consistency#model","text":"\u53c2\u89c1 infogalactic Consistency model : Strict consistency Sequential Consistency Causal consistency Processor consistency","title":"Consistency model"},{"location":"TODO/20201108-contract/#chomsky#hierarchy","text":"\u53c2\u89c1 wikipedia Chomsky hierarchy","title":"Chomsky hierarchy"},{"location":"TODO/20201108-contract/#database#normalization","text":"\u53c2\u89c1 infogalactic Database normalization","title":"Database normalization"},{"location":"TODO/20201108-contract/#c#memory#model","text":"In the first approach, the C++ memory model defines a contract. This contract is established between the programmer and the system. The system consists of the compiler, which compiles the program into assembler instructions, the processor, which performs the assembler instructions and the different caches, which stores the state of the program. The contract requires from the programmer to obey certain rules and gives the system the full power to optimise the program as far as no rules are broken. The result is - in the good case - a well-defined program, that is maximal optimised. Precisely spoken, there is not only a single contract, but a fine-grained set of contracts. Or to say it differently. The weaker the rules are the programmer has to follow, the more potential is there for the system to generate a highly optimised executable.","title":"C++ Memory Model"},{"location":"Thundering-herd-problem/","text":"Thundering herd problem \u4e00\u3001\"Thundering Herd\" \u7684\u610f\u601d\u662f \"\u60ca\u7fa4\u6548\u5e94\"\u3002 \u4e8c\u3001\u5b83\u7684\u672c\u8d28\u8fd8\u662fmany to one: \u5f53many wait on one\uff0c\u5219\u4e00\u65e6\u6761\u4ef6\u6ee1\u8db3\uff0c\u5982\u679cnotify all\uff0c\u5c31\u4f1a\u5bfc\u81f4\"Thundering Herd\"\uff1b \u89e3\u51b3\u7684\u65b9\u6cd5: notify one\u3001serialize(\u4e32\u884c) wanweibaike Thundering herd problem In computer science , the thundering herd problem occurs when a large number of processes or threads waiting for an event are awoken when that event occurs, but only one process is able to handle the event. After all the processes wake up, they will start to handle the event, but only one will win. All processes will compete for resources, possibly freezing the computer, until the herd is calmed down again.[ 1] [ 2] Mitigation NOTE: \u603b\u7684\u6765\u8bf4\uff0c\u73af\u8282\u7684\u65b9\u6cd5\u5c31\u662fnotify one\u3001serialize The Linux-kernel will serialize(\u4e32\u884c) responses for requests to a single file descriptor, so only one thread (process) is woken up.[ 3] NOTE: \u5728 uwsgi Serializing accept(), AKA Thundering Herd, AKA the Zeeg Problem \u4e2d\uff0c\u7ed9\u51fa\u4e86\u5b9e\u73b0\u65b9\u6848 Similarly in Microsoft Windows, I/O completion ports can mitigate the thundering herd problem, as they can be configured such that only one of the threads waiting on the completion port is woken up when an event occurs.[ 4] \u5404\u4e2a\u9886\u57df\u4e2d\u5e94\u4ed8thundering herd\u603b\u7ed3 Concurrent server server \u53c2\u8003\u6587\u7ae0 Nginx Thundering-herd-in-Nginx uwsgi uwsgi-docs Serializing accept(), AKA Thundering Herd, AKA the Zeeg Problem Apache httpd accept Serialization - Multiple Sockets \u3001 accept Serialization - Single Socket Distributed computing \u6587\u7ae0 zookeeper zookeeper A Guide to Creating Higher-level Constructs with ZooKeeper#Leader Election Thundering-Herd-in-distributed-computing Linux OS \u53c2\u89c1 Thundering-Herd-in-Linux \u7ae0\u8282 Multithread \u53c2\u89c1 Concurrent-computing\\Concurrency-control\\Lock\\Spinlock \u7ae0\u8282\u3002","title":"Introduction"},{"location":"Thundering-herd-problem/#thundering#herd#problem","text":"\u4e00\u3001\"Thundering Herd\" \u7684\u610f\u601d\u662f \"\u60ca\u7fa4\u6548\u5e94\"\u3002 \u4e8c\u3001\u5b83\u7684\u672c\u8d28\u8fd8\u662fmany to one: \u5f53many wait on one\uff0c\u5219\u4e00\u65e6\u6761\u4ef6\u6ee1\u8db3\uff0c\u5982\u679cnotify all\uff0c\u5c31\u4f1a\u5bfc\u81f4\"Thundering Herd\"\uff1b \u89e3\u51b3\u7684\u65b9\u6cd5: notify one\u3001serialize(\u4e32\u884c)","title":"Thundering herd problem"},{"location":"Thundering-herd-problem/#wanweibaike#thundering#herd#problem","text":"In computer science , the thundering herd problem occurs when a large number of processes or threads waiting for an event are awoken when that event occurs, but only one process is able to handle the event. After all the processes wake up, they will start to handle the event, but only one will win. All processes will compete for resources, possibly freezing the computer, until the herd is calmed down again.[ 1] [ 2]","title":"wanweibaike Thundering herd problem"},{"location":"Thundering-herd-problem/#mitigation","text":"NOTE: \u603b\u7684\u6765\u8bf4\uff0c\u73af\u8282\u7684\u65b9\u6cd5\u5c31\u662fnotify one\u3001serialize The Linux-kernel will serialize(\u4e32\u884c) responses for requests to a single file descriptor, so only one thread (process) is woken up.[ 3] NOTE: \u5728 uwsgi Serializing accept(), AKA Thundering Herd, AKA the Zeeg Problem \u4e2d\uff0c\u7ed9\u51fa\u4e86\u5b9e\u73b0\u65b9\u6848 Similarly in Microsoft Windows, I/O completion ports can mitigate the thundering herd problem, as they can be configured such that only one of the threads waiting on the completion port is woken up when an event occurs.[ 4]","title":"Mitigation"},{"location":"Thundering-herd-problem/#thundering#herd","text":"","title":"\u5404\u4e2a\u9886\u57df\u4e2d\u5e94\u4ed8thundering herd\u603b\u7ed3"},{"location":"Thundering-herd-problem/#concurrent#server","text":"server \u53c2\u8003\u6587\u7ae0 Nginx Thundering-herd-in-Nginx uwsgi uwsgi-docs Serializing accept(), AKA Thundering Herd, AKA the Zeeg Problem Apache httpd accept Serialization - Multiple Sockets \u3001 accept Serialization - Single Socket","title":"Concurrent server"},{"location":"Thundering-herd-problem/#distributed#computing","text":"\u6587\u7ae0 zookeeper zookeeper A Guide to Creating Higher-level Constructs with ZooKeeper#Leader Election Thundering-Herd-in-distributed-computing","title":"Distributed computing"},{"location":"Thundering-herd-problem/#linux#os","text":"\u53c2\u89c1 Thundering-Herd-in-Linux \u7ae0\u8282","title":"Linux OS"},{"location":"Thundering-herd-problem/#multithread","text":"\u53c2\u89c1 Concurrent-computing\\Concurrency-control\\Lock\\Spinlock \u7ae0\u8282\u3002","title":"Multithread"},{"location":"Thundering-herd-problem/TODO-louqibin-Robinhood-%E5%AE%95%E6%9C%BA-%E5%B9%95%E5%90%8E%E9%BB%91%E6%89%8BThundering-Herd/","text":"louqibin Robinhood\u53c8\u53cc\u53d2\u53d5\u5b95\u673a\u4e86\u2014\u2014\u6d45\u8c08\u5e55\u540e\u9ed1\u624b\u201cThundering Herd\u201d","title":"Introduction"},{"location":"Thundering-herd-problem/TODO-louqibin-Robinhood-%E5%AE%95%E6%9C%BA-%E5%B9%95%E5%90%8E%E9%BB%91%E6%89%8BThundering-Herd/#louqibin#robinhoodthundering#herd","text":"","title":"louqibin Robinhood\u53c8\u53cc\u53d2\u53d5\u5b95\u673a\u4e86\u2014\u2014\u6d45\u8c08\u5e55\u540e\u9ed1\u624b\u201cThundering Herd\u201d"},{"location":"Thundering-herd-problem/Thundering-Herd-in-distributed-computing/","text":"\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684Thundering Herd\u6548\u5e94 zookeeper A Guide to Creating Higher-level Constructs with ZooKeeper#Leader Election cnblogs UNIX \u5386\u53f2\u95ee\u9898 \u5206\u5e03\u5f0f\u7cfb\u7edf\u7684Thundering Herd\u6548\u5e94 \u60ca\u7fa4\u6548\u5e94","title":"Introduction"},{"location":"Thundering-herd-problem/Thundering-Herd-in-distributed-computing/#thundering#herd","text":"zookeeper A Guide to Creating Higher-level Constructs with ZooKeeper#Leader Election","title":"\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684Thundering Herd\u6548\u5e94"},{"location":"Thundering-herd-problem/Thundering-Herd-in-distributed-computing/#cnblogs#unix#thundering#herd","text":"","title":"cnblogs UNIX \u5386\u53f2\u95ee\u9898 \u5206\u5e03\u5f0f\u7cfb\u7edf\u7684Thundering Herd\u6548\u5e94 \u60ca\u7fa4\u6548\u5e94"},{"location":"Thundering-herd-problem/Thundering-Herd-in-distributed-computing/zhihu-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%9A%84Thundering-Herd%E6%95%88%E5%BA%94/","text":"zhihu \u5206\u5e03\u5f0f\u7cfb\u7edf\u7684Thundering Herd\u6548\u5e94","title":"Introduction"},{"location":"Thundering-herd-problem/Thundering-Herd-in-distributed-computing/zhihu-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%9A%84Thundering-Herd%E6%95%88%E5%BA%94/#zhihu#thundering#herd","text":"","title":"zhihu \u5206\u5e03\u5f0f\u7cfb\u7edf\u7684Thundering Herd\u6548\u5e94"},{"location":"software-Memcached/","text":"Memcached","title":"Introduction"},{"location":"software-Memcached/#memcached","text":"","title":"Memcached"},{"location":"software-Memcached/Thread-model/","text":"\u7ebf\u7a0b\u6a21\u578b itpub memcache\u7684\u7ebf\u7a0b\u6a21\u578b MC\u91c7\u7528\u4e00master\u591aworker\u7684\u5de5\u4f5c\u6a21\u578b\uff0c\u7531master\u8d1f\u8d23accept\u5ba2\u6237\u7aef\u8bf7\u6c42\uff0c\u7136\u540e\u4ee5RR\u5206\u53d1\u7ed9worker\uff1b -t \u7ebf\u7a0b\u6570\uff0c\u7528\u4e8e\u5904\u7406\u8bf7\u6c42\uff0c\u9ed8\u8ba4\u4e3a4 -b backlog\u961f\u5217\u957f\u5ea6\uff0c\u9ed8\u8ba41024 calixwu Memcached\u6e90\u7801\u5206\u6790\u4e4b\u7ebf\u7a0b\u6a21\u578b","title":"Introduction"},{"location":"software-Memcached/Thread-model/#_1","text":"","title":"\u7ebf\u7a0b\u6a21\u578b"},{"location":"software-Memcached/Thread-model/#itpub#memcache","text":"MC\u91c7\u7528\u4e00master\u591aworker\u7684\u5de5\u4f5c\u6a21\u578b\uff0c\u7531master\u8d1f\u8d23accept\u5ba2\u6237\u7aef\u8bf7\u6c42\uff0c\u7136\u540e\u4ee5RR\u5206\u53d1\u7ed9worker\uff1b -t \u7ebf\u7a0b\u6570\uff0c\u7528\u4e8e\u5904\u7406\u8bf7\u6c42\uff0c\u9ed8\u8ba4\u4e3a4 -b backlog\u961f\u5217\u957f\u5ea6\uff0c\u9ed8\u8ba41024","title":"itpub memcache\u7684\u7ebf\u7a0b\u6a21\u578b"},{"location":"software-Memcached/Thread-model/#calixwu#memcached","text":"","title":"calixwu Memcached\u6e90\u7801\u5206\u6790\u4e4b\u7ebf\u7a0b\u6a21\u578b"},{"location":"software-ZooKeeper/","text":"Apache ZooKeeper ZooKeeper\u7684\u5173\u952e\u5b57\u662f\"coordination\"\uff0c\u5373\"\u534f\u8c03\"\u3002 \u5b98\u7f51 Apache ZooKeeper Doc ZooKeeper: A Distributed Coordination Service for Distributed Applications wikipedia Apache ZooKeeper#Use cases Typical use cases for ZooKeeper are: 1) Naming service 2) Configuration management 3) Data Synchronization 4) Leader election 5) Message queue 6) Notification system Q&A zookeeper\u4e3b\u5907\u5207\u6362\u3002 \u4e00\u4e2azookeeper process\u9700\u8981\u4fa6\u542c\u4e09\u4e2a\u7aef\u53e3\uff1f ||| | -------------------------- | ------------------------------------------------------------ | | server.id=host:port1:port2 | \u5176\u4e2did\u4e3a\u4e00\u4e2a\u6570\u5b57\uff0c\u8868\u793azk\u8fdb\u7a0b\u7684id\uff0c\u8fd9\u4e2aid\u4e5f\u662fdataDir\u76ee\u5f55\u4e0bmyid\u6587\u4ef6\u7684\u5185\u5bb9\u3002 host\u662f\u8be5zk\u8fdb\u7a0b\u6240\u5728\u673a\u5668\u7684IP\u5730\u5740\uff0cport1\u8868\u793afollower\u548cleader\u4ea4\u6362\u6d88\u606f\u6240\u4f7f\u7528\u7684\u7aef\u53e3\uff0cport2\u8868\u793a\u9009\u4e3eleader\u6240\u4f7f\u7528\u7684\u7aef\u53e3\u3002 | | dataDir | \u5176\u914d\u7f6e\u7684\u542b\u4e49\u8ddf\u5355\u673a\u6a21\u5f0f\u4e0b\u7684\u542b\u4e49\u7c7b\u4f3c\uff0c\u4e0d\u540c\u7684\u662f\u96c6\u7fa4\u6a21\u5f0f\u4e0b\u8fd8\u6709\u4e00\u4e2amyid\u6587\u4ef6\u3002myid\u6587\u4ef6\u7684\u5185\u5bb9\u53ea\u6709\u4e00\u884c\uff0c\u4e14\u5185\u5bb9\u53ea\u80fd\u4e3a1 - 255\u4e4b\u95f4\u7684\u6570\u5b57\uff0c\u8fd9\u4e2a\u6570\u5b57\u4ea6\u5373\u4e0a\u9762\u4ecb\u7ecdserver.id\u4e2d\u7684id\uff0c\u8868\u793azk\u8fdb\u7a0b\u7684id\u3002 | | Clientport | \u662f\u6307\u5916\u90e8\u5ba2\u6237\u7aef\u8fde\u63a5\u7684\u7aef\u53e3\uff1b | leader \u548c follower \u7684\u89d2\u8272\u5982\u4f55\u8fdb\u884c\u6307\u5b9a\uff1f \u4e0eredis\u4e0d\u540c\u7684\u662f\uff0czookeeper\u4e2d\uff0cleader\u662f\u7531\u9009\u4e3e\u4ea7\u751f\u7684\uff0c\u800c\u4e0d\u662f\u7531\u7528\u6237\u76f4\u63a5\u8fdb\u884c\u6307\u5b9a\u3002","title":"Introduction"},{"location":"software-ZooKeeper/#apache#zookeeper","text":"ZooKeeper\u7684\u5173\u952e\u5b57\u662f\"coordination\"\uff0c\u5373\"\u534f\u8c03\"\u3002","title":"Apache ZooKeeper"},{"location":"software-ZooKeeper/#apache#zookeeper_1","text":"","title":"\u5b98\u7f51 Apache ZooKeeper"},{"location":"software-ZooKeeper/#doc","text":"ZooKeeper: A Distributed Coordination Service for Distributed Applications","title":"Doc"},{"location":"software-ZooKeeper/#wikipedia#apache#zookeeperuse#cases","text":"Typical use cases for ZooKeeper are: 1) Naming service 2) Configuration management 3) Data Synchronization 4) Leader election 5) Message queue 6) Notification system","title":"wikipedia Apache ZooKeeper#Use cases"},{"location":"software-ZooKeeper/#qa","text":"zookeeper\u4e3b\u5907\u5207\u6362\u3002 \u4e00\u4e2azookeeper process\u9700\u8981\u4fa6\u542c\u4e09\u4e2a\u7aef\u53e3\uff1f ||| | -------------------------- | ------------------------------------------------------------ | | server.id=host:port1:port2 | \u5176\u4e2did\u4e3a\u4e00\u4e2a\u6570\u5b57\uff0c\u8868\u793azk\u8fdb\u7a0b\u7684id\uff0c\u8fd9\u4e2aid\u4e5f\u662fdataDir\u76ee\u5f55\u4e0bmyid\u6587\u4ef6\u7684\u5185\u5bb9\u3002 host\u662f\u8be5zk\u8fdb\u7a0b\u6240\u5728\u673a\u5668\u7684IP\u5730\u5740\uff0cport1\u8868\u793afollower\u548cleader\u4ea4\u6362\u6d88\u606f\u6240\u4f7f\u7528\u7684\u7aef\u53e3\uff0cport2\u8868\u793a\u9009\u4e3eleader\u6240\u4f7f\u7528\u7684\u7aef\u53e3\u3002 | | dataDir | \u5176\u914d\u7f6e\u7684\u542b\u4e49\u8ddf\u5355\u673a\u6a21\u5f0f\u4e0b\u7684\u542b\u4e49\u7c7b\u4f3c\uff0c\u4e0d\u540c\u7684\u662f\u96c6\u7fa4\u6a21\u5f0f\u4e0b\u8fd8\u6709\u4e00\u4e2amyid\u6587\u4ef6\u3002myid\u6587\u4ef6\u7684\u5185\u5bb9\u53ea\u6709\u4e00\u884c\uff0c\u4e14\u5185\u5bb9\u53ea\u80fd\u4e3a1 - 255\u4e4b\u95f4\u7684\u6570\u5b57\uff0c\u8fd9\u4e2a\u6570\u5b57\u4ea6\u5373\u4e0a\u9762\u4ecb\u7ecdserver.id\u4e2d\u7684id\uff0c\u8868\u793azk\u8fdb\u7a0b\u7684id\u3002 | | Clientport | \u662f\u6307\u5916\u90e8\u5ba2\u6237\u7aef\u8fde\u63a5\u7684\u7aef\u53e3\uff1b | leader \u548c follower \u7684\u89d2\u8272\u5982\u4f55\u8fdb\u884c\u6307\u5b9a\uff1f \u4e0eredis\u4e0d\u540c\u7684\u662f\uff0czookeeper\u4e2d\uff0cleader\u662f\u7531\u9009\u4e3e\u4ea7\u751f\u7684\uff0c\u800c\u4e0d\u662f\u7531\u7528\u6237\u76f4\u63a5\u8fdb\u884c\u6307\u5b9a\u3002","title":"Q&amp;A"},{"location":"software-ZooKeeper/Developers/","text":"ZooKeeper Programmer's Guide Introduction It does not contain source code, but it does assume a familiarity with the problems associated with distributed computing. NOTE: \u5173\u4e8e\"the problems associated with distributed computing\"\uff0c\u53c2\u89c1 Distributed-computing\\Theory\\The-Trouble-with-Distributed-Systems \u3002 The ZooKeeper Data Model ZooKeeper has a hierarchal namespace, much like a distributed file system. NOTE: \u5173\u4e8ehierarchal \uff0c\u53c2\u89c1\u5de5\u7a0bdiscrete\u7684 Relation-structure-computation\\Model\\Hierarchy-relation-model \u7ae0\u8282\uff1b\u4e0a\u8ff0**hierarchal** namespace\uff0c\u5176\u5b9e\u5c31\u662f\u4e00\u68f5\u6811\u3002 ZNodes Every node in a ZooKeeper tree is referred to as a znode . Znodes maintain a stat structure that includes: 1) version numbers for data changes, acl changes 2) timestamps The version number, together with the timestamp, allows ZooKeeper to validate the cache and to coordinate updates. Each time a znode's data changes, the version number increases. NOTE: \u4e00\u3001\u8fd9\u662fvector version\u6280\u672f\uff0c\u53c2\u89c1 Vector-version \uff0c\u5728distributed system\u4e2d\u6709\u7740\u975e\u5e38\u91cd\u8981\u7684\u5e94\u7528\uff0c\u5176\u5b9e\u5b83\u80cc\u540e\u7684\u601d\u60f3\u6709: 1\u3001make it computational-ordering 2\u3001lamport clock For instance, whenever a client retrieves data, it also receives the version of the data. And when a client performs an update or a delete, it must supply the version of the data of the znode it is changing. If the version it supplies doesn't match the actual version of the data, the update will fail. NOTE: ZK\u5c06version\u66b4\u9732\u7ed9\u4e86\u7528\u6237 Znodes are the main entity that a programmer access. They have several characteristics that are worth mentioning here. Watches Clients can set watches on znodes. Changes to that znode trigger the watch and then clear the watch. When a watch triggers, ZooKeeper sends the client a notification. More information about watches can be found in the section ZooKeeper Watches . NOTE: redis\u4e5f\u6709\u7c7b\u4f3c\u7684\u529f\u80fd Data Access The data stored at each znode in a namespace is read and written atomically . NOTE: atomicity\u662f\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u7279\u6027 Coordination data ZooKeeper was not designed to be a general database or large object store. Instead, it manages coordination data . This data can come in the form of configuration , status information , rendezvous , etc. A common property of the various forms of coordination data is that they are relatively small: measured in kilobytes. If large data storage is needed, the usual pattern of dealing with such data is to store it on a bulk storage system , such as NFS or HDFS, and store pointers to the storage locations in ZooKeeper. Ephemeral Nodes ZooKeeper also has the notion of ephemeral nodes. These znodes exists as long as the session that created the znode is active. When the session ends the znode is deleted. Sequence Nodes -- Unique Naming Time in ZooKeeper ZooKeeper tracks time multiple ways: 1\u3001 Zxid Every change to the ZooKeeper state receives a stamp in the form of a zxid (ZooKeeper Transaction Id). This exposes the total ordering of all changes to ZooKeeper. Each change will have a unique zxid and if zxid1 is smaller than zxid2 then zxid1 happened before zxid2. NOTE: \" total ordering \"\u4f53\u73b0\u4e86ZK\u7684ordering \"happened-before\"\u662fdistributed system\u4e2d\uff0c\u975e\u5e38\u91cd\u8981\u7684\u4e00\u79cdrelation\uff0c\u5b83\u662f\u7531Leslie Lamport \u63d0\u51fa\u7684\uff0c\u53c2\u89c1: 1\u3001wikipedia Happened-before 2\u3001 https://amturing.acm.org/p558-lamport.pdf 2\u3001 Version numbers Every change to a node will cause an increase to one of the version numbers of that node. The three version numbers are version (number of changes to the data of a znode), cversion (number of changes to the children of a znode), and aversion (number of changes to the ACL of a znode). 3\u3001 Ticks When using multi-server ZooKeeper, servers use ticks to define timing of events such as status uploads, session timeouts, connection timeouts between peers, etc. The tick time is only indirectly exposed through the minimum session timeout (2 times the tick time); if a client requests a session timeout less than the minimum session timeout, the server will tell the client that the session timeout is actually the minimum session timeout. 4\u3001 Real time ZooKeeper doesn't use real time, or clock time, at all except to put timestamps into the stat structure on znode creation and znode modification. ZooKeeper Stat Structure ZooKeeper Sessions A ZooKeeper client establishes a session with the ZooKeeper service by creating a handle to the service using a language binding. NOTE: \u5178\u578b\u7684finite state machine ZooKeeper Watches All of the read operations in ZooKeeper - getData() , getChildren() , and exists() - have the option of setting a watch as a side effect. Here is ZooKeeper's definition of a watch: a watch event is one-time trigger, sent to the client that set the watch, which occurs when the data for which the watch was set changes. There are three key points to consider in this definition of a watch: NOTE: one-time trigger\u8ba9\u6211\u60f3\u5230\u4e86 epoll Triggering modes \uff0c\u663e\u7136\u5b83\u7c7b\u4f3c\u4e8e edge-triggered ZooKeeper access control using ACLs Pluggable ZooKeeper authentication Consistency Guarantees Bindings Building Blocks: A Guide to ZooKeeper Operations","title":"Introduction"},{"location":"software-ZooKeeper/Developers/#zookeeper#programmers#guide","text":"","title":"ZooKeeper Programmer's Guide"},{"location":"software-ZooKeeper/Developers/#introduction","text":"It does not contain source code, but it does assume a familiarity with the problems associated with distributed computing. NOTE: \u5173\u4e8e\"the problems associated with distributed computing\"\uff0c\u53c2\u89c1 Distributed-computing\\Theory\\The-Trouble-with-Distributed-Systems \u3002","title":"Introduction"},{"location":"software-ZooKeeper/Developers/#the#zookeeper#data#model","text":"ZooKeeper has a hierarchal namespace, much like a distributed file system. NOTE: \u5173\u4e8ehierarchal \uff0c\u53c2\u89c1\u5de5\u7a0bdiscrete\u7684 Relation-structure-computation\\Model\\Hierarchy-relation-model \u7ae0\u8282\uff1b\u4e0a\u8ff0**hierarchal** namespace\uff0c\u5176\u5b9e\u5c31\u662f\u4e00\u68f5\u6811\u3002","title":"The ZooKeeper Data Model"},{"location":"software-ZooKeeper/Developers/#znodes","text":"Every node in a ZooKeeper tree is referred to as a znode . Znodes maintain a stat structure that includes: 1) version numbers for data changes, acl changes 2) timestamps The version number, together with the timestamp, allows ZooKeeper to validate the cache and to coordinate updates. Each time a znode's data changes, the version number increases. NOTE: \u4e00\u3001\u8fd9\u662fvector version\u6280\u672f\uff0c\u53c2\u89c1 Vector-version \uff0c\u5728distributed system\u4e2d\u6709\u7740\u975e\u5e38\u91cd\u8981\u7684\u5e94\u7528\uff0c\u5176\u5b9e\u5b83\u80cc\u540e\u7684\u601d\u60f3\u6709: 1\u3001make it computational-ordering 2\u3001lamport clock For instance, whenever a client retrieves data, it also receives the version of the data. And when a client performs an update or a delete, it must supply the version of the data of the znode it is changing. If the version it supplies doesn't match the actual version of the data, the update will fail. NOTE: ZK\u5c06version\u66b4\u9732\u7ed9\u4e86\u7528\u6237 Znodes are the main entity that a programmer access. They have several characteristics that are worth mentioning here.","title":"ZNodes"},{"location":"software-ZooKeeper/Developers/#watches","text":"Clients can set watches on znodes. Changes to that znode trigger the watch and then clear the watch. When a watch triggers, ZooKeeper sends the client a notification. More information about watches can be found in the section ZooKeeper Watches . NOTE: redis\u4e5f\u6709\u7c7b\u4f3c\u7684\u529f\u80fd","title":"Watches"},{"location":"software-ZooKeeper/Developers/#data#access","text":"The data stored at each znode in a namespace is read and written atomically . NOTE: atomicity\u662f\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u7279\u6027","title":"Data Access"},{"location":"software-ZooKeeper/Developers/#coordination#data","text":"ZooKeeper was not designed to be a general database or large object store. Instead, it manages coordination data . This data can come in the form of configuration , status information , rendezvous , etc. A common property of the various forms of coordination data is that they are relatively small: measured in kilobytes. If large data storage is needed, the usual pattern of dealing with such data is to store it on a bulk storage system , such as NFS or HDFS, and store pointers to the storage locations in ZooKeeper.","title":"Coordination data"},{"location":"software-ZooKeeper/Developers/#ephemeral#nodes","text":"ZooKeeper also has the notion of ephemeral nodes. These znodes exists as long as the session that created the znode is active. When the session ends the znode is deleted.","title":"Ephemeral Nodes"},{"location":"software-ZooKeeper/Developers/#sequence#nodes#--#unique#naming","text":"","title":"Sequence Nodes -- Unique Naming"},{"location":"software-ZooKeeper/Developers/#time#in#zookeeper","text":"ZooKeeper tracks time multiple ways: 1\u3001 Zxid Every change to the ZooKeeper state receives a stamp in the form of a zxid (ZooKeeper Transaction Id). This exposes the total ordering of all changes to ZooKeeper. Each change will have a unique zxid and if zxid1 is smaller than zxid2 then zxid1 happened before zxid2. NOTE: \" total ordering \"\u4f53\u73b0\u4e86ZK\u7684ordering \"happened-before\"\u662fdistributed system\u4e2d\uff0c\u975e\u5e38\u91cd\u8981\u7684\u4e00\u79cdrelation\uff0c\u5b83\u662f\u7531Leslie Lamport \u63d0\u51fa\u7684\uff0c\u53c2\u89c1: 1\u3001wikipedia Happened-before 2\u3001 https://amturing.acm.org/p558-lamport.pdf 2\u3001 Version numbers Every change to a node will cause an increase to one of the version numbers of that node. The three version numbers are version (number of changes to the data of a znode), cversion (number of changes to the children of a znode), and aversion (number of changes to the ACL of a znode). 3\u3001 Ticks When using multi-server ZooKeeper, servers use ticks to define timing of events such as status uploads, session timeouts, connection timeouts between peers, etc. The tick time is only indirectly exposed through the minimum session timeout (2 times the tick time); if a client requests a session timeout less than the minimum session timeout, the server will tell the client that the session timeout is actually the minimum session timeout. 4\u3001 Real time ZooKeeper doesn't use real time, or clock time, at all except to put timestamps into the stat structure on znode creation and znode modification.","title":"Time in ZooKeeper"},{"location":"software-ZooKeeper/Developers/#zookeeper#stat#structure","text":"","title":"ZooKeeper Stat Structure"},{"location":"software-ZooKeeper/Developers/#zookeeper#sessions","text":"A ZooKeeper client establishes a session with the ZooKeeper service by creating a handle to the service using a language binding. NOTE: \u5178\u578b\u7684finite state machine","title":"ZooKeeper Sessions"},{"location":"software-ZooKeeper/Developers/#zookeeper#watches","text":"All of the read operations in ZooKeeper - getData() , getChildren() , and exists() - have the option of setting a watch as a side effect. Here is ZooKeeper's definition of a watch: a watch event is one-time trigger, sent to the client that set the watch, which occurs when the data for which the watch was set changes. There are three key points to consider in this definition of a watch: NOTE: one-time trigger\u8ba9\u6211\u60f3\u5230\u4e86 epoll Triggering modes \uff0c\u663e\u7136\u5b83\u7c7b\u4f3c\u4e8e edge-triggered","title":"ZooKeeper Watches"},{"location":"software-ZooKeeper/Developers/#zookeeper#access#control#using#acls","text":"","title":"ZooKeeper access control using ACLs"},{"location":"software-ZooKeeper/Developers/#pluggable#zookeeper#authentication","text":"","title":"Pluggable ZooKeeper authentication"},{"location":"software-ZooKeeper/Developers/#consistency#guarantees","text":"","title":"Consistency Guarantees"},{"location":"software-ZooKeeper/Developers/#bindings","text":"","title":"Bindings"},{"location":"software-ZooKeeper/Developers/#building#blocks#a#guide#to#zookeeper#operations","text":"","title":"Building Blocks: A Guide to ZooKeeper Operations"},{"location":"software-ZooKeeper/Developers/Application/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u8ba8\u8bbaZK\u7684application\u3002 zookeeper ZooKeeper Recipes and Solutions NOTE: \u539f\u6587\u63cf\u8ff0\u7684\u5404\u79cdsolution\u5728distributed computing\u4e2d\u662f\u5178\u578b\u7684\u6280\u672f\u3002 In this article, you'll find guidelines for using ZooKeeper to implement higher order functions. All of them are conventions implemented at the client and do not require special support from ZooKeeper. Hopefully the community will capture these conventions in client-side libraries to ease their use and to encourage standardization. One of the most interesting things about ZooKeeper is that even though ZooKeeper uses asynchronous notifications, you can use it to build synchronous consistency primitives, such as queues and locks. As you will see, this is possible because ZooKeeper imposes an overall order on updates, and has mechanisms to expose this ordering. NOTE: ZK\u5c06\u5b83\u7684order\u66b4\u9732\u7ed9\u4e86user Out of the Box Applications: Name Service, Configuration, Group Membership NOTE: \u5f00\u7bb1\u5373\u7528\u7684\u529f\u80fd Name service and configuration are two of the primary applications of ZooKeeper. These two functions are provided directly by the ZooKeeper API. Another function directly provided by ZooKeeper is group membership . The group is represented by a node. Members of the group create ephemeral nodes under the group node. Nodes of the members that fail abnormally will be removed automatically when ZooKeeper detects the failure. Barriers Queues Locks Two-phased Commit Leader Election \u53c2\u89c1 ./Lead-election zhuanlan.zhihu \u4e3a\u4ec0\u4e48\u9700\u8981 Zookeeper","title":"Introduction"},{"location":"software-ZooKeeper/Developers/Application/#_1","text":"\u672c\u7ae0\u8ba8\u8bbaZK\u7684application\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"software-ZooKeeper/Developers/Application/#zookeeper#zookeeper#recipes#and#solutions","text":"NOTE: \u539f\u6587\u63cf\u8ff0\u7684\u5404\u79cdsolution\u5728distributed computing\u4e2d\u662f\u5178\u578b\u7684\u6280\u672f\u3002 In this article, you'll find guidelines for using ZooKeeper to implement higher order functions. All of them are conventions implemented at the client and do not require special support from ZooKeeper. Hopefully the community will capture these conventions in client-side libraries to ease their use and to encourage standardization. One of the most interesting things about ZooKeeper is that even though ZooKeeper uses asynchronous notifications, you can use it to build synchronous consistency primitives, such as queues and locks. As you will see, this is possible because ZooKeeper imposes an overall order on updates, and has mechanisms to expose this ordering. NOTE: ZK\u5c06\u5b83\u7684order\u66b4\u9732\u7ed9\u4e86user","title":"zookeeper ZooKeeper Recipes and Solutions"},{"location":"software-ZooKeeper/Developers/Application/#out#of#the#box#applications#name#service#configuration#group#membership","text":"NOTE: \u5f00\u7bb1\u5373\u7528\u7684\u529f\u80fd Name service and configuration are two of the primary applications of ZooKeeper. These two functions are provided directly by the ZooKeeper API. Another function directly provided by ZooKeeper is group membership . The group is represented by a node. Members of the group create ephemeral nodes under the group node. Nodes of the members that fail abnormally will be removed automatically when ZooKeeper detects the failure.","title":"Out of the Box Applications: Name Service, Configuration, Group Membership"},{"location":"software-ZooKeeper/Developers/Application/#barriers","text":"","title":"Barriers"},{"location":"software-ZooKeeper/Developers/Application/#queues","text":"","title":"Queues"},{"location":"software-ZooKeeper/Developers/Application/#locks","text":"","title":"Locks"},{"location":"software-ZooKeeper/Developers/Application/#two-phased#commit","text":"","title":"Two-phased Commit"},{"location":"software-ZooKeeper/Developers/Application/#leader#election","text":"\u53c2\u89c1 ./Lead-election","title":"Leader Election"},{"location":"software-ZooKeeper/Developers/Application/#zhuanlanzhihu#zookeeper","text":"","title":"zhuanlan.zhihu \u4e3a\u4ec0\u4e48\u9700\u8981 Zookeeper"},{"location":"software-ZooKeeper/Developers/Application/Lead-election/","text":"Leader Election \u672c\u7ae0\u8ba8\u8bbazookeeper\u5b9e\u73b0Leader Election\uff0c\u540c\u65f6\u4e5f\u80fd\u591f\u5b9e\u73b0automatic failover\u3002 zookeeper A Guide to Creating Higher-level Constructs with ZooKeeper#Leader Election A simple way of doing leader election with ZooKeeper is to use the SEQUENCE|EPHEMERAL flags when creating znodes that represent \"proposals\" of clients. The idea is to have a znode, say \" /election \", such that each znode creates a child znode \" /election/guid-n_ \" with both flags SEQUENCE|EPHEMERAL . With the sequence flag, ZooKeeper automatically appends a sequence number that is greater than any one previously appended to a child of \" /election \". The proces(\u8fdb\u7a0b) that created the znode with the smallest appended sequence number is the leader. That's not all, though. It is important to watch for failures of the leader , so that a new client arises as the new leader in the case the current leader fails. A trivial solution is to have all application processes watching upon the current smallest znode , and checking if they are the new leader when the smallest znode goes away (note that the smallest znode will go away if the leader fails because the node is ephemeral ). But this causes a herd effect (\u7f8a\u7fa4\u6548\u5e94): upon a failure of the current leader, all other processes receive a notification, and execute getChildren on \" /election \" to obtain the current list of children of \" /election \". If the number of clients is large, it causes a spike(\u6bdb\u523a) on the number of operations that ZooKeeper servers have to process. To avoid the herd effect , it is sufficient to watch for the next znode down on the sequence of znodes. If a client receives a notification that the znode it is watching is gone, then it becomes the new leader in the case that there is no smaller znode. Note that this avoids the herd effect by not having all clients watching the same znode. Here's the pseudo code: Let ELECTION be a path of choice of the application. To volunteer to be a leader: Create znode z with path \" ELECTION/guid-n_ \" with both SEQUENCE and EPHEMERAL flags; Let C be the children of \" ELECTION \", and i be the sequence number of z ; Watch for changes on \" ELECTION/guid-n_j \", where j is the largest sequence number such that j < i and n_j is a znode in C; Upon receiving a notification of znode deletion: Let C be the new set of children of ELECTION ; If z is the smallest node in C , then execute leader procedure; Otherwise, watch for changes on \" ELECTION/guid-n_j \", where j is the largest sequence number such that j < i and n_j is a znode in C; Notes: Note that the znode having no preceding znode on the list of children do not imply that the creator of this znode is aware that it is the current leader. Applications may consider creating a separate znode to acknowledge that the leader has executed the leader procedure. See the note for Locks on how to use the guid in the node. hadoopinrealworld What is ZooKeeper and it\u2019s Use Case Leader Election Leader election is one of the common use case for ZooKeeper. Let\u2019s see how it works. Let\u2019s say we have 2 resource managers. Each resource manager will participate in the leader election process to decide who becomes the active resource manager. Resource managers use ZooKeeper to elect a leader among themselves. Let\u2019s see how it works. Both resource mangers will attempt to create a znode named ActiveStandbyElectorLock in ZooKeeper and only one resource manager will be able to create ActiveStandbyElectorLock znode. Whoever creates this znode first will become the active resource manager . So if resource manger 1 creates ActiveStandbyElectorLock first, it will become the active node and if resource manger 2 creates ActiveStandbyElectorLock , it will be become the active node. Let\u2019s say resource manger 1 created the ActiveStandbyElectorLock znode first, resource manager 2 will also attempt to create ActiveStandbyElectorLock but it won\u2019t be able to because ActiveStandbyElectorLock is already created by resource manager 1 so resource manager 1 becomes the active resource manager and resource manager 2 becomes the stand by node and goes in to stand by mode. So far clear? So we have now elected a leader. Resource manager 1 is the active resource manager now. Failover With Resource Manager 1 as the active node, everything is working good, up until one fine morning when the RAM in resource manager 1 has gone bad. So resource manager 1 went down. Now our expectation is resource manager 2 should become the active node automatically. Let\u2019s now understand how failover from active node to standby node works. With ZooKeeper, we can add a watch to any node, watchers are extremely helpful because if we are watching a znode and when that znode gets updated or deleted, whoever is watching the znode will get notified. Remember, whoever creates the ActiveStandbyElectorLock ephemeral znode first becomes the active resoure manager. In our case resource manager 1 beat resource manger 2 by creating ActiveStandbyElectorLock first. When resource manager 2 tried to create ActiveStandbyElectorLock znode, it couldn\u2019t because it was already created. So resource manager 2 realized that there is already an active resource manager elected and it became the stand by node . Before resource manager 2 becomes stand by, it will add a watch on the ActiveStandbyElectorLock znode. When resource manager 1 goes down due to the RAM failure, it\u2019s session with ZooKeeper will become dead and since ActiveStandbyElectorLock is an ephemeral node, ZooKeeper will then delete the ActiveStandbyElectorLock node as soon the application who created the znode loses connection with ZooKeeper. Since resource manger 2 is watching the ActiveStandbyElectorLock node, it will get a notification stating ActiveStandbyElectorLock is gone. Resource Manager 2 will now attempt to create ActiveStandbyElectorLock node and since it is the only one trying to create the node, it will succeed in creating the ActiveStandbyElectorLock znode and will become the active resource manager. Fencing(\u5251\u51fb) A RAM failure is not recoverable but assume resource manager 1 experienced a connectivity issue with ZooKeeper for a brief moment. This will cause ZooKeeper to delete the ActiveStandbyElectorLock ephemeral znode, because for ZooKeeper it lost the session with resource manger 1 \u2013 it doesn\u2019t matter few seconds or minutes. As soon the session is lost, ZooKeeper will delete the ActiveStandbyElectorLock ephemeral znode. But in this case resource manager 1 is still active at least it thinks it is active. By this time resource manger 2 becomes active. We don\u2019t want 2 resource managers thinking or acting as active at the same time, this will cause lot of issues for the applications running in the cluster and it is referred to as split brain scenario. So before resource manager 2 becomes active it will read ActiveBreadCrumb znode to find who was the active node, in our case it will be resource manager 1 and will attempt to kill the resource manager 1 process, this process is referred to as fencing. This way we are certain that there is only one active resource manager at time. After the fencing attempt, resource manager 2 will write it\u2019s information to the ActiveBreadCrumb znode since it is now the active resource manager. This is how ZooKeeper is used in leader election and failover process with YARN or resource manager high availability. This same concept is applied to achieve NameNode high availability as well. So when you have multiple process or application trying to perform the same action but you need coordination between the applications so they don\u2019t step on each other \u201ctoes\u201d you can use ZooKeeper as a coordinator for the applications. cnblogs \u57fa\u4e8ezookeeper\u7684\u4e3b\u5907\u5207\u6362\u65b9\u6cd5 \u7ee7\u627f CZookeeperHelper \u5373\u53ef\u5feb\u901f\u5b9e\u73b0\u4e3b\u5907\u5207\u6362\uff1a https://github.com/eyjian/libmooon/blob/master/include/mooon/net/zookeeper_helper.h zookeeper\u7684 ZOO_EPHEMERAL \u8282\u70b9\uff08\u5982\u679c ZOO_EPHEMERAL \u6ee1\u8db3\u4e0d\u4e86\u9700\u6c42\uff0c\u53ef\u4ee5\u8003\u8651\u548c ZOO_SEQUENCE \u7ed3\u5408\u4f7f\u7528\uff09\uff0c\u5728\u4f1a\u8bdd\u5173\u95ed\u6216\u8fc7\u671f\u65f6\uff0c\u4f1a\u81ea\u52a8\u5220\u9664\uff0c\u5229\u7528\u8fd9\u4e00**\u7279\u6027**\u53ef\u4ee5\u5b9e\u73b0\u4e24\u4e2a\u6216\u591a\u8282\u70b9\u95f4\u7684\u4e3b\u5907\u5207\u6362\u3002 \u5b9e\u73b0\u65b9\u6cd5 1\uff09\u5728\u8fdb\u7a0b\u542f\u52a8\u65f6\u8c03\u7528 zookeeper_init() \u521d\u59cb\u5316\uff1a bool X :: init_zookeeper () { // \u7b2c\u4e00\u6b21\u8c03\u7528\u65f6_clientid\u603b\u662f\u4e3aNULL\uff0c // \u72b6\u6001\u4e3aZOO_EXPIRED_SESSION_STATE\u65f6\uff0c\u9700\u8981\u91cd\u65b0\u8c03\u7528zookeeper_init\uff0c // \u8fd9\u4e2a\u65f6\u5019\u53ef\u4f20\u5165\u7684_clientid\u4e3a\u524d\u4e00\u6b21zookeeper_init()\u4ea7\u751f\u7684_clientid // \u8bf7\u6ce8\u610fzookeeper_init()\u662f\u4e00\u4e2a\u5f02\u6b65\u8c03\u7528\uff0c\u8fd4\u56de\u975eNULL\u5e76\u4e0d\u8868\u793a\u4f1a\u8bdd\u5efa\u7acb\u6210\u529f\uff0c // \u53ea\u6709\u5f53zk_watcher\u4e2d\u7684type\u4e3aZOO_SESSION_EVENT\u548cstate\u4e3aZOO_CONNECTED_STATE\u65f6\uff0c // \u624d\u771f\u6b63\u8868\u793a\u4f1a\u8bdd\u5efa\u7acb\u6210\u529f\u3002 _zhandle = zookeeper_init ( zk_hosts , zk_watcher , 5000 , _clientid , this , 0 ); if ( NULL == _zhandle ) { MYLOG_ERROR ( \"init zookeeper failed: %s \\n \" , zerror ( errno )); return false ; } MYLOG_INFO ( \"init zookeeper(%s) successfully \\n \" , zk_hosts ); return true ; } 2\uff09\u8fdb\u5165\u5de5\u4f5c\u4e4b\u524d\uff0c\u5148\u5c1d\u8bd5\u5207\u6362\u6210\u4e3b\uff0c\u53ea\u6709\u6210\u529f\u5207\u6362\u6210\u4e3b\u540e\u624d\u8fdb\u5165work bool X :: run () { while ( true ) { int num_items = 0 ; // \u5907\u673a\u6700\u7b80\u5355\u7684\u65b9\u6cd5\u662f\u6bcf\u9694\u4e00\u5b9a\u65f6\u95f4\uff0c\u59821\u79d2\u5c31\u5c1d\u8bd5\u8f6c\u6210master\uff0c // \u5982\u679c\u4e0d\u4f7f\u7528\u8f6e\u8be2\uff0c\u5219\u53ef\u4ee5\u91c7\u7528\u76d1\u89c6_zk_path\u7684\u65b9\u5f0f mooon :: sys :: CUtils :: millisleep ( 1000 ); // \u5982\u679c\u4e0d\u662fmaster\uff0c\u5219\u5c1d\u8bd5\u8f6c\u6210master\uff0c\u5982\u679c\u8f6c\u6210\u4e0d\u6210\u529f\u5219\u7ee7\u7eed\u4e0b\u4e00\u6b21\u5c1d\u8bd5 if ( ! is_master () && ! change_to_master ()) continue ; do_work (); } } bool X :: is_master () const { return _is_master ; } bool X :: change_to_master () { static uint64_t log_counter = 0 ; // \u6253log\u8ba1\u6570\u5668\uff0c\u5907\u72b6\u6001\u65f6\u7684\u65e5\u5fd7\u8f93\u51fa // ZOO_EPHEMERAL|ZOO_SEQUENCE // _myip\u4e3a\u672c\u5730IP\u5730\u5740\uff0c\u53ef\u4ee5\u901a\u8fc7\u5b83\u6765\u5224\u65ad\u5f53\u524d\u8c01\u662fmaster // _zk_path\u503c\u793a\u4f8b\uff1a/master/test\uff0c\u6ce8\u610f\u9700\u8981\u5148\u4fdd\u8bc1/master\u5df2\u5b58\u5728 int errcode = zoo_create ( _zhandle , _zk_path . c_str (), _myip . c_str (), _myip . size () + 1 , & ZOO_OPEN_ACL_UNSAFE , ZOO_EPHEMERAL , NULL , 0 ); // (-4)connection loss\uff0c\u6bd4\u5982\u4e3azookeeper_init()\u6307\u5b9a\u4e86\u65e0\u6548\u7684hosts\uff08\u4e00\u4e2a\u6709\u6548\u7684host\u4e5f\u6ca1\u6709\uff09 if ( errcode != ZOK ) { _is_master = false ; // \u51cf\u5c11\u4e3a\u5907\u72b6\u6001\u65f6\u7684\u65e5\u5fd7\u8f93\u51fa if ( 0 == log_counter ++ % 600 ) { MYLOG_DEBUG ( \"become master[%s] failed: (%d)%s \\n \" , _zk_path . c_str (), errcode , zerror ( errcode )); } return false ; } else { _is_master = true ; log_counter = 0 ; MYLOG_INFO ( \"becase master[%s] \\n \" , _zk_path . c_str ()); // sleep\u4e00\u4e0b\uff0c\u4ee5\u4fbf\u8ba9\u539fmaster\u6b63\u5728\u8fdb\u884c\u7684\u5b8c\u6210 mooon :: sys :: CUtils :: millisleep ( 2000 ); return true ; } } 3\uff09\u5f53zookeeper\u4f1a\u8bdd\u6210\u529f\u5efa\u7acb\u6216\u8fc7\u671f\u65f6\u5747\u4f1a\u89e6\u53d1 zk_watcher \uff0c\u53ef\u901a\u8fc7 type \u548c state \u6765\u533a\u5206 void zk_watcher ( zhandle_t * zh , int type , int state , const char * path , void * context ) { X * x = static_cast < X *> ( context ); MYLOG_DEBUG ( \"zh=%p, type=%d, state=%d, context=%p, path=%s \\n \" , zh , type , state , context , path ); // zookeeper_init\u6210\u529f\u65f6type\u4e3aZOO_SESSION_EVENT\uff0cstate\u4e3aZOO_CONNECTED_STATE if (( ZOO_SESSION_EVENT == type ) && ( ZOO_CONNECTED_STATE == state )) { x -> on_zookeeper_connected ( path ); } else if (( ZOO_SESSION_EVENT == type ) && ( ZOO_EXPIRED_SESSION_STATE == state )) { // \u9700\u8981\u91cd\u65b0\u8c03\u7528zookeeper_init()\uff0c\u7b80\u5355\u70b9\u53ef\u4ee5\u9000\u51fa\u5f53\u524d\u8fdb\u7a0b\u91cd\u542f x -> on_zookeeper_expired (); } } Implementation github Automatic Redis Failover NOTE: Ruby\u8bed\u8a00\u5b9e\u73b0\u7684 This gem (built using ZK ) attempts to address these failover scenarios. One or more Node Manager daemons run as background processes and monitor all of your configured master/slave nodes. When the daemon starts up, it automatically discovers the current master/slaves . Background watchers are setup for each of the redis nodes . As soon as a node is detected as being offline, it will be moved to an \"unavailable\" state. If the node that went offline was the master, then one of the slaves will be promoted as the new master. All existing slaves will be automatically reconfigured to point to the new master for replication. All nodes marked as unavailable will be periodically checked to see if they have been brought back online. If so, the newly available nodes will be configured as slaves and brought back into the list of available nodes. Note that detection of a node going down should be nearly instantaneous, since the mechanism used to keep tabs on a node is via a blocking Redis BLPOP call (no polling). This call fails nearly immediately when the node actually goes offline. To avoid false positives (i.e., intermittent flaky network interruption), the Node Manager will only mark a node as unavailable if it fails to communicate with it 3 times (this is configurable via --max-failures, see configuration options below). Note that you can (and should) deploy multiple Node Manager daemons since they each report periodic health reports/snapshots of the redis servers. A \"node strategy\" is used to determine if a node is actually unavailable. By default a majority strategy is used, but you can also configure \"consensus\" or \"single\" as well. github maweina / zookeeper-leader-election NOTE: Java\u8bed\u8a00\u5b9e\u73b0\u7684 github tgockel / zookeeper-cpp NOTE: C++\u8bed\u8a00\u5b9e\u73b0\u7684 TODO cnblogs zookeeper\u4e3b\u5907\u5207\u6362\u5b66\u4e60","title":"Introduction"},{"location":"software-ZooKeeper/Developers/Application/Lead-election/#leader#election","text":"\u672c\u7ae0\u8ba8\u8bbazookeeper\u5b9e\u73b0Leader Election\uff0c\u540c\u65f6\u4e5f\u80fd\u591f\u5b9e\u73b0automatic failover\u3002","title":"Leader Election"},{"location":"software-ZooKeeper/Developers/Application/Lead-election/#zookeeper#a#guide#to#creating#higher-level#constructs#with#zookeeperleader#election","text":"A simple way of doing leader election with ZooKeeper is to use the SEQUENCE|EPHEMERAL flags when creating znodes that represent \"proposals\" of clients. The idea is to have a znode, say \" /election \", such that each znode creates a child znode \" /election/guid-n_ \" with both flags SEQUENCE|EPHEMERAL . With the sequence flag, ZooKeeper automatically appends a sequence number that is greater than any one previously appended to a child of \" /election \". The proces(\u8fdb\u7a0b) that created the znode with the smallest appended sequence number is the leader. That's not all, though. It is important to watch for failures of the leader , so that a new client arises as the new leader in the case the current leader fails. A trivial solution is to have all application processes watching upon the current smallest znode , and checking if they are the new leader when the smallest znode goes away (note that the smallest znode will go away if the leader fails because the node is ephemeral ). But this causes a herd effect (\u7f8a\u7fa4\u6548\u5e94): upon a failure of the current leader, all other processes receive a notification, and execute getChildren on \" /election \" to obtain the current list of children of \" /election \". If the number of clients is large, it causes a spike(\u6bdb\u523a) on the number of operations that ZooKeeper servers have to process. To avoid the herd effect , it is sufficient to watch for the next znode down on the sequence of znodes. If a client receives a notification that the znode it is watching is gone, then it becomes the new leader in the case that there is no smaller znode. Note that this avoids the herd effect by not having all clients watching the same znode. Here's the pseudo code: Let ELECTION be a path of choice of the application. To volunteer to be a leader: Create znode z with path \" ELECTION/guid-n_ \" with both SEQUENCE and EPHEMERAL flags; Let C be the children of \" ELECTION \", and i be the sequence number of z ; Watch for changes on \" ELECTION/guid-n_j \", where j is the largest sequence number such that j < i and n_j is a znode in C; Upon receiving a notification of znode deletion: Let C be the new set of children of ELECTION ; If z is the smallest node in C , then execute leader procedure; Otherwise, watch for changes on \" ELECTION/guid-n_j \", where j is the largest sequence number such that j < i and n_j is a znode in C; Notes: Note that the znode having no preceding znode on the list of children do not imply that the creator of this znode is aware that it is the current leader. Applications may consider creating a separate znode to acknowledge that the leader has executed the leader procedure. See the note for Locks on how to use the guid in the node.","title":"zookeeper A Guide to Creating Higher-level Constructs with ZooKeeper#Leader Election"},{"location":"software-ZooKeeper/Developers/Application/Lead-election/#hadoopinrealworld#what#is#zookeeper#and#its#use#case","text":"","title":"hadoopinrealworld What is ZooKeeper and it\u2019s Use Case"},{"location":"software-ZooKeeper/Developers/Application/Lead-election/#leader#election_1","text":"Leader election is one of the common use case for ZooKeeper. Let\u2019s see how it works. Let\u2019s say we have 2 resource managers. Each resource manager will participate in the leader election process to decide who becomes the active resource manager. Resource managers use ZooKeeper to elect a leader among themselves. Let\u2019s see how it works. Both resource mangers will attempt to create a znode named ActiveStandbyElectorLock in ZooKeeper and only one resource manager will be able to create ActiveStandbyElectorLock znode. Whoever creates this znode first will become the active resource manager . So if resource manger 1 creates ActiveStandbyElectorLock first, it will become the active node and if resource manger 2 creates ActiveStandbyElectorLock , it will be become the active node. Let\u2019s say resource manger 1 created the ActiveStandbyElectorLock znode first, resource manager 2 will also attempt to create ActiveStandbyElectorLock but it won\u2019t be able to because ActiveStandbyElectorLock is already created by resource manager 1 so resource manager 1 becomes the active resource manager and resource manager 2 becomes the stand by node and goes in to stand by mode. So far clear? So we have now elected a leader. Resource manager 1 is the active resource manager now.","title":"Leader Election"},{"location":"software-ZooKeeper/Developers/Application/Lead-election/#failover","text":"With Resource Manager 1 as the active node, everything is working good, up until one fine morning when the RAM in resource manager 1 has gone bad. So resource manager 1 went down. Now our expectation is resource manager 2 should become the active node automatically. Let\u2019s now understand how failover from active node to standby node works. With ZooKeeper, we can add a watch to any node, watchers are extremely helpful because if we are watching a znode and when that znode gets updated or deleted, whoever is watching the znode will get notified. Remember, whoever creates the ActiveStandbyElectorLock ephemeral znode first becomes the active resoure manager. In our case resource manager 1 beat resource manger 2 by creating ActiveStandbyElectorLock first. When resource manager 2 tried to create ActiveStandbyElectorLock znode, it couldn\u2019t because it was already created. So resource manager 2 realized that there is already an active resource manager elected and it became the stand by node . Before resource manager 2 becomes stand by, it will add a watch on the ActiveStandbyElectorLock znode. When resource manager 1 goes down due to the RAM failure, it\u2019s session with ZooKeeper will become dead and since ActiveStandbyElectorLock is an ephemeral node, ZooKeeper will then delete the ActiveStandbyElectorLock node as soon the application who created the znode loses connection with ZooKeeper. Since resource manger 2 is watching the ActiveStandbyElectorLock node, it will get a notification stating ActiveStandbyElectorLock is gone. Resource Manager 2 will now attempt to create ActiveStandbyElectorLock node and since it is the only one trying to create the node, it will succeed in creating the ActiveStandbyElectorLock znode and will become the active resource manager.","title":"Failover"},{"location":"software-ZooKeeper/Developers/Application/Lead-election/#fencing","text":"A RAM failure is not recoverable but assume resource manager 1 experienced a connectivity issue with ZooKeeper for a brief moment. This will cause ZooKeeper to delete the ActiveStandbyElectorLock ephemeral znode, because for ZooKeeper it lost the session with resource manger 1 \u2013 it doesn\u2019t matter few seconds or minutes. As soon the session is lost, ZooKeeper will delete the ActiveStandbyElectorLock ephemeral znode. But in this case resource manager 1 is still active at least it thinks it is active. By this time resource manger 2 becomes active. We don\u2019t want 2 resource managers thinking or acting as active at the same time, this will cause lot of issues for the applications running in the cluster and it is referred to as split brain scenario. So before resource manager 2 becomes active it will read ActiveBreadCrumb znode to find who was the active node, in our case it will be resource manager 1 and will attempt to kill the resource manager 1 process, this process is referred to as fencing. This way we are certain that there is only one active resource manager at time. After the fencing attempt, resource manager 2 will write it\u2019s information to the ActiveBreadCrumb znode since it is now the active resource manager. This is how ZooKeeper is used in leader election and failover process with YARN or resource manager high availability. This same concept is applied to achieve NameNode high availability as well. So when you have multiple process or application trying to perform the same action but you need coordination between the applications so they don\u2019t step on each other \u201ctoes\u201d you can use ZooKeeper as a coordinator for the applications.","title":"Fencing(\u5251\u51fb)"},{"location":"software-ZooKeeper/Developers/Application/Lead-election/#cnblogs#zookeeper","text":"\u7ee7\u627f CZookeeperHelper \u5373\u53ef\u5feb\u901f\u5b9e\u73b0\u4e3b\u5907\u5207\u6362\uff1a https://github.com/eyjian/libmooon/blob/master/include/mooon/net/zookeeper_helper.h zookeeper\u7684 ZOO_EPHEMERAL \u8282\u70b9\uff08\u5982\u679c ZOO_EPHEMERAL \u6ee1\u8db3\u4e0d\u4e86\u9700\u6c42\uff0c\u53ef\u4ee5\u8003\u8651\u548c ZOO_SEQUENCE \u7ed3\u5408\u4f7f\u7528\uff09\uff0c\u5728\u4f1a\u8bdd\u5173\u95ed\u6216\u8fc7\u671f\u65f6\uff0c\u4f1a\u81ea\u52a8\u5220\u9664\uff0c\u5229\u7528\u8fd9\u4e00**\u7279\u6027**\u53ef\u4ee5\u5b9e\u73b0\u4e24\u4e2a\u6216\u591a\u8282\u70b9\u95f4\u7684\u4e3b\u5907\u5207\u6362\u3002","title":"cnblogs \u57fa\u4e8ezookeeper\u7684\u4e3b\u5907\u5207\u6362\u65b9\u6cd5"},{"location":"software-ZooKeeper/Developers/Application/Lead-election/#_1","text":"1\uff09\u5728\u8fdb\u7a0b\u542f\u52a8\u65f6\u8c03\u7528 zookeeper_init() \u521d\u59cb\u5316\uff1a bool X :: init_zookeeper () { // \u7b2c\u4e00\u6b21\u8c03\u7528\u65f6_clientid\u603b\u662f\u4e3aNULL\uff0c // \u72b6\u6001\u4e3aZOO_EXPIRED_SESSION_STATE\u65f6\uff0c\u9700\u8981\u91cd\u65b0\u8c03\u7528zookeeper_init\uff0c // \u8fd9\u4e2a\u65f6\u5019\u53ef\u4f20\u5165\u7684_clientid\u4e3a\u524d\u4e00\u6b21zookeeper_init()\u4ea7\u751f\u7684_clientid // \u8bf7\u6ce8\u610fzookeeper_init()\u662f\u4e00\u4e2a\u5f02\u6b65\u8c03\u7528\uff0c\u8fd4\u56de\u975eNULL\u5e76\u4e0d\u8868\u793a\u4f1a\u8bdd\u5efa\u7acb\u6210\u529f\uff0c // \u53ea\u6709\u5f53zk_watcher\u4e2d\u7684type\u4e3aZOO_SESSION_EVENT\u548cstate\u4e3aZOO_CONNECTED_STATE\u65f6\uff0c // \u624d\u771f\u6b63\u8868\u793a\u4f1a\u8bdd\u5efa\u7acb\u6210\u529f\u3002 _zhandle = zookeeper_init ( zk_hosts , zk_watcher , 5000 , _clientid , this , 0 ); if ( NULL == _zhandle ) { MYLOG_ERROR ( \"init zookeeper failed: %s \\n \" , zerror ( errno )); return false ; } MYLOG_INFO ( \"init zookeeper(%s) successfully \\n \" , zk_hosts ); return true ; } 2\uff09\u8fdb\u5165\u5de5\u4f5c\u4e4b\u524d\uff0c\u5148\u5c1d\u8bd5\u5207\u6362\u6210\u4e3b\uff0c\u53ea\u6709\u6210\u529f\u5207\u6362\u6210\u4e3b\u540e\u624d\u8fdb\u5165work bool X :: run () { while ( true ) { int num_items = 0 ; // \u5907\u673a\u6700\u7b80\u5355\u7684\u65b9\u6cd5\u662f\u6bcf\u9694\u4e00\u5b9a\u65f6\u95f4\uff0c\u59821\u79d2\u5c31\u5c1d\u8bd5\u8f6c\u6210master\uff0c // \u5982\u679c\u4e0d\u4f7f\u7528\u8f6e\u8be2\uff0c\u5219\u53ef\u4ee5\u91c7\u7528\u76d1\u89c6_zk_path\u7684\u65b9\u5f0f mooon :: sys :: CUtils :: millisleep ( 1000 ); // \u5982\u679c\u4e0d\u662fmaster\uff0c\u5219\u5c1d\u8bd5\u8f6c\u6210master\uff0c\u5982\u679c\u8f6c\u6210\u4e0d\u6210\u529f\u5219\u7ee7\u7eed\u4e0b\u4e00\u6b21\u5c1d\u8bd5 if ( ! is_master () && ! change_to_master ()) continue ; do_work (); } } bool X :: is_master () const { return _is_master ; } bool X :: change_to_master () { static uint64_t log_counter = 0 ; // \u6253log\u8ba1\u6570\u5668\uff0c\u5907\u72b6\u6001\u65f6\u7684\u65e5\u5fd7\u8f93\u51fa // ZOO_EPHEMERAL|ZOO_SEQUENCE // _myip\u4e3a\u672c\u5730IP\u5730\u5740\uff0c\u53ef\u4ee5\u901a\u8fc7\u5b83\u6765\u5224\u65ad\u5f53\u524d\u8c01\u662fmaster // _zk_path\u503c\u793a\u4f8b\uff1a/master/test\uff0c\u6ce8\u610f\u9700\u8981\u5148\u4fdd\u8bc1/master\u5df2\u5b58\u5728 int errcode = zoo_create ( _zhandle , _zk_path . c_str (), _myip . c_str (), _myip . size () + 1 , & ZOO_OPEN_ACL_UNSAFE , ZOO_EPHEMERAL , NULL , 0 ); // (-4)connection loss\uff0c\u6bd4\u5982\u4e3azookeeper_init()\u6307\u5b9a\u4e86\u65e0\u6548\u7684hosts\uff08\u4e00\u4e2a\u6709\u6548\u7684host\u4e5f\u6ca1\u6709\uff09 if ( errcode != ZOK ) { _is_master = false ; // \u51cf\u5c11\u4e3a\u5907\u72b6\u6001\u65f6\u7684\u65e5\u5fd7\u8f93\u51fa if ( 0 == log_counter ++ % 600 ) { MYLOG_DEBUG ( \"become master[%s] failed: (%d)%s \\n \" , _zk_path . c_str (), errcode , zerror ( errcode )); } return false ; } else { _is_master = true ; log_counter = 0 ; MYLOG_INFO ( \"becase master[%s] \\n \" , _zk_path . c_str ()); // sleep\u4e00\u4e0b\uff0c\u4ee5\u4fbf\u8ba9\u539fmaster\u6b63\u5728\u8fdb\u884c\u7684\u5b8c\u6210 mooon :: sys :: CUtils :: millisleep ( 2000 ); return true ; } } 3\uff09\u5f53zookeeper\u4f1a\u8bdd\u6210\u529f\u5efa\u7acb\u6216\u8fc7\u671f\u65f6\u5747\u4f1a\u89e6\u53d1 zk_watcher \uff0c\u53ef\u901a\u8fc7 type \u548c state \u6765\u533a\u5206 void zk_watcher ( zhandle_t * zh , int type , int state , const char * path , void * context ) { X * x = static_cast < X *> ( context ); MYLOG_DEBUG ( \"zh=%p, type=%d, state=%d, context=%p, path=%s \\n \" , zh , type , state , context , path ); // zookeeper_init\u6210\u529f\u65f6type\u4e3aZOO_SESSION_EVENT\uff0cstate\u4e3aZOO_CONNECTED_STATE if (( ZOO_SESSION_EVENT == type ) && ( ZOO_CONNECTED_STATE == state )) { x -> on_zookeeper_connected ( path ); } else if (( ZOO_SESSION_EVENT == type ) && ( ZOO_EXPIRED_SESSION_STATE == state )) { // \u9700\u8981\u91cd\u65b0\u8c03\u7528zookeeper_init()\uff0c\u7b80\u5355\u70b9\u53ef\u4ee5\u9000\u51fa\u5f53\u524d\u8fdb\u7a0b\u91cd\u542f x -> on_zookeeper_expired (); } }","title":"\u5b9e\u73b0\u65b9\u6cd5"},{"location":"software-ZooKeeper/Developers/Application/Lead-election/#implementation","text":"","title":"Implementation"},{"location":"software-ZooKeeper/Developers/Application/Lead-election/#github#automatic#redis#failover","text":"NOTE: Ruby\u8bed\u8a00\u5b9e\u73b0\u7684 This gem (built using ZK ) attempts to address these failover scenarios. One or more Node Manager daemons run as background processes and monitor all of your configured master/slave nodes. When the daemon starts up, it automatically discovers the current master/slaves . Background watchers are setup for each of the redis nodes . As soon as a node is detected as being offline, it will be moved to an \"unavailable\" state. If the node that went offline was the master, then one of the slaves will be promoted as the new master. All existing slaves will be automatically reconfigured to point to the new master for replication. All nodes marked as unavailable will be periodically checked to see if they have been brought back online. If so, the newly available nodes will be configured as slaves and brought back into the list of available nodes. Note that detection of a node going down should be nearly instantaneous, since the mechanism used to keep tabs on a node is via a blocking Redis BLPOP call (no polling). This call fails nearly immediately when the node actually goes offline. To avoid false positives (i.e., intermittent flaky network interruption), the Node Manager will only mark a node as unavailable if it fails to communicate with it 3 times (this is configurable via --max-failures, see configuration options below). Note that you can (and should) deploy multiple Node Manager daemons since they each report periodic health reports/snapshots of the redis servers. A \"node strategy\" is used to determine if a node is actually unavailable. By default a majority strategy is used, but you can also configure \"consensus\" or \"single\" as well.","title":"github Automatic Redis Failover"},{"location":"software-ZooKeeper/Developers/Application/Lead-election/#github#maweinazookeeper-leader-election","text":"NOTE: Java\u8bed\u8a00\u5b9e\u73b0\u7684","title":"github maweina/zookeeper-leader-election"},{"location":"software-ZooKeeper/Developers/Application/Lead-election/#github#tgockelzookeeper-cpp","text":"NOTE: C++\u8bed\u8a00\u5b9e\u73b0\u7684","title":"github tgockel/zookeeper-cpp"},{"location":"software-ZooKeeper/Developers/Application/Lead-election/#todo","text":"cnblogs zookeeper\u4e3b\u5907\u5207\u6362\u5b66\u4e60","title":"TODO"},{"location":"software-ZooKeeper/Developers/Application/Lead-election/Automatic-failover-using-ZK/","text":"Automatic failover Automatic failover\u662f\u5b9e\u73b0HA\u7684\u5173\u952e\uff0c\u672c\u6587\u603b\u7ed3\u5982\u4f55\u4f7f\u7528ZK\u6765\u5b9e\u73b0automatic failover\uff0c\u4e0d\u540c\u7684architecture \u5b9e\u73b0\u7684\u65b9\u5f0f\u6709\u6240\u4e0d\u540c\uff0c\u6240\u4ee5\u672c\u6587\u5206\u7c7b\u8ba8\u8bba\u3002 Master/slave architecture automatic failover \u57fa\u4e8emaster/slave\u67b6\u6784\u7684automatic failover\uff0c\u6838\u5fc3\u662f\u5982\u4f55\u8fdb\u884cleader selection\uff0c\u5728 ./Lead-election \u7ae0\u8282\u8fdb\u884c\u4e86\u4ecb\u7ecd\u3002 TODO stackoverflow zookeeper failover for kafka cluster","title":"Automatic-failover-using-ZK"},{"location":"software-ZooKeeper/Developers/Application/Lead-election/Automatic-failover-using-ZK/#automatic#failover","text":"Automatic failover\u662f\u5b9e\u73b0HA\u7684\u5173\u952e\uff0c\u672c\u6587\u603b\u7ed3\u5982\u4f55\u4f7f\u7528ZK\u6765\u5b9e\u73b0automatic failover\uff0c\u4e0d\u540c\u7684architecture \u5b9e\u73b0\u7684\u65b9\u5f0f\u6709\u6240\u4e0d\u540c\uff0c\u6240\u4ee5\u672c\u6587\u5206\u7c7b\u8ba8\u8bba\u3002","title":"Automatic failover"},{"location":"software-ZooKeeper/Developers/Application/Lead-election/Automatic-failover-using-ZK/#masterslave#architecture#automatic#failover","text":"\u57fa\u4e8emaster/slave\u67b6\u6784\u7684automatic failover\uff0c\u6838\u5fc3\u662f\u5982\u4f55\u8fdb\u884cleader selection\uff0c\u5728 ./Lead-election \u7ae0\u8282\u8fdb\u884c\u4e86\u4ecb\u7ecd\u3002","title":"Master/slave architecture automatic failover"},{"location":"software-ZooKeeper/Developers/Application/Lead-election/Automatic-failover-using-ZK/#todo","text":"stackoverflow zookeeper failover for kafka cluster","title":"TODO"},{"location":"software-ZooKeeper/Developers/Application/Lead-election/Herd-Effect-in-ZK/","text":"Herd Effect in ZK \u5728\u5982\u4e0b\u6587\u7ae0\u4e2d\u5bf9\u6b64\u8fdb\u884c\u4e86\u8bf4\u660e: 1) zookeeper A Guide to Creating Higher-level Constructs with ZooKeeper 2) developer.aliyun \u3010ZooKeeper Notes 16\u3011\u907f\u514d\u7f8a\u7fa4\u6548\u5e94\uff08Herd Effect\uff09","title":"Introduction"},{"location":"software-ZooKeeper/Developers/Application/Lead-election/Herd-Effect-in-ZK/#herd#effect#in#zk","text":"\u5728\u5982\u4e0b\u6587\u7ae0\u4e2d\u5bf9\u6b64\u8fdb\u884c\u4e86\u8bf4\u660e: 1) zookeeper A Guide to Creating Higher-level Constructs with ZooKeeper 2) developer.aliyun \u3010ZooKeeper Notes 16\u3011\u907f\u514d\u7f8a\u7fa4\u6548\u5e94\uff08Herd Effect\uff09","title":"Herd Effect in ZK"},{"location":"software-ZooKeeper/Developers/Application/Lock/","text":"Lock zookeeper A Guide to Creating Higher-level Constructs with ZooKeeper#Locks zhuanlan.zhihu \u5206\u5e03\u5f0f\u9501\u7528 Redis \u8fd8\u662f Zookeeper zhuanlan.zhihu \u5206\u5e03\u5f0f\u9501\u7528 Redis \u8fd8\u662f Zookeeper\uff1f","title":"Introduction"},{"location":"software-ZooKeeper/Developers/Application/Lock/#lock","text":"","title":"Lock"},{"location":"software-ZooKeeper/Developers/Application/Lock/#zookeeper#a#guide#to#creating#higher-level#constructs#with#zookeeperlocks","text":"","title":"zookeeper A Guide to Creating Higher-level Constructs with ZooKeeper#Locks"},{"location":"software-ZooKeeper/Developers/Application/Lock/#zhuanlanzhihu#redis#zookeeper","text":"","title":"zhuanlan.zhihu \u5206\u5e03\u5f0f\u9501\u7528 Redis \u8fd8\u662f Zookeeper"},{"location":"software-ZooKeeper/Developers/Application/Lock/#zhuanlanzhihu#redis#zookeeper_1","text":"","title":"zhuanlan.zhihu \u5206\u5e03\u5f0f\u9501\u7528 Redis \u8fd8\u662f Zookeeper\uff1f"},{"location":"software-ZooKeeper/Developers/ZK-client/","text":"ZK client \u5f88\u591aZK client\u90fd\u5df2\u7ecf\u5c06ZK\u7684\u5404\u79cdapplication\u7ed9\u5c01\u88c5\u597d\u4e86\uff0c\u4f7f\u7528\u8fd9\u4e9bclient\uff0c\u6211\u4eec\u53ef\u4ee5\u5feb\u901f\u5730\u4f7f\u7528\u54cd\u5e94\u7684\u529f\u80fd\u3002 C++ github ZooKeeper C++ \u8f83\u597d github libzookeeper_master \u4e00\u822c C \u5b98\u65b9\u63d0\u4f9b\u4e86C client\u3002 Python \u5b98\u65b9\u63d0\u4f9b\u4e86Python client\u3002 Java Apache Curator","title":"Introduction"},{"location":"software-ZooKeeper/Developers/ZK-client/#zk#client","text":"\u5f88\u591aZK client\u90fd\u5df2\u7ecf\u5c06ZK\u7684\u5404\u79cdapplication\u7ed9\u5c01\u88c5\u597d\u4e86\uff0c\u4f7f\u7528\u8fd9\u4e9bclient\uff0c\u6211\u4eec\u53ef\u4ee5\u5feb\u901f\u5730\u4f7f\u7528\u54cd\u5e94\u7684\u529f\u80fd\u3002","title":"ZK client"},{"location":"software-ZooKeeper/Developers/ZK-client/#c","text":"github ZooKeeper C++ \u8f83\u597d github libzookeeper_master \u4e00\u822c","title":"C++"},{"location":"software-ZooKeeper/Developers/ZK-client/#c_1","text":"\u5b98\u65b9\u63d0\u4f9b\u4e86C client\u3002","title":"C"},{"location":"software-ZooKeeper/Developers/ZK-client/#python","text":"\u5b98\u65b9\u63d0\u4f9b\u4e86Python client\u3002","title":"Python"},{"location":"software-ZooKeeper/Developers/ZK-client/#java","text":"Apache Curator","title":"Java"},{"location":"software-ZooKeeper/Developers/ZK-client/Java-Apache-Curator/","text":"Apache Curator Welcome to Apache Curator Apache Curator is a Java/JVM client library for Apache ZooKeeper , a distributed coordination service. It includes a highlevel API framework and utilities to make using Apache ZooKeeper much easier and more reliable. It also includes recipes for common use cases and extensions such as service discovery and a Java 8 asynchronous DSL . NOTE: \u5173\u4e8easynchronous DSL\uff0c\u53c2\u89c1 Programming-model\\Asynchronous-programming \u3002","title":"Introduction"},{"location":"software-ZooKeeper/Developers/ZK-client/Java-Apache-Curator/#apache#curator","text":"","title":"Apache Curator"},{"location":"software-ZooKeeper/Developers/ZK-client/Java-Apache-Curator/#welcome#to#apache#curator","text":"Apache Curator is a Java/JVM client library for Apache ZooKeeper , a distributed coordination service. It includes a highlevel API framework and utilities to make using Apache ZooKeeper much easier and more reliable. It also includes recipes for common use cases and extensions such as service discovery and a Java 8 asynchronous DSL . NOTE: \u5173\u4e8easynchronous DSL\uff0c\u53c2\u89c1 Programming-model\\Asynchronous-programming \u3002","title":"Welcome to Apache Curator"},{"location":"software-ZooKeeper/Developers/ZK-client/Zookeeper-cpp/","text":"ZooKeeper C++ github tgockel / zookeeper-cpp","title":"Introduction"},{"location":"software-ZooKeeper/Developers/ZK-client/Zookeeper-cpp/#zookeeper#c","text":"","title":"ZooKeeper C++"},{"location":"software-ZooKeeper/Developers/ZK-client/Zookeeper-cpp/#github#tgockelzookeeper-cpp","text":"","title":"github tgockel/zookeeper-cpp"},{"location":"software-ZooKeeper/Developers/ZK-client/Zookeeper-cpp/Read-code/","text":"","title":"Introduction"},{"location":"software-ZooKeeper/Overview/","text":"ZooKeeper: A Distributed Coordination Service for Distributed Applications Design Goals ZooKeeper is simple. ZooKeeper allows distributed processes to coordinate with each other through a shared hierarchical namespace which is organized similarly to a standard file system. NOTE: \u8fd9\u662fZooKeeper\u7684simple\u7279\u6027\u7684\u5185\u6db5: \u4f7f\u7528\u5bb9\u6613\u7406\u89e3\u7684data model\u6765\u5b9e\u73b0distributed processes\u7684coordination\u3002data model\u5728\u540e\u9762\u7684\u7ae0\u8282\u4f1a\u8fdb\u884c\u63cf\u8ff0\u3002 The reliability aspects keep it from being a single point of failure. The strict ordering means that sophisticated synchronization primitives can be implemented at the client. NOTE: \u7531client\u6765\u5b9e\u73b0synchronization ZooKeeper is replicated The servers that make up the ZooKeeper service must all know about each other. NOTE: \u4ece\u4e0a\u56fe\u53ef\u4ee5\u770b\u51fa\uff0c\u6bcf\u4e2aslave server\u53ea\u548cleader server\u8fdb\u884c\u8fde\u63a5\uff0c\u800cslave server\u4e4b\u95f4\u5e76\u6ca1\u6709connection\uff0c\u90a3\u8fd9\u5e76\u4e0d\u662f\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u6240\u63cf\u8ff0\u7684\"know about each other\" ZooKeeper is ordered ZooKeeper stamps(\u5b83\u7684\u542b\u4e49\u975e\u5e38\u7c7b\u4f3c\u4e8etimestamp) each update with a number that reflects the order of all ZooKeeper transactions. Subsequent operations can use the order to implement higher-level abstractions, such as synchronization primitives. NOTE: \u4e00\u3001\u8fd9\u79cd\u6280\u672f\u53eb\u4ec0\u4e48\u540d\u79f0\uff1f\u975e\u5e38\u7c7b\u4f3c\u4e8eMVCC\uff1b\u5728 ZooKeeper Programmer's Guide \u4e2d\uff0c\u5bf9\u6b64\u8fdb\u884c\u4e86\u4ecb\u7ecd\uff1b \u4e8c\u3001\u4e0a\u8ff0update\u5177\u4f53\u662f\u6307\u4ec0\u4e48\uff1f ZooKeeper is fast It is especially fast in \"read-dominant\" workloads. NOTE: \u8fd9\u8ba9\u6211\u60f3\u5230\u4e86: 1\u3001RCU Data model and the hierarchical namespace ZooKeeper's Hierarchical Namespace Nodes and ephemeral(\u77ed\u6682\u7684) nodes (ZooKeeper was designed to store coordination data : status information, configuration, location information, etc., so the data stored at each node is usually small, in the byte to kilobyte range.) NOTE: \u4e0a\u8ff0**coordination data**\uff0c\u53ef\u4ee5\u662fconsensus We use the term znode to make it clear that we are talking about ZooKeeper data nodes. Znodes maintain a stat structure that includes version numbers for data changes, ACL changes, and timestamps, to allow cache validations and coordinated updates . Each time a znode's data changes, the version number increases. For instance, whenever a client retrieves data it also receives the version of the data. NOTE: **stat structure**\u7684\u542b\u4e49\"\u7edf\u8ba1\u7ed3\u6784\"\u3002 The data stored at each znode in a namespace is read and written atomically . Reads get all the data bytes associated with a znode and a write replaces all the data. Each node has an Access Control List (ACL) that restricts who can do what. ZooKeeper also has the notion of ephemeral nodes. These znodes exists as long as the session that created the znode is active. When the session ends the znode is deleted. Conditional updates and watches ZooKeeper supports the concept of watches . Clients can set a watch on a znode. NOTE: \u548credis\u975e\u5e38\u7c7b\u4f3c Guarantees These are: 1\u3001Sequential Consistency - Updates from a client will be applied in the order that they were sent. NOTE: \u5173\u4e8eSequential Consistency \uff0c\u53c2\u89c1 Theory\\CAP\\Consistency \u7ae0\u8282 2\u3001Atomicity - Updates either succeed or fail. No partial results. 3\u3001Single System Image - A client will see the same view of the service regardless of the server that it connects to. i.e., a client will never see an older view of the system even if the client fails over to a different server with the same session. 4\u3001Reliability - Once an update has been applied, it will persist from that time forward until a client overwrites the update. NOTE: \u4e00\u65e6\u5e94\u7528\u4e86\u66f4\u65b0\uff0c\u5b83\u5c06\u4ece\u90a3\u65f6\u5f00\u59cb\u6301\u7eed\u5b58\u5728\uff0c\u76f4\u5230\u5ba2\u6237\u673a\u8986\u76d6\u8be5\u66f4\u65b0 5\u3001Timeliness - The clients view of the system is guaranteed to be up-to-date within a certain time bound. NOTE: \u53ca\u65f6\u6027 Simple API One of the design goals of ZooKeeper is providing a very simple programming interface. As a result, it supports only these operations: 1\u3001 create : creates a node at a location in the tree 2\u3001 delete : deletes a node 3\u3001 exists : tests if a node exists at a location 4\u3001 get data : reads the data from a node 5\u3001 set data : writes data to a node 6\u3001 get children : retrieves a list of children of a node 7\u3001 sync : waits for data to be propagated NOTE: \u5c31\u662f\u5bf9tree node\u7684\u64cd\u4f5c\uff0c\u57fa\u672c\u4e0a\u90fd\u662f\"\u589e\u5220\u6539\u67e5\" Implementation ZooKeeper Components shows the high-level components of the ZooKeeper service. With the exception of the request processor, each of the servers that make up the ZooKeeper service replicates its own copy of each of the components. NOTE: 1\u3001ZooKeeper \u4f7f\u7528\u4e86\u81ea\u5df1\u7684consistency protocol: ZAB The replicated database is an in-memory database containing the entire data tree. Updates are logged to disk for recoverability, and writes are serialized to disk before they are applied to the in-memory database. NOTE: \u5148\u6301\u4e45\u5316\u5230\u6587\u4ef6\uff0c\u7136\u540e\u518d\u66f4\u65b0\u5230memory\uff0c\u8fd9\u79cd\u505a\u6cd5\u63d0\u9ad8\u4e86Reliability Every ZooKeeper server services clients. Clients connect to exactly one server to submit requests. Read requests are serviced from the local replica of each server database. Requests that change the state of the service, write requests, are processed by an agreement protocol . NOTE: read request\u80fd\u591f\u76f4\u63a5\u8bfb\u53d6local replica\uff0c\u4f46\u662fwrite request\u5219\u9700\u8981\u7531agreement protocol\u6765\u5904\u7406\uff0c\u6d89\u53ca\u5230\u6240\u6709\u7684\u8282\u70b9\uff0c\u5e76\u4e14\u4ece\u4e0b\u9762\u7684\u63cf\u8ff0\u6765\u770b\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u662f\u6bd4\u8f83\u590d\u6742\u7684: As part of the agreement protocol all write requests from clients are forwarded to a single server, called the leader . The rest of the ZooKeeper servers, called followers , receive message proposals(\u5efa\u8bae\u4e66) from the leader and agree upon message delivery. The messaging layer takes care of replacing leaders on failures and syncing followers with leaders. NOTE: \u8fd9\u79cd\u534f\u8bae\u53eb\u505a\u4ec0\u4e48\uff1f\u5b83\u662f\u5426\u53ef\u9760\uff1f ZooKeeper uses a custom atomic messaging protocol . Since the messaging layer is atomic , ZooKeeper can guarantee that the local replicas never diverge(\u5206\u79bb\u3001\u4ea7\u751f\u5dee\u5f02). When the leader receives a write request , it calculates what the state of the system is when the write is to be applied and transforms this into a transaction that captures this new state. NOTE: zookeeper\u662f\u4f7f\u7528\u7684\u5355\u4e2aleader\uff0c\u8fd9\u548credis\u6709\u4ec0\u4e48\u5dee\u5f02\u5462\uff1f Performance ZooKeeper is designed to be highly performance . But is it? The results of the ZooKeeper's development team at Yahoo! Research indicate that it is. (See ZooKeeper Throughput as the Read-Write Ratio Varies .) It is especially high performance in applications where reads outnumber writes, since writes involve synchronizing the state of all servers. (Reads outnumbering writes is typically the case for a coordination service .)","title":"Introduction"},{"location":"software-ZooKeeper/Overview/#zookeeper#a#distributed#coordination#service#for#distributed#applications","text":"","title":"ZooKeeper: A Distributed Coordination Service for Distributed Applications"},{"location":"software-ZooKeeper/Overview/#design#goals","text":"","title":"Design Goals"},{"location":"software-ZooKeeper/Overview/#zookeeper#is#simple","text":"ZooKeeper allows distributed processes to coordinate with each other through a shared hierarchical namespace which is organized similarly to a standard file system. NOTE: \u8fd9\u662fZooKeeper\u7684simple\u7279\u6027\u7684\u5185\u6db5: \u4f7f\u7528\u5bb9\u6613\u7406\u89e3\u7684data model\u6765\u5b9e\u73b0distributed processes\u7684coordination\u3002data model\u5728\u540e\u9762\u7684\u7ae0\u8282\u4f1a\u8fdb\u884c\u63cf\u8ff0\u3002 The reliability aspects keep it from being a single point of failure. The strict ordering means that sophisticated synchronization primitives can be implemented at the client. NOTE: \u7531client\u6765\u5b9e\u73b0synchronization","title":"ZooKeeper is simple."},{"location":"software-ZooKeeper/Overview/#zookeeper#is#replicated","text":"The servers that make up the ZooKeeper service must all know about each other. NOTE: \u4ece\u4e0a\u56fe\u53ef\u4ee5\u770b\u51fa\uff0c\u6bcf\u4e2aslave server\u53ea\u548cleader server\u8fdb\u884c\u8fde\u63a5\uff0c\u800cslave server\u4e4b\u95f4\u5e76\u6ca1\u6709connection\uff0c\u90a3\u8fd9\u5e76\u4e0d\u662f\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u6240\u63cf\u8ff0\u7684\"know about each other\"","title":"ZooKeeper is replicated"},{"location":"software-ZooKeeper/Overview/#zookeeper#is#ordered","text":"ZooKeeper stamps(\u5b83\u7684\u542b\u4e49\u975e\u5e38\u7c7b\u4f3c\u4e8etimestamp) each update with a number that reflects the order of all ZooKeeper transactions. Subsequent operations can use the order to implement higher-level abstractions, such as synchronization primitives. NOTE: \u4e00\u3001\u8fd9\u79cd\u6280\u672f\u53eb\u4ec0\u4e48\u540d\u79f0\uff1f\u975e\u5e38\u7c7b\u4f3c\u4e8eMVCC\uff1b\u5728 ZooKeeper Programmer's Guide \u4e2d\uff0c\u5bf9\u6b64\u8fdb\u884c\u4e86\u4ecb\u7ecd\uff1b \u4e8c\u3001\u4e0a\u8ff0update\u5177\u4f53\u662f\u6307\u4ec0\u4e48\uff1f","title":"ZooKeeper is ordered"},{"location":"software-ZooKeeper/Overview/#zookeeper#is#fast","text":"It is especially fast in \"read-dominant\" workloads. NOTE: \u8fd9\u8ba9\u6211\u60f3\u5230\u4e86: 1\u3001RCU","title":"ZooKeeper is fast"},{"location":"software-ZooKeeper/Overview/#data#model#and#the#hierarchical#namespace","text":"","title":"Data model and the hierarchical namespace"},{"location":"software-ZooKeeper/Overview/#zookeepers#hierarchical#namespace","text":"","title":"ZooKeeper's Hierarchical Namespace"},{"location":"software-ZooKeeper/Overview/#nodes#and#ephemeral#nodes","text":"(ZooKeeper was designed to store coordination data : status information, configuration, location information, etc., so the data stored at each node is usually small, in the byte to kilobyte range.) NOTE: \u4e0a\u8ff0**coordination data**\uff0c\u53ef\u4ee5\u662fconsensus We use the term znode to make it clear that we are talking about ZooKeeper data nodes. Znodes maintain a stat structure that includes version numbers for data changes, ACL changes, and timestamps, to allow cache validations and coordinated updates . Each time a znode's data changes, the version number increases. For instance, whenever a client retrieves data it also receives the version of the data. NOTE: **stat structure**\u7684\u542b\u4e49\"\u7edf\u8ba1\u7ed3\u6784\"\u3002 The data stored at each znode in a namespace is read and written atomically . Reads get all the data bytes associated with a znode and a write replaces all the data. Each node has an Access Control List (ACL) that restricts who can do what. ZooKeeper also has the notion of ephemeral nodes. These znodes exists as long as the session that created the znode is active. When the session ends the znode is deleted.","title":"Nodes and ephemeral(\u77ed\u6682\u7684) nodes"},{"location":"software-ZooKeeper/Overview/#conditional#updates#and#watches","text":"ZooKeeper supports the concept of watches . Clients can set a watch on a znode. NOTE: \u548credis\u975e\u5e38\u7c7b\u4f3c","title":"Conditional updates and watches"},{"location":"software-ZooKeeper/Overview/#guarantees","text":"These are: 1\u3001Sequential Consistency - Updates from a client will be applied in the order that they were sent. NOTE: \u5173\u4e8eSequential Consistency \uff0c\u53c2\u89c1 Theory\\CAP\\Consistency \u7ae0\u8282 2\u3001Atomicity - Updates either succeed or fail. No partial results. 3\u3001Single System Image - A client will see the same view of the service regardless of the server that it connects to. i.e., a client will never see an older view of the system even if the client fails over to a different server with the same session. 4\u3001Reliability - Once an update has been applied, it will persist from that time forward until a client overwrites the update. NOTE: \u4e00\u65e6\u5e94\u7528\u4e86\u66f4\u65b0\uff0c\u5b83\u5c06\u4ece\u90a3\u65f6\u5f00\u59cb\u6301\u7eed\u5b58\u5728\uff0c\u76f4\u5230\u5ba2\u6237\u673a\u8986\u76d6\u8be5\u66f4\u65b0 5\u3001Timeliness - The clients view of the system is guaranteed to be up-to-date within a certain time bound. NOTE: \u53ca\u65f6\u6027","title":"Guarantees"},{"location":"software-ZooKeeper/Overview/#simple#api","text":"One of the design goals of ZooKeeper is providing a very simple programming interface. As a result, it supports only these operations: 1\u3001 create : creates a node at a location in the tree 2\u3001 delete : deletes a node 3\u3001 exists : tests if a node exists at a location 4\u3001 get data : reads the data from a node 5\u3001 set data : writes data to a node 6\u3001 get children : retrieves a list of children of a node 7\u3001 sync : waits for data to be propagated NOTE: \u5c31\u662f\u5bf9tree node\u7684\u64cd\u4f5c\uff0c\u57fa\u672c\u4e0a\u90fd\u662f\"\u589e\u5220\u6539\u67e5\"","title":"Simple API"},{"location":"software-ZooKeeper/Overview/#implementation","text":"ZooKeeper Components shows the high-level components of the ZooKeeper service. With the exception of the request processor, each of the servers that make up the ZooKeeper service replicates its own copy of each of the components. NOTE: 1\u3001ZooKeeper \u4f7f\u7528\u4e86\u81ea\u5df1\u7684consistency protocol: ZAB The replicated database is an in-memory database containing the entire data tree. Updates are logged to disk for recoverability, and writes are serialized to disk before they are applied to the in-memory database. NOTE: \u5148\u6301\u4e45\u5316\u5230\u6587\u4ef6\uff0c\u7136\u540e\u518d\u66f4\u65b0\u5230memory\uff0c\u8fd9\u79cd\u505a\u6cd5\u63d0\u9ad8\u4e86Reliability Every ZooKeeper server services clients. Clients connect to exactly one server to submit requests. Read requests are serviced from the local replica of each server database. Requests that change the state of the service, write requests, are processed by an agreement protocol . NOTE: read request\u80fd\u591f\u76f4\u63a5\u8bfb\u53d6local replica\uff0c\u4f46\u662fwrite request\u5219\u9700\u8981\u7531agreement protocol\u6765\u5904\u7406\uff0c\u6d89\u53ca\u5230\u6240\u6709\u7684\u8282\u70b9\uff0c\u5e76\u4e14\u4ece\u4e0b\u9762\u7684\u63cf\u8ff0\u6765\u770b\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u662f\u6bd4\u8f83\u590d\u6742\u7684: As part of the agreement protocol all write requests from clients are forwarded to a single server, called the leader . The rest of the ZooKeeper servers, called followers , receive message proposals(\u5efa\u8bae\u4e66) from the leader and agree upon message delivery. The messaging layer takes care of replacing leaders on failures and syncing followers with leaders. NOTE: \u8fd9\u79cd\u534f\u8bae\u53eb\u505a\u4ec0\u4e48\uff1f\u5b83\u662f\u5426\u53ef\u9760\uff1f ZooKeeper uses a custom atomic messaging protocol . Since the messaging layer is atomic , ZooKeeper can guarantee that the local replicas never diverge(\u5206\u79bb\u3001\u4ea7\u751f\u5dee\u5f02). When the leader receives a write request , it calculates what the state of the system is when the write is to be applied and transforms this into a transaction that captures this new state. NOTE: zookeeper\u662f\u4f7f\u7528\u7684\u5355\u4e2aleader\uff0c\u8fd9\u548credis\u6709\u4ec0\u4e48\u5dee\u5f02\u5462\uff1f","title":"Implementation"},{"location":"software-ZooKeeper/Overview/#performance","text":"ZooKeeper is designed to be highly performance . But is it? The results of the ZooKeeper's development team at Yahoo! Research indicate that it is. (See ZooKeeper Throughput as the Read-Write Ratio Varies .) It is especially high performance in applications where reads outnumber writes, since writes involve synchronizing the state of all servers. (Reads outnumbering writes is typically the case for a coordination service .)","title":"Performance"},{"location":"software-ZooKeeper/ZAB/","text":"ZAB ZAB\u662fzookeeper\u7684\u4e00\u81f4\u6027\u534f\u8bae\u3002 apache Zab1.0 This documents the Zab, an atomic broadcast protocol used by ZooKeeper to propagate state changes.","title":"Introduction"},{"location":"software-ZooKeeper/ZAB/#zab","text":"ZAB\u662fzookeeper\u7684\u4e00\u81f4\u6027\u534f\u8bae\u3002","title":"ZAB"},{"location":"software-ZooKeeper/ZAB/#apache#zab10","text":"This documents the Zab, an atomic broadcast protocol used by ZooKeeper to propagate state changes.","title":"apache Zab1.0"},{"location":"wikipedia-Parallel-computing/","text":"wikipedia Parallel computing NOTE: \u201cParallel computing\u201d\u5373\u201c\u5e76\u884c\u8ba1\u7b97\u201d\u3002 Parallel computing is a type of computation in which many calculations or the execution of processes are carried out simultaneously . Large problems can often be divided into smaller ones, which can then be solved at the same time. NOTE: \u4e0a\u8ff0**Parallel computing**\u7684\u5b9a\u4e49\u662f\u975e\u5e38\u5bbd\u6cdb\u7684\uff0c\u6309\u7167\u8fd9\u4e2a\u5b9a\u4e49\uff0c\u5c06\u4e00\u4e2a\u5927\u95ee\u9898\u62c6\u89e3\u4e3a\u5c0f\u95ee\u9898\uff0c\u7136\u540e**\u540c\u65f6**\u89e3\u51b3\u8fd9\u4e9b\u5c0f\u95ee\u9898\u7684\u90fd\u662f**parallel computing**\u3002\u663e\u7136\uff0c\u975e\u5e38\u591a\u7684 computation \u5f62\u6001\u90fd\u53ef\u4ee5\u5f52\u4e3aparallel computing\uff0c\u53c2\u89c1\u4e0b\u9762\u7684Types of parallelism\u3002 There are several different forms of parallel computing : bit-level , instruction-level , data , and task parallelism . NOTE: \u5bf9\u4e8esoftware engineer\u800c\u8a00\uff0c\u66f4\u591a\u7684\u662f\u5173\u6ce8 task parallelism \u3002\u5173\u4e8e\u5206\u7c7b\u7684\u8be6\u7ec6\u5185\u5bb9\uff0c\u53c2\u89c1 Types of parallelism \u7ae0\u8282\u3002 Parallelism has long been employed in high-performance computing , but it's gaining broader interest due to the physical constraints preventing frequency scaling . As power consumption (and consequently heat generation) by computers has become a concern in recent years, parallel computing has become the dominant paradigm in computer architecture , mainly in the form of multi-core processors . NOTE: \u65f6\u4ee3\u80cc\u666f Parallel computing is closely related to concurrent computing \u2014they are frequently used together, and often conflated(\u5408\u5e76), though the two are distinct: it is possible to have parallelism without concurrency (such as bit-level parallelism ), and concurrency without parallelism (such as multitasking by time-sharing on a single-core CPU). In parallel computing, a computational task is typically broken down into several, often many, very similar subtasks that can be processed independently and whose results are combined afterwards, upon completion. In contrast, in concurrent computing, the various processes often do not address related tasks; when they do, as is typical in distributed computing , the separate tasks may have a varied nature and often require some inter-process communication during execution. NOTE\uff1a\u5173\u4e8econcurrent computing\u548cparallelism\u4e4b\u95f4\u7684\u5173\u8054\u4e0e\u5dee\u522b\uff0csoftwareengineering the-difference-between-concurrent-and-parallel-execution \u8fd9\u4e2a\u56de\u7b54 \u6bd4\u8f83\u597d\uff1a Concurrency and parallelism are two related but distinct concepts. Concurrency means, essentially, that task A and task B both need to happen independently of each other, and A starts running, and then B starts before A is finished. There are various different ways of accomplishing concurrency. One of them is parallelism--having multiple CPUs working on the different tasks at the same time. But that's not the only way. Another is by task switching, which works like this: Task A works up to a certain point, then the CPU working on it stops and switches over to task B, works on it for a while, and then switches back to task A. If the time slices are small enough, it may appear to the user that both things are being run in parallel, even though they're actually being processed in serial by a multitasking CPU. Parallel computers can be roughly classified according to the level at which the hardware supports parallelism, with multi-core and multi-processor computers having multiple processing elements within a single machine, while clusters , MPPs , and grids use multiple computers to work on the same task. Specialized parallel computer architectures are sometimes used alongside traditional processors, for accelerating specific tasks. In some cases parallelism is transparent to the programmer, such as in bit-level or instruction-level parallelism, but explicitly parallel algorithms , particularly those that use concurrency , are more difficult to write than sequential ones, because concurrency introduces several new classes of potential software bugs , of which race conditions are the most common. Communication and synchronization between the different subtasks are typically some of the greatest obstacles\uff08\u969c\u788d) to getting good parallel program performance. A theoretical upper bound on the speed-up of a single program as a result of parallelization is given by Amdahl's law . Background(\u65f6\u4ee3\u80cc\u666f) To deal with the problem of power consumption and overheating the major central processing unit (CPU or processor) manufacturers started to produce power efficient processors with multiple cores. The core is the computing unit of the processor and in multi-core processors each core is independent and can access the same memory concurrently. Multi-core processors have brought parallel computing to desktop computers . Thus parallelisation of serial programmes has become a mainstream programming task. \u603b\u7ed3\uff1a\u4ece Frequency scaling \u5230 parallel computing Dependencies Understanding data dependencies is fundamental in implementing parallel algorithms . No program can run more quickly than the longest chain of dependent calculations (known as the critical path ), since calculations that depend upon prior calculations in the chain must be executed in order. However, most algorithms do not consist of just a long chain of dependent calculations; there are usually opportunities to execute independent calculations in parallel. \u603b\u7ed3\uff1a\u4e0a\u8ff0 critical path \u7684\u6982\u5ff5\u662f\u975e\u5e38\u91cd\u8981\u7684\uff1b Race conditions, mutual exclusion, synchronization, and parallel slowdown NOTE: \u8fd9\u4e9b\u5185\u5bb9\u5728Unix-like-operating-system\u4e2d\u5df2\u7ecf\u5305\u542b\u4e86\uff0c\u6b64\u5904\u7701\u7565\u3002\u4ec5\u4ec5\u7ed9\u51fa\u4e00\u4e9b\u94fe\u63a5\u3002 Race condition Lock (computer science) Mutual exclusion Critical section Software lockout Deadlock Non-blocking algorithm Parallel slowdown Barrier (computer science) Semaphore (programming) Synchronization (computer science) Fine-grained, coarse-grained, and embarrassing parallelism Applications are often classified according to how often their subtasks need to synchronize or communicate with each other. An application exhibits fine-grained parallelism if its subtasks must communicate many times per second; it exhibits coarse-grained parallelism if they do not communicate many times per second, and it exhibits embarrassing parallelism if they rarely or never have to communicate. Embarrassingly parallel applications are considered the easiest to parallelize. Consistency models Main article: Consistency model Parallel programming languages and parallel computers must have a consistency model (also known as a memory model). The consistency model defines rules for how operations on computer memory occur and how results are produced. Types of parallelism Bit-level parallelism Main article: Bit-level parallelism Instruction-level parallelism Main article: Instruction-level parallelism Task parallelism Main article: Task parallelism Hardware Memory and communication NOTE: \u8fd9\u4e00\u6bb5\u5185\u5bb9\u4e0e\u5e95\u5c42hardware\u5173\u8054\uff0c\u5bf9\u4e8esoftware engineer\u800c\u8a00\uff0c\u53ef\u4ee5\u8df3\u8fc7 Classes of parallel computers Parallel computers can be roughly classified according to the level at which the hardware supports parallelism. This classification is broadly analogous to the distance between basic computing nodes . These are not mutually exclusive; for example, clusters of symmetric multiprocessors are relatively common. Multi-core computing Main article: Multi-core processor A multi-core processor is a processor that includes multiple processing units (called \"cores\") on the same chip. Symmetric multiprocessing Main article: Symmetric multiprocessing A symmetric multiprocessor (SMP) is a computer system with multiple identical processors that share memory and connect via a bus. Distributed computing Main article: Distributed computing A distributed computer (also known as a distributed memory multiprocessor) is a distributed memory computer system in which the processing elements are connected by a network. Distributed computers are highly scalable. The terms \" concurrent computing \", \" parallel computing \", and \" distributed computing \" have a lot of overlap, and no clear distinction exists between them. The same system may be characterized both as \"parallel\" and \"distributed\"; the processors in a typical distributed system run concurrently in parallel. Cluster computing Main article: Computer cluster Massively parallel computing Main article: Massively parallel (computing) Grid computing Main article: Grid computing Specialized parallel computers Within parallel computing, there are specialized parallel devices that remain niche areas of interest. While not domain-specific , they tend to be applicable to only a few classes of parallel problems. Software Parallel programming languages Main article: List of concurrent and parallel programming languages Algorithmic methods Fault tolerance Further information: Fault-tolerant computer system As parallel computers become larger and faster, we are now able to solve problems that had previously taken too long to run. Fields as varied as bioinformatics (for protein folding and sequence analysis ) and economics (for mathematical finance ) have taken advantage of parallel computing. Common types of problems in parallel computing applications include: Dense linear algebra Sparse linear algebra Spectral methods (such as Cooley\u2013Tukey fast Fourier transform ) N -body problems (such as Barnes\u2013Hut simulation ) structured grid problems (such as Lattice Boltzmann methods ) Unstructured grid problems (such as found in finite element analysis ) Monte Carlo method Combinational logic (such as brute-force cryptographic techniques ) Graph traversal (such as sorting algorithms ) Dynamic programming Branch and bound methods Graphical models (such as detecting hidden Markov models and constructing Bayesian networks ) Finite-state machine simulation","title":"Introduction"},{"location":"wikipedia-Parallel-computing/#wikipedia#parallel#computing","text":"NOTE: \u201cParallel computing\u201d\u5373\u201c\u5e76\u884c\u8ba1\u7b97\u201d\u3002 Parallel computing is a type of computation in which many calculations or the execution of processes are carried out simultaneously . Large problems can often be divided into smaller ones, which can then be solved at the same time. NOTE: \u4e0a\u8ff0**Parallel computing**\u7684\u5b9a\u4e49\u662f\u975e\u5e38\u5bbd\u6cdb\u7684\uff0c\u6309\u7167\u8fd9\u4e2a\u5b9a\u4e49\uff0c\u5c06\u4e00\u4e2a\u5927\u95ee\u9898\u62c6\u89e3\u4e3a\u5c0f\u95ee\u9898\uff0c\u7136\u540e**\u540c\u65f6**\u89e3\u51b3\u8fd9\u4e9b\u5c0f\u95ee\u9898\u7684\u90fd\u662f**parallel computing**\u3002\u663e\u7136\uff0c\u975e\u5e38\u591a\u7684 computation \u5f62\u6001\u90fd\u53ef\u4ee5\u5f52\u4e3aparallel computing\uff0c\u53c2\u89c1\u4e0b\u9762\u7684Types of parallelism\u3002 There are several different forms of parallel computing : bit-level , instruction-level , data , and task parallelism . NOTE: \u5bf9\u4e8esoftware engineer\u800c\u8a00\uff0c\u66f4\u591a\u7684\u662f\u5173\u6ce8 task parallelism \u3002\u5173\u4e8e\u5206\u7c7b\u7684\u8be6\u7ec6\u5185\u5bb9\uff0c\u53c2\u89c1 Types of parallelism \u7ae0\u8282\u3002 Parallelism has long been employed in high-performance computing , but it's gaining broader interest due to the physical constraints preventing frequency scaling . As power consumption (and consequently heat generation) by computers has become a concern in recent years, parallel computing has become the dominant paradigm in computer architecture , mainly in the form of multi-core processors . NOTE: \u65f6\u4ee3\u80cc\u666f Parallel computing is closely related to concurrent computing \u2014they are frequently used together, and often conflated(\u5408\u5e76), though the two are distinct: it is possible to have parallelism without concurrency (such as bit-level parallelism ), and concurrency without parallelism (such as multitasking by time-sharing on a single-core CPU). In parallel computing, a computational task is typically broken down into several, often many, very similar subtasks that can be processed independently and whose results are combined afterwards, upon completion. In contrast, in concurrent computing, the various processes often do not address related tasks; when they do, as is typical in distributed computing , the separate tasks may have a varied nature and often require some inter-process communication during execution. NOTE\uff1a\u5173\u4e8econcurrent computing\u548cparallelism\u4e4b\u95f4\u7684\u5173\u8054\u4e0e\u5dee\u522b\uff0csoftwareengineering the-difference-between-concurrent-and-parallel-execution \u8fd9\u4e2a\u56de\u7b54 \u6bd4\u8f83\u597d\uff1a Concurrency and parallelism are two related but distinct concepts. Concurrency means, essentially, that task A and task B both need to happen independently of each other, and A starts running, and then B starts before A is finished. There are various different ways of accomplishing concurrency. One of them is parallelism--having multiple CPUs working on the different tasks at the same time. But that's not the only way. Another is by task switching, which works like this: Task A works up to a certain point, then the CPU working on it stops and switches over to task B, works on it for a while, and then switches back to task A. If the time slices are small enough, it may appear to the user that both things are being run in parallel, even though they're actually being processed in serial by a multitasking CPU. Parallel computers can be roughly classified according to the level at which the hardware supports parallelism, with multi-core and multi-processor computers having multiple processing elements within a single machine, while clusters , MPPs , and grids use multiple computers to work on the same task. Specialized parallel computer architectures are sometimes used alongside traditional processors, for accelerating specific tasks. In some cases parallelism is transparent to the programmer, such as in bit-level or instruction-level parallelism, but explicitly parallel algorithms , particularly those that use concurrency , are more difficult to write than sequential ones, because concurrency introduces several new classes of potential software bugs , of which race conditions are the most common. Communication and synchronization between the different subtasks are typically some of the greatest obstacles\uff08\u969c\u788d) to getting good parallel program performance. A theoretical upper bound on the speed-up of a single program as a result of parallelization is given by Amdahl's law .","title":"wikipedia Parallel computing"},{"location":"wikipedia-Parallel-computing/#background","text":"To deal with the problem of power consumption and overheating the major central processing unit (CPU or processor) manufacturers started to produce power efficient processors with multiple cores. The core is the computing unit of the processor and in multi-core processors each core is independent and can access the same memory concurrently. Multi-core processors have brought parallel computing to desktop computers . Thus parallelisation of serial programmes has become a mainstream programming task. \u603b\u7ed3\uff1a\u4ece Frequency scaling \u5230 parallel computing","title":"Background(\u65f6\u4ee3\u80cc\u666f)"},{"location":"wikipedia-Parallel-computing/#dependencies","text":"Understanding data dependencies is fundamental in implementing parallel algorithms . No program can run more quickly than the longest chain of dependent calculations (known as the critical path ), since calculations that depend upon prior calculations in the chain must be executed in order. However, most algorithms do not consist of just a long chain of dependent calculations; there are usually opportunities to execute independent calculations in parallel. \u603b\u7ed3\uff1a\u4e0a\u8ff0 critical path \u7684\u6982\u5ff5\u662f\u975e\u5e38\u91cd\u8981\u7684\uff1b","title":"Dependencies"},{"location":"wikipedia-Parallel-computing/#race#conditions#mutual#exclusion#synchronization#and#parallel#slowdown","text":"NOTE: \u8fd9\u4e9b\u5185\u5bb9\u5728Unix-like-operating-system\u4e2d\u5df2\u7ecf\u5305\u542b\u4e86\uff0c\u6b64\u5904\u7701\u7565\u3002\u4ec5\u4ec5\u7ed9\u51fa\u4e00\u4e9b\u94fe\u63a5\u3002 Race condition Lock (computer science) Mutual exclusion Critical section Software lockout Deadlock Non-blocking algorithm Parallel slowdown Barrier (computer science) Semaphore (programming) Synchronization (computer science)","title":"Race conditions, mutual exclusion, synchronization, and parallel slowdown"},{"location":"wikipedia-Parallel-computing/#fine-grained#coarse-grained#and#embarrassing#parallelism","text":"Applications are often classified according to how often their subtasks need to synchronize or communicate with each other. An application exhibits fine-grained parallelism if its subtasks must communicate many times per second; it exhibits coarse-grained parallelism if they do not communicate many times per second, and it exhibits embarrassing parallelism if they rarely or never have to communicate. Embarrassingly parallel applications are considered the easiest to parallelize.","title":"Fine-grained, coarse-grained, and embarrassing parallelism"},{"location":"wikipedia-Parallel-computing/#consistency#models","text":"Main article: Consistency model Parallel programming languages and parallel computers must have a consistency model (also known as a memory model). The consistency model defines rules for how operations on computer memory occur and how results are produced.","title":"Consistency models"},{"location":"wikipedia-Parallel-computing/#types#of#parallelism","text":"","title":"Types of parallelism"},{"location":"wikipedia-Parallel-computing/#bit-level#parallelism","text":"Main article: Bit-level parallelism","title":"Bit-level parallelism"},{"location":"wikipedia-Parallel-computing/#instruction-level#parallelism","text":"Main article: Instruction-level parallelism","title":"Instruction-level parallelism"},{"location":"wikipedia-Parallel-computing/#task#parallelism","text":"Main article: Task parallelism","title":"Task parallelism"},{"location":"wikipedia-Parallel-computing/#hardware","text":"","title":"Hardware"},{"location":"wikipedia-Parallel-computing/#memory#and#communication","text":"NOTE: \u8fd9\u4e00\u6bb5\u5185\u5bb9\u4e0e\u5e95\u5c42hardware\u5173\u8054\uff0c\u5bf9\u4e8esoftware engineer\u800c\u8a00\uff0c\u53ef\u4ee5\u8df3\u8fc7","title":"Memory and communication"},{"location":"wikipedia-Parallel-computing/#classes#of#parallel#computers","text":"Parallel computers can be roughly classified according to the level at which the hardware supports parallelism. This classification is broadly analogous to the distance between basic computing nodes . These are not mutually exclusive; for example, clusters of symmetric multiprocessors are relatively common.","title":"Classes of parallel computers"},{"location":"wikipedia-Parallel-computing/#multi-core#computing","text":"Main article: Multi-core processor A multi-core processor is a processor that includes multiple processing units (called \"cores\") on the same chip.","title":"Multi-core computing"},{"location":"wikipedia-Parallel-computing/#symmetric#multiprocessing","text":"Main article: Symmetric multiprocessing A symmetric multiprocessor (SMP) is a computer system with multiple identical processors that share memory and connect via a bus.","title":"Symmetric multiprocessing"},{"location":"wikipedia-Parallel-computing/#distributed#computing","text":"Main article: Distributed computing A distributed computer (also known as a distributed memory multiprocessor) is a distributed memory computer system in which the processing elements are connected by a network. Distributed computers are highly scalable. The terms \" concurrent computing \", \" parallel computing \", and \" distributed computing \" have a lot of overlap, and no clear distinction exists between them. The same system may be characterized both as \"parallel\" and \"distributed\"; the processors in a typical distributed system run concurrently in parallel.","title":"Distributed computing"},{"location":"wikipedia-Parallel-computing/#cluster#computing","text":"Main article: Computer cluster","title":"Cluster computing"},{"location":"wikipedia-Parallel-computing/#massively#parallel#computing","text":"Main article: Massively parallel (computing)","title":"Massively parallel computing"},{"location":"wikipedia-Parallel-computing/#grid#computing","text":"Main article: Grid computing","title":"Grid computing"},{"location":"wikipedia-Parallel-computing/#specialized#parallel#computers","text":"Within parallel computing, there are specialized parallel devices that remain niche areas of interest. While not domain-specific , they tend to be applicable to only a few classes of parallel problems.","title":"Specialized parallel computers"},{"location":"wikipedia-Parallel-computing/#software","text":"","title":"Software"},{"location":"wikipedia-Parallel-computing/#parallel#programming#languages","text":"Main article: List of concurrent and parallel programming languages","title":"Parallel programming languages"},{"location":"wikipedia-Parallel-computing/#algorithmic#methods","text":"","title":"Algorithmic methods"},{"location":"wikipedia-Parallel-computing/#fault#tolerance","text":"Further information: Fault-tolerant computer system As parallel computers become larger and faster, we are now able to solve problems that had previously taken too long to run. Fields as varied as bioinformatics (for protein folding and sequence analysis ) and economics (for mathematical finance ) have taken advantage of parallel computing. Common types of problems in parallel computing applications include: Dense linear algebra Sparse linear algebra Spectral methods (such as Cooley\u2013Tukey fast Fourier transform ) N -body problems (such as Barnes\u2013Hut simulation ) structured grid problems (such as Lattice Boltzmann methods ) Unstructured grid problems (such as found in finite element analysis ) Monte Carlo method Combinational logic (such as brute-force cryptographic techniques ) Graph traversal (such as sorting algorithms ) Dynamic programming Branch and bound methods Graphical models (such as detecting hidden Markov models and constructing Bayesian networks ) Finite-state machine simulation","title":"Fault tolerance"}]}